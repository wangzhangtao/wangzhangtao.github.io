<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>linux-tmp2</title>
      <link href="/2021/01/02/linux-tmp2/"/>
      <url>/2021/01/02/linux-tmp2/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是Pipeline"><a href="#什么是Pipeline" class="headerlink" title="什么是Pipeline"></a>什么是Pipeline</h2><p>​        Jenkins Pipeline是一组插件，支持在Jenkins上实现和集成持续交付的管道。Pipeline这个单词是水管、流水线的意思。这里持续集成（CI）和持续交付（CD）。</p><p>​        Jenkins为了更好支持CI和CD，通过Groovy语言这么DSL（动态描述语言）来开发Pipeline组件。在Jenkins中有一句话，Pipeline as code，Pipeline是Jenkins中最优雅的存在。之前Jenkins上UI操作动作，都可以在Pipeline中代码实现，主要你对Jenkins和Groovy语言有足够多掌握。</p><p>​        以后我们经常说CI Pipeline和CD Pipeline，你现在大致可以理解为，要实现CD，先要实现CI。CD Pipeline就是一个代码文件，里面把你项目业务场景都通过Groovy代码和Pipeline语法实现，一个一个业务串联起来，全部实现自动化，从代码仓库到生产环境完成部署的自动化流水线。这个过程就是一个典型的CD Pipeline</p><p>​       官网建议我们把Pipeline代码放在一个名称为Jenkinsfile的文本文件中，并且把这个文件放在你项目代码的根目录，采用版本管理工具管理。Jenkinsfile我后面会具体例子来介绍。当然，我们也可以把Pipeline代码用一个Hello.groovy这样的文件去保存在代码库，这也是没问题的。</p><h2 id="为什么要选择使用Pipeline"><a href="#为什么要选择使用Pipeline" class="headerlink" title="为什么要选择使用Pipeline"></a>为什么要选择使用Pipeline</h2><p>​        现在Jenkins是一个非常著名的CI服务器平台，支持很多不同第三方（插件的形式）集成自动化测试。Jenkins UI 配置已经满足不了这么复杂的自动化需求，加入Pipeline功能之后，Jenkins 表现更强大，Pipeline主要有一下特点。</p><p>代码：Pipeline是用代码去实现，并且支持check in到代码仓库，这样项目团队人员就可以修改，更新Pipeline脚本代码，支持代码迭代。</p><p>耐用：Pipeline支持在Jenkins master(主节点)上计划之内或计划外的重启下也能使用。</p><p>可暂停：Pipeline支持可选的停止和恢复或者等待批准之后再跑Pipeline代码。</p><p>丰富功能：Pipeline支持复杂和实时的CD需求，包括循环，拉取代码，和并行执行的能力。</p><p>可扩展性：Pipeline支持DSL的自定义插件扩展和支持和其他插件的集成。</p><h2 id="Blue-Ocean界面"><a href="#Blue-Ocean界面" class="headerlink" title="Blue Ocean界面"></a>Blue Ocean界面</h2><h2 id="测试案例"><a href="#测试案例" class="headerlink" title="测试案例"></a>测试案例</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent any </span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&#39;Build&#39;) &#123; </span><br><span class="line">            steps &#123;</span><br><span class="line">                println &quot;Build&quot; </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&#39;Test&#39;) &#123; </span><br><span class="line">            steps &#123;</span><br><span class="line">                println &quot;Test&quot; </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&#39;Deploy&#39;) &#123; </span><br><span class="line">            steps &#123;</span><br><span class="line">                println &quot;Deploy&quot; </span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">使用mvn</span><br><span class="line">pipeline &#123;</span><br><span class="line">    agent &#123; docker &#39;maven:3.3.3&#39; &#125;</span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&#39;build&#39;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                sh &#39;mvn --version&#39;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 Linux、BSD 和 Mac OS（类 Unix ) 系统中的 shell 命令， 对应于 Pipeline 中的一个 sh 步骤（step）。</p><h2 id="重试和超时"><a href="#重试和超时" class="headerlink" title="重试和超时"></a>重试和超时</h2><p>重复执行 flakey-deploy.sh 脚本3次，然后等待 health-check.sh 脚本最长执行3分钟</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">steps &#123;</span><br><span class="line">    retry(3) &#123;</span><br><span class="line">        sh &#39;.&#x2F;flakey-deploy.sh&#39;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    timeout(time: 3, unit: &#39;MINUTES&#39;) &#123;</span><br><span class="line">        sh &#39;.&#x2F;health-check.sh&#39;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>内嵌类型的步骤，例如 timeout 和 retry 可以包含其他的步骤，包括 timeout 和 retry</p><p>重试部署任务 5 次，但是总共花费的时间不能超过 3 分钟。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">steps &#123;</span><br><span class="line">    timeout(time: 3, unit: &#39;MINUTES&#39;) &#123;</span><br><span class="line">        retry(5) &#123;</span><br><span class="line">            sh &#39;.&#x2F;flakey-deploy.sh&#39;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="完成时动作"><a href="#完成时动作" class="headerlink" title="完成时动作"></a>完成时动作</h2><p>当 Pipeline 运行完成时，你可能需要做一些清理工作或者基于 Pipeline 的运行结果执行不同的操作， 这些操作可以放在 post 部分。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">post &#123;</span><br><span class="line">    always &#123;</span><br><span class="line">        echo &#39;This will always run&#39;</span><br><span class="line">    &#125;</span><br><span class="line">    success &#123;</span><br><span class="line">        echo &#39;This will run only if successful&#39;</span><br><span class="line">    &#125;</span><br><span class="line">    failure &#123;</span><br><span class="line">        echo &#39;This will run only if failed&#39;</span><br><span class="line">    &#125;</span><br><span class="line">    unstable &#123;</span><br><span class="line">        echo &#39;This will run only if the run was marked as unstable&#39;</span><br><span class="line">    &#125;</span><br><span class="line">    changed &#123;</span><br><span class="line">        echo &#39;This will run only if the state of the Pipeline has changed&#39;</span><br><span class="line">        echo &#39;For example, if the Pipeline was previously failing but is now successful&#39;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h2 id="agent定义执行环境"><a href="#agent定义执行环境" class="headerlink" title="agent定义执行环境"></a>agent定义执行环境</h2><p>agent 指令告诉Jenkins在哪里以及如何执行Pipeline或者Pipeline子集</p><ul><li>所有在块block中的步骤steps会被Jenkins保存在一个执行队列中。 一旦一个执行器 <a href="https://jenkins.io/zh/doc/pipeline/tour/agents/#../../book/glossary/#executor" target="_blank" rel="noopener">executor</a> 是可以利用的，这些步骤将会开始执行。</li><li>一个工作空间 <a href="https://jenkins.io/zh/doc/pipeline/tour/agents/#../../book/glossary/#workspace" target="_blank" rel="noopener">workspace</a> 将会被分配， 工作空间中会包含来自远程仓库的文件和一些用于Pipeline的工作文件</li></ul><h2 id="使用环境变量"><a href="#使用环境变量" class="headerlink" title="使用环境变量"></a>使用环境变量</h2><p>环境变量可以是全局，也可以是阶段（stage）级别的</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">    agent any</span><br><span class="line"></span><br><span class="line">    environment &#123;</span><br><span class="line">        DISABLE_AUTH &#x3D; &#39;true&#39;</span><br><span class="line">        DB_ENGINE    &#x3D; &#39;sqlite&#39;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    stages &#123;</span><br><span class="line">        stage(&#39;Build&#39;) &#123;</span><br><span class="line">            steps &#123;</span><br><span class="line">                sh &#39;printenv&#39;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="记录测试和构建结果"><a href="#记录测试和构建结果" class="headerlink" title="记录测试和构建结果"></a>记录测试和构建结果</h2><h3 id="测试和通知"><a href="#测试和通知" class="headerlink" title="测试和通知"></a>测试和通知</h3><p>电子邮件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">post &#123;</span><br><span class="line">    failure &#123;</span><br><span class="line">        mail to: &#39;team@example.com&#39;,</span><br><span class="line">             subject: &quot;Failed Pipeline: $&#123;currentBuild.fullDisplayName&#125;&quot;,</span><br><span class="line">             body: &quot;Something is wrong with $&#123;env.BUILD_URL&#125;&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>人工确认</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">stage(&#39;Sanity check&#39;) &#123;</span><br><span class="line">    steps &#123;</span><br><span class="line">        input &quot;Does the staging environment look ok?&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 其他 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux-tmp</title>
      <link href="/2021/01/01/linux-tmp/"/>
      <url>/2021/01/01/linux-tmp/</url>
      
        <content type="html"><![CDATA[<h1 id="维护节点"><a href="#维护节点" class="headerlink" title="维护节点"></a>维护节点</h1><p>如果需要重启节点（例如内核升级、libc 升级、硬件维修等），且停机时间很短时， kubelet 重启后，将尝试重启调度到节点上的 Pod。如果重启花费较长时间（默认时间为 5 分钟，由 控制器管理器的 <code>--pod-eviction-timeout</code> 控制），节点控制器将会结束绑定到这个不可用节点上的 Pod。 如果存在对应的 ReplicaSet（或者 ReplicationController），则将在另一个节点上启动 Pod 的新副本。 所以，如果所有的 Pod 都是多副本的，那么在不是所有节点都同时停机的前提下，升级可以在不需要特殊 调整情况下完成。</p><p>如果你希望对升级过程有更多的控制，可以使用下面的工作流程：</p><p>使用 <code>kubectl drain</code> 体面地结束节点上的所有 Pod 并同时<strong>标记节点为不可调度</strong>：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl drain $NODENAME --grace-period&#x3D;600 --ignore-daemonsets</span><br></pre></td></tr></table></figure><p>在你试图使节点离线时，这样做将阻止新的 Pod 落到它们上面。</p><p>对于有 ReplicaSet 的 Pod 来说，它们将会被新的 Pod 替换并且将被调度到一个新的节点。 此外，如果 Pod 是一个 Service 的一部分，则客户端将被自动重定向到新的 Pod。</p><p>在节点上执行维护工作。</p><p>重新使节点可调度：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl uncordon $NODENAME</span><br></pre></td></tr></table></figure><p>如果删除了节点的虚拟机实例并重新创建，那么一个新的可调度节点资源将被自动创建 （只在你使用支持节点发现的云服务提供商时；当前只有 Google Compute Engine， 不包括在 Google Compute Engine 上使用 kube-register 的 CoreOS）。 相关详细信息，请查阅<a href="https://kubernetes.io/zh/docs/concepts/architecture/nodes/" target="_blank" rel="noopener">节点</a>。</p><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/cluster-management/#维护节点" target="_blank" rel="noopener">维护节点</a></li></ul><h2 id="k8s下线node节点"><a href="#k8s下线node节点" class="headerlink" title="k8s下线node节点"></a>k8s下线node节点</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get node -o wide  # 获取节点列表</span><br><span class="line">kubectl cordon node1  # 设置不可调度</span><br><span class="line">kubectl drain node1  # 驱逐节点上的Pod</span><br><span class="line">kubectl delete node1   # 删除节点</span><br></pre></td></tr></table></figure><h1 id="清空节点"><a href="#清空节点" class="headerlink" title="清空节点"></a>清空节点</h1><h2 id="前提条件："><a href="#前提条件：" class="headerlink" title="前提条件："></a>前提条件：</h2><ul><li>使用的 Kubernetes 版本 &gt;= 1.5。</li><li>以下两项，具备其一：<ol><li>在节点清空期间，不要求应用程序具有高可用性</li><li>你已经了解了 <a href="https://kubernetes.io/zh/docs/concepts/workloads/pods/disruptions/" target="_blank" rel="noopener">PodDisruptionBudget 的概念</a>，并为需要它的应用程序<a href="https://kubernetes.io/zh/docs/tasks/run-application/configure-pdb/" target="_blank" rel="noopener">配置了 PodDisruptionBudget</a>。</li></ol></li></ul><h2 id="使用drain清空节点"><a href="#使用drain清空节点" class="headerlink" title="使用drain清空节点"></a>使用drain清空节点</h2><ul><li>使用 <code>kubectl drain</code> 从服务中删除一个节点</li><li>并行清空多个节点</li><li>驱逐 API</li><li>驱逐阻塞</li></ul><h2 id="参考文档-1"><a href="#参考文档-1" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><p><a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/safely-drain-node/" target="_blank" rel="noopener">清空一个节点</a></p></li><li><p><a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands/#drain" target="_blank" rel="noopener">kubectl drain</a> </p></li></ul><h1 id="pod干扰"><a href="#pod干扰" class="headerlink" title="pod干扰"></a>pod干扰</h1><h2 id="自愿干扰和非自愿干扰"><a href="#自愿干扰和非自愿干扰" class="headerlink" title="自愿干扰和非自愿干扰"></a>自愿干扰和非自愿干扰</h2><p>Pod 不会消失，除非有人（用户或控制器）将其销毁，或者出现了不可避免的硬件或软件系统错误。</p><h2 id="处理干扰"><a href="#处理干扰" class="headerlink" title="处理干扰"></a>处理干扰</h2><p>以下是减轻非自愿干扰的一些方法：</p><ul><li>确保 Pod 在请求中给出<a href="https://kubernetes.io/zh/docs/tasks/configure-pod-container/assign-memory-resource/" target="_blank" rel="noopener">所需资源</a>。</li><li>如果需要更高的可用性，请复制应用程序。 （了解有关运行多副本的<a href="https://kubernetes.io/zh/docs/tasks/run-application/run-stateless-application-deployment/" target="_blank" rel="noopener">无状态</a> 和<a href="https://kubernetes.io/zh/docs/tasks/run-application/run-replicated-stateful-application/" target="_blank" rel="noopener">有状态</a>应用程序的信息。）</li><li>为了在运行复制应用程序时获得更高的可用性，请跨机架（使用 <a href="https://kubernetes.io/zh/docs/concepts/scheduling-eviction/assign-pod-node/" target="_blank" rel="noopener">反亲和性</a>）或跨区域 （如果使用<a href="https://kubernetes.io/docs/setup/best-practices/multiple-zones/" target="_blank" rel="noopener">多区域集群</a>）扩展应用程序。</li></ul><h2 id="干扰预算"><a href="#干扰预算" class="headerlink" title="干扰预算"></a>干扰预算</h2><p>Kubernetes 提供特性来满足在出现频繁自愿干扰的同时运行高可用的应用程序。我们称这些特性为 <em>干扰预算（Disruption Budget）</em>。</p><p>应用程序所有者可以为每个应用程序创建 <code>PodDisruptionBudget</code> 对象（PDB）。 PDB 将限制在同一时间因自愿干扰导致的复制应用程序中宕机的 pod 数量。 例如，基于票选机制的应用程序希望确保运行的副本数永远不会低于仲裁所需的数量。 Web 前端可能希望确保提供负载的副本数量永远不会低于总数的某个百分比。</p><p>集群管理员和托管提供商应该使用遵循 Pod Disruption Budgets 的接口 （通过调用<a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#the-eviction-api" target="_blank" rel="noopener">Eviction API</a>）， 而不是直接删除 Pod 或 Deployment。</p><p>PDB 指定应用程序可以容忍的副本数量（相当于应该有多少副本）。</p><h3 id="PDB例外"><a href="#PDB例外" class="headerlink" title="PDB例外"></a>PDB例外</h3><p>由于应用程序的滚动升级而被删除或不可用的 Pod 确实会计入干扰预算， 但是控制器（如 Deployment 和 StatefulSet）在进行滚动升级时不受 PDB 的限制。应用程序更新期间的故障处理方式是在对应的工作负载资源的 <code>spec</code> 中配置的。</p><p>当使用驱逐 API 驱逐 Pod 时，Pod 会被体面地 <a href="https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination" target="_blank" rel="noopener">终止</a>，期间会 参考 <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#podspec-v1-core" target="_blank" rel="noopener">PodSpec</a> 中的 <code>terminationGracePeriodSeconds</code> 配置值。</p><h2 id="PDB-例子"><a href="#PDB-例子" class="headerlink" title="PDB 例子"></a>PDB 例子</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: policy&#x2F;v1beta1</span><br><span class="line">kind: PodDisruptionBudget</span><br><span class="line">metadata:</span><br><span class="line">  name: zk-pdb</span><br><span class="line">spec:</span><br><span class="line">  minAvailable: 2</span><br><span class="line">  # maxUnavailable: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: zookeeper</span><br></pre></td></tr></table></figure><p>查看pdb</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get poddisruptionbudgets</span><br></pre></td></tr></table></figure><h2 id="如何在集群上执行干扰性操作"><a href="#如何在集群上执行干扰性操作" class="headerlink" title="如何在集群上执行干扰性操作"></a>如何在集群上执行干扰性操作</h2><p>如果你是集群管理员，并且需要对集群中的所有节点执行干扰操作，例如节点或系统软件升级，则可以使用以下选项</p><ul><li>接受升级期间的停机时间。</li><li>故障转移到另一个完整的副本集群。<ul><li>没有停机时间，但是对于重复的节点和人工协调成本可能是昂贵的。</li></ul></li><li>编写可容忍干扰的应用程序和使用 PDB。<ul><li>不停机。</li><li>最小的资源重复。</li><li>允许更多的集群管理自动化。</li><li>编写可容忍干扰的应用程序是棘手的，但对于支持容忍自愿干扰所做的工作，和支持自动扩缩和容忍非 自愿干扰所做工作相比，有大量的重叠</li></ul></li></ul><h2 id="参考文档-2"><a href="#参考文档-2" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://kubernetes.io/zh/docs/concepts/workloads/pods/disruptions/" target="_blank" rel="noopener">pod干扰</a></li></ul><h1 id="设置干扰预算"><a href="#设置干扰预算" class="headerlink" title="设置干扰预算"></a>设置干扰预算</h1><h2 id="用-PodDisruptionBudget-来保护应用"><a href="#用-PodDisruptionBudget-来保护应用" class="headerlink" title="用 PodDisruptionBudget 来保护应用"></a>用 PodDisruptionBudget 来保护应用</h2><ul><li><p>确定要保护的应用</p></li><li><p>考虑应用对干扰的反应</p></li><li><p>指定 PodDisruptionBudget</p></li></ul><h2 id="参考文档-3"><a href="#参考文档-3" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://kubernetes.io/zh/docs/tasks/run-application/configure-pdb/" target="_blank" rel="noopener">设置干扰预算</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Linux </category>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 其他 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>wangzt/test/k8s-know</title>
      <link href="/2020/11/10/wangzt-test-k8s-know/"/>
      <url>/2020/11/10/wangzt-test-k8s-know/</url>
      
        <content type="html"><![CDATA[<h1 id="其他知识"><a href="#其他知识" class="headerlink" title="其他知识"></a>其他知识</h1><p> lvs, lvm, linux启动顺序，内核调优，</p><h2 id="阿里云主机服务网络迁移"><a href="#阿里云主机服务网络迁移" class="headerlink" title="阿里云主机服务网络迁移"></a>阿里云主机服务网络迁移</h2><h2 id="dubbo是什么？springcloud了解多少"><a href="#dubbo是什么？springcloud了解多少" class="headerlink" title="dubbo是什么？springcloud了解多少"></a>dubbo是什么？springcloud了解多少</h2><p>Spring Cloud是一个微服务框架的规范，注意，<strong>只是规范，他不是任何具体的框架</strong>。</p><p>它规定大概要有以下几种功能。</p><ol><li>服务的注册与发现</li><li>负载均衡</li><li>服务熔断和限流</li><li>智能路由</li><li>控制总线</li><li>链路监控</li></ol><h3 id="Spring-Cloud和Spring-boot的关系"><a href="#Spring-Cloud和Spring-boot的关系" class="headerlink" title="Spring Cloud和Spring boot的关系"></a>Spring Cloud和Spring boot的关系</h3><p>与其说他们有什么关系，不如说他们就没有什么关系。只是现在微服务当道，而实现一个服务最快的办法就是用spring  boot。不然你还要自己搭项目自己找jar包自己搞配置，还有兼容性等情况。那你的服务化进程注定是缓慢的。所以他们之间是没有关系的，只是因为微服务所需要的小应用很多，而spring boot恰恰又是实现小应用最快的方式。</p><h3 id="Spring-Cloud和dubbo的关系"><a href="#Spring-Cloud和dubbo的关系" class="headerlink" title="Spring Cloud和dubbo的关系"></a>Spring Cloud和dubbo的关系</h3><p>dubbo是阿里搞得一套框架，是基于RPC调用的，而Spring Cloud  Netflix是基于HTTP的，所以效率上应该dubbo更快（如果你不能理解什么是RPC，当我没说，反正dubbo更快就是了）。但是dubbo的组件不是很齐全，他的很多功能比如服务注册与发现你需要借助于类似zookeeper等组件才能实现，而Spring Cloud Netflix则是提供了一站式解决方案。从使用广度来说，在国内几年前dubbo的使用人数远多于Spring  Cloud的，但是近来Spring Cloud慢慢的有了后来居上的趋势。</p><h2 id="使用nginx时，后段服务挂掉怎么办"><a href="#使用nginx时，后段服务挂掉怎么办" class="headerlink" title="使用nginx时，后段服务挂掉怎么办"></a>使用nginx时，后段服务挂掉怎么办</h2><h2 id="filebeat服务故障时，如果保证日志不丢失"><a href="#filebeat服务故障时，如果保证日志不丢失" class="headerlink" title="filebeat服务故障时，如果保证日志不丢失"></a>filebeat服务故障时，如果保证日志不丢失</h2><h2 id="kafka的实现原理"><a href="#kafka的实现原理" class="headerlink" title="kafka的实现原理"></a>kafka的实现原理</h2><h2 id="prometheus的相关原理"><a href="#prometheus的相关原理" class="headerlink" title="prometheus的相关原理"></a>prometheus的相关原理</h2><h2 id="docker的网络分几种"><a href="#docker的网络分几种" class="headerlink" title="docker的网络分几种"></a>docker的网络分几种</h2><ol><li>bridge桥接模式：将容器的网络连接到虚拟网桥上（默认模式）。–net=bridge</li><li>host主机模式：容器不会虚拟自己的网卡，配置自己的IP，而会使用宿主机的IP地址和端口</li><li>container共享容器网络模式：新建的容器和已经存在的容器共享一个网络</li><li>none无网络模式：容器拥有自己的Network Namespace, 但是不进行任何的网络设置</li></ol><h2 id="Jenkins-pipeline"><a href="#Jenkins-pipeline" class="headerlink" title="Jenkins pipeline"></a>Jenkins pipeline</h2><h1 id="k8s的原理"><a href="#k8s的原理" class="headerlink" title="k8s的原理"></a>k8s的原理</h1><h2 id="k8s有哪些组件"><a href="#k8s有哪些组件" class="headerlink" title="k8s有哪些组件"></a>k8s有哪些组件</h2><h3 id="master"><a href="#master" class="headerlink" title="master"></a>master</h3><ul><li>etcd：Etcd是用Go编程语言编写的，是一个分布式键值存储，用于协调分布式工作。因此，Etcd存储Kubernetes集群的配置数据，表示在任何给定时间点的集群状态。</li><li>kube-apiserver:遵循横向扩展架构，是主节点控制面板的前端。这将公开Kubernetes主节点组件的所有API，并负责在Kubernetes节点和Kubernetes主组件之间建立通信。</li><li>kube-controller-manager:多个控制器进程在主节点上运行，但是一起编译为单个进程运行，即Kubernetes控制器管理器。因此，Controller Manager是一个嵌入控制器并执行命名空间创建和垃圾收集的守护程序。它拥有责任并与API服务器通信以管理端点。</li><li>kube-scheduler:负责工作节点上工作负载的分配和管理。因此，它根据资源需求选择最合适的节点来运行未调度的pod，并跟踪资源利用率。它确保不在已满的节点上调度工作负载。</li></ul><h3 id="node"><a href="#node" class="headerlink" title="node"></a>node</h3><ul><li>kubelet:</li><li>kube-proxy:Kube-proxy可以在每个节点上运行，并且可以跨后端网络服务进行简单的TCP /  UDP数据包转发。基本上，它是一个网络代理，它反映了每个节点上Kubernetes  API中配置的服务。因此，Docker可链接的兼容环境变量提供由代理打开的群集IP和端口。</li></ul><h3 id="addons"><a href="#addons" class="headerlink" title="addons"></a>addons</h3><ul><li>flannel:</li><li>CoreDNS:</li><li>ingress:</li><li>dashboard:</li></ul><h2 id="你在部署k8s集群时，都遇到过哪些问题，你是怎么解决的"><a href="#你在部署k8s集群时，都遇到过哪些问题，你是怎么解决的" class="headerlink" title="你在部署k8s集群时，都遇到过哪些问题，你是怎么解决的"></a>你在部署k8s集群时，都遇到过哪些问题，你是怎么解决的</h2><p>安装flannal时，subnet配置错误，最后修改的etcd数据</p><h2 id="你在使用k8s集群时，都遇到过哪些问题，你是怎么解决的"><a href="#你在使用k8s集群时，都遇到过哪些问题，你是怎么解决的" class="headerlink" title="你在使用k8s集群时，都遇到过哪些问题，你是怎么解决的"></a>你在使用k8s集群时，都遇到过哪些问题，你是怎么解决的</h2><ol><li>通过describe pod查看输出信息</li><li>通过kubectl logs查看日志信息</li><li>通过kubectl exec 查看进程信息</li></ol><h2 id="在k8s里部署过有状态的应用吗"><a href="#在k8s里部署过有状态的应用吗" class="headerlink" title="在k8s里部署过有状态的应用吗"></a>在k8s里部署过有状态的应用吗</h2><p>部署过zk3.6</p><h2 id="阿里云k8s托管版的架构"><a href="#阿里云k8s托管版的架构" class="headerlink" title="阿里云k8s托管版的架构"></a>阿里云k8s托管版的架构</h2><h2 id="kubeadm创建k8s集群过程"><a href="#kubeadm创建k8s集群过程" class="headerlink" title="kubeadm创建k8s集群过程"></a>kubeadm创建k8s集群过程</h2><h2 id="k8s组件的升级顺序"><a href="#k8s组件的升级顺序" class="headerlink" title="k8s组件的升级顺序"></a>k8s组件的升级顺序</h2><p>k8s升级规划：</p><ul><li>官方建议每次升级不要跨越两个版本，升级顺序为: master，addons，salve。</li><li>slave节点的升级是滚动升级，官方建议首先使用<code>kubectl drain</code>驱逐pod之后，然后升级kubelet。</li><li>apiserver升级之前需要确保resource version被正常支持。</li><li>备份etcd数据库数据。</li></ul><h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h3><ul><li><a href="https://www.cnblogs.com/gaorong/p/11266629.html" target="_blank" rel="noopener">kubernetes集群升级的正确姿势</a></li></ul><h2 id="k8s如何下线一个节点"><a href="#k8s如何下线一个节点" class="headerlink" title="k8s如何下线一个节点"></a>k8s如何下线一个节点</h2><h3 id="参考文档-1"><a href="#参考文档-1" class="headerlink" title="参考文档"></a>参考文档</h3><ul><li><a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/safely-drain-node/" target="_blank" rel="noopener">清空一个节点</a></li></ul><h2 id="etcd的高可用配置和原理"><a href="#etcd的高可用配置和原理" class="headerlink" title="etcd的高可用配置和原理"></a>etcd的高可用配置和原理</h2><h2 id="kube-apiserver是如何实现高可用的"><a href="#kube-apiserver是如何实现高可用的" class="headerlink" title="kube-apiserver是如何实现高可用的"></a>kube-apiserver是如何实现高可用的</h2><p>keepalive+lvs</p><h2 id="pod的IP地址是谁分配的"><a href="#pod的IP地址是谁分配的" class="headerlink" title="pod的IP地址是谁分配的"></a>pod的IP地址是谁分配的</h2><p>分析：k8s的CNI网络插件</p><p>kubelet通过CRI接口调用docker, docker启动服务时加载CNI 网络插件flannel时分配IP地址。</p><h2 id="configmap怎么使用，支持热更新吗"><a href="#configmap怎么使用，支持热更新吗" class="headerlink" title="configmap怎么使用，支持热更新吗"></a>configmap怎么使用，支持热更新吗</h2><p>configmap热更新分两种情况：</p><ul><li><p>更新已经在数据卷中使用的 ConfigMap 时，已映射的键最终也会被更新。</p></li><li><p>使用 ConfigMap 作为 <a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/#using-subpath" target="_blank" rel="noopener">subPath</a> 的数据卷将不会收到 ConfigMap 更新</p></li></ul><h2 id="pod生命周期"><a href="#pod生命周期" class="headerlink" title="pod生命周期"></a>pod生命周期</h2><p>容器的状态有三种：<code>Waiting</code>（等待）、<code>Running</code>（运行中）和 <code>Terminated</code>（已终止）。</p><p>pod生命周期有：</p><table><thead><tr><th>取值</th><th>描述</th></tr></thead><tbody><tr><td><code>Pending</code>（悬决）</td><td>Pod 已被 Kubernetes 系统接受，但有一个或者多个容器尚未创建亦未运行。此阶段包括等待 Pod 被调度的时间和通过网络下载镜像的时间，</td></tr><tr><td><code>Running</code>（运行中）</td><td>Pod 已经绑定到了某个节点，Pod 中所有的容器都已被创建。至少有一个容器仍在运行，或者正处于启动或重启状态。</td></tr><tr><td><code>Succeeded</code>（成功）</td><td>Pod 中的所有容器都已成功终止，并且不会再重启。</td></tr><tr><td><code>Failed</code>（失败）</td><td>Pod 中的所有容器都已终止，并且至少有一个容器是因为失败终止。也就是说，容器以非 0 状态退出或者被系统终止。</td></tr><tr><td><code>Unknown</code>（未知）</td><td>因为某些原因无法取得 Pod 的状态。这种情况通常是因为与 Pod 所在主机通信失败。</td></tr></tbody></table><p>如果某节点死掉或者与集群中其他节点失联，Kubernetes 会实施一种策略，将失去的节点上运行的所有 Pod 的 <code>phase</code> 设置为 <code>Failed</code>。</p><h3 id="参考文档-2"><a href="#参考文档-2" class="headerlink" title="参考文档"></a>参考文档</h3><ul><li><a href="https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions" target="_blank" rel="noopener">Pod 的生命周期</a></li></ul><h2 id="描述一个pod的创建过程"><a href="#描述一个pod的创建过程" class="headerlink" title="描述一个pod的创建过程"></a>描述一个pod的创建过程</h2><h3 id="参考文档-3"><a href="#参考文档-3" class="headerlink" title="参考文档"></a>参考文档</h3><ul><li><a href="https://mp.weixin.qq.com/s?__biz=MzA4MzIwNTc4NQ==&mid=2247484143&idx=1&sn=63552dbb768cf8ec5488575375f07311&chksm=9ffb4919a88cc00f7f411ce4b2cc6dfdb3eb3aa67f05791ba5ffeaac9650dbdeb7c6548ad4e5&mpshare=1&scene=23&srcid=&sharer_sharetime=1579044356619&sharer_shareid=85ecb9201cba173f8c63eaf47b0c5559#rd" target="_blank" rel="noopener">kubectl 创建 Pod 背后到底发生了什么?</a></li></ul><h2 id="如果服务是长连接，如何保证pod升级时，服务不受影响"><a href="#如果服务是长连接，如何保证pod升级时，服务不受影响" class="headerlink" title="如果服务是长连接，如何保证pod升级时，服务不受影响"></a>如果服务是长连接，如何保证pod升级时，服务不受影响</h2><p>可以有以下几种：</p><ol><li>设置服务重设机制</li><li>设置prestop机制</li><li>增加体面终止时间</li></ol><h2 id="如何缩小容器的镜像"><a href="#如何缩小容器的镜像" class="headerlink" title="如何缩小容器的镜像"></a>如何缩小容器的镜像</h2><p>缩小容器镜像的方法有：</p><ul><li>使用Alpine基础镜像</li><li>服务镜像和基础镜像分离</li><li>使用pv挂载数据</li></ul><h2 id="对k8s做了哪些监控？有对服务的信息做分析吗"><a href="#对k8s做了哪些监控？有对服务的信息做分析吗" class="headerlink" title="对k8s做了哪些监控？有对服务的信息做分析吗"></a>对k8s做了哪些监控？有对服务的信息做分析吗</h2><p>对k8s做了：</p><ul><li>节点的CPU，内存，磁盘等常用监控</li><li>etcd等服务监控</li><li>pod的可用性监控</li></ul><h2 id="Pod出现异常时，怎么排查问题"><a href="#Pod出现异常时，怎么排查问题" class="headerlink" title="Pod出现异常时，怎么排查问题"></a>Pod出现异常时，怎么排查问题</h2><p>分析：排查问题的思虑</p><p>Pod出现异常时，应该：</p><ul><li>通过kubectl describe pod xxx 查看pod信息</li><li>查看pod日志</li><li>查看/var/log/message</li></ul><h2 id="pod的优先级"><a href="#pod的优先级" class="headerlink" title="pod的优先级"></a>pod的优先级</h2><p>分析：主要考察QoS 模型</p><h3 id="QoS（Quality-of-Service，服务质量）-模型主要分为："><a href="#QoS（Quality-of-Service，服务质量）-模型主要分为：" class="headerlink" title="QoS（Quality of Service，服务质量） 模型主要分为："></a>QoS（Quality of Service，服务质量） 模型主要分为：</h3><ul><li><p>Guaranteed：当 Pod 里的每一个 Container 都同时设置了 requests 和 limits，并且 requests 和 limits 值相等的时候。</p><p>当这个 Pod 创建之后，它的 qosClass 字段就会被 Kubernetes 自动设置为 Guaranteed。需要注意的是，当  Pod 仅设置了 limits 没有设置 requests 的时候，Kubernetes 会自动为它设置与 limits 相同的  requests 值，所以，这也属于 Guaranteed 情况。</p></li><li><p>Burstable：当 Pod 不满足 Guaranteed 的条件，但至少有一个 Container 设置了 requests。</p></li><li><p>BestEffort：一个 Pod 既没有设置 requests，也没有设置 limits</p></li></ul><p>QoS 模型作用： QoS 划分的主要应用场景，是当宿主机资源紧张的时候，kubelet 对 Pod 进行 Eviction（即资源回收）时需要用到的。</p><h3 id="通过PriorityClass设置-Pod-优先级"><a href="#通过PriorityClass设置-Pod-优先级" class="headerlink" title="通过PriorityClass设置 Pod 优先级"></a>通过PriorityClass设置 Pod 优先级</h3><h3 id="参考文档-4"><a href="#参考文档-4" class="headerlink" title="参考文档"></a>参考文档</h3><ul><li><a href="https://wangzhangtao.com/2020/10/02/40-44-%E4%BD%9C%E4%B8%9A%E8%B0%83%E5%BA%A6%E5%92%8C%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/#QoS-%E6%A8%A1%E5%9E%8B" target="_blank" rel="noopener">作业调度和资源管理</a></li></ul><h2 id="pod发生驱逐的情况"><a href="#pod发生驱逐的情况" class="headerlink" title="pod发生驱逐的情况"></a>pod发生驱逐的情况</h2><p>分析：主要考察可压缩资源和不可压缩资源的区分</p><p>pod发生驱逐的情况有：</p><ul><li>内存不足</li><li>宿主机磁盘空间</li><li>容器运行时镜像存储空间</li></ul><h2 id="pod原地升级"><a href="#pod原地升级" class="headerlink" title="pod原地升级"></a>pod原地升级</h2><p>总结：这种只更新 Pod 中某一个或多个容器版本、而不影响整个 Pod 对象、其余容器的升级方式，被我们称为 Kubernetes 中的原地升级。</p><p>原地升级为发布效率带来了以下优化点：</p><ol><li>节省了调度的耗时，Pod 的位置、资源都不发生变化；</li><li>节省了分配网络的耗时，Pod 还使用原有的 IP；</li><li>节省了分配、挂载远程盘的耗时，Pod 还使用原有的 PV（且都是已经在 Node 上挂载好的）；</li><li>节省了大部分拉取镜像的耗时，因为 Node 上已经存在了应用的旧镜像，当拉取新版本镜像时只需要下载很少的几层 layer。</li></ol><h3 id="参考文档-5"><a href="#参考文档-5" class="headerlink" title="参考文档"></a>参考文档</h3><ul><li><a href="https://mp.weixin.qq.com/s/vw-bO5ekqvrKPzlqkzh7SA" target="_blank" rel="noopener">k8s原地升级</a></li></ul><h1 id="k8s网络"><a href="#k8s网络" class="headerlink" title="k8s网络"></a>k8s网络</h1><h2 id="service"><a href="#service" class="headerlink" title="service"></a>service</h2><h3 id="参考文档-6"><a href="#参考文档-6" class="headerlink" title="参考文档"></a>参考文档</h3><ul><li><a href="https://v1-16.docs.kubernetes.io/zh/docs/concepts/services-networking/service/#proxy-mode-ipvs" target="_blank" rel="noopener">官网service介绍</a></li></ul><h2 id="“网络栈”有哪些组件"><a href="#“网络栈”有哪些组件" class="headerlink" title="“网络栈”有哪些组件"></a>“网络栈”有哪些组件</h2><p>“网络栈”组建为：a.网卡，b.回环设备，c.路由表，d.iptables规则。这些要素构成了一台主机的网络环境。</p><h2 id="flannel的几种网络模式"><a href="#flannel的几种网络模式" class="headerlink" title="flannel的几种网络模式"></a>flannel的几种网络模式</h2><p>flannel的三种网络模式：</p><ul><li>udp：</li><li>路由模式：</li><li>vxlan：</li></ul><h2 id="service的实现机制"><a href="#service的实现机制" class="headerlink" title="service的实现机制"></a>service的实现机制</h2><h2 id="iptables和ipvs区别"><a href="#iptables和ipvs区别" class="headerlink" title="iptables和ipvs区别"></a>iptables和ipvs区别</h2><p>ipvsadm命令行用法和iptables命令行用法非常相似，毕竟是兄弟，比如<code>-L</code>列举，<code>-A</code>添加，<code>-D</code>删除。</p><h3 id="3-4-总结"><a href="#3-4-总结" class="headerlink" title="3.4 总结"></a>3.4 总结</h3><p>Kubernetes的ClusterIP和NodePort都是通过ipvs service实现的，Pod当作ipvs service的server，通过NAT MQSQ实现转发。</p><p>简单来说kube-proxy主要在所有的Node节点做如下三件事:</p><ol><li>如果没有dummy类型虚拟网卡，则创建一个，默认名称为<code>kube-ipvs0</code>;</li><li>把Kubernetes ClusterIP地址添加到<code>kube-ipvs0</code>，同时添加到ipset中。</li><li>创建ipvs service，ipvs service地址为ClusterIP以及Cluster Port，ipvs server为所有的Endpoint地址，即Pod IP及端口。</li></ol><p>使用ipvs作为kube-proxy后端，不仅提高了转发性能，结合ipset还使iptables规则变得更“干净”清楚，从此再也不怕iptables。</p><h3 id="参考文档-7"><a href="#参考文档-7" class="headerlink" title="参考文档"></a>参考文档</h3><ul><li><a href="https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/" target="_blank" rel="noopener">IPVS 代理模式</a></li><li><a href="https://zhuanlan.zhihu.com/p/94418251" target="_blank" rel="noopener">IPVS从入门到精通kube-proxy实现原理</a></li></ul><h2 id="iptables和ipvs实现过程"><a href="#iptables和ipvs实现过程" class="headerlink" title="iptables和ipvs实现过程"></a>iptables和ipvs实现过程</h2><p>iptables通过防火墙规则实现后端pod转发</p><p>ipvs通过内核和iptables实现</p><h3 id="ipvs实现过程"><a href="#ipvs实现过程" class="headerlink" title="ipvs实现过程"></a>ipvs实现过程</h3><p>IPVS 中有三种代理模式：NAT（masq），IPIP 和 DR。 只有 NAT 模式支持端口映射。 Kube-proxy 利用 NAT 模式进行端口映射。 以下示例显示 IPVS 服务端口3080到Pod端口8080的映射。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@ikj-128 ~]# ipvsadm -ln   | grep 23018</span><br><span class="line">TCP  127.0.0.1:23018 nq</span><br><span class="line">TCP  172.16.128.1:23018 nq</span><br><span class="line">TCP  192.168.70.128:23018 nq</span><br></pre></td></tr></table></figure><p>IPVS 代理中的 Iptables 和 Ipset</p><p>IPVS 用于负载均衡，它无法处理 kube-proxy 中的其他问题，例如 包过滤，数据包欺骗，SNAT 等</p><p>IPVS proxier 在上述场景中利用 iptables。 具体来说，ipvs proxier 将在以下4种情况下依赖于 iptables：</p><ul><li>kube-proxy 以 –masquerade-all = true 开头</li><li>在 kube-proxy 启动中指定集群 CIDR</li><li>支持 Loadbalancer 类型服务</li><li>支持 NodePort 类型的服务</li></ul><p>但是，我们不想创建太多的 iptables 规则。 所以我们采用 ipset 来减少 iptables 规则。 以下是 IPVS proxier 维护的 ipset 集表：</p><h2 id="service地址可以ping通吗"><a href="#service地址可以ping通吗" class="headerlink" title="service地址可以ping通吗"></a>service地址可以ping通吗</h2><p>service地址分两种情况：</p><ul><li>iptables的地址不可以ping通</li><li>ipvs地址可以ping通</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ip addr show kube-ipvs0</span><br><span class="line">4: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default</span><br><span class="line">    link&#x2F;ether 46:6b:9e:af:b0:60 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.96.0.1&#x2F;32 brd 10.96.0.1 scope global kube-ipvs0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 10.96.0.10&#x2F;32 brd 10.96.0.10 scope global kube-ipvs0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 10.96.54.11&#x2F;32 brd 10.96.54.11 scope global kube-ipvs0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"># ethtool -i kube-ipvs0 | grep driver</span><br><span class="line">driver: dummy</span><br></pre></td></tr></table></figure><p>ip addr show kube-ipvs0</p><p>可见kube-proxy首先会创建一个dummy虚拟网卡kube-ipvs0，然后把所有的Service IP添加到kube-ipvs0中。</p><p>我们知道基于iptables的Service，ClusterIP是一个虚拟的IP，因此这个IP是ping不通的，但ipvs中这个IP是在每个节点上真实存在的，因此可以ping通:</p><h2 id="k8s服务之间是如何调用的"><a href="#k8s服务之间是如何调用的" class="headerlink" title="k8s服务之间是如何调用的"></a>k8s服务之间是如何调用的</h2><p>通过service来进行调用</p><h2 id="外部服务访问k8s内部pod的几种方式"><a href="#外部服务访问k8s内部pod的几种方式" class="headerlink" title="外部服务访问k8s内部pod的几种方式"></a>外部服务访问k8s内部pod的几种方式</h2><ul><li><strong>hostNetwork</strong>：true  在pod中使用该配置，在这种Pod中运行的应用程序可以直接看到pod启动的主机的网络接口。</li><li><strong>hostPort</strong>：直接将容器的端口与所调度的节点上的端口路由，这样用户就可以通过主机的IP来访问Pod了</li><li><strong>NodePort：</strong>让外部能够直接访问service，需要将service type修改为 <code>nodePort</code>。</li><li><strong>LoadBalancer</strong>: <code>LoadBalancer</code> 只能在service上定义。这是公有云提供的负载均衡器</li><li><strong>Ingress</strong>: 一个像nginx或HAProxy的负载均衡器和一个控制器守护进程。控制器守护程序从Kubernetes接收所需的Ingress配置。</li></ul><h3 id="参考文档："><a href="#参考文档：" class="headerlink" title="参考文档："></a>参考文档：</h3><ul><li><a href="https://jimmysong.io/blog/accessing-kubernetes-pods-from-outside-of-the-cluster/" target="_blank" rel="noopener">从外部访问Kubernetes中的Pod</a></li></ul><h2 id="Kubernetes之Network-Policy"><a href="#Kubernetes之Network-Policy" class="headerlink" title="Kubernetes之Network Policy"></a>Kubernetes之Network Policy</h2><p><strong>Network policy</strong>：设置pod进出网络的策略，k8s本身并不支持，主要靠以下网络插件来支持。</p><ul><li>calico</li><li>canal</li><li>Cilium</li><li>Romana</li><li>Weave</li></ul><h3 id="参考文档-8"><a href="#参考文档-8" class="headerlink" title="参考文档"></a>参考文档</h3><ul><li><a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy" target="_blank" rel="noopener">网络和网络规则</a></li></ul><h2 id="Headless-Service-zookeeper"><a href="#Headless-Service-zookeeper" class="headerlink" title="Headless Service: zookeeper"></a>Headless Service: zookeeper</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>数据盘存储</title>
      <link href="/2020/11/03/%E6%95%B0%E6%8D%AE%E7%9B%98%E5%AD%98%E5%82%A8/"/>
      <url>/2020/11/03/%E6%95%B0%E6%8D%AE%E7%9B%98%E5%AD%98%E5%82%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="阿里云创建动态云盘卷"><a href="#阿里云创建动态云盘卷" class="headerlink" title="阿里云创建动态云盘卷"></a>阿里云创建动态云盘卷</h1><h2 id="创建StorageClass"><a href="#创建StorageClass" class="headerlink" title="创建StorageClass"></a>创建StorageClass</h2><p>创建storage-class-csi.yaml文件。                              </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: storage.k8s.io&#x2F;v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: alicloud-disk-ssd-b</span><br><span class="line">provisioner: diskplugin.csi.alibabacloud.com</span><br><span class="line">parameters:</span><br><span class="line">  type: cloud_ssd</span><br><span class="line">  regionId: cn-shenzhen</span><br><span class="line">  zoneId: cn-shenzhen-b</span><br><span class="line">reclaimPolicy: Retain</span><br><span class="line">allowVolumeExpansion: true</span><br><span class="line">volumeBindingMode: Immediate</span><br></pre></td></tr></table></figure><table><thead><tr><th>参数</th><th>说明</th></tr></thead><tbody><tr><td>provisioner</td><td>配置为diskplugin.csi.alibabacloud.com。标识StorageClass使用阿里云云盘provisioner插件创建。</td></tr><tr><td>type</td><td>标识云盘类型，支持cloud_efficiency、cloud_ssd、cloud_essd、available四种参数，其中available会对ESSD、SSD、高效依次尝试创建，直到创建成功。                                                                                                                                                                                          <strong>说明</strong> 部分ECS机型不支持ESSD云盘挂载，请参见<a href="https://help.aliyun.com/document_detail/40551.html#concept-40551-zh" target="_blank" rel="noopener">块存储FAQ</a>。                                                                                             注意</td></tr><tr><td>regionId</td><td>可选参数。期望创建云盘的区域。</td></tr><tr><td>zoneId</td><td>可选参数。期望创建云盘的可用区。</td></tr><tr><td>reclaimPolicy</td><td>云盘的回收策略，默认为Delete，支持Retain。                                                                                              Delete模式：删除PVC的时候，PV和云盘会一起删除。                                                Retain模式：删除PVC的时候，PV和云盘数据不会被删除，需要您手动删除。                                                                                          如果数据安全性要求高，推荐使用Retain方式以免误删数据。</td></tr><tr><td>encrypted</td><td>可选参数，标识创建的云盘是否加密。默认情况是false，创建的云盘不加密。</td></tr></tbody></table><p>执行以下命令，创建StorageClass。   </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f storage-class-csi.yaml</span><br></pre></td></tr></table></figure><h2 id="创建PVC-PV"><a href="#创建PVC-PV" class="headerlink" title="创建PVC/PV"></a>创建PVC/PV</h2><p>以下模板为创建动态卷PV的示例YAML。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: disk-pvc</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 25Gi</span><br><span class="line">  storageClassName: alicloud-disk-ssd</span><br></pre></td></tr></table></figure><p><strong>说明</strong> <code>storage</code>：定义申请云盘大小，最小为20GiB。                     </p><h2 id="创建应用"><a href="#创建应用" class="headerlink" title="创建应用"></a>创建应用</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    name: web</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">---</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  serviceName: &quot;nginx&quot;</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: web</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: pvc-disk</span><br><span class="line">          mountPath: &#x2F;data</span><br><span class="line">      volumes:</span><br><span class="line">        - name: pvc-disk</span><br><span class="line">          persistentVolumeClaim:</span><br><span class="line">            claimName: disk-pvc</span><br></pre></td></tr></table></figure><p>我们查看pvc的状态</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@daily test]# kubectl get pvc</span><br><span class="line">disk-pvc       Bound    d-2ze1gqesu0zjm6duqhfl                     20Gi       RWO            alicloud-disk-ssd-f   44s</span><br></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://help.aliyun.com/document_detail/134859.html?spm=a2c4g.11186623.6.814.55ce2430cxBSMW" target="_blank" rel="noopener">动态云盘卷</a></li><li><a href="https://help.aliyun.com/document_detail/167551.html?spm=a2c4g.11186623.2.16.6c2e7531CciIAf" target="_blank" rel="noopener">CSI云盘在线扩容</a></li></ul><h2 id="阿里云磁盘在线扩容"><a href="#阿里云磁盘在线扩容" class="headerlink" title="阿里云磁盘在线扩容"></a>阿里云磁盘在线扩容</h2><h3 id="不重启容器实现在线扩容"><a href="#不重启容器实现在线扩容" class="headerlink" title="不重启容器实现在线扩容"></a>不重启容器实现在线扩容</h3><p>执行以下命令获取PV信息。  </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@daily test]# kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                            STORAGECLASS                   REASON   AGE</span><br><span class="line">d-2ze1gqesu0zjm6duqhfl                     20Gi       RWO            Retain           Bound      stage&#x2F;disk-pvc                   alicloud-disk-ssd-f                     23m</span><br></pre></td></tr></table></figure><p>查看pod挂载的磁盘大小</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@daily test]# kubectl exec web-0 df &#x2F;data</span><br><span class="line">Filesystem     1K-blocks  Used Available Use% Mounted on</span><br><span class="line">&#x2F;dev&#x2F;vdb        20511312 45080  20449848   1% &#x2F;data</span><br></pre></td></tr></table></figure><p>在符合<a href="https://help.aliyun.com/document_detail/167551.html?spm=a2c4g.11186623.2.16.6c2e7531CciIAf#section-p0l-kfz-nhq" target="_blank" rel="noopener">使用说明</a>的各个条件下，执行以下命令进行数据卷扩容。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@daily test]# kubectl patch pvc disk-pvc -p &#39;&#123;&quot;spec&quot;:&#123;&quot;resources&quot;:&#123;&quot;requests&quot;:&#123;&quot;storage&quot;:&quot;30Gi&quot;&#125;&#125;&#125;&#125;&#39;         </span><br><span class="line">persistentvolumeclaim&#x2F;disk-pvc patched</span><br></pre></td></tr></table></figure><p>等待一定时间（一分钟以内）后扩容完成，检查状态如下。                            </p><p>执行以下命令查看PV信息。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@daily test]# kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                            STORAGECLASS                   REASON   AGE</span><br><span class="line">d-2ze1gqesu0zjm6duqhfl                     30Gi       RWO            Retain           Bound      stage&#x2F;disk-pvc                   alicloud-disk-ssd-f                     23m</span><br></pre></td></tr></table></figure><h1 id="CSI-卷克隆"><a href="#CSI-卷克隆" class="headerlink" title="CSI 卷克隆"></a>CSI 卷克隆</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/#csi" target="_blank" rel="noopener">CSI</a> 卷克隆功能增加了通过在 <code>dataSource</code> 字段中指定存在的 <a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/" target="_blank" rel="noopener">PVC</a>， 来表示用户想要克隆的 <a href="https://kubernetes.io/zh/docs/concepts/storage/volumes/" target="_blank" rel="noopener">卷</a>。</p><p>克隆，意思是为已有的 Kubernetes 卷创建副本，它可以像任何其它标准卷一样被使用。 唯一的区别就是配置后，后端设备将创建指定完全相同的副本，而不是创建一个“新的”空卷。</p><p>从 Kubernetes API 的角度看，克隆的实现只是在创建新的 PVC 时， 增加了指定一个现有 PVC 作为数据源的能力。源 PVC 必须是 bound 状态且可用的（不在使用中）。</p><p>用户在使用该功能时，需要注意以下事项：</p><ul><li>克隆支持（<code>VolumePVCDataSource</code>）仅适用于 CSI 驱动。</li><li>克隆支持仅适用于 动态供应器。</li><li>CSI 驱动可能实现，也可能未实现卷克隆功能。</li><li>仅当 PVC 与目标 PVC 存在于同一命名空间（源和目标 PVC 必须在相同的命名空间）时，才可以克隆 PVC。</li><li>仅在同一存储类中支持克隆。<ul><li>目标卷必须和源卷具有相同的存储类</li><li>可以使用默认的存储类并且 storageClassName 字段在规格中忽略了</li></ul></li><li>克隆只能在两个使用相同 VolumeMode 设置的卷中进行 （如果请求克隆一个块存储模式的卷，源卷必须也是块存储模式）。</li></ul><h2 id="供应"><a href="#供应" class="headerlink" title="供应"></a>供应</h2><p>克隆卷与其他任何 PVC 一样配置，除了需要增加 dataSource 来引用同一命名空间中现有的 PVC。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">    name: clone-of-pvc-1</span><br><span class="line">    namespace: myns</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  storageClassName: cloning</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 5Gi</span><br><span class="line">  dataSource:</span><br><span class="line">    kind: PersistentVolumeClaim</span><br><span class="line">    name: pvc-1</span><br></pre></td></tr></table></figure><blockquote><p><strong>说明：</strong> 你必须为 <code>spec.resources.requests.storage</code> 指定一个值，并且你指定的值必须大于或等于源卷的值。</p></blockquote><p>结果是一个名称为 <code>clone-of-pvc-1</code> 的新 PVC 与指定的源 <code>pvc-1</code> 拥有相同的内容。</p><h2 id="参考文档："><a href="#参考文档：" class="headerlink" title="参考文档："></a>参考文档：</h2><ul><li><a href="https://kubernetes.io/zh/docs/concepts/storage/volume-pvc-datasource/" target="_blank" rel="noopener">CSI 卷克隆</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> kubernets </category>
          
          <category> volumes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernets </tag>
            
            <tag> volumes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker-compost的安装和使用</title>
      <link href="/2020/11/02/docker-compost%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
      <url>/2020/11/02/docker-compost%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="Docker-Compose介绍"><a href="#Docker-Compose介绍" class="headerlink" title="Docker Compose介绍"></a>Docker Compose介绍</h2><p><code>Docker Compose</code>是<code>Docker</code>官方编排（Orchestration）项目之一，负责快速的部署分布式应用。其代码目前在<a href="https://github.com/docker/compose上开源。Compose" target="_blank" rel="noopener">https://github.com/docker/compose上开源。Compose</a> 定位是 「定义和运行多个 Docker 容器的应用（Defining and running multi-container Docker applications）」，其前身是开源项目<code>Fig</code>。</p><p>它允许用户通过一个单独的 <code>docker-compose.yml</code>模板文件（YAML 格式）来定义一组相关联的应用容器为一个项目（project）。</p><p>Compose 中有两个重要的概念：</p><ul><li>服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。</li><li>项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。</li></ul><p>Compose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。Compose 项目由 Python  编写，实现上调用了 Docker 服务提供的 API 来对容器进行管理。所以只要所操作的平台支持 Docker API，就可以在其上利用  Compose 来进行编排管理。</p><h2 id="Docker-Compose-的安装"><a href="#Docker-Compose-的安装" class="headerlink" title="Docker Compose 的安装"></a>Docker Compose 的安装</h2><p>在 Linux 上的也安装十分简单，从 官方 GitHub Release 处直接下载编译好的二进制文件即可。例如，在 Linux 64 位系统上直接下载对应的二进制包。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo curl -L https:&#x2F;&#x2F;github.com&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;1.17.1&#x2F;docker-compose-&#96;uname -s&#96;-&#96;uname -m&#96; &gt; &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br><span class="line">$ sudo chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br><span class="line">$ &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose version</span><br></pre></td></tr></table></figure><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>下面我们用 Python 来建立一个能够记录页面访问次数的 web 网站。 新建文件夹，在该目录中编写<code>app.py</code>文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import time</span><br><span class="line">import redis</span><br><span class="line">from flask import Flask</span><br><span class="line"></span><br><span class="line">app &#x3D; Flask(__name__)</span><br><span class="line">cache &#x3D; redis.Redis(host&#x3D;&#39;redis&#39;, port&#x3D;6379)</span><br><span class="line"></span><br><span class="line">def get_hit_count():</span><br><span class="line">    retries &#x3D; 5</span><br><span class="line">    while True:</span><br><span class="line">        try:</span><br><span class="line">            return cache.incr(&#39;hits&#39;)</span><br><span class="line">        except redis.exceptions.ConnectionError as exc:</span><br><span class="line">            if retries &#x3D;&#x3D; 0:</span><br><span class="line">                raise exc</span><br><span class="line">            retries -&#x3D; 1</span><br><span class="line">            time.sleep(0.5)</span><br><span class="line"></span><br><span class="line">@app.route(&#39;&#x2F;&#39;)</span><br><span class="line">def hello():</span><br><span class="line">    count &#x3D; get_hit_count()</span><br><span class="line">    return &#39;Hello World! I have been seen &#123;&#125; times.\n&#39;.format(count)</span><br><span class="line"></span><br><span class="line">if __name__ &#x3D;&#x3D; &quot;__main__&quot;:</span><br><span class="line">    app.run(host&#x3D;&quot;0.0.0.0&quot;, debug&#x3D;True)</span><br></pre></td></tr></table></figure><p>接着编写<code>Dockerfile</code>文件，内容为:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM python:3.6-alpine</span><br><span class="line">ADD . &#x2F;code</span><br><span class="line">WORKDIR &#x2F;code</span><br><span class="line">RUN pip install redis flask</span><br><span class="line">CMD [&quot;python&quot;, &quot;app.py&quot;]</span><br></pre></td></tr></table></figure><p>然后是编写<code>docker-compose.yml</code>文件，这个是 Compose 使用的主模板文件。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">version: &#39;3&#39;</span><br><span class="line">services:</span><br><span class="line">  web:    </span><br><span class="line">    build: .    </span><br><span class="line">    ports:    </span><br><span class="line">    - &quot;5000:5000&quot;</span><br><span class="line">    volumes:</span><br><span class="line">         - .:&#x2F;code</span><br><span class="line">  redis:    </span><br><span class="line">    image: &quot;redis:alpine&quot;</span><br></pre></td></tr></table></figure><p>运行 compose 项目:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker-compose up</span><br></pre></td></tr></table></figure><p>此时访问本地 5000 端口，每次刷新页面，计数就会加 1。</p><h3 id="Compose-命令"><a href="#Compose-命令" class="headerlink" title="Compose 命令"></a>Compose 命令</h3><p>对于 Compose 来说，大部分命令的对象既可以是项目本身，也可以指定为项目中的服务或者容器。如果没有特别的说明，命令对象将是项目，这意味着项目中所有的服务都会受到命令影响。</p><p>执行<strong>docker-compose [COMMAND] –help</strong>或者<strong>docker-compose help [COMMAND]</strong>可以查看具体某个命令的使用格式。</p><p>docker-compose 命令的基本的使用格式是:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker-compose [-f&#x3D;&lt;arg&gt;...] [options] [COMMAND] [ARGS...]</span><br></pre></td></tr></table></figure><p>命令选项：</p><ul><li>-f, –file FILE 指定使用的 Compose 模板文件，默认为 docker-compose.yml，可以多次指定。</li><li>-p, –project-name NAME 指定项目名称，默认将使用所在目录名称作为项目名。</li><li>–x-networking 使用 Docker 的可拔插网络后端特性</li><li>–x-network-driver DRIVER 指定网络后端的驱动，默认为 bridge</li><li>–verbose 输出更多调试信息。</li><li>-v, –version 打印版本并退出。</li></ul><p>build 格式为<strong>docker-compose build [options] [SERVICE…]</strong>。 构建（重新构建）项目中的服务容器。服务容器一旦构建后，将会带上一个标记名，例如对于 web 项目中的一个 db 容器，可能是 web_db。可以随时在项目目录下运行<code>docker-compose build</code>来重新构建服务。选项包括：</p><ul><li>–force-rm 删除构建过程中的临时容器。</li><li>–no-cache 构建镜像过程中不使用 cache（这将加长构建过程）。</li><li>–pull 始终尝试通过 pull 来获取更新版本的镜像。</li></ul><p>config: 验证 Compose 文件格式是否正确，若正确则显示配置，若格式错误显示错误原因。</p><p>down：此命令将会停止 up 命令所启动的容器，并移除网络</p><p>exec：进入指定的容器。</p><p>help：获得一个命令的帮助。</p><p>images：列出 Compose 文件中包含的镜像。</p><p>kill：格式为<strong>docker-compose kill [options] [SERVICE…]</strong>。通过发送<code>SIGKILL</code>信号来强制停止服务容器。支持通过<code>-s</code>参数来指定发送的信号，例如通过如下指令发送<code>SIGINT</code>信号。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker-compose kill -s SIGINT</span><br></pre></td></tr></table></figure><p>logs：格式为<strong>docker-compose logs [options] [SERVICE…]</strong>，查看服务容器的输出。默认情况下，docker-compose 将对不同的服务输出使用不同的颜色来区分。可以通过 <code>--no-color</code>来关闭颜色。该命令在调试问题的时候十分有用。</p><p>pause：格式为<strong>docker-compose pause [SERVICE…]</strong>，暂停一个服务容器。</p><p>port：格式为<strong>docker-compose port [options] SERVICE PRIVATE_PORT</strong>，打印某个容器端口所映射的公共端口。选项：</p><ul><li>–protocol=proto 指定端口协议，tcp（默认值）或者 udp。</li><li>–index=index 如果同一服务存在多个容器，指定命令对象容器的序号（默认为 1）。</li></ul><p>ps：格式为<strong>docker-compose ps [options] [SERVICE…]</strong>，列出项目中目前的所有容器。选项：</p><ul><li><code>-q</code>只打印容器的 ID 信息。</li></ul><p>pull：格式为<code>docker-compose pull [options] [SERVICE...]</code>，拉取服务依赖的镜像。选项：</p><ul><li>–ignore-pull-failures 忽略拉取镜像过程中的错误。</li></ul><p>push：推送服务依赖的镜像到 Docker 镜像仓库。</p><p>restart：格式为<strong>docker-compose restart [options] [SERVICE…]</strong>，重启项目中的服务。选项：</p><ul><li>-t, –timeout TIMEOUT 指定重启前停止容器的超时（默认为 10 秒）。</li></ul><p>rm：格式为<strong>docker-compose rm [options] [SERVICE…]</strong>,删除所有（停止状态的）服务容器。推荐先执行 <code>docker-compose stop</code>命令来停止容器。选项：</p><ul><li>-f, –force 强制直接删除，包括非停止状态的容器。一般尽量不要使用该选项。</li><li>-v 删除容器所挂载的数据卷。</li></ul><p>run：格式为<strong>docker-compose run [options] [-p PORT…] [-e KEY=VAL…] SERVICE [COMMAND] [ARGS…]</strong>，在指定服务上执行一个命令。例如：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker-compose run ubuntu ping docker.com</span><br></pre></td></tr></table></figure><p>将会启动一个 ubuntu 服务容器，并执行 ping docker.com 命令。默认情况下，如果存在关联，则所有关联的服务将会自动被启动，除非这些服务已经在运行中。</p><p>该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照配置自动创建。</p><p>给定命令将会覆盖原有的自动运行命令； 不会自动创建端口，以避免冲突。</p><p>如果不希望自动启动关联的容器，可以使用<code>--no-deps</code>选项，例如:</p><ul><li>-d 后台运行容器。</li><li>–name NAME 为容器指定一个名字。</li><li>–entrypoint CMD 覆盖默认的容器启动指令。</li><li>-e KEY=VAL 设置环境变量值，可多次使用选项来设置多个环境变量。</li><li>-u, –user=”” 指定运行容器的用户名或者 uid。</li><li>–no-deps 不自动启动关联的服务容器。</li><li>–rm 运行命令后自动删除容器，d 模式下将忽略。</li><li>-p, –publish=[] 映射容器端口到本地主机。</li><li>–service-ports 配置服务端口并映射到本地主机。</li><li>-T 不分配伪 tty，意味着依赖 tty 的指令将无法运行。</li></ul><p>scale：格式为<strong>docker-compose scale [options] [SERVICE=NUM…]</strong>，设置指定服务运行的容器个数。 通过 service=num 的参数来设置数量。例如：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ docker-compose scale web&#x3D;3 db&#x3D;2</span><br></pre></td></tr></table></figure><p>将启动 3 个容器运行 web 服务，2 个容器运行 db 服务。</p><p>一般的，当指定数目多于该服务当前实际运行容器，将新创建并启动容器；反之，将停止容器。选项：</p><ul><li>-t, –timeout TIMEOUT 停止容器时候的超时（默认为 10 秒）。</li></ul><p>start：格式为<strong>docker-compose start [SERVICE…]</strong>，启动已经存在的服务容器。</p><p>stop：格式为<strong>docker-compose stop [options] [SERVICE…]</strong>, 停止已经处于运行状态的容器，但不删除它。通过<code>docker-compose start</code>可以再次启动这些容器。选项：</p><ul><li>-t, –timeout TIMEOUT 停止容器时候的超时（默认为 10 秒）。</li></ul><p>top：查看各个服务容器内运行的进程。</p><p>unpause：格式为<strong>docker-compose unpause [SERVICE…]</strong>，恢复处于暂停状态中的服务。</p><p>up：格式为<strong>docker-compose up [options] [SERVICE…]</strong>，该命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。链接的服务都将会被自动启动，除非已经处于运行状态。 可以说，大部分时候都可以直接通过该命令来启动一个项目。</p><p>默认情况，<code>docker-compose up</code>启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试。 当通过 Ctrl-C 停止命令时，所有容器将会停止。</p><p>如果使用<code>docker-compose up -d</code>，将会在后台启动并运行所有的容器。一般推荐生产环境下使用该选项。</p><p>默认情况，如果服务容器已经存在，<code>docker-compose up</code>将会尝试停止容器，然后重新创建（保持使用 volumes-from 挂载的卷），以保证新启动的服务匹配 docker-compose.yml 文件的最新内容。如果用户不希望容器被停止并重新创建，可以使用 <code>docker-compose up --no-recreate</code>。这样将只会启动处于停止状态的容器，而忽略已经运行的服务。</p><p>如果用户只想重新部署某个服务，可以使用<code>docker-compose up --no-deps -d &lt;SERVICE_NAME&gt;</code>来重新创建服务并后台停止旧服务，启动新服务，并不会影响到其所依赖的服务。选项：</p><ul><li>-d 在后台运行服务容器。</li><li>–no-color 不使用颜色来区分不同的服务的控制台输出。</li><li>–no-deps 不启动服务所链接的容器。</li><li>–force-recreate 强制重新创建容器，不能与<code>--no-recreate</code>同时使用。 <code>--no-recreate</code>如果容器已经存在了，则不重新创建，不能与 <code>--force-recreate</code>同时使用。 <code>--no-build</code>不自动构建缺失的服务镜像。</li><li>-t, –timeout TIMEOUT 停止容器时候的超时（默认为 10 秒）。</li></ul><h2 id="Compose-命令演示"><a href="#Compose-命令演示" class="headerlink" title="Compose 命令演示"></a>Compose 命令演示</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 compose]# docker-compose config</span><br><span class="line">services:</span><br><span class="line">  redis:</span><br><span class="line">    image: redis:alpine</span><br><span class="line">  web:</span><br><span class="line">    build:</span><br><span class="line">      context: &#x2F;root&#x2F;compose</span><br><span class="line">    ports:</span><br><span class="line">    - 5000:5000&#x2F;tcp</span><br><span class="line">    volumes:</span><br><span class="line">    - &#x2F;root&#x2F;compose:&#x2F;code:rw</span><br><span class="line">version: &#39;3.0&#39;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 compose]# docker-compose up -d</span><br><span class="line">Starting compose_web_1 ... </span><br><span class="line">Starting compose_redis_1 ... </span><br><span class="line">Starting compose_web_1</span><br><span class="line">Starting compose_redis_1 ... done</span><br><span class="line"></span><br><span class="line">[root@wang-18 compose]# docker-compose down</span><br><span class="line">Stopping compose_web_1   ... done</span><br><span class="line">Stopping compose_redis_1 ... done</span><br><span class="line">Removing compose_web_1   ... done</span><br><span class="line">Removing compose_redis_1 ... done</span><br><span class="line">Removing network compose_default</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 compose]# docker-compose exec web &#x2F;bin&#x2F;sh  </span><br><span class="line">&#x2F;code #</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 compose]# docker-compose images</span><br><span class="line">   Container      Repository     Tag       Image Id      Size  </span><br><span class="line">---------------------------------------------------------------</span><br><span class="line">compose_redis_1   redis         alpine   8d1b562adb1c   29.7 MB</span><br><span class="line">compose_web_1     compose_web   latest   692a2d143d9e   49.1 MB</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 compose]# docker-compose kill -s 1</span><br><span class="line">Killing compose_redis_1 ... done</span><br><span class="line">Killing compose_web_1   ... done</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 compose]# docker-compose logs -f web</span><br><span class="line">Attaching to compose_web_1</span><br><span class="line">web_1    |  * Serving Flask app &quot;app&quot; (lazy loading)</span><br><span class="line">web_1    |  * Environment: production</span><br><span class="line">web_1    |    WARNING: This is a development server. Do not use it in a production deployment.</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 compose]# docker-compose pause web</span><br><span class="line">Pausing compose_web_1 ... done</span><br><span class="line">[root@wang-18 compose]# docker-compose unpause web</span><br><span class="line">Unpausing compose_web_1 ... done</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 compose]# docker-compose port web 5000</span><br><span class="line">0.0.0.0:5000</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 compose]# docker-compose ps</span><br><span class="line">     Name                    Command               State           Ports         </span><br><span class="line">---------------------------------------------------------------------------------</span><br><span class="line">compose_redis_1   docker-entrypoint.sh redis ...   Up      6379&#x2F;tcp              </span><br><span class="line">compose_web_1     python app.py                    Up      0.0.0.0:5000-&gt;5000&#x2F;tcp</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 compose]# docker-compose pull redis</span><br><span class="line">Pulling redis (redis:alpine)...</span><br><span class="line">alpine: Pulling from library&#x2F;redis</span><br><span class="line">Digest: sha256:cac5bbb2a6dd455e503951d296e11966060bdc6e3be1eadc5551adde58052700</span><br><span class="line">Status: Image is up to date for redis:alpine</span><br><span class="line"></span><br><span class="line">[root@wang-18 compose]# docker-compose push redis</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 compose]# docker-compose restart </span><br><span class="line">Restarting compose_redis_1 ... done</span><br><span class="line">Restarting compose_web_1   ... done</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 compose]# docker-compose rm web</span><br><span class="line">No stopped containers</span><br><span class="line">[root@wang-18 compose]# docker-compose stop web</span><br><span class="line">Stopping compose_web_1 ... done</span><br><span class="line">[root@wang-18 compose]# docker-compose rm web  </span><br><span class="line">Going to remove compose_web_1</span><br><span class="line">Are you sure? [yN] y</span><br><span class="line">Removing compose_web_1 ... done</span><br><span class="line">[root@wang-18 compose]# docker-compose rm redis</span><br><span class="line">No stopped containers</span><br><span class="line">[root@wang-18 compose]# docker-compose rm -f redis</span><br><span class="line">No stopped containers</span><br></pre></td></tr></table></figure><h2 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h2><h3 id="报错：service-‘build’-must-be-a-mapping-not-a-string"><a href="#报错：service-‘build’-must-be-a-mapping-not-a-string" class="headerlink" title="报错：service ‘build’ must be a mapping not a string"></a>报错：service ‘build’ must be a mapping not a string</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 test1]# docker-compose config</span><br><span class="line">ERROR: In file &#39;.&#x2F;docker-compose.yml&#39;, service &#39;build&#39; must be a mapping not a string.</span><br></pre></td></tr></table></figure><p>因为docker-compose.yml中格式错了，请看看再看看格式。</p><p>我一开始复制粘贴，结果如下，web后的下一列，没有缩紧。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">version: &#39;3&#39;</span><br><span class="line">services:</span><br><span class="line">  web:    </span><br><span class="line">  build: .    </span><br><span class="line">  ports:    </span><br><span class="line">  - &quot;5000:5000&quot;</span><br><span class="line">  volumes:</span><br><span class="line">       - .:&#x2F;code</span><br><span class="line">  redis:    </span><br><span class="line">  image: &quot;redis:alpine&quot;</span><br></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li>阳明的 <a href="https://www.qikqiak.com/k8s-book/docs/8.Docker%20Compose.html" target="_blank" rel="noopener">Docker compose</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deployment实现应用部署</title>
      <link href="/2020/11/02/deployment%E5%AE%9E%E7%8E%B0%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2/"/>
      <url>/2020/11/02/deployment%E5%AE%9E%E7%8E%B0%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="Deployment实现应用部署"><a href="#Deployment实现应用部署" class="headerlink" title="Deployment实现应用部署"></a>Deployment实现应用部署</h1><h2 id="Deployment模版文件"><a href="#Deployment模版文件" class="headerlink" title="Deployment模版文件"></a>Deployment模版文件</h2><img src="https://wangzhangtao.com/img/body/1.linux-tmp/198738-20200221181724013-1936975800.png" alt="img" style="zoom:67%;max-width: 67%" /><h2 id="实现水平扩展-收缩"><a href="#实现水平扩展-收缩" class="headerlink" title="实现水平扩展/收缩"></a>实现水平扩展/收缩</h2><p>  水平扩展/收缩非常容易实现,Deployment Controller只需要修改它所控制的ReplicaSet的Pod副本个数就可以了</p><p>  把值从3改成4那么Deployment所对应的ReplicaSet,就会根据修改后的值自动创建一个新的 Pod.这就是“水平扩展”了.“水平收缩”则反之</p><p>  kubectl scale deployment nginx-deployment –replicas=4</p><h2 id="实现滚动更新"><a href="#实现滚动更新" class="headerlink" title="实现滚动更新"></a>实现滚动更新</h2><h3 id="deployment-显示的字段含义"><a href="#deployment-显示的字段含义" class="headerlink" title="deployment 显示的字段含义"></a>deployment 显示的字段含义</h3><p><img src="https://wangzhangtao.com/img/body/1.linux-tmp/198738-20200221183755564-450860101.png" alt="img"></p><ul><li>READY  表示当前处于Running状态的Pod个数,但容器的镜像版本不一定是最新的</li><li>UP-TO-DATE 表示当前处于最新版本的Pod的个数 最新版本指的是Pod的Spec字段和Deployment里Pod模板中定义的完全一致</li><li>AVAILABLE 表示当前已经可用的Pod的个数 即是Running状态又是最新版本并且容器已经Ready状态的Pod个数,描述的是用户期望的最终状态的个数</li></ul><p><img src="https://wangzhangtao.com/img/body/1.linux-tmp/198738-20200221190726638-411262384.png" alt="img"></p><h3 id="滚动更新流程"><a href="#滚动更新流程" class="headerlink" title="滚动更新流程:"></a>滚动更新流程:</h3><ol><li>当修改了Deployment里的Pod定义之后,Deployment Controller会使用这个修改后的Pod模板创建一个新的ReplicaSet.这个新的 ReplicaSet 的初始Pod副本数是0</li><li>然后Deployment Controller 开始将这个新的ReplicaSet 所控制的Pod副本数从0个变成1个.即:“水平扩展”出一个副本</li><li>Deployment Controller又将旧的ReplicaSet所控制的旧Pod 副本数减少一个.即:“水平收缩”成两个副本</li><li>如此交替进行,新ReplicaSet管理的Pod副本数,从0个变成1个,再变成 2个,最后变成 3个.而旧的ReplicaSet管理的Pod副本数则从3个变成 2个,再变成 1个,最后变成 0个.这样就完成了这一组 Pod 的版本升级过程</li></ol><p>​    将一个集群中正在运行的多个Pod版本 交替地逐一升级的过程,就是“滚动更新”</p><p>​    在升级刚开始的时候,集群里只有1个新版本的 Pod.如果这时新版本Pod有问题启动不起来,那么“滚动更新”就会停止.而在这个过程中,由于应用本身还有两个旧版本的Pod在线,所以服务并不会受到太大的影响</p><h3 id="滚动更新指定比例的Pod"><a href="#滚动更新指定比例的Pod" class="headerlink" title="滚动更新指定比例的Pod"></a>滚动更新指定比例的Pod</h3><p>Deployment Controller还会确保在任何时间窗口内,只有指定比例的Pod处于离线状态.同时它也会确保,在任何时间窗口内只有指定比例的新Pod被创建出来.这两个比例的值都是可以配置的默认都是DESIRED值的25%</p><p>在RollingUpdateStrategy 的配置中:</p><ul><li>maxSurge=1指定的是除了DESIRED数量之外在一次“滚动”中,Deployment 控制器还可以创建多少个新Pod</li><li>maxSurge=50%指的是我们最多可以一次创建 “50%*DESIRED 数量” 个新版本 Pod</li><li>maxUnavailable=1指的是在一次“滚动”中,Deployment控制器可以删除多少个旧Pod</li><li>maxUnavailable=50%指的是我们最多可以一次删除 “50%*DESIRED 数量” 个旧版本 Pod</li></ul><h2 id="deployment控制流程"><a href="#deployment控制流程" class="headerlink" title="deployment控制流程"></a>deployment控制流程</h2><img src="https://wangzhangtao.com/img/body/1.linux-tmp/198738-20200221194033982-1209661333.png" alt="img" style="zoom:67%;max-width: 67%" /><ol><li><p>Deployment 的控制器实际上控制的是ReplicaSet 的数目以及每个ReplicaSet的属性</p></li><li><p>一个应用的版本对应的正是一个ReplicaSet这个版本应用的Pod数量.则由ReplicaSet通过它自己的控制器(ReplicaSet Controller)来保证</p></li><li><p>这样的多个ReplicaSet对象Kubernetes项目就实现了对多个“应用版本”的描述</p></li><li><p>应用版本和ReplicaSet是 一一对应的关系</p><p>我们对 Deployment 进行的每一次更新操作,默认都会生成一个新的ReplicaSet对象 相当于创建一个程序的版本</p></li></ol><h3 id="deployment暂停控制"><a href="#deployment暂停控制" class="headerlink" title="deployment暂停控制"></a>deployment暂停控制</h3><p>Kubernetes 项目还提供了一个指令,使得我们对Deployment的多次更新操作最后只生成一个ReplicaSet 相当于多次更新操作的集合作为一个程序版本</p><ol><li>在更新Deployment 前你要先执行一条 kubectl rollout pause 指令</li><li>由于此时 Deployment 正处于“暂停”状态所以我们对 Deployment 的所有修改,都不会触发新的“滚动更新”也不会创建新的ReplicaSet</li><li>等到我们对 Deployment 修改操作都完成之后,只需要再执行一条 kubectl rollout resume 指令就可以把这个Deployment“恢复”回来</li></ol><p>而在这个kubectl rollout resume指令执行之前,在kubectl rollout pause 指令之后的这段时间里我们对 Deployment 进行的所有修改最后只会触发一次“滚动更新相当于只会创建一个ReplicaSet(表示一个程序版本)</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> deployment </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> deployment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s的部署策略</title>
      <link href="/2020/11/02/k8s%E7%9A%84%E9%83%A8%E7%BD%B2%E7%AD%96%E7%95%A5/"/>
      <url>/2020/11/02/k8s%E7%9A%84%E9%83%A8%E7%BD%B2%E7%AD%96%E7%95%A5/</url>
      
        <content type="html"><![CDATA[<h1 id="k8s的部署策略"><a href="#k8s的部署策略" class="headerlink" title="k8s的部署策略"></a>k8s的部署策略</h1><ul><li>recreate(重建)模式 </li><li>ramped(滚动)模式</li><li>blue/green(蓝色/绿色)模式</li><li>canary(金丝雀)模式</li></ul><h2 id="recreate-重建-模式"><a href="#recreate-重建-模式" class="headerlink" title="recreate(重建)模式"></a>recreate(重建)模式</h2><p>一次性终止所有的旧版本后并一次性发布新版本</p><p>定义的部署将终止所有正在运行的实例,然后使用较新的版本重新创建它们  最适合开发环境的发布</p><p>优点: 应用状态完全更新</p><p>缺点: 停机时间取决于应用程序的关闭和启动持续时间</p><h2 id="ramped-滚动-模式"><a href="#ramped-滚动-模式" class="headerlink" title="ramped(滚动)模式"></a>ramped(滚动)模式</h2><p>以滚动更新的方式发布新版本,成功创建一个新pod后再终止一个旧版本的Pod</p><p>渐变部署以滚动更新方式更新 pod,使用应用程序的新版本创建辅助ReplicaSet,然后减少旧版本的副本数,并增加新版本,直到达到正确的副本数</p><p>优点：</p><ul><li>版本会在实例之间缓慢发布</li><li>对于可以处理数据重新平衡的有状态应用程序很方便</li></ul><p>缺点：</p><ul><li>推出/回滚可能需要一些时间</li><li>支持多个API很难</li><li>无法控制流量</li></ul><h2 id="重建模式和滚动模式的区别"><a href="#重建模式和滚动模式的区别" class="headerlink" title="重建模式和滚动模式的区别"></a>重建模式和滚动模式的区别</h2><p>Deployment控制器支持两种更新策略： 滚动更新（rolling update）和  重新创建（recreate），默认为滚动更新。字段：deploy.spec.strategy</p><p>重新创建更新首先删除现有的Pod对象,而后由 控制器基于新模板重新创建出新版本资源对象.通常,只应该在应用的新旧版本不兼容（  如依赖的后端数据库的schema不同且无法兼容）时运行时才会使用recreate策略,因为它会导致应用替换期间暂时不可用,好处在于它不存在中间状态,用户访问到的要么是应用的 新版本,要么是旧版本</p><p>滚动升级是默认的更新策略,它在删除一部分旧版本Pod资源的同时,补充创建一部分  新版本的Pod对象进行应用升级,其优势是升级期间,容器中应用提供的服务不会中断，但要求应用程序能够应对新旧版本同时工作的情形,例如新旧版本兼容同一个 数据库方案等.不过更新操作期间,不同客户端得到的响应内容可能会来自不同版本的 应用</p><p>Deployment控制器的滚动更新操作并非在同一个ReplicaSet控制器对象下删除并 创建Pod资源,而是将它们分置于两个不同的控制器之下：旧控制器的Pod对象数量 不断减少的同时,新控制器的Pod对象数量不断增加,直到旧控制器不再拥有Pod对象</p><h2 id="blue-green-蓝色-绿色-模式"><a href="#blue-green-蓝色-绿色-模式" class="headerlink" title="blue/green(蓝色/绿色)模式"></a>blue/green(蓝色/绿色)模式</h2><p>与旧版本一起发布新版本,然后切换流量</p><p>蓝色/绿色部署与渐变部署不同,因为应用程序的“绿色”版本与“蓝色”版本同时部署.在测试新版本满足要求之后,我们更新Kubernetes Service对象</p><p>该对象扮演负载平衡器的角色,<strong>通过替换选择器字段中的版本标签将流量发送到新版本.</strong></p><p>优点:</p><ul><li>即时部署/回滚</li><li>避免版本问题,一次性更改整个群集状态</li></ul><p>缺点：</p><ul><li>需要两倍的资源</li><li>在正式发布之前,应该对整个平台进行适当的测试</li><li>处理有状态的应用程序可能很困难</li></ul><h2 id="canary-金丝雀-模式"><a href="#canary-金丝雀-模式" class="headerlink" title="canary(金丝雀)模式"></a>canary(金丝雀)模式</h2><p>向一部分用户发布新版本,然后进行全面部署</p><p>金丝雀部署将用户的子集路由到新功能.在k8s中,可以使用两个具有通用Pod标签的部署来完成金丝雀部署.新版本的一个副本与旧版本一起发布</p><p>在一段时间后,如果未检测到错误,请<strong>按比例增加新版本的副本数量并删除旧的部署</strong></p><p>优点：</p><ul><li>为部分用户发布的版本</li><li>错率和性能监控</li><li>快速回滚</li></ul><p>缺点：</p><ul><li>缓慢推出</li><li>精细调整的流量分配可能会很昂贵(99％A / 1％B = 99个Pod A，1个Pod B)</li></ul><h2 id="蓝绿模式和金丝雀模式的区别"><a href="#蓝绿模式和金丝雀模式的区别" class="headerlink" title="蓝绿模式和金丝雀模式的区别"></a>蓝绿模式和金丝雀模式的区别</h2><p>相同点：</p><ul><li>都是新旧版本共存</li><li>通过替换选择器字段中的版本标签来选择不同的pod</li></ul><p>不同点：</p><ul><li>蓝绿模式同时存在新旧版本，通过手动更改 svc的标签来切换服务的不同版本（pod）</li><li>金丝雀模式为新旧pod共存，手动更改新旧的ds控制的pod数量</li></ul><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://www.cnblogs.com/yxh168/p/12240134.html" target="_blank" rel="noopener">k8s的部署策略</a></li></ul><h2 id="容器重启策略"><a href="#容器重启策略" class="headerlink" title="容器重启策略"></a>容器重启策略</h2><p>Pod 的 <code>spec</code> 中包含一个 <code>restartPolicy</code> 字段，其可能取值包括 Always、OnFailure 和 Never。默认值是 Always。</p><p><code>restartPolicy</code> 适用于 Pod 中的所有容器。<code>restartPolicy</code> 仅针对同一节点上 <code>kubelet</code> 的容器重启动作。当 Pod 中的容器退出时，<code>kubelet</code> 会按指数回退 方式计算重启的延迟（10s、20s、40s、…），其最长延迟为 5 分钟。 一旦某容器执行了 10 分钟并且没有出现问题，<code>kubelet</code> 对该容器的重启回退计时器执行 重置操作。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 部署策略 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 部署策略 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>本人推荐</title>
      <link href="/2020/10/20/%E6%9C%AC%E4%BA%BA%E6%8E%A8%E8%8D%90/"/>
      <url>/2020/10/20/%E6%9C%AC%E4%BA%BA%E6%8E%A8%E8%8D%90/</url>
      
        <content type="html"><![CDATA[<h2 id="我的k8s学习路线"><a href="#我的k8s学习路线" class="headerlink" title="我的k8s学习路线"></a>我的k8s学习路线</h2><ol><li>官方的 <a href="https://kubernetes.io/" target="_blank" rel="noopener">kuberntes官网</a>，特点：官网最权威，没事常看看</li><li>马永亮的<a href="https://ke.qq.com/course/332538" target="_blank" rel="noopener">《马哥的Kubernetes》</a> (2018)，特点：理论基础特别好</li><li>阳明的<a href="https://youdianzhishi.com/web/course/1007" target="_blank" rel="noopener">《从 Docker 到 Kubernetes 进阶》</a>(2018)，特点：综合使用特别好</li><li>极客时间张磊的<a href="https://time.geekbang.org/column/intro/100015201" target="_blank" rel="noopener">《深入剖析Kubernetes》</a>（2018），特点：原理透彻，通俗易懂</li><li>老男孩的<a href="">《Kubernetes实战》</a>（2019），特点：二进制部署和k8s使用技巧</li></ol><h2 id="本站相关文档"><a href="#本站相关文档" class="headerlink" title="本站相关文档"></a>本站相关文档</h2><h3 id="二进制安装"><a href="#二进制安装" class="headerlink" title="二进制安装"></a>二进制安装</h3><ul><li><p><a href="https://wangzhangtao.com/2020/05/28/一、k8s-实验环境说明/" target="_blank" rel="noopener">一、k8s-实验环境说明</a></p></li><li><p><a href="https://wangzhangtao.com/2020/05/30/二、k8s-前期环境准备/" target="_blank" rel="noopener">二、k8s-前期环境准备</a></p></li><li><p><a href="https://wangzhangtao.com/2020/05/31/三、k8s-部署master组件/" target="_blank" rel="noopener">三、k8s-部署master组件</a></p></li><li><p><a href="https://wangzhangtao.com/2020/06/02/四、k8s-部署node组件/" target="_blank" rel="noopener">四、k8s-部署node组件</a></p></li><li><p><a href="https://wangzhangtao.com/2020/06/03/五、k8s-部署addons组件/" target="_blank" rel="noopener">五、k8s-部署Addons组件</a></p></li><li><p><a href="https://wangzhangtao.com/2020/06/04/六、k8s-集群平滑升级/" target="_blank" rel="noopener">六、k8s-集群平滑升级</a></p></li><li><p><a href="https://wangzhangtao.com/2020/06/04/七、k8s-错误收集/" target="_blank" rel="noopener">七、k8s-错误收集</a></p></li></ul><h3 id="部署k8s生态"><a href="#部署k8s生态" class="headerlink" title="部署k8s生态"></a>部署k8s生态</h3><ul><li><p><a href="https://wangzhangtao.com/2020/06/05/一、交付-基础环境准备/" target="_blank" rel="noopener">一、交付-基础环境准备</a></p></li><li><p><a href="https://wangzhangtao.com/2020/06/05/二、交付-使用blue-ocean流水线构建/" target="_blank" rel="noopener">二、交付-使用blue ocean流水线构建</a></p></li><li><p><a href="https://wangzhangtao.com/2020/06/08/三、交付-使用apollo配置中心/" target="_blank" rel="noopener">三、交付-使用apollo配置中心</a></p></li><li><p><a href="https://wangzhangtao.com/2020/06/09/四、生态-prometheus系统监控/" target="_blank" rel="noopener">四、生态-prometheus系统监控</a></p></li><li><p><a href="https://wangzhangtao.com/2020/06/10/五、生态-elk收集系统日志/" target="_blank" rel="noopener">五、生态-elk收集系统日志</a></p></li></ul><h3 id="部署服务"><a href="#部署服务" class="headerlink" title="部署服务"></a>部署服务</h3><ul><li><a href="https://wangzhangtao.com/2020/07/22/k8s添加开发使用的serviceaccount/" target="_blank" rel="noopener">k8s添加开发使用的ServiceAccount</a></li><li><a href="https://wangzhangtao.com/2020/07/21/k8s部署动态持久化存储storageclass/" target="_blank" rel="noopener">k8s部署动态持久化存储StorageClass</a></li><li><a href="https://wangzhangtao.com/2020/09/23/部署zookeeper3-6-1集群/" target="_blank" rel="noopener">部署zookeeper3.6.1集群</a></li><li><a href="https://wangzhangtao.com/2020/08/17/k8s部署dubbo-admin/" target="_blank" rel="noopener">部署dubbo-admin</a></li><li><a href="https://wangzhangtao.com/2020/10/10/helm的安装和使用/" target="_blank" rel="noopener">helm的安装和使用</a></li></ul><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul><li><a href="https://wangzhangtao.com/2020/05/28/基础环境部署/" target="_blank" rel="noopener">基础环境部署</a></li><li><a href="https://wangzhangtao.com/2020/06/24/操作系统vmware-esxi安装/" target="_blank" rel="noopener">操作系统VMware-esxi安装</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 基础环境 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>helm的安装和使用</title>
      <link href="/2020/10/10/helm%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
      <url>/2020/10/10/helm%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<blockquote><p> 本文档主要为参考阳明的文档摘抄，并从其他文章参考而来，主要文章链接，请查看结尾</p></blockquote><h1 id="Helm安装使用"><a href="#Helm安装使用" class="headerlink" title="Helm安装使用"></a>Helm安装使用</h1><h2 id="Helm简介"><a href="#Helm简介" class="headerlink" title="Helm简介"></a>Helm简介</h2><p><code>Helm</code>这个东西其实早有耳闻，但是一直没有用在生产环境，最近我们的k8s文件需要大的改动，发现没有变量真的很难，所以返回来再次学习。这次使用helm3做练习。</p><p>Helm是一个Kubernetes的包管理工具，就像Linux下的包管理器，如yum/apt等，可以很方便的将之前打包好的yaml文件部署到kubernetes上。</p><p>Helm有3个重要概念：</p><ul><li><strong>helm：</strong>一个命令行客户端工具，主要用于Kubernetes应用chart的创建、打包、发布和管理。</li><li><strong>Chart：</strong>应用描述，一系列用于描述 k8s 资源相关文件的集合。</li><li><strong>Release：</strong>基于Chart的部署实体，一个 chart 被 Helm 运行后将会生成对应的一个 release；将在k8s中创建出真实运行的资源对象。</li></ul><p>做为 Kubernetes 的一个包管理工具，<code>Helm</code>具有如下功能：</p><ul><li>创建新的 chart</li><li>chart 打包成 tgz 格式</li><li>上传 chart 到 chart 仓库或从仓库中下载 chart</li><li>在<code>Kubernetes</code>集群中安装或卸载 chart</li><li>管理用<code>Helm</code>安装的 chart 的发布周期</li></ul><h2 id="Helm3变化"><a href="#Helm3变化" class="headerlink" title="Helm3变化"></a>Helm3变化</h2><ol><li>最明显的变化是 <code>Tiller</code>的删除</li><li>Release 不再是全局资源，而是存储在各自命名空间内</li><li>Values 支持 JSON Schema校验器，自动检查所有输入的变量格式</li><li>helm install 不再默认生成一个 Release 的名称，除非指定了 –generate-name。</li><li>Helm CLI 个别更名</li></ol><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">helm delete更名为 helm uninstall</span><br><span class="line">helm inspect更名为 helm show</span><br><span class="line">helm fetch更名为 helm pull</span><br></pre></td></tr></table></figure><p>但以上旧的命令当前仍能使用。</p><h2 id="部署helm客户端"><a href="#部署helm客户端" class="headerlink" title="部署helm客户端"></a>部署helm客户端</h2><p>下载helm-v3.3.3安装包：<a href="https://get.helm.sh/helm-v3.3.3-linux-amd64.tar.gz" target="_blank" rel="noopener">https://get.helm.sh/helm-v3.3.3-linux-amd64.tar.gz</a></p><h3 id="安装helm"><a href="#安装helm" class="headerlink" title="安装helm"></a>安装helm</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;k8s&#x2F;helm-v3.3.3-linux-amd64.tar.gz -O &#x2F;opt&#x2F;src&#x2F;helm-v3.3.3-linux-amd64.tar.gz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;helm-v3.3.3-linux-amd64.tar.gz -C &#x2F;usr&#x2F;local&#x2F;</span><br><span class="line">mv &#x2F;usr&#x2F;local&#x2F;linux-amd64 &#x2F;usr&#x2F;local&#x2F;helm-v3.3.3</span><br><span class="line">ln -s &#x2F;usr&#x2F;local&#x2F;helm-v3.3.3&#x2F;helm &#x2F;usr&#x2F;bin&#x2F;helm</span><br><span class="line">helm version</span><br></pre></td></tr></table></figure><h3 id="配置仓库"><a href="#配置仓库" class="headerlink" title="配置仓库"></a>配置仓库</h3><ul><li><p>微软仓库（<a href="http://mirror.azure.cn/kubernetes/charts/）这个仓库强烈推荐，基本上官网有的chart这里都有。" target="_blank" rel="noopener">http://mirror.azure.cn/kubernetes/charts/）这个仓库强烈推荐，基本上官网有的chart这里都有。</a></p></li><li><p>阿里云仓库（<a href="https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts" target="_blank" rel="noopener">https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</a>  ） </p></li><li><p>官方仓库（<a href="https://hub.kubeapps.com/charts/incubator）官方chart仓库，国内有点不好使。" target="_blank" rel="noopener">https://hub.kubeapps.com/charts/incubator）官方chart仓库，国内有点不好使。</a></p></li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">helm repo add stable http:&#x2F;&#x2F;mirror.azure.cn&#x2F;kubernetes&#x2F;charts</span><br><span class="line">helm repo add aliyun https:&#x2F;&#x2F;kubernetes.oss-cn-hangzhou.aliyuncs.com&#x2F;charts </span><br><span class="line">helm repo update</span><br></pre></td></tr></table></figure><h3 id="查看配置的存储库"><a href="#查看配置的存储库" class="headerlink" title="查看配置的存储库"></a>查看配置的存储库</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">helm repo list</span><br><span class="line">helm search repo stable</span><br></pre></td></tr></table></figure><h2 id="案例模拟"><a href="#案例模拟" class="headerlink" title="案例模拟"></a>案例模拟</h2><h3 id="创建Chart"><a href="#创建Chart" class="headerlink" title="创建Chart"></a>创建Chart</h3><p>我们现在了尝试创建一个 Chart：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm create hello-helm</span><br><span class="line">Creating hello-helm</span><br><span class="line">[root@wang-200 test]# tree  hello-helm&#x2F;</span><br><span class="line">hello-helm&#x2F;</span><br><span class="line">├── charts</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── hpa.yaml</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   ├── serviceaccount.yaml</span><br><span class="line">│   ├── service.yaml</span><br><span class="line">│   └── tests</span><br><span class="line">│       └── test-connection.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">3 directories, 10 files</span><br></pre></td></tr></table></figure><p>我们通过查看<code>templates</code>目录下面的<code>deployment.yaml</code>文件可以看出默认创建的 Chart 是一个 nginx 服务，具体的每个文件是干什么用的，我们可以前往 <a href="https://docs.helm.sh/developing_charts/#charts" target="_blank" rel="noopener">Helm 官方文档</a>进行查看，后面会和大家详细讲解的。</p><p>我们来安装 1.7.9 这个版本的 nginx，则我们更改 value.yaml 文件下面的 image tag 即可，将默认的 stable 更改为 1.7.9，为了测试方便，我们把 Service 的类型也改成 NodePort</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">image:</span><br><span class="line">  repository: nginx</span><br><span class="line">  tag: 1.7.9</span><br><span class="line">  pullPolicy: IfNotPresent</span><br><span class="line"></span><br><span class="line">nameOverride: &quot;&quot;</span><br><span class="line">fullnameOverride: &quot;&quot;</span><br><span class="line"></span><br><span class="line">service:</span><br><span class="line">  type: NodePort</span><br><span class="line">  port: 80</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="安装chart"><a href="#安装chart" class="headerlink" title="安装chart"></a>安装chart</h3><p>现在我们来尝试安装下这个 Chart :</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm install hello-helm .&#x2F;hello-helm</span><br><span class="line">NAME: hello-helm</span><br><span class="line">LAST DEPLOYED: Sat Oct 10 14:52:44 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: deployed</span><br><span class="line">REVISION: 1</span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  export NODE_PORT&#x3D;$(kubectl get --namespace default -o jsonpath&#x3D;&quot;&#123;.spec.ports[0].nodePort&#125;&quot; services hello-helm)</span><br><span class="line">  export NODE_IP&#x3D;$(kubectl get nodes --namespace default -o jsonpath&#x3D;&quot;&#123;.items[0].status.addresses[0].address&#125;&quot;)</span><br><span class="line">  echo http:&#x2F;&#x2F;$NODE_IP:$NODE_PORT</span><br></pre></td></tr></table></figure><h3 id="结果展示"><a href="#结果展示" class="headerlink" title="结果展示"></a>结果展示</h3><p>等到 Pod 创建完成后，我们可以根据创建的 Service 的 NodePort 来访问该服务了，然后在浏览器中就可以正常的访问我们刚刚部署的 nginx 应用了。</p><img src="http://wangzhangtao.com/img/body/helm/helm-demo1.png" alt="nginx" style="zoom:80%;" /><h3 id="helm查看，打包和删除"><a href="#helm查看，打包和删除" class="headerlink" title="helm查看，打包和删除"></a>helm查看，打包和删除</h3><p>查看<code>release</code>：</p><img src="http://wangzhangtao.com/img/body/helm/image-20201010145950906.png" alt="image-20201010145950906"  /><p>打包<code>chart</code>：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm package hello-helm</span><br><span class="line">Successfully packaged chart and saved it to: &#x2F;root&#x2F;test&#x2F;hello-helm-0.1.0.tgz</span><br></pre></td></tr></table></figure><p>然后我们就可以将打包的<code>tgz</code>文件分发到任意的服务器上，通过<code>helm fetch</code>就可以获取到该 Chart 了。</p><p>删除<code>release</code>：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm uninstall hello-helm </span><br><span class="line">release &quot;hello-helm&quot; uninstalled</span><br></pre></td></tr></table></figure><p>然后我们看到<code>kubernetes</code>集群上的该 nginx 服务也已经被删除了。</p><h1 id="Helm-客户端常用命令"><a href="#Helm-客户端常用命令" class="headerlink" title="Helm 客户端常用命令"></a>Helm 客户端常用命令</h1><h2 id="helm-命令使用"><a href="#helm-命令使用" class="headerlink" title="helm 命令使用"></a>helm 命令使用</h2><ul><li>helm repo list；查看当前的仓库配置。</li><li>helm search repo mysql；查找服务 chart。</li><li>helm inspect chart stable/mysql；查看一个 chart 的详细信息。</li><li>helm install mydb stable/mysql；安装新的软件包。</li><li>helm ls ；查看先当前的 release。</li><li>helm history mydb；查看 release 的历史版本。</li><li>helm rollback mydb 1；回滚到某一个历史版本。</li><li>helm uninstall mydb； 删除release</li><li>helm package hello-helm； 打包release</li><li>helm pull/fetch`；拉去镜像</li></ul><h2 id="helm-常见的命令"><a href="#helm-常见的命令" class="headerlink" title="helm 常见的命令"></a>helm 常见的命令</h2><table><thead><tr><th><strong>命令</strong></th><th><strong>描述</strong></th></tr></thead><tbody><tr><td>create</td><td>创建一个chart并指定名字</td></tr><tr><td>dependency</td><td>管理chart依赖</td></tr><tr><td>get</td><td>下载一个release。可用子命令：all、hooks、manifest、notes、values</td></tr><tr><td>history</td><td>获取release历史</td></tr><tr><td>install</td><td>安装一个chart</td></tr><tr><td>list</td><td>列出release</td></tr><tr><td>package</td><td>将chart目录打包到chart存档文件中</td></tr><tr><td>pull</td><td>从远程仓库中下载chart并解压到本地  # helm pull stable/mysql –untar</td></tr><tr><td>repo</td><td>添加，列出，移除，更新和索引chart仓库。可用子命令：add、index、list、remove、update</td></tr><tr><td>rollback</td><td>从之前版本回滚</td></tr><tr><td>search</td><td>根据关键字搜索chart。可用子命令：hub、repo</td></tr><tr><td>show</td><td>查看chart详细信息。可用子命令：all、chart、readme、values</td></tr><tr><td>status</td><td>显示已命名版本的状态</td></tr><tr><td>template</td><td>本地呈现模板</td></tr><tr><td>uninstall</td><td>卸载一个release</td></tr><tr><td>upgrade</td><td>更新一个release</td></tr><tr><td>version</td><td>查看helm客户端版本</td></tr></tbody></table><h1 id="Helm-模板之内置函数和Values"><a href="#Helm-模板之内置函数和Values" class="headerlink" title="Helm 模板之内置函数和Values"></a>Helm 模板之内置函数和Values</h1><p>chart 包的目录上节课我们就已经学习过了，这里我们再来仔细看看 templates 目录下面的文件:</p><ul><li>NOTES.txt：chart 的 “帮助文本”。这会在用户运行 helm install 时显示给用户。</li><li>deployment.yaml：创建 Kubernetes deployment 的基本 manifest</li><li>service.yaml：为 deployment 创建 service 的基本 manifest</li><li>ingress.yaml: 创建 ingress 对象的资源清单文件</li><li>_helpers.tpl：放置模板助手的地方，可以在整个 chart 中重复使用</li></ul><p>Helm 为我们提供了<code>--dry-run --debug</code>这个可选参数，进行调试</p><h2 id="内置对象"><a href="#内置对象" class="headerlink" title="内置对象"></a>内置对象</h2><p>下面是一些常用的内置对象：</p><ul><li><strong>Release</strong>：这个对象描述了 release 本身。它里面有几个对象：<ul><li>Release.Name：release 名称</li><li>Release.Time：release 的时间</li><li>Release.Namespace：release 的 namespace（如果清单未覆盖）</li><li>Release.Revision：此 release 的修订版本号，从1开始累加。</li><li>Release.IsUpgrade：如果当前操作是升级或回滚，则将其设置为 true。</li><li>Release.IsInstall：如果当前操作是安装，则设置为 true。</li></ul></li><li><strong>Values</strong>：从<code>values.yaml</code>文件和用户提供的文件传入模板的值。默认情况下，Values 是空的。</li><li>Chart：<code>Chart.yaml</code>文件的内容。所有的 Chart 对象都将从该文件中获取。chart 指南中<a href="https://github.com/kubernetes/helm/blob/master/docs/charts.md#the-chartyaml-file" target="_blank" rel="noopener">Charts Guide</a>列出了可用字段，可以前往查看。</li><li>Files：这提供对 chart 中所有非特殊文件的访问。虽然无法使用它来访问模板，但可以使用它来访问 chart 中的其他文件。请参阅 “访问文件” 部分。<ul><li>Files.Get 是一个按名称获取文件的函数（.Files.Get config.ini）</li><li>Files.GetBytes 是将文件内容作为字节数组而不是字符串获取的函数。这对于像图片这样的东西很有用。</li></ul></li><li>Capabilities：这提供了关于 Kubernetes 集群支持的功能的信息。<ul><li>Capabilities.APIVersions 是一组版本信息。</li><li>Capabilities.APIVersions.Has $version 指示是否在群集上启用版本（batch/v1）。</li><li>Capabilities.KubeVersion 提供了查找 Kubernetes  版本的方法。它具有以下值：Major，Minor，GitVersion，GitCommit，GitTreeState，BuildDate，GoVersion，Compiler，和 Platform。</li><li>Capabilities.TillerVersion 提供了查找 Tiller 版本的方法。它具有以下值：SemVer，GitCommit，和 GitTreeState。</li></ul></li><li>Template：包含有关正在执行的当前模板的信息</li><li>Name：到当前模板的文件路径（例如 mychart/templates/mytemplate.yaml）</li><li>BasePath：当前 chart 模板目录的路径（例如 mychart/templates）。</li></ul><p>上面这些值可用于任何顶级模板，要注意内置值始终以大写字母开头。这也符合<code>Go</code>的命名约定。当你创建自己的名字时，你可以自由地使用适合你的团队的惯例。</p><h2 id="values-文件"><a href="#values-文件" class="headerlink" title="values 文件"></a>values 文件</h2><p>Values 对象的值有4个来源：</p><ul><li>chart 包中的 values.yaml 文件</li><li>父 chart 包的 values.yaml 文件</li><li>通过 helm install 或者 helm upgrade 的<code>-f</code>或者<code>--values</code>参数传入的自定义的 yaml 文件(上节课我们已经学习过)</li><li>通过<code>--set</code> 参数传入的值</li></ul><h2 id="常见函数"><a href="#常见函数" class="headerlink" title="常见函数"></a>常见函数</h2><ul><li>quote: 字符串</li><li>upper: 大写字母</li><li>title: 首字母大写</li><li>default: 设置默认值</li><li>indent: 设置缩进</li><li>eq：等于，除此以外，还有ne、lt、 gt、 and、 or 等运算符</li></ul><h3 id="模板函数"><a href="#模板函数" class="headerlink" title="模板函数"></a>模板函数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123; quote .Values.course.k8s &#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123; .Values.course.k8s | upper | quote &#125;&#125;</span><br><span class="line">&#123;&#123; .Values.course.python | repeat 3 | quote &#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="default-函数"><a href="#default-函数" class="headerlink" title="default 函数"></a>default 函数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123; .Values.hello | default  &quot;Hello World&quot; | quote &#125;&#125;</span><br></pre></td></tr></table></figure><h2 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h2><ul><li>if/else 条件块</li><li>with执行范围</li><li>range 循环</li></ul><h3 id="if-else-条件"><a href="#if-else-条件" class="headerlink" title="if/else 条件"></a>if/else 条件</h3><p><code>if/else</code>块是用于在模板中有条件地包含文本块的方法，条件块的基本结构如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123; if PIPELINE &#125;&#125;</span><br><span class="line">  # Do something</span><br><span class="line">&#123;&#123; else if OTHER PIPELINE &#125;&#125;</span><br><span class="line">  # Do something else</span><br><span class="line">&#123;&#123; else &#125;&#125;</span><br><span class="line">  # Default case</span><br><span class="line">&#123;&#123; end &#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="空格控制"><a href="#空格控制" class="headerlink" title="空格控制"></a>空格控制</h3><p>使用在模板标识  &lt;!–￼28–&gt; 前面添加一个空格和破折号 -}} 表示应该删除右边的空格。另外需要注意的是换行符也是空格！</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123;- if eq .Values.course.python &quot;django&quot; &#125;&#125;</span><br><span class="line">web: true</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="使用-with-修改范围"><a href="#使用-with-修改范围" class="headerlink" title="使用 with 修改范围"></a>使用 with 修改范围</h3><p>关键词<code>with</code>用来控制变量作用域。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123; with PIPELINE &#125;&#125;</span><br><span class="line">  #  restricted scope</span><br><span class="line">  &#123;&#123;- toYaml . | nindent 8 &#125;&#125;</span><br><span class="line">&#123;&#123; end &#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="range-循环"><a href="#range-循环" class="headerlink" title="range 循环"></a>range 循环</h3><p>在<code>values.yaml</code>文件中添加上一个课程列表：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">courselist:</span><br><span class="line">- k8s</span><br><span class="line">- python</span><br><span class="line">- search</span><br><span class="line">- golang</span><br></pre></td></tr></table></figure><p>循环打印出该列表： 点.代表当前循环的值</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">toppings: |-</span><br><span class="line">  &#123;&#123;- range .Values.courselist &#125;&#125;</span><br><span class="line">  - &#123;&#123; . | title | quote &#125;&#125;</span><br><span class="line">  &#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123;- $releaseName :&#x3D; .Release.Name -&#125;&#125;</span><br></pre></td></tr></table></figure><h2 id="声明和使用命名模板"><a href="#声明和使用命名模板" class="headerlink" title="声明和使用命名模板"></a>声明和使用命名模板</h2><p>使用<code>define</code>关键字就可以允许我们在模板文件内部创建一个命名模板，它的语法格式如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123; define &quot;ChartName.TplName&quot; &#125;&#125;</span><br><span class="line"># 模板内容区域</span><br><span class="line">&#123;&#123; end &#125;&#125;</span><br></pre></td></tr></table></figure><p>templates 目录下面默认会生成一个<code>_helpers.tpl</code>文件，就是模板文件。</p><h2 id="模板范围template"><a href="#模板范围template" class="headerlink" title="模板范围template"></a>模板范围template</h2><p>当命名模板被渲染时，它会接收由 template  调用时传入的作用域，要在  template 后面加上作用域范围即可：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123;- template &quot;mychart.labels&quot; . &#125;&#125;</span><br></pre></td></tr></table></figure><h2 id="include-函数"><a href="#include-函数" class="headerlink" title="include 函数"></a>include 函数</h2><p>使用<code>include</code>函数，在需要控制空格的地方使用<code>indent</code>管道函数来自己控制，比如上面的例子我们替换成<code>include</code>函数：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123;- include &quot;mychart.labels&quot; . | nindent 2 &#125;&#125;</span><br></pre></td></tr></table></figure><h2 id="NOTES-txt-文件"><a href="#NOTES-txt-文件" class="headerlink" title="NOTES.txt 文件"></a>NOTES.txt 文件</h2><p>注释信息快速了解我们的 chart 包的使用方法，这些信息就是编写在 NOTES.txt  文件之中的。</p><h2 id="子-chart"><a href="#子-chart" class="headerlink" title="子 chart"></a>子 chart</h2><p>在创建 mychart 包的时候，在根目录下面有一个空文件夹 charts 目录。这就是我们的子 chart 所在的目录，在该目录下面添加一个新的 chart：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# cd mychart&#x2F;charts&#x2F;</span><br><span class="line">[root@wang-200 charts]# helm create mysubchart</span><br><span class="line">Creating mysubchart</span><br><span class="line">[root@wang-200 charts]# helm install mysubchart .&#x2F;mysubchart&#x2F; --dry-run --debug</span><br></pre></td></tr></table></figure><h2 id="值覆盖"><a href="#值覆盖" class="headerlink" title="值覆盖"></a>值覆盖</h2><p>在 <strong>mychart 根目录</strong>中执行调试命令，子 chart 中的值已经被顶层的值给覆盖，可以查看到子 chart 也被一起渲染了。</p><h2 id="全局值"><a href="#全局值" class="headerlink" title="全局值"></a>全局值</h2><p>values 对象中有一个保留的属性是<code>Values.global</code>，就可以被用来设置全局值。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">global:</span><br><span class="line">  allin: helm</span><br></pre></td></tr></table></figure><h2 id="Helm-Hooks"><a href="#Helm-Hooks" class="headerlink" title="Helm Hooks"></a>Helm Hooks</h2><p>和 Kubernetes 里面的容器一样，Helm 也提供了 <a href="https://docs.helm.sh/developing_charts/#hooks" target="_blank" rel="noopener">Hook</a> 的机制，允许 chart 开发人员在 release 的生命周期中的某些节点来进行干预，比如我们可以利用 Hooks 来做下面的这些事情：</p><ul><li>在加载任何其他 chart 之前，在安装过程中加载 ConfigMap 或 Secret</li><li>在安装新 chart 之前执行作业以备份数据库，然后在升级后执行第二个作业以恢复数据</li><li>在删除 release 之前运行作业，以便在删除 release 之前优雅地停止服务</li></ul><h1 id="helm使用harbor做存储仓库"><a href="#helm使用harbor做存储仓库" class="headerlink" title="helm使用harbor做存储仓库"></a>helm使用harbor做存储仓库</h1><h2 id="启用Harbor的Char仓库服务"><a href="#启用Harbor的Char仓库服务" class="headerlink" title="启用Harbor的Char仓库服务"></a>启用Harbor的Char仓库服务</h2><p>安装harbor时，需要添加参数。(harbor版本需1.6及以上) 当看到Helm Charts这一列，代表安装成功了</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;install.sh --with-clair --with-chartmuseum</span><br></pre></td></tr></table></figure><img src="http://wangzhangtao.com/img/body/helm/image-20201012141240610.png" alt="image-20201012141240610" style="zoom:100%;" /><p>clair是CoreOS提供的一款根据CVE的信息确认镜像各层安全状况的开源工具</p><h2 id="helm添加harbor作为存储仓库"><a href="#helm添加harbor作为存储仓库" class="headerlink" title="helm添加harbor作为存储仓库"></a>helm添加harbor作为存储仓库</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# helm repo add --username&#x3D;admin --password&#x3D;Harbor12345 test http:&#x2F;&#x2F;harbor.od.com&#x2F;chartrepo&#x2F;library   </span><br><span class="line">&quot;test&quot; has been added to your repositories</span><br></pre></td></tr></table></figure><h2 id="安装push-插件"><a href="#安装push-插件" class="headerlink" title="安装push 插件"></a>安装push 插件</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# helm plugin install https:&#x2F;&#x2F;github.com&#x2F;chartmuseum&#x2F;helm-push</span><br><span class="line">Downloading and installing helm-push v0.8.1 ...</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;chartmuseum&#x2F;helm-push&#x2F;releases&#x2F;download&#x2F;v0.8.1&#x2F;helm-push_0.8.1_linux_amd64.tar.gz</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>如果网络下载不了，也可以自己下载<a href="https://github.com/chartmuseum/helm-push/releases/download/v0.8.1/helm-push_0.8.1_linux_amd64.tar.gz" target="_blank" rel="noopener">helm-push_0.8.1_linux_amd64.tar.gz</a>：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ tar xf helm-push_0.8.1_linux_amd64.tar.gz </span><br><span class="line">$ mkdir -p &#x2F;root&#x2F;.local&#x2F;share&#x2F;helm&#x2F;plugins&#x2F;helm-push</span><br><span class="line">$ chmod +x bin&#x2F;*</span><br><span class="line">$ mv bin plugin.yaml &#x2F;root&#x2F;.local&#x2F;share&#x2F;helm&#x2F;plugins&#x2F;helm-push</span><br></pre></td></tr></table></figure><h2 id="推送release到harbor仓库"><a href="#推送release到harbor仓库" class="headerlink" title="推送release到harbor仓库"></a>推送release到harbor仓库</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm push mychart test</span><br><span class="line">Pushing mychart-0.1.0.tgz to test...</span><br><span class="line">Done.</span><br></pre></td></tr></table></figure><p>推送成功后，就可以看到如示例图里面的图像了。</p><h1 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h1><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li>阳明的 <a href="https://www.qikqiak.com/k8s-book/docs/42.Helm%E5%AE%89%E8%A3%85.html" target="_blank" rel="noopener">42. Helm安装使用</a> </li><li>李振良的 <a href="https://blog.51cto.com/13812615/2523543" target="_blank" rel="noopener">kubernetes(十七) Helm V3 入门到放弃</a></li><li><a href="https://youendless.com/post/helm_plugin_install/" target="_blank" rel="noopener">Helm插件安装原理详解</a></li><li><a href="https://www.icode9.com/content-4-681793.html" target="_blank" rel="noopener">Helm 从入门到放弃系列</a>  helm安装常用服务</li></ul>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> helm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> helm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>helm的练习案例</title>
      <link href="/2020/10/09/helm%E7%9A%84%E7%BB%83%E4%B9%A0%E6%A1%88%E4%BE%8B/"/>
      <url>/2020/10/09/helm%E7%9A%84%E7%BB%83%E4%B9%A0%E6%A1%88%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文档主要为参考阳明的文档摘抄，并从其他文章参考而来，主要文章链接，请查看结尾</p></blockquote><h1 id="43-Helm-的基本使用"><a href="#43-Helm-的基本使用" class="headerlink" title="43. Helm 的基本使用"></a>43. Helm 的基本使用</h1><h2 id="仓库"><a href="#仓库" class="headerlink" title="仓库"></a>仓库</h2><p>可以用<code>helm repo list</code>来查看当前的仓库配置</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ helm repo list</span><br><span class="line">NAME       URL</span><br><span class="line">stable     https:&#x2F;&#x2F;kubernetes-charts.storage.googleapis.com&#x2F;</span><br><span class="line">local      http:&#x2F;&#x2F;127.0.0.1:8879&#x2F;charts</span><br></pre></td></tr></table></figure><h2 id="查找-chart"><a href="#查找-chart" class="headerlink" title="查找 chart"></a>查找 chart</h2><p>Helm 将 Charts 包安装到 Kubernetes 集群中，一个安装实例就是一个新的 Release，要找到新的 Chart，我们可以通过搜索命令完成。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm search </span><br><span class="line">Usage:</span><br><span class="line">  helm search [command]</span><br><span class="line"></span><br><span class="line">Available Commands:</span><br><span class="line">  hub         search for charts in the Helm Hub or an instance of Monocular</span><br><span class="line">  repo        search repositories for a keyword in charts</span><br></pre></td></tr></table></figure><p>查看repo的stable案例</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm search repo stable</span><br><span class="line">NAME                                    CHART VERSION   APP VERSION             DESCRIPTION                                       </span><br><span class="line">stable&#x2F;acs-engine-autoscaler            2.2.2           2.1.1                   DEPRECATED Scales worker nodes within agent pools </span><br><span class="line">stable&#x2F;aerospike                        0.3.3           v4.5.0.5                A Helm chart for Aerospike in Kubernetes          </span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>如果没有使用过滤条件，helm search 显示所有可用的 charts。可以通过使用过滤条件进行搜索来缩小搜索的结果范围：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm search repo mysql</span><br><span class="line">NAME                                    CHART VERSION   APP VERSION     DESCRIPTION                                       </span><br><span class="line">aliyun&#x2F;mysql                            0.3.5                           Fast, reliable, scalable, and easy to use open-...</span><br><span class="line">stable&#x2F;mysql                            1.6.7           5.7.30          Fast, reliable, scalable, and easy to use open-...</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>使用 inspect 命令来查看一个 chart 的详细信息：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm inspect chart stable&#x2F;mysql</span><br><span class="line">apiVersion: v1</span><br><span class="line">appVersion: 5.7.30</span><br><span class="line">description: Fast, reliable, scalable, and easy to use open-source relational database</span><br><span class="line">  system.</span><br><span class="line">home: https:&#x2F;&#x2F;www.mysql.com&#x2F;</span><br><span class="line">icon: https:&#x2F;&#x2F;www.mysql.com&#x2F;common&#x2F;logos&#x2F;logo-mysql-170x115.png</span><br><span class="line">keywords:</span><br><span class="line">- mysql</span><br><span class="line">- database</span><br><span class="line">- sql</span><br><span class="line">maintainers:</span><br><span class="line">- email: o.with@sportradar.com</span><br><span class="line">  name: olemarkus</span><br><span class="line">- email: viglesias@google.com</span><br><span class="line">  name: viglesiasce</span><br><span class="line">name: mysql</span><br><span class="line">sources:</span><br><span class="line">- https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;charts</span><br><span class="line">- https:&#x2F;&#x2F;github.com&#x2F;docker-library&#x2F;mysql</span><br><span class="line">version: 1.6.7</span><br></pre></td></tr></table></figure><h2 id="安装-chart"><a href="#安装-chart" class="headerlink" title="安装 chart"></a>安装 chart</h2><p>要安装新的软件包，直接使用 helm install 命令即可。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm install mydb stable&#x2F;mysql </span><br><span class="line">NAME: mydb</span><br><span class="line">LAST DEPLOYED: Sat Oct 10 15:33:41 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>在安装过程中，helm 客户端将打印有关创建哪些资源的有用信息，release 的状态以及其他有用的配置信息，比如这里的有访问 mysql 服务的方法、获取 root 用户的密码以及连接 mysql 的方法等信息。</p><p>要跟踪 release 状态或重新读取配置信息，可以使用 helm status 查看：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm status mydb</span><br><span class="line">NAME: mydb</span><br><span class="line">LAST DEPLOYED: Sat Oct 10 15:33:41 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: deployed</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>可以看到当前 release 的状态是<code>DEPLOYED</code>，下面还有一些安装的时候出现的信息。</p><h2 id="自定义-chart"><a href="#自定义-chart" class="headerlink" title="自定义 chart"></a>自定义 chart</h2><p>上面的安装方式是使用 chart 的默认配置选项。但是在很多时候，我们都需要自定义 chart 以满足自身的需求，要自定义 chart，我们就需要知道我们使用的 chart 支持的可配置选项才行。</p><p>要查看 chart 上可配置的选项，使用<code>helm inspect values</code>命令即可，比如我们这里查看上面的 mysql 的配置选项：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm inspect values stable&#x2F;mysql</span><br><span class="line">## mysql image version</span><br><span class="line">## ref: https:&#x2F;&#x2F;hub.docker.com&#x2F;r&#x2F;library&#x2F;mysql&#x2F;tags&#x2F;</span><br><span class="line">##</span><br><span class="line">image: &quot;mysql&quot;</span><br><span class="line">imageTag: &quot;5.7.30&quot;</span><br><span class="line"></span><br><span class="line">strategy:</span><br><span class="line">  type: Recreate</span><br><span class="line"></span><br><span class="line">busybox:</span><br><span class="line">  image: &quot;busybox&quot;</span><br><span class="line">  tag: &quot;1.32&quot;</span><br></pre></td></tr></table></figure><p>然后，我们可以直接在 YAML 格式的文件中来覆盖上面的任何配置，在安装的时候直接使用该配置文件即可：(config.yaml)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysqlUser: haimaxyUser</span><br><span class="line">mysqlDatabase: haimaxyDB</span><br><span class="line">service:</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure><p>我们来安装的时候直接指定该 yaml 文件：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm install mydb stable&#x2F;mysql -f config.yaml </span><br><span class="line">NAME: mydb</span><br><span class="line">LAST DEPLOYED: Sat Oct 10 15:37:58 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>可以看到当前 release 的名字已经变成 mydb 了。然后可以查看下 mydb 关联的 Service 变成 NodePort 类型的了。</p><p>helm 更新的用法，我们这里来直接禁用掉数据持久化，可以在上面的 config.yaml 文件中设置：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">persistence:</span><br><span class="line">  enabled: false</span><br></pre></td></tr></table></figure><p>另外一种方法就是在安装过程中使用<code>--set</code>来覆盖对应的 value 值，比如禁用数据持久化，我们这里可以这样来覆盖：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">helm install mydb stable&#x2F;mysql --set persistence.enabled&#x3D;false</span><br></pre></td></tr></table></figure><h2 id="升级"><a href="#升级" class="headerlink" title="升级"></a>升级</h2><p>我们这里将数据持久化禁用掉来对上面的 mydb 进行升级：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# echo config.yaml</span><br><span class="line">mysqlUser: haimaxyUser</span><br><span class="line">mysqlDatabase: haimaxyDB</span><br><span class="line">service:</span><br><span class="line">  type: NodePort</span><br><span class="line">persistence:</span><br><span class="line">  enabled: false</span><br><span class="line">  </span><br><span class="line">[root@wang-200 test]# helm upgrade mydb stable&#x2F;mysql -f config.yaml </span><br><span class="line">Release &quot;mydb&quot; has been upgraded. Happy Helming!</span><br><span class="line">NAME: mydb</span><br><span class="line">LAST DEPLOYED: Sat Oct 10 15:41:12 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: deployed</span><br></pre></td></tr></table></figure><p>使用 helm ls 命令查看先当前的 release：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm ls</span><br><span class="line">NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION</span><br><span class="line">mydb    default         2               2020-10-10 15:41:12.15738066 +0800 CST  deployed        mysql-1.6.7     5.7.30</span><br></pre></td></tr></table></figure><p>可以看到 mydb 这个 release 的<code>REVISION</code>已经变成<strong>2</strong>了，这是因为 release 的版本是递增的，每次安装、升级或者回滚，版本号都会加<strong>1</strong>，第一个版本号始终为<strong>1</strong>，同样我们可以使用 helm history 命令查看 release 的历史版本：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm history mydb</span><br><span class="line">REVISION        UPDATED                         STATUS          CHART           APP VERSION     DESCRIPTION     </span><br><span class="line">1               Sat Oct 10 15:37:58 2020        superseded      mysql-1.6.7     5.7.30          Install complete</span><br><span class="line">2               Sat Oct 10 15:41:12 2020        deployed        mysql-1.6.7     5.7.30          Upgrade complete</span><br></pre></td></tr></table></figure><p>当然如果我们要回滚到某一个版本的话，使用 helm rollback 命令即可，比如我们将 mydb 回滚到上一个版本：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm rollback mydb 1</span><br><span class="line">Rollback was a success! Happy Helming!</span><br></pre></td></tr></table></figure><p>通过<code>helm history mydb</code> 查看，版本依然在增加。</p><h2 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h2><p>直接使用 helm uninstall 命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm uninstall mydb</span><br><span class="line">release &quot;mydb&quot; uninstalled</span><br></pre></td></tr></table></figure><h1 id="44-Helm-模板之内置函数和Values"><a href="#44-Helm-模板之内置函数和Values" class="headerlink" title="44. Helm 模板之内置函数和Values"></a>44. Helm 模板之内置函数和Values</h1><p>这节课来和大家一起定义一个<code>chart</code>包，了解 Helm 中模板的一些使用方法。</p><h2 id="定义-chart"><a href="#定义-chart" class="headerlink" title="定义 chart"></a>定义 chart</h2><p>一个 chart 包就是一个文件夹的集合，文件夹名称就是 chart 包的名称，比如创建一个 mychart 的 chart 包：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm create mychart</span><br><span class="line">Creating mychart</span><br><span class="line"></span><br><span class="line">[root@wang-200 test]# tree mychart&#x2F;</span><br><span class="line">mychart&#x2F;</span><br><span class="line">├── charts</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── hpa.yaml</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   ├── serviceaccount.yaml</span><br><span class="line">│   ├── service.yaml</span><br><span class="line">│   └── tests</span><br><span class="line">│       └── test-connection.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">3 directories, 10 files</span><br></pre></td></tr></table></figure><p>chart 包的目录上节课我们就已经学习过了，这里我们再来仔细看看 templates 目录下面的文件:</p><ul><li>NOTES.txt：chart 的 “帮助文本”。这会在用户运行 helm install 时显示给用户。</li><li>deployment.yaml：创建 Kubernetes deployment 的基本 manifest</li><li>service.yaml：为 deployment 创建 service 的基本 manifest</li><li>ingress.yaml: 创建 ingress 对象的资源清单文件</li><li>_helpers.tpl：放置模板助手的地方，可以在整个 chart 中重复使用</li></ul><p>这里我们明白每一个文件是干嘛的就行，然后我们把 templates 目录下面所有文件全部删除掉，这里我们自己来创建模板文件：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# rm -rf mychart&#x2F;templates&#x2F;*.*</span><br></pre></td></tr></table></figure><h2 id="创建模板"><a href="#创建模板" class="headerlink" title="创建模板"></a>创建模板</h2><p>这里我们来创建一个非常简单的模板 ConfigMap，在 templates 目录下面新建一个<code>configmap.yaml</code>文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# cat mychart&#x2F;templates&#x2F;configmap.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: mychart-configmap</span><br><span class="line">data:</span><br><span class="line">  myvalue: &quot;Hello World&quot;</span><br></pre></td></tr></table></figure><p>实际上现在我们就有一个可安装的 chart 包了，通过<code>helm install</code>命令来进行安装：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm install cmtest .&#x2F;mychart&#x2F;</span><br><span class="line">NAME: cmtest</span><br><span class="line">LAST DEPLOYED: Sat Oct 10 15:58:21 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: deployed</span><br><span class="line">REVISION: 1</span><br><span class="line">TEST SUITE: None</span><br></pre></td></tr></table></figure><p>通过命令，我们可以看到cm已经创建</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# kubectl get cm mychart-configmap</span><br><span class="line">NAME                DATA   AGE</span><br><span class="line">mychart-configmap   1      51s</span><br></pre></td></tr></table></figure><h2 id="添加一个简单的模板"><a href="#添加一个简单的模板" class="headerlink" title="添加一个简单的模板"></a>添加一个简单的模板</h2><p>上面我们定义的 ConfigMap 的名字是固定的，但往往这并不是一种很好的做法，我们可以通过插入 release  的名称来生成资源的名称，比如这里 ConfigMap 的名称我们希望是：ringed-lynx-configmap，这就需要用到 Chart  的模板定义方法了。</p><p>Helm Chart 模板使用的是<a href="https://golang.org/pkg/text/template/" target="_blank" rel="noopener"><code>Go</code>语言模板</a>编写而成，并添加了<a href="https://github.com/Masterminds/sprig" target="_blank" rel="noopener"><code>Sprig</code>库</a>中的50多个附件模板函数以及一些其他<a href="https://github.com/kubernetes/helm/blob/master/docs/charts_tips_and_tricks.md" target="_blank" rel="noopener">特殊的函</a>。</p><blockquote><p>需要注意的是<code>kubernetes</code>资源对象的 labels 和 name 定义被<a href="http://kubernetes.io/docs/user-guide/labels/#syntax-and-character-set" target="_blank" rel="noopener">限制 63个字符</a>，所以需要注意名称的定义。</p></blockquote><p>现在我们来重新定义下上面的 configmap.yaml 文件：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# cat mychart&#x2F;templates&#x2F;configmap.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: &#123;&#123; .Release.Name &#125;&#125;-configmap</span><br><span class="line">data:</span><br><span class="line">  myvalue: &quot;Hello World&quot;</span><br><span class="line">  </span><br><span class="line">[root@wang-200 test]# helm install cmtest .&#x2F;mychart</span><br></pre></td></tr></table></figure><p>现在我们来重新安装我们的 Chart 包，注意观察 ConfigMap 资源对象的名称：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# kubectl get cm | grep configmap</span><br><span class="line">cmtest-configmap   1      27s</span><br></pre></td></tr></table></figure><p>可以看到现在生成的名称变成了cmtest-configmap.</p><h2 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h2><p>Helm 为我们提供了<code>--dry-run --debug</code>这个可选参数，进行调试</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# helm install cmtest .&#x2F;mychart --dry-run --debug </span><br><span class="line">install.go:172: [debug] Original chart version: &quot;&quot;</span><br><span class="line">install.go:189: [debug] CHART PATH: &#x2F;root&#x2F;test&#x2F;mychart</span><br><span class="line"></span><br><span class="line">NAME: cmtest</span><br><span class="line">LAST DEPLOYED: Sat Oct 10 16:05:54 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: pending-install</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">HOOKS:</span><br><span class="line">MANIFEST:</span><br><span class="line">---</span><br><span class="line"># Source: mychart&#x2F;templates&#x2F;configmap.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: cmtest-configmap</span><br><span class="line">data:</span><br><span class="line">  myvalue: &quot;Hello World&quot;</span><br></pre></td></tr></table></figure><p>但是要注意的是这不能确保 Kubernetes 本身就一定会接受生成的模板，在调试完成后，还是需要去安装一个实际的 release 实例来进行验证的。</p><h2 id="内置对象"><a href="#内置对象" class="headerlink" title="内置对象"></a>内置对象</h2><p>下面是一些常用的内置对象：</p><ul><li><strong>Release</strong>：这个对象描述了 release 本身。它里面有几个对象：<ul><li>Release.Name：release 名称</li><li>Release.Time：release 的时间</li><li>Release.Namespace：release 的 namespace（如果清单未覆盖）</li><li>Release.Revision：此 release 的修订版本号，从1开始累加。</li><li>Release.IsUpgrade：如果当前操作是升级或回滚，则将其设置为 true。</li><li>Release.IsInstall：如果当前操作是安装，则设置为 true。</li></ul></li><li><strong>Values</strong>：从<code>values.yaml</code>文件和用户提供的文件传入模板的值。默认情况下，Values 是空的。</li><li>Chart：<code>Chart.yaml</code>文件的内容。所有的 Chart 对象都将从该文件中获取。chart 指南中<a href="https://github.com/kubernetes/helm/blob/master/docs/charts.md#the-chartyaml-file" target="_blank" rel="noopener">Charts Guide</a>列出了可用字段，可以前往查看。</li><li>Files：这提供对 chart 中所有非特殊文件的访问。虽然无法使用它来访问模板，但可以使用它来访问 chart 中的其他文件。请参阅 “访问文件” 部分。<ul><li>Files.Get 是一个按名称获取文件的函数（.Files.Get config.ini）</li><li>Files.GetBytes 是将文件内容作为字节数组而不是字符串获取的函数。这对于像图片这样的东西很有用。</li></ul></li><li>Capabilities：这提供了关于 Kubernetes 集群支持的功能的信息。<ul><li>Capabilities.APIVersions 是一组版本信息。</li><li>Capabilities.APIVersions.Has $version 指示是否在群集上启用版本（batch/v1）。</li><li>Capabilities.KubeVersion 提供了查找 Kubernetes  版本的方法。它具有以下值：Major，Minor，GitVersion，GitCommit，GitTreeState，BuildDate，GoVersion，Compiler，和 Platform。</li><li>Capabilities.TillerVersion 提供了查找 Tiller 版本的方法。它具有以下值：SemVer，GitCommit，和 GitTreeState。</li></ul></li><li>Template：包含有关正在执行的当前模板的信息</li><li>Name：到当前模板的文件路径（例如 mychart/templates/mytemplate.yaml）</li><li>BasePath：当前 chart 模板目录的路径（例如 mychart/templates）。</li></ul><p>上面这些值可用于任何顶级模板，要注意内置值始终以大写字母开头。这也符合<code>Go</code>的命名约定。当你创建自己的名字时，你可以自由地使用适合你的团队的惯例。</p><h2 id="values-文件"><a href="#values-文件" class="headerlink" title="values 文件"></a>values 文件</h2><p>Values 对象的值有4个来源：</p><ul><li>chart 包中的 values.yaml 文件</li><li>父 chart 包的 values.yaml 文件</li><li>通过 helm install 或者 helm upgrade 的<code>-f</code>或者<code>--values</code>参数传入的自定义的 yaml 文件(上节课我们已经学习过)</li><li>通过<code>--set</code> 参数传入的值</li></ul><p>chart 的 values.yaml 提供的值可以被用户提供的 values 文件覆盖，而该文件同样可以被<code>--set</code>提供的参数所覆盖。</p><p>比如（values.yaml）：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">course:</span><br><span class="line">  k8s: devops</span><br></pre></td></tr></table></figure><p>使用方式(configmap.yaml)：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data:</span><br><span class="line">  myvalue: &quot;Hello World&quot;</span><br><span class="line">  course: &#123;&#123; .Values.course.k8s &#125;&#125;</span><br></pre></td></tr></table></figure><p>建议保持 value 树浅一些，平一些，这样维护起来要简单一点。</p><h1 id="45-Helm-模板之模板函数与管道"><a href="#45-Helm-模板之模板函数与管道" class="headerlink" title="45. Helm 模板之模板函数与管道"></a>45. Helm 模板之模板函数与管道</h1><p>上节课我们学习了如何将信息渲染到模板之中，但是这些信息都是直接传入模板引擎中进行渲染的，有的时候我们想要转换一下这些数据才进行渲染，这就需要使用到 Go 模板语言中的一些其他用法。</p><p>查看生成的结果</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">helm install cmtest . --dry-run --debug</span><br></pre></td></tr></table></figure><h2 id="模板函数"><a href="#模板函数" class="headerlink" title="模板函数"></a>模板函数</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 mychart]# cat values.yaml </span><br><span class="line">course:</span><br><span class="line">  k8s: devops</span><br><span class="line">  python: django</span><br></pre></td></tr></table></figure><p>从<code>.Values</code>中读取的值变成字符串的时候就可以通过调用<code>quote</code>模板函数来实现(templates/configmap.yaml)：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: &#123;&#123; .Release.Name &#125;&#125;-configmap</span><br><span class="line">data:</span><br><span class="line">  myvalue: &quot;Hello World&quot;</span><br><span class="line">  k8s: &#123;&#123; quote .Values.course.k8s &#125;&#125;</span><br><span class="line">  python: &#123;&#123; .Values.course.python &#125;&#125;</span><br></pre></td></tr></table></figure><p>使用<code>quote</code>前后结构对比，结果加上了双引号””：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 使用前</span><br><span class="line">data:</span><br><span class="line">  myvalue: &quot;Hello World&quot;</span><br><span class="line">  k8s: devops</span><br><span class="line">  python: django</span><br><span class="line"></span><br><span class="line"># 使用后</span><br><span class="line">data:</span><br><span class="line">  myvalue: &quot;Hello World&quot;</span><br><span class="line">  k8s: &quot;devops&quot;</span><br><span class="line">  python: django</span><br></pre></td></tr></table></figure><p>使用的<code>quote</code>函数就是<code>Sprig 模板库</code>提供的一种字符串函数，用途就是用双引号将字符串括起来，如果需要双引号<code>&quot;</code>，则需要添加<code>\</code>来进行转义，而<code>squote</code>函数的用途则是用双引号将字符串括起来，而不会对内容进行转义。</p><h2 id="管道"><a href="#管道" class="headerlink" title="管道"></a>管道</h2><p>和<code>UNIX</code>中一样，管道我们通常称为<code>Pipeline</code>，是一个链在一起的一系列模板命令的工具，以紧凑地表达一系列转换。简单来说，管道是可以按顺序完成一系列事情的一种方法。比如我们用管道来重写上面的 ConfigMap 模板（templates/configmap.yaml）：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">k8s: &#123;&#123; .Values.course.k8s | quote &#125;&#125;</span><br></pre></td></tr></table></figure><p>k8s 的 value 值被渲染后是大写的字符串，则我们就可以使用管道来修改：（templates/configmap.yaml）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">k8s: &#123;&#123; .Values.course.k8s | upper | quote &#125;&#125;</span><br></pre></td></tr></table></figure><p>这里我们在管道中增加了一个<code>upper</code>函数，该函数同样是<a href="https://godoc.org/github.com/Masterminds/sprig" target="_blank" rel="noopener"><code>Sprig 模板库</code></a>提供的，表示将字符串每一个字母都变成大写。然后我们用<code>debug</code>模式来查看下上面的模板最终渲染的结果。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data:</span><br><span class="line">  k8s: &quot;DEVOPS&quot;</span><br></pre></td></tr></table></figure><p>使用管道操作的时候，前面的操作结果会作为参数传递给后面的模板函数，比如我们这里希望将上面模板中的 python 的值渲染为重复出现3次的字符串，则我们就可以使用到<a href="https://godoc.org/github.com/Masterminds/sprig" target="_blank" rel="noopener"><code>Sprig 模板库</code></a>提供的<code>repeat</code>函数，不过该函数需要传入一个参数<code>repeat COUNT STRING</code>表示重复的次数：（templates/configmap.yaml）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python: &#123;&#123; .Values.course.python | repeat 3 | quote &#125;&#125;</span><br></pre></td></tr></table></figure><p>输出结果为</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data:</span><br><span class="line">  python: &quot;djangodjangodjango&quot;</span><br></pre></td></tr></table></figure><h2 id="default-函数"><a href="#default-函数" class="headerlink" title="default 函数"></a>default 函数</h2><p><code>default 函数</code>：<code>default DEFAULT_VALUE GIVEN_VALUE</code>。该函数允许我们在模板内部指定默认值，以防止该值被忽略掉了。比如我们来修改上面的 ConfigMap 的模板：（templates/configmap.yaml）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">myvalue: &#123;&#123; .Values.hello | default  &quot;Hello World&quot; | quote &#125;&#125;</span><br></pre></td></tr></table></figure><p>由于我们的<code>values.yaml</code>文件中只定义了 course 结构的信息，并没有定义 hello 的值，所以使用默认值：<code>Hello World</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data:</span><br><span class="line">  myvalue: &quot;Hello World&quot;</span><br></pre></td></tr></table></figure><h1 id="46-Helm-模板之控制流程"><a href="#46-Helm-模板之控制流程" class="headerlink" title="46. Helm 模板之控制流程"></a>46. Helm 模板之控制流程</h1><p>控制流程为我们提供了控制模板生成流程的一种能力，Helm 的模板语言提供了以下几种流程控制：</p><ul><li><code>if/else</code> 条件块</li><li><code>with</code> 指定范围</li><li><code>range</code> 循环块</li></ul><p>除此之外，它还提供了一些声明和使用命名模板段的操作：</p><ul><li><code>define</code>在模板中声明一个新的命名模板</li><li><code>template</code>导入一个命名模板</li><li><code>block</code>声明了一种特殊的可填写的模板区域</li></ul><h2 id="if-else-条件"><a href="#if-else-条件" class="headerlink" title="if/else 条件"></a>if/else 条件</h2><p><code>if/else</code>块是用于在模板中有条件地包含文本块的方法，条件块的基本结构如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123; if PIPELINE &#125;&#125;</span><br><span class="line">  # Do something</span><br><span class="line">&#123;&#123; else if OTHER PIPELINE &#125;&#125;</span><br><span class="line">  # Do something else</span><br><span class="line">&#123;&#123; else &#125;&#125;</span><br><span class="line">  # Default case</span><br><span class="line">&#123;&#123; end &#125;&#125;</span><br></pre></td></tr></table></figure><p>当然要使用条件块就得判断条件是否为真，如果值为下面的几种情况，则管道的结果为 false：</p><ul><li>一个布尔类型的<code>假</code></li><li>一个数字<code>零</code></li><li>一个<code>空</code>的字符串</li><li>一个<code>nil</code>（空或<code>null</code>）</li><li>一个空的集合（<code>map</code>、<code>slice</code>、<code>tuple</code>、<code>dict</code>、<code>array</code>）</li></ul><p>除了上面的这些情况外，其他所有条件都为<code>真</code>。</p><p>还是以上面的 ConfigMap 模板文件为例，添加一个简单的条件判断，如果 python 被设置为 django，则添加一个<code>web: true</code>：（tempaltes/configmap.yaml）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123; if eq .Values.course.python &quot;django&quot; &#125;&#125; web: true &#123;&#123; end &#125;&#125;</span><br></pre></td></tr></table></figure><p>其中运算符<code>eq</code>是判断是否相等的操作，除此之外，还有<code>ne</code>、<code>lt</code>、<code>gt</code>、<code>and</code>、<code>or</code>等运算符都是 Helm 模板已经实现了的，直接使用即可。</p><h2 id="空格控制"><a href="#空格控制" class="headerlink" title="空格控制"></a>空格控制</h2><p>如果是一下格式，则会有空行</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123; if eq .Values.course.python &quot;django&quot; &#125;&#125;</span><br><span class="line">web: true</span><br><span class="line">&#123;&#123; end &#125;&#125;</span><br></pre></td></tr></table></figure><p>结果示例：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data:</span><br><span class="line">  myvalue: &quot;Hello World&quot;</span><br><span class="line">  k8s: devops</span><br><span class="line">  python: django</span><br><span class="line">  </span><br><span class="line">  web: true</span><br></pre></td></tr></table></figure><p>我们可以看到渲染出来会有多余的空行，这是因为当模板引擎运行时，它将一些值渲染过后，之前的指令被删除，但它之前所占的位置完全按原样保留剩余的空白了，所以就出现了多余的空行。<code>YAML</code>文件中的空格是非常严格的，所以对于空格的管理非常重要，一不小心就会导致你的<code>YAML</code>文件格式错误。</p><p>我们可以通过使用在模板标识 { 后面添加破折号和空格 前面添加一个空格和破折号 -}} 表示应该删除右边的空格，另外需要注意的是<strong>换行符也是空格！</strong></p><p>使用这个语法，我们来修改我们上面的模板文件去掉多余的空格：（templates/configmap.yaml）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123;- if eq .Values.course.python &quot;django&quot; &#125;&#125;</span><br><span class="line">web: true</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><h2 id="使用-with-修改范围"><a href="#使用-with-修改范围" class="headerlink" title="使用 with 修改范围"></a>使用 with 修改范围</h2><p>关键词<code>with</code>用来控制变量作用域。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123; with PIPELINE &#125;&#125;</span><br><span class="line">  #  restricted scope</span><br><span class="line">&#123;&#123; end &#125;&#125;</span><br></pre></td></tr></table></figure><p><code>with</code>语句可以允许将当前范围<code>.</code>设置为特定的对象，比如我们前面一直使用的<code>.Values.course</code>，我们可以使用<code>with</code>来将<code>.</code>范围指向<code>.Values.course</code>：(templates/configmap.yaml)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data:</span><br><span class="line">  myvalue: &#123;&#123; .Values.hello | default  &quot;Hello World&quot; | quote &#125;&#125;</span><br><span class="line">  &#123;&#123;- with .Values.course &#125;&#125;</span><br><span class="line">  k8s: &#123;&#123; .k8s | upper | quote &#125;&#125;</span><br><span class="line">  python: &#123;&#123; .python | repeat 3 | quote &#125;&#125;</span><br><span class="line">  &#123;&#123;- if eq .python &quot;django&quot; &#125;&#125;</span><br><span class="line">  web: true</span><br><span class="line">  &#123;&#123;- end &#125;&#125;</span><br><span class="line">  &#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><p>可以看到上面我们增加了一个<code>xxx</code>的一个块，这样的话我们就可以在当前的块里面直接引用<code>.python</code>和<code>.k8s</code>了，而不需要进行限定了，这是因为该<code>with</code>声明将<code>.</code>指向了<code>.Values.course</code>，在<code></code>后<code>.</code>就会复原其之前的作用范围了，我们可以使用模板引擎来渲染上面的模板查看是否符合预期结果。</p><p>不过需要注意的是在<code>with</code>声明的范围内，此时将无法从父范围访问到其他对象了，比如下面的模板渲染的时候将会报错，因为显然<code>.Release</code>根本就不在当前的<code>.</code>范围内，当然如果我们最后两行交换下位置就正常了，因为<code></code>之后范围就被重置了：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123;- with .Values.course &#125;&#125;</span><br><span class="line">k8s: &#123;&#123; .k8s | upper | quote &#125;&#125;</span><br><span class="line">python: &#123;&#123; .python | repeat 3 | quote &#125;&#125;</span><br><span class="line">release: &#123;&#123; .Release.Name &#125;&#125;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><h2 id="range-循环"><a href="#range-循环" class="headerlink" title="range 循环"></a>range 循环</h2><p>在 Helm 模板语言中，是使用<code>range</code>关键字来进行循环操作。</p><p>我们在<code>values.yaml</code>文件中添加上一个课程列表：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">course:</span><br><span class="line">  k8s: devops</span><br><span class="line">  python: django</span><br><span class="line">courselist:</span><br><span class="line">- k8s</span><br><span class="line">- python</span><br><span class="line">- search</span><br><span class="line">- golang</span><br></pre></td></tr></table></figure><p>修改 ConfigMap 模板文件来循环打印出该列表：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">toppings: |-</span><br><span class="line">  &#123;&#123;- range .Values.courselist &#125;&#125;</span><br><span class="line">  - &#123;&#123; . | title | quote &#125;&#125;</span><br><span class="line">  &#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><p>可以看到最下面我们使用了一个<code>range</code>函数，该函数将会遍历 &lt;!–￼82–&gt;  列表，循环内部我们使用的是一个<code>.</code>，这是因为当前的作用域就在当前循环内，这个<code>.</code>从列表的第一个元素一直遍历到最后一个元素，然后在遍历过程中使用了<code>title</code>和<code>quote</code>这两个函数，前面这个函数是将字符串首字母变成大写，后面就是加上双引号变成字符串，所以按照上面这个模板被渲染过后的结果为：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">toppings: |-</span><br><span class="line">  - &quot;Mushrooms&quot;</span><br><span class="line">  - &quot;Cheese&quot;</span><br><span class="line">  - &quot;Peppers&quot;</span><br><span class="line">  - &quot;Onions&quot;</span><br></pre></td></tr></table></figure><p>我们可以看到<code>courselist</code>按照我们的要求循环出来了。除了 list 或者 tuple，range 还可以用于遍历具有键和值的集合（如map 或 dict），这个就需要用到变量的概念了。</p><h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p>在 Helm 模板中，使用变量的场合不是特别多，但是在合适的时候使用变量可以很好的解决我们的问题。如下面的模板：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123;- with .Values.course &#125;&#125;</span><br><span class="line">k8s: &#123;&#123; .k8s | upper | quote &#125;&#125;</span><br><span class="line">python: &#123;&#123; .python | repeat 3 | quote &#125;&#125;</span><br><span class="line">release: &#123;&#123; .Release.Name &#125;&#125;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><p>我们在<code>with</code>语句块内添加了一个<code>.Release.Name</code>对象，但这个模板是错误的，编译的时候会失败，这是因为<code>.Release.Name</code>不在该<code>with</code>语句块限制的作用范围之内，我们可以将该对象赋值给一个变量可以来解决这个问题：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data:</span><br><span class="line">  &#123;&#123;- $releaseName :&#x3D; .Release.Name -&#125;&#125;</span><br><span class="line">  &#123;&#123;- with .Values.course &#125;&#125;</span><br><span class="line">  k8s: &#123;&#123; .k8s | upper | quote &#125;&#125;</span><br><span class="line">  python: &#123;&#123; .python | repeat 3 | quote &#125;&#125;</span><br><span class="line">  release: &#123;&#123; $releaseName &#125;&#125;</span><br><span class="line">  &#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><p>我们可以看到我们在<code>with</code>语句上面增加了一句 &lt;!–￼83–&gt; ，其中<code>$releaseName</code>就是后面的对象的一个引用变量，它的形式就是<code>$name</code>，赋值操作使用<code>:=</code>，这样<code>with</code>语句块内部的<code>$releaseName</code>变量仍然指向的是<code>.Release.Name</code>。</p><p>另外变量在<code>range</code>循环中也非常有用，我们可以在循环中用变量来同时捕获索引的值：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">toppings: |-</span><br><span class="line">  &#123;&#123;- range $index, $course :&#x3D; .Values.courselist &#125;&#125;</span><br><span class="line">  - &#123;&#123; $index &#125;&#125;: &#123;&#123; $course | title | quote &#125;&#125;</span><br><span class="line">  &#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><p>显示结果</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data:</span><br><span class="line">  k8s: &quot;DEVOPS&quot;</span><br><span class="line">  python: &quot;djangodjangodjango&quot;</span><br><span class="line">  release: cmtest</span><br><span class="line">  toppings: |-</span><br><span class="line">    - 0: &quot;K8s&quot;</span><br><span class="line">    - 1: &quot;Python&quot;</span><br><span class="line">    - 2: &quot;Search&quot;</span><br><span class="line">    - 3: &quot;Golang&quot;</span><br></pre></td></tr></table></figure><h1 id="47-Helm模板之命名模板"><a href="#47-Helm模板之命名模板" class="headerlink" title="47. Helm模板之命名模板"></a>47. Helm模板之命名模板</h1><p>命名模板我们也可以称为子模板，是限定在一个文件内部的模板，然后给一个名称。在使用命名模板的时候有一个需要特别注意的是：<strong>模板名称是全局的</strong>，如果我们声明了两个相同名称的模板，最后加载的一个模板会覆盖掉另外的模板，由于子 chart 中的模板也是和顶层的模板一起编译的，所以在命名的时候一定要注意，不要重名了。</p><p>为了避免重名，有个通用的约定就是为每个定义的模板添加上 chart 名称：&lt;!–￼84–&gt; ，<code>define</code>关键字就是用来声明命名模板的，加上 chart 名称就可以避免不同 chart 间的模板出现冲突的情况。</p><h2 id="声明和使用命名模板"><a href="#声明和使用命名模板" class="headerlink" title="声明和使用命名模板"></a>声明和使用命名模板</h2><p>使用<code>define</code>关键字就可以允许我们在模板文件内部创建一个命名模板，它的语法格式如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123; define &quot;ChartName.TplName&quot; &#125;&#125;</span><br><span class="line"># 模板内容区域</span><br><span class="line">&#123;&#123; end &#125;&#125;</span><br></pre></td></tr></table></figure><p>然后我们可以将该模板嵌入到现有的 ConfigMap 中，然后使用<code>template</code>关键字在需要的地方包含进来即可：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123;- define &quot;mychart.labels&quot; &#125;&#125;</span><br><span class="line">  labels:</span><br><span class="line">    from: helm</span><br><span class="line">    date: &#123;&#123; now | htmlDate &#125;&#125;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: &#123;&#123; .Release.Name &#125;&#125;-configmap</span><br><span class="line">  &#123;&#123;- template &quot;mychart.labels&quot; &#125;&#125;</span><br><span class="line">data:</span><br><span class="line">  &#123;&#123;- range $key, $value :&#x3D; .Values.course &#125;&#125;</span><br><span class="line">  &#123;&#123; $key &#125;&#125;: &#123;&#123; $value | quote &#125;&#125;</span><br><span class="line">  &#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><p>我们可以看到<code>define</code>区域定义的命名模板被嵌入到了<code>template</code>所在的区域，但是如果我们将命名模板全都写入到一个模板文件中的话无疑也会增大模板的复杂性。</p><p>还记得我们在创建 chart 包的时候，templates 目录下面默认会生成一个<code>_helpers.tpl</code>文件吗？我们前面也提到过 templates 目录下面除了<code>NOTES.txt</code>文件和以下划线<code>_</code>开头命令的文件之外，都会被当做 kubernetes 的资源清单文件，而这个下划线开头的文件不会被当做资源清单外，还可以被其他 chart 模板中调用，这个就是 Helm 中的<code>partials</code>文件，所以其实我们完全就可以将命名模板定义在这些<code>partials</code>文件中，默认就是<code>_helpers.tpl</code>文件了。</p><p>现在我们将上面定义的命名模板移动到 templates/_helpers.tpl 文件中去：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123;&#x2F;* 生成基本的 labels 标签 *&#x2F;&#125;&#125;</span><br><span class="line">&#123;&#123;- define &quot;mychart.labels&quot; &#125;&#125;</span><br><span class="line">  labels:</span><br><span class="line">    from: helm</span><br><span class="line">    date: &#123;&#123; now | htmlDate &#125;&#125;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><p>一般情况下面，我们都会在命名模板头部加一个简单的文档块，用<code>/**/</code>包裹起来，用来描述我们这个命名模板的用途的。</p><p>现在我们讲命名模板从模板文件 templates/configmap.yaml 中移除，当然还是需要保留 template 来嵌入命名模板内容，名称还是之前的 mychart.lables，这是因为<strong>模板名称是全局</strong>的，所以我们可以能够直接获取到。</p><h2 id="模板范围"><a href="#模板范围" class="headerlink" title="模板范围"></a>模板范围</h2><p>上面我们定义的命名模板中，没有使用任何对象，只是使用了一个简单的函数，如果我们在里面来使用 chart 对象相关信息呢：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123;&#x2F;* 生成基本的 labels 标签 *&#x2F;&#125;&#125;</span><br><span class="line">&#123;&#123;- define &quot;mychart.labels&quot; &#125;&#125;</span><br><span class="line">  labels:</span><br><span class="line">    from: helm</span><br><span class="line">    date: &#123;&#123; now | htmlDate &#125;&#125;</span><br><span class="line">    chart: &#123;&#123; .Chart.Name &#125;&#125;</span><br><span class="line">    version: &#123;&#123; .Chart.Version &#125;&#125;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><p>如果这样的直接进行渲染测试的话，是不会得到我们的预期结果的,直接报错：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Error: unable to build kubernetes objects from release manifest: error validating &quot;&quot;: error validating data: [unknown object type &quot;nil&quot; in ConfigMap.metadata.labels.chart, unknown object type &quot;nil&quot; in ConfigMap.metadata.labels.version]</span><br></pre></td></tr></table></figure><p><code>ConfigMap.metadata.labels.chart</code> 的值不对。 （正好是我们yaml文件的层级结构）</p><p>chart 的名称和版本都没有正确被渲染，这是因为他们不在我们定义的模板范围内，当命名模板被渲染时，它会接收由 template  调用时传入的作用域，由于我们这里并没有传入对应的作用域，因此模板中我们无法调用到 .Chart 对象，要解决也非常简单，我们只需要在  template 后面加上作用域范围即可：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: &#123;&#123; .Release.Name &#125;&#125;-configmap</span><br><span class="line">  &#123;&#123;- template &quot;mychart.labels&quot; . &#125;&#125;</span><br><span class="line">data:</span><br><span class="line">  &#123;&#123;- range $key, $value :&#x3D; .Values.course &#125;&#125;</span><br><span class="line">  &#123;&#123; $key &#125;&#125;: &#123;&#123; $value | quote &#125;&#125;</span><br><span class="line">  &#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><p>我们在 template 末尾传递了<code>.</code>，表示当前的最顶层的作用范围，如果我们想要在命名模板中使用<code>.Values</code>范围内的数据，当然也是可以的。</p><h2 id="include-函数"><a href="#include-函数" class="headerlink" title="include 函数"></a>include 函数</h2><p>假如现在我们将上面的定义的 labels 单独提取出来放置到 _helpers.tpl 文件中：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123;&#x2F;* 生成基本的 labels 标签 *&#x2F;&#125;&#125;</span><br><span class="line">&#123;&#123;- define &quot;mychart.labels&quot; &#125;&#125;</span><br><span class="line">from: helm</span><br><span class="line">date: &#123;&#123; now | htmlDate &#125;&#125;</span><br><span class="line">chart: &#123;&#123; .Chart.Name &#125;&#125;</span><br><span class="line">version: &#123;&#123; .Chart.Version &#125;&#125;</span><br><span class="line">&#123;&#123;- end &#125;&#125;</span><br></pre></td></tr></table></figure><p>现在我们将该命名模板插入到 configmap 模板文件的 labels 部分和 data 部分：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: &#123;&#123; .Release.Name &#125;&#125;-configmap</span><br><span class="line">  labels:</span><br><span class="line">    &#123;&#123;- template &quot;mychart.labels&quot; . &#125;&#125;</span><br><span class="line">data:</span><br><span class="line">  &#123;&#123;- range $key, $value :&#x3D; .Values.course &#125;&#125;</span><br><span class="line">  &#123;&#123; $key &#125;&#125;: &#123;&#123; $value | quote &#125;&#125;</span><br><span class="line">  &#123;&#123;- end &#125;&#125;</span><br><span class="line">  &#123;&#123;- template &quot;mychart.labels&quot; . &#125;&#125;</span><br></pre></td></tr></table></figure><p>我们可以看到渲染结果是有问题的，不是一个正常的 YAML 文件格式，这是因为<code>template</code>只是表示一个嵌入动作而已，不是一个函数，所以原本命名模板中是怎样的格式就是怎样的格式被嵌入进来了，比如我们可以在命名模板中给内容区域都空了两个空格，再来查看下渲染的结构：</p><p>为了解决这个问题，Helm 提供了另外一个方案来代替<code>template</code>，那就是使用<code>include</code>函数，在需要控制空格的地方使用<code>indent</code>管道函数来自己控制，比如上面的例子我们替换成<code>include</code>函数：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: &#123;&#123; .Release.Name &#125;&#125;-configmap</span><br><span class="line">  labels:</span><br><span class="line">&#123;&#123;- include &quot;mychart.labels&quot; . | indent 4 &#125;&#125;</span><br><span class="line">data:</span><br><span class="line">  &#123;&#123;- range $key, $value :&#x3D; .Values.course &#125;&#125;</span><br><span class="line">  &#123;&#123; $key &#125;&#125;: &#123;&#123; $value | quote &#125;&#125;</span><br><span class="line">  &#123;&#123;- end &#125;&#125;</span><br><span class="line">&#123;&#123;- include &quot;mychart.labels&quot; . | indent 2 &#125;&#125;</span><br></pre></td></tr></table></figure><p>可以看到是符合我们的预期，所以在 Helm 模板中我们使用 include 函数要比 template 更好，可以更好地处理 YAML 文件输出格式。</p><h1 id="48-Helm模板之其他注意事项"><a href="#48-Helm模板之其他注意事项" class="headerlink" title="48. Helm模板之其他注意事项"></a>48. Helm模板之其他注意事项</h1><h2 id="NOTES-txt-文件"><a href="#NOTES-txt-文件" class="headerlink" title="NOTES.txt 文件"></a>NOTES.txt 文件</h2><p>我们前面在使用 helm install 命令的时候，Helm 都会为我们打印出一大堆介绍信息，这样当别的用户在使用我们的 chart  包的时候就可以根据这些注释信息快速了解我们的 chart 包的使用方法，这些信息就是编写在 NOTES.txt  文件之中的，这个文件是纯文本的，但是它和其他模板一样，具有所有可用的普通模板函数和对象。</p><p>现在我们在前面的示例中 templates 目录下面创建一个 NOTES.txt 文件：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Thank you for installing &#123;&#123; .Chart.Name &#125;&#125;.</span><br><span class="line"></span><br><span class="line">Your release is named &#123;&#123; .Release.Name &#125;&#125;.</span><br><span class="line"></span><br><span class="line">To learn more about the release, try:</span><br><span class="line"></span><br><span class="line">  $ helm status &#123;&#123; .Release.Name &#125;&#125;</span><br><span class="line">  $ helm get &#123;&#123; .Release.Name &#125;&#125;</span><br></pre></td></tr></table></figure><p>我们可以看到我们在 NOTES.txt 文件中也使用 Chart 和 Release 对象，现在我们在 mychart 包根目录下面执行安装命令查看是否能够得到上面的注释信息：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NOTES:</span><br><span class="line">Thank you for installing mychart.</span><br><span class="line"></span><br><span class="line">Your release is named cmtest.</span><br><span class="line"></span><br><span class="line">To learn more about the release, try:</span><br><span class="line"></span><br><span class="line">  $ helm status cmtest</span><br><span class="line">  $ helm get cmtest</span><br></pre></td></tr></table></figure><h2 id="子-chart-包"><a href="#子-chart-包" class="headerlink" title="子 chart 包"></a>子 chart 包</h2><p>我们到目前为止都只用了一个 chart，但是 chart 也可以有 子 chart 的依赖关系，它们也有自己的值和模板，在学习字 chart 之前，我们需要了解几点关于子 chart 的说明：</p><ul><li>子 chart 是<strong>独立</strong>的，所以子 chart 不能明确依赖于其父 chart</li><li>子 chart 无法访问其父 chart 的值</li><li>父 chart 可以覆盖子 chart 的值</li><li>Helm 中有全局值的概念，可以被所有的 chart 访问</li></ul><h3 id="创建子-chart"><a href="#创建子-chart" class="headerlink" title="创建子 chart"></a>创建子 chart</h3><p>在创建 mychart 包的时候，在根目录下面有一个空文件夹 charts 目录。这就是我们的子 chart 所在的目录，在该目录下面添加一个新的 chart：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# cd mychart&#x2F;charts&#x2F;</span><br><span class="line">[root@wang-200 charts]# helm create mysubchart</span><br><span class="line">Creating mysubchart</span><br><span class="line">[root@wang-200 charts]# rm -rf mysubchart&#x2F;templates&#x2F;*</span><br><span class="line">[root@wang-200 charts]# tree ..</span><br><span class="line">..</span><br><span class="line">├── charts</span><br><span class="line">│   └── mysubchart</span><br><span class="line">│       ├── charts</span><br><span class="line">│       ├── Chart.yaml</span><br><span class="line">│       ├── templates</span><br><span class="line">│       └── values.yaml</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── configmap.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   └── NOTES.txt</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">5 directories, 7 files</span><br></pre></td></tr></table></figure><p>同样的，我们将子 chart 模板中的文件全部删除了，接下来，我们为子 chart 创建一个简单的模板和 values 文件了。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cat &gt; mysubchart&#x2F;values.yaml &lt;&lt;EOF</span><br><span class="line">in: mysub</span><br><span class="line">EOF</span><br><span class="line">$ cat &gt; mysubchart&#x2F;templates&#x2F;configmap.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: &#123;&#123; .Release.Name &#125;&#125;-configmap2</span><br><span class="line">data:</span><br><span class="line">  in: &#123;&#123; .Values.in &#125;&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>我们上面已经提到过每个子 chart 都是独立的 chart，所以我们可以单独给 mysubchart 进行测试：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 charts]# helm install mysubchart .&#x2F;mysubchart&#x2F; --dry-run --debug</span><br><span class="line">install.go:172: [debug] Original chart version: &quot;&quot;</span><br><span class="line">install.go:189: [debug] CHART PATH: &#x2F;root&#x2F;test&#x2F;mychart&#x2F;charts&#x2F;mysubchart</span><br><span class="line">---</span><br><span class="line"># Source: mysubchart&#x2F;templates&#x2F;configmap.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: mysubchart-configmap2</span><br><span class="line">data:</span><br><span class="line">  in: mysub</span><br></pre></td></tr></table></figure><h3 id="值覆盖"><a href="#值覆盖" class="headerlink" title="值覆盖"></a>值覆盖</h3><p>现在 mysubchart 这个子 chart 就属于 mychart 这个父 chart 了，由于 mychart 是父级，所以我们可以在  mychart 的 values.yaml 文件中直接配置子 chart 中的值，比如我们可以在 mychart/values.yaml  文件中添加上子 chart 的值：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">course:</span><br><span class="line">  k8s: devops</span><br><span class="line">  python: django</span><br><span class="line">courselist:</span><br><span class="line">- k8s</span><br><span class="line">- python</span><br><span class="line">- search</span><br><span class="line">- golang</span><br><span class="line"></span><br><span class="line">mysubchart:</span><br><span class="line">  in: parent</span><br></pre></td></tr></table></figure><p>注意最后两行，mysubchart 部分内的任何指令都会传递到 mysubchart 这个子 chart 中去的，现在我们在 <strong>mychart 根目录</strong>中执行调试命令，可以查看到子 chart 也被一起渲染了：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 mychart]# helm install mychart . --dry-run --debug   </span><br><span class="line">...</span><br><span class="line">---</span><br><span class="line"># Source: mychart&#x2F;charts&#x2F;mysubchart&#x2F;templates&#x2F;configmap.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: ideal-ostrich-configmap2</span><br><span class="line">data:</span><br><span class="line">  in: parent</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>我们可以看到子 chart 中的值已经被顶层的值给覆盖了。但是在某些场景下面我们还是希望某些值在所有模板中都可以使用，这就需要用到全局 chart 值了。</p><h3 id="全局值"><a href="#全局值" class="headerlink" title="全局值"></a>全局值</h3><p>全局值可以从任何 chart 或者子 chart中进行访问使用，values 对象中有一个保留的属性是<code>Values.global</code>，就可以被用来设置全局值，比如我们在父 chart 的 values.yaml 文件中添加一个全局值：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">course:</span><br><span class="line">  k8s: devops</span><br><span class="line">  python: django</span><br><span class="line">courselist:</span><br><span class="line">- k8s</span><br><span class="line">- python</span><br><span class="line">- search</span><br><span class="line">- golang</span><br><span class="line"></span><br><span class="line">mysubchart:</span><br><span class="line">  in: parent</span><br><span class="line"></span><br><span class="line">global:</span><br><span class="line">  allin: helm</span><br></pre></td></tr></table></figure><p>我们在 values.yaml 文件中添加了一个 global 的属性，这样的话无论在父 chart 中还是在子 chart 中我们都可以通过 &lt;!–￼85–&gt; 来访问这个全局值了。比如我们在 mychart/templates/configmap.yaml 和 mychart/charts/mysubchart/templates/configmap.yaml 文件的 data 区域下面都添加上如下内容：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">data:</span><br><span class="line">  allin: &#123;&#123; .Values.global.allin &#125;&#125;</span><br></pre></td></tr></table></figure><p>父 chart 和子 chart 可以共享模板。任何 chart 中的任何定义块都可用于其他 chart，所以我们在给命名模板定义名称的时候添加了 chart 名称这样的前缀，避免冲突。</p><h1 id="49-Helm-Hooks"><a href="#49-Helm-Hooks" class="headerlink" title="49. Helm Hooks"></a>49. Helm Hooks</h1><p>和 Kubernetes 里面的容器一样，Helm 也提供了 <a href="https://docs.helm.sh/developing_charts/#hooks" target="_blank" rel="noopener">Hook</a> 的机制，允许 chart 开发人员在 release 的生命周期中的某些节点来进行干预，比如我们可以利用 Hooks 来做下面的这些事情：</p><ul><li>在加载任何其他 chart 之前，在安装过程中加载 ConfigMap 或 Secret</li><li>在安装新 chart 之前执行作业以备份数据库，然后在升级后执行第二个作业以恢复数据</li><li>在删除 release 之前运行作业，以便在删除 release 之前优雅地停止服务</li></ul><p>值得注意的是 Hooks 和普通模板一样工作，但是它们具有特殊的注释，可以使 Helm 以不同的方式使用它们。</p><p>Hook 在资源清单中的 metadata 部分用 annotations 的方式进行声明：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: ...</span><br><span class="line">kind: ....</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    &quot;helm.sh&#x2F;hook&quot;: &quot;pre-install&quot;</span><br><span class="line"># ...</span><br></pre></td></tr></table></figure><p>接下来我们就来和大家介绍下 Helm Hooks 的一些基本使用方法。</p><h2 id="Hooks"><a href="#Hooks" class="headerlink" title="Hooks"></a>Hooks</h2><p>在 Helm 中定义了如下一些可供我们使用的 Hooks：</p><ul><li>预安装<code>pre-install</code>：在模板渲染后，kubernetes 创建任何资源之前执行</li><li>安装后<code>post-install</code>：在所有 kubernetes 资源安装到集群后执行</li><li>预删除<code>pre-delete</code>：在从 kubernetes 删除任何资源之前执行删除请求</li><li>删除后<code>post-delete</code>：删除所有 release 的资源后执行</li><li>升级前<code>pre-upgrade</code>：在模板渲染后，但在任何资源升级之前执行</li><li>升级后<code>post-upgrade</code>：在所有资源升级后执行</li><li>预回滚<code>pre-rollback</code>：在模板渲染后，在任何资源回滚之前执行</li><li>回滚后<code>post-rollback</code>：在修改所有资源后执行回滚请求</li><li><code>crd-install</code>：在运行其他检查之前添加 CRD 资源，只能用于 chart 中其他的资源清单定义的 CRD 资源。</li></ul><h2 id="生命周期"><a href="#生命周期" class="headerlink" title="生命周期"></a>生命周期</h2><p>待定</p><h2 id="写一个-hook"><a href="#写一个-hook" class="headerlink" title="写一个 hook"></a>写一个 hook</h2><p>上面我们也说了 hook 和普通模板一样，也可以使用普通的模板函数和常用的一些对象，比如<code>Values</code>、<code>Chart</code>、<code>Release</code>等等，唯一和普通模板不太一样的地方就是在资源清单文件中的 metadata 部分会有一些特殊的注释 annotation。</p><p>例如，现在我们来创建一个 hook，在前面的示例 templates 目录中添加一个 post-install-job.yaml 的文件，表示安装后执行的一个 hook：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: batch&#x2F;v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: &#123;&#123; .Release.Name &#125;&#125;-post-install-job</span><br><span class="line">  lables:</span><br><span class="line">    release: &#123;&#123; .Release.Name &#125;&#125;</span><br><span class="line">    chart: &#123;&#123; .Chart.Name &#125;&#125;</span><br><span class="line">    version: &#123;&#123; .Chart.Version &#125;&#125;</span><br><span class="line">  annotations:</span><br><span class="line">    # 注意，如果没有下面的这个注释的话，当前的这个Job就会被当成release的一部分</span><br><span class="line">    &quot;helm.sh&#x2F;hook&quot;: post-install</span><br><span class="line">    &quot;helm.sh&#x2F;hook-weight&quot;: &quot;-5&quot;</span><br><span class="line">    &quot;helm.sh&#x2F;hook-delete-policy&quot;: hook-succeeded</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: &#123;&#123; .Release.Name &#125;&#125;</span><br><span class="line">      labels:</span><br><span class="line">        release: &#123;&#123; .Release.Name &#125;&#125;</span><br><span class="line">        chart: &#123;&#123; .Chart.Name &#125;&#125;</span><br><span class="line">        version: &#123;&#123; .Chart.Version &#125;&#125;</span><br><span class="line">    spec:</span><br><span class="line">      restartPolicy: Never</span><br><span class="line">      containers:</span><br><span class="line">      - name: post-install-job</span><br><span class="line">        image: alpine</span><br><span class="line">        command: [&quot;&#x2F;bin&#x2F;sleep&quot;, &quot;&#123;&#123; default &quot;10&quot; .Values.sleepTime &#125;&#125;&quot;]</span><br></pre></td></tr></table></figure><p>上面的 Job 资源中我们添加一个 annotations，要注意的是，如果我们没有添加下面这行注释的话，这个资源就会被当成是 release 的一部分资源：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">annotations:</span><br><span class="line">  &quot;helm.sh&#x2F;hook&quot;: post-install</span><br></pre></td></tr></table></figure><p>当然一个资源中我们也可以同时部署多个 hook，比如我们还可以添加一个<code>post-upgrade</code>的钩子：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">annotations:</span><br><span class="line">  &quot;helm.sh&#x2F;hook&quot;: post-install,post-upgrade</span><br></pre></td></tr></table></figure><p>另外值得注意的是我们为 hook 定义了一个权重，这有助于建立一个确定性的执行顺序，权重可以是正数也可以是负数，但是必须是字符串才行。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">annotations:</span><br><span class="line">  &quot;helm.sh&#x2F;hook-weight&quot;: &quot;-5&quot;</span><br></pre></td></tr></table></figure><p>最后还添加了一个删除 hook 资源的策略：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">annotations:</span><br><span class="line">  &quot;helm.sh&#x2F;hook-delete-policy&quot;: hook-succeeded</span><br></pre></td></tr></table></figure><p>删除资源的策略可供选择的注释值：</p><ul><li><code>hook-succeeded</code>：表示 Tiller 在 hook 成功执行后删除 hook 资源</li><li><code>hook-failed</code>：表示如果 hook 在执行期间失败了，Tiller 应该删除 hook 资源</li><li><code>before-hook-creation</code>：表示在删除新的 hook 之前应该删除以前的 hook</li></ul><p>当 helm 的 release 更新时，有可能 hook 资源已经存在于群集中。默认情况下，helm 会尝试创建资源，并抛出错误<strong>“… already exists”</strong>。</p><p>我们可以选择 “helm.sh/hook-delete-policy”: “before-hook-creation”，取代 “helm.sh/hook-delete-policy”: “hook-succeeded,hook-failed” 因为：</p><p>例如为了手动调试，将错误的 hook 作业资源保存在 kubernetes 中是很方便的。 出于某种原因，可能有必要将成功的 hook 资源保留在 kubernetes 中。同时，在 helm release  升级之前进行手动资源删除是不可取的。 “helm.sh/hook-delete-policy”: “before-hook-creation” 在 hook 中的注释，如果在新的  hook 启动前有一个 hook 的话，会使 Tiller 将以前的release 中的 hook 删除，而这个 hook  同时它可能正在被其他一个策略使用。</p><h1 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h1><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li>阳明的 <a href="https://www.qikqiak.com/k8s-book/docs/42.Helm%E5%AE%89%E8%A3%85.html" target="_blank" rel="noopener">42. Helm安装使用</a> </li></ul>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> helm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> helm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>51-56.k8s的未来和结束语</title>
      <link href="/2020/10/06/51-56-k8s%E7%9A%84%E6%9C%AA%E6%9D%A5%E5%92%8C%E7%BB%93%E6%9D%9F%E8%AF%AD/"/>
      <url>/2020/10/06/51-56-k8s%E7%9A%84%E6%9C%AA%E6%9D%A5%E5%92%8C%E7%BB%93%E6%9D%9F%E8%AF%AD/</url>
      
        <content type="html"><![CDATA[<p>极客时间课程张磊的 <a href="https://time.geekbang.org/column/intro/116" target="_blank" rel="noopener">深入剖析Kuernetes</a> </p><h1 id="51-谈谈Kubernetes开源社区和未来走向"><a href="#51-谈谈Kubernetes开源社区和未来走向" class="headerlink" title="51 | 谈谈Kubernetes开源社区和未来走向"></a>51 | 谈谈Kubernetes开源社区和未来走向</h1><h2 id="Kubernetes和google的关系"><a href="#Kubernetes和google的关系" class="headerlink" title="Kubernetes和google的关系"></a>Kubernetes和google的关系</h2><p> Kubernetes 这个项目是托管在 CNCF 基金会下面的。但是，CNCF 跟 Kubernetes 的关系，并不是传统意义上的基金会与托管项目的关系，CNCF 实际上扮演的，是 Kubernetes 项目的 Marketing 的角色。</p><p>本来 Kubernetes 项目应该是由 Google 公司一家维护、运营和推广的。但是为了表示中立，并且吸引更多的贡献者加入，Kubernetes 项目从一开始就选择了由基金会托管的模式。而这里的关键在于，这个基金会本身，就是 Kubernetes 背后的“大佬们”一手创建出来的，然后以中立的方式，对 Kubernetes 项目进行运营和 推广。</p><p>通过这种方式，Kubernetes 项目既避免了因为 Google 公司在开源社区里的“不良作风”和非中立角色被竞争对手口诛笔伐，又可以站在开源基金会的制高点上团结社区里所有跟容器相关的力量。而随后 CNCF 基金会的迅速发展和壮大，也印证了这个思路其实是非常正确和有先见之明的。</p><p>由于 Kubernetes 项目的巨大成功，CNCF 在云计算领域已经取得了极高的声誉和认可度，也填补了以往 Linux 基金会在这一领域的空白。</p><h2 id="开源项目和基金会"><a href="#开源项目和基金会" class="headerlink" title="开源项目和基金会"></a>开源项目和基金会</h2><p>不过，需要指出的是<strong>，对于开源项目和开源社区的运作来说，第三方基金会从来就不是一个必要条件。</strong>事实上，这个世界上绝大多数成功的开源项目和社区，都来自于一个聪明的想法或者一帮杰出的黑客。在这些项目的发展过程中，一个独立的、第三方基金会的作用，更多是在该项目发展到一定程度后主动进行商业运作的一部分。开源项目与基金会间的这一层关系，希望你不要本末倒置了。</p><h2 id="Kubernetes-社区"><a href="#Kubernetes-社区" class="headerlink" title="Kubernetes 社区"></a>Kubernetes 社区</h2><p>Kubernetes 项目的治理方式，其实还是比较贴近 Google 风格的，即：重视代码，重视社区的民主性。它是一个没有“Maintainer”的项目。取而代之的，则是 approver+reviewer 机制。</p><p>GitHub 比 GitLab 或者其他代码托管平台相比， 它的庞大的 API 和插件生态，才是这个产品最具吸引力的地方。</p><p>其实，到目前为止，Kubernetes 社区最大的一个优点，就是把“搞政治”的人和“搞技术”的人分得比较清楚。相信你也不难理解，这两种角色在一个活跃的开源社区里其实都是需要的，但是，如果这两部分人发生了大量的重合，那对于一个开源社区来说，恐怕就是个灾难了。</p><h1 id="52-答疑：在问题中解决问题，在思考中产生思考"><a href="#52-答疑：在问题中解决问题，在思考中产生思考" class="headerlink" title="52 | 答疑：在问题中解决问题，在思考中产生思考"></a>52 | 答疑：在问题中解决问题，在思考中产生思考</h1><p>这一章主要以前课后问题的总结，要多看。  <a href="https://time.geekbang.org/column/article/73790" target="_blank" rel="noopener">链接地址</a></p><h2 id="课外扩展"><a href="#课外扩展" class="headerlink" title="课外扩展"></a>课外扩展</h2><p>k8s使用动态资源边界调整和应用画像能力来选择最合理的limit值。</p><p>看了阿里的声东写的关于k8s的pdf，跟老师讲得很多内容都贴合。</p><h1 id="53-特别放送-2019-年，容器技术生态会发生些什么？"><a href="#53-特别放送-2019-年，容器技术生态会发生些什么？" class="headerlink" title="53 特别放送 | 2019 年，容器技术生态会发生些什么？"></a>53 特别放送 | 2019 年，容器技术生态会发生些什么？</h1><p>1.Kubernetes 项目被采纳度将持续增长。</p><p>2.“Serverless 化”与“多样性”将成为上层应用服务生态的两大关键词。</p><p>3.看得见、摸得着、能落地的“云原生”。</p><h1 id="54-特别放送-基于-Kubernetes-的云原生应用管理，到底应该怎么做？"><a href="#54-特别放送-基于-Kubernetes-的云原生应用管理，到底应该怎么做？" class="headerlink" title="54 特别放送 | 基于 Kubernetes 的云原生应用管理，到底应该怎么做？"></a>54 特别放送 | 基于 Kubernetes 的云原生应用管理，到底应该怎么做？</h1><p>现在的 Kubernetes 项目里，最有价值的部分到底是哪些呢？</p><p>在《为什么我们需要 Pod？》这篇文章中，为了讲解 Pod 里容器间关系（即：容器设计模式）的典型场景，我举了一个“WAR 包与 Web 服务器解耦”的例子。在这个例子中，我既没有让你通过 Volume 的方式将 WAR 包挂载到 Tomcat 容器中，也没有建议你把 WAR 包和 Tomcat 打包在一个镜像里，而是用了一个 InitContainer 将 WAR 包“注入”给了 Tomcat 容器。</p><p>不过，不同用户面对的场景不同，对问题的思考角度也不一样。提出的问题总结起来，主要为两个问题：</p><ol><li>第一个问题：如果 WAR 包更新了，那不是也得重新制作 WAR 包容器的镜像么？这和重新打 Tomcat 镜像有很大区别吗？</li><li>第二个问题：当用户通过 YAML 文件将 WAR 包镜像更新后，整个 Pod 不会重建么？Tomcat 需要重启么？</li></ol><p>这里的两个问题，实际上都聚焦在了这样一个对于 Kubernetes 项目至关重要的核心问题之上：<strong>基于 Kubernetes 的应用管理，到底应该怎么做？</strong></p><h2 id="第一个问题"><a href="#第一个问题" class="headerlink" title="第一个问题"></a>第一个问题</h2><p>如果 WAR 包更新了，那不是也得重新制作 WAR 包容器的镜像么？这和重新打 Tomcat 像有很大区别吗？</p><h3 id="第一个好处：自动化"><a href="#第一个好处：自动化" class="headerlink" title="第一个好处：自动化"></a>第一个好处：自动化</h3><p>一般来说，如果组织的规模不大、发布和迭代次数不多的话，将 WAR 包（应用代码）的发布流程和 Tomcat （Web 服务器）的发布流程解耦，实际上很难有较强的体感。在这些团队中，Tomcat 本身很可能就是开发人员自己负责管理的，甚至被认为是应用的一部分，无需进行很明确的分离。</p><p>而如果WAR 包和Tomcat ，由两个团队维护，那么在 Pod 的定义中直接将两个容器解耦，相比于每次发布前都必须先将两个镜像“融合”成一个镜像然后再发布，就要自动化得多了。这个原因是显而易见的：开发人员不需要额外维护一个“重新打包”应用的脚本、甚至手动地去做这个步骤了。</p><h3 id="第二个好处：自描述"><a href="#第二个好处：自描述" class="headerlink" title="第二个好处：自描述"></a>第二个好处：自描述</h3><p>然而，相比于 Volume 挂载的方式，通过在 Pod 定义中解耦上述两个容器，其实还会带来另一个更重要的好处，叫作：自描述。一个良好编写的 Pod 的 YAML 文件应该是“自描述”的，它直接描述了这个应用本身的所有信息。</p><p>要想 “Volume 挂载”的方式真正能工作，可行方法只有一种：那就是写一个专门的 Kubernetes Volume 插件（比如，Flexvolume 或者 CSI 插件） 。</p><h3 id="声明式API"><a href="#声明式API" class="headerlink" title="声明式API"></a>声明式API</h3><p>Kubernetes 项目最强大的能力，就是“声明式”的应用定义方式。这个“声明式”背后的设计思想，是在 YAML 文件（Kubernetes API  对象）中描述应用的“终态”。然后 Kubernetes 负责通过“控制器模式”不断地将整个系统的实际状态向这个“终态”逼近并且达成一致。</p><p>“声明式”带来最大的好处，其实正是“自动化”。作为一个 Kubernetes 用户，你只需要在 YAML 里描述清楚这个应用长什么样子，那么剩下的所有事情，就都可以放心地交给 Kubernetes 自动完成了：它会通过控制器模式确保这个系统里的应用状态，最终并且始终跟你在 YAML 文件里的描述完全一致。</p><p>这种“把简单交给用户，把复杂留给自己”的精神，正是一个“声明式”项目的精髓所在了。</p><h2 id="第二个问题"><a href="#第二个问题" class="headerlink" title="第二个问题"></a>第二个问题</h2><p>当通过 YAML 文件将 WAR 包镜像更新后，整个 Pod 不会重建么？Tomcat 需要重启么？</p><p>实际上，当一个  Pod 里的容器镜像被更新后，kubelet 本身就能够判断究竟是哪个容器需要更新，而不会“无脑”地重建整个 Pod。当然，你的 Tomcat 需要配置好 reloadable=“true”，这样就不需要重启 Tomcat 服务器了，这是一个非常常见的做法。</p><p>但是，这里还有一个细节需要注意。即使 kubelet 本身能够“智能”地单独重建被更新的容器，但如果你的 Pod 是用  Deployment 管理的话，它会按照自己的发布策略（RolloutStrategy） 来通过重建的方式更新 Pod。</p><p>这时候，如果这个 Pod 被重新调度到其他机器上，那么 kubelet “单独重建被更新的容器”的能力就没办法发挥作用了。所以说，要让这个案例中的“解耦”能力发挥到最佳程度，你还需要一个“原地升级”的功能，即：允许 Kubernetes 在原地进行 Pod 的更新，避免重调度带来的各种麻烦。原地升级能力，在 Kubernetes 的默认控制器中都是不支持的，需要你自己开启。</p><h2 id="云原生应用管理"><a href="#云原生应用管理" class="headerlink" title="云原生应用管理"></a>云原生应用管理</h2><p>实际上，通过深入地讲解 “Tomcat 与 WAR 包解耦”这个案例，你可以看到 Kubernetes 的“声明式 API”“容器设计模式”“控制器原理”，以及 kubelet 的工作机制等很多核心知识点，实际上是可以通过一条主线贯穿起来的。这条主线，从“应用如何描述”开始，到“容器如何运行”结束。</p><p><strong>这条主线，正是 Kubernetes 项目中最具价值的那个部分，即：云原生应用管理（Cloud Native Application Management）。</strong>它是一条连接 Kubernetes 项目绝大多数核心特性的关键线索，也是 Kubernetes 项目乃至整个云原生社区这五年来飞速发展背后唯一不变的精髓。</p><h1 id="55-结束语-Kubernetes：赢开发者赢天下"><a href="#55-结束语-Kubernetes：赢开发者赢天下" class="headerlink" title="55 结束语 | Kubernetes：赢开发者赢天下"></a>55 结束语 | Kubernetes：赢开发者赢天下</h1><h2 id="Kubernetes-项目适应了绝大多数开发者"><a href="#Kubernetes-项目适应了绝大多数开发者" class="headerlink" title="Kubernetes 项目适应了绝大多数开发者"></a>Kubernetes 项目适应了绝大多数开发者</h2><p>Kubernetes 项目之所以能赢，最重要的原因在于它争取到了云计算生态里的绝大多数开发者。它的成功，是成千上万云计算平台上的开发者用脚投票的结果。云计算平台上的开发者们所关心的，并不是调度，也不是资源管理，更不是网络或者存储，他们关心的只有一件事，那就是 Kubernetes 的 API。</p><p>这也是为什么，在 Kubernetes 这个项目里，只要是跟 API 相关的事情，那就都是大事儿；只要是想要在这个社区构建影响力的人或者组织，就一定会在 API 层面展开角逐。这一层 “API 为王”的思路，早已经深入到了 Kubernetes 里每一个 API 对象的每一个字段的设计过程当中。</p><p>所以说，<strong>Kubernetes 项目的本质其实只有一个，那就是“控制器模式”。</strong>这个思想，不仅仅是 Kubernetes 项目里每一个组件的“设计模板”，也是 Kubernetes 项目能够将开发者们紧紧团结到自己身边的重要原因。作为一个云计算平台的用户，能够用一个 YAML 文件表达我开发的应用的最终运行状态，并且自动地对我的应用进行运维和管理。这种信赖关系，就是连接 Kubernetes 项目和开发者们最重要的纽带。更重要的是，当这个 API 趋向于足够稳定和完善的时候，越来越多的开发者会自动汇集到这个 API 上来，依托它所提供的能力构建出一个全新的生态。</p><h2 id="Kubernetes-项目未来很重要"><a href="#Kubernetes-项目未来很重要" class="headerlink" title="Kubernetes 项目未来很重要"></a>Kubernetes 项目未来很重要</h2><p>Kubernetes 项目里最重要的，是它的“容器设计模式”，是它的 API 对象，是它的 API 编程范式。这些，都是未来云计算时代的每一个开发者需要融会贯通、融化到自己开发基因里的关键所在。也只有这样，作为一个开发者，你才能够开发和构建出符合未来云计算形态的应用。而更重要的是，也只有这样，你才能够借助云计算的力量，让自己的应用真正产生价值。</p><p>所以说，当你不太理解为什么要学习 Kubernetes 项目的时候，或者，你在学习 Kubernetes 项目感到困难的时候，不妨想象一下 Kubernetes 就是未来的 Linux 操作系统。在这个云计算以前所未有的速度迅速普及的世界里，Kubernetes 项目很快就会像操作系统一样，成为每一个技术从业者必备的基础知识。而现在，你不仅牢牢把握住了这个项目的精髓，也就是声明式 API 和控制器模式；掌握了这个 API 独有的编程范式，即 Controller 和 Operator；还以此为基础详细地了解了这个项目每一个核心模块和功能的设计与实现方法。那么，对于这个未来云计算时代的操作系统，你还有什么好担心的呢？</p><h1 id="56-结课测试｜这些Kubernetes的相关知识，你都掌握了吗？"><a href="#56-结课测试｜这些Kubernetes的相关知识，你都掌握了吗？" class="headerlink" title="56 结课测试｜这些Kubernetes的相关知识，你都掌握了吗？"></a>56 结课测试｜这些Kubernetes的相关知识，你都掌握了吗？</h1><p>看看我第一次自测的结果吧！这个结果有点惨，刚刚及格。</p><p>分析了一下，我认真的学习了，但是思考不够深入，要和别人多沟通。以后需要多看看问题，带着这自己的思考，反复学习。</p><p>小伙子，加油。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 深入剖析Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 深入剖析Kuernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>48-50.容器监控和日志</title>
      <link href="/2020/10/05/48-50-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%92%8C%E6%97%A5%E5%BF%97/"/>
      <url>/2020/10/05/48-50-%E5%AE%B9%E5%99%A8%E7%9B%91%E6%8E%A7%E5%92%8C%E6%97%A5%E5%BF%97/</url>
      
        <content type="html"><![CDATA[<p>极客时间课程张磊的 <a href="https://time.geekbang.org/column/intro/116" target="_blank" rel="noopener">深入剖析Kuernetes</a> </p><h1 id="48-Prometheus、Metrics-Server与Kubernetes监控体系"><a href="#48-Prometheus、Metrics-Server与Kubernetes监控体系" class="headerlink" title="48 | Prometheus、Metrics Server与Kubernetes监控体系"></a>48 | Prometheus、Metrics Server与Kubernetes监控体系</h1><h2 id="Prometheus-项目"><a href="#Prometheus-项目" class="headerlink" title="Prometheus 项目"></a>Prometheus 项目</h2><p>Prometheus 项目作为一个监控系统，他的的作用和工作方式，其实可以用如下所示的一张官方示意图来解释。</p><img src="/img/body/jike/2ada1ece66fcc81d704c2ba46f9dd7d3.png" alt="Prometheus 工作方式官方示意图" style="zoom: 67%; max-width: 70%;" /><p>可以看到，Prometheus 项目工作的核心，是使用 Pull （抓取）的方式去搜集被监控对象的 Metrics 数据（监控指标数据），然后，再把这些数据保存在一个 TSDB （时间序列数据库，比如 OpenTSDB、InfluxDB 等）当中，以便后续可以按照时间进行检索。</p><p>有了这套核心监控机制， Prometheus 剩下的组件就是用来配合这套机制的运行。比如 Pushgateway，可以允许被监控对象以 Push 的方式向 Prometheus 推送 Metrics 数据。而 Alertmanager，则可以根据 Metrics 信息灵活地设置报警。当然， Prometheus 最受用户欢迎的功能，还是通过 Grafana 对外暴露出的、可以灵活配置的监控数据可视化界面。</p><p>有了 Prometheus 之后，我们就可以按照 Metrics 数据的来源，来对 Kubernetes 的监控体系做一个汇总了。</p><h3 id="第一种-Metrics：宿主机的监控数据。"><a href="#第一种-Metrics：宿主机的监控数据。" class="headerlink" title="第一种 Metrics：宿主机的监控数据。"></a>第一种 Metrics：宿主机的监控数据。</h3><p>这部分数据的提供，需要借助一个由 Prometheus 维护的Node Exporter 工具。一般来说，Node Exporter 会以 DaemonSet 的方式运行在宿主机上。其实，所谓的 Exporter，就是代替被监控对象来对 Prometheus 暴露出可以被“抓取”的 Metrics 信息的一个辅助进程。</p><p>而 Node Exporter 可以暴露给 Prometheus 采集的 Metrics 数据， 也不单单是节点的负载（Load）、CPU 、内存、磁盘以及网络这样的常规信息，它的 Metrics 指标可以说是“包罗万象”，你可以查看<a href="https://github.com/prometheus/node_exporter#enabled-by-default" target="_blank" rel="noopener">这个列表</a>来感受一下。</p><h3 id="第二种-Metrics：来自于-API-Server、kubelet-等组件的-metrics-API。"><a href="#第二种-Metrics：来自于-API-Server、kubelet-等组件的-metrics-API。" class="headerlink" title="第二种 Metrics：来自于 API Server、kubelet 等组件的 /metrics API。"></a>第二种 Metrics：来自于 API Server、kubelet 等组件的 /metrics API。</h3><p>除了常规的 CPU、内存的信息外，这部分信息还主要包括了各个组件的核心监控指标。比如，对于 API Server 来说，它就会在 /metrics API 里，暴露出各个 Controller 的工作队列（Work Queue）的长度、请求的 QPS 和延迟数据等等。这些信息，是检查 Kubernetes 本身工作情况的主要依据。</p><h3 id="第三种-Metrics：Kubernetes-相关的监控数据。"><a href="#第三种-Metrics：Kubernetes-相关的监控数据。" class="headerlink" title="第三种 Metrics：Kubernetes 相关的监控数据。"></a>第三种 Metrics：Kubernetes 相关的监控数据。</h3><p>这部分数据，一般叫作 Kubernetes 核心监控数据（core metrics）。这其中包括了 Pod、Node、容器、Service 等主要 Kubernetes 核心概念的 Metrics。</p><p>其中，容器相关的 Metrics 主要来自于 kubelet 内置的 cAdvisor 服务。在 kubelet 启动后，cAdvisor 服务也随之启动，而它能够提供的信息，可以细化到每一个容器的 CPU 、文件系统、内存、网络等资源的使用情况。</p><h2 id="Metrics-Server"><a href="#Metrics-Server" class="headerlink" title="Metrics Server"></a>Metrics Server</h2><p>需要注意的是，这里提到的 Kubernetes 核心监控数据，其实使用的是 Kubernetes 的一个非常重要的扩展能力，叫作 Metrics Server。</p><p>Metrics Server，则把 Kubernetes 监控数据（比如 Pod 和 Node 的资源使用情况），通过标准的 Kubernetes API 暴露了出来。</p><p>而有了 Metrics Server 之后，用户就可以通过标准的 Kubernetes API 来访问到这些监控数据了。比如，下面这个 URL：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;127.0.0.1:8001&#x2F;apis&#x2F;metrics.k8s.io&#x2F;v1beta1&#x2F;namespaces&#x2F;&lt;namespace-name&gt;&#x2F;pods&#x2F;&lt;pod-name&gt;</span><br></pre></td></tr></table></figure><p>当你访问这个 Metrics API 时，它就会为你返回一个 Pod 的监控数据，而这些数据，其实是从 kubelet 的 Summary API （即 <kubelet_ip>:<kubelet_port>/stats/summary）采集而来的。Summary API 返回的信息，既包括了 cAdVisor 的监控数据，也包括了 kubelet 本身汇总的信息。</p><h2 id="Aggregator-插件"><a href="#Aggregator-插件" class="headerlink" title="Aggregator 插件"></a>Aggregator 插件</h2><p>要指出的是， Metrics Server 并不是 kube-apiserver 的一部分，而是通过 Aggregator 这种插件机制，在独立部署的情况下同 kube-apiserver 一起统一对外服务的。</p><p>这里，Aggregator APIServer 的工作原理，可以用如下所示的一幅示意图来表示清楚：</p><img src="/img/body/jike/0b767b5224ad1906ddc4cce075618809.png" alt="img" style="zoom:67%;max-width: 60%" /><p>可以看到，当 Kubernetes 的 API Server 开启了 Aggregator 模式之后，你再访问 apis/metrics.k8s.io/v1beta1 的时候，实际上访问到的是一个叫作 kube-aggregator 的代理。而 kube-apiserver，正是这个代理的一个后端；而 Metrics Server，则是另一个后端。</p><p>而且，在这个机制下，你还可以添加更多的后端给这个 kube-aggregator。所以 kube-aggregator 其实就是一个根据 URL 选择具体的 API 后端的代理服务器。通过这种方式，我们就可以很方便地扩展 Kubernetes 的 API 了。</p><p>而 Aggregator 模式的开启也非常简单：</p><ul><li>如果你是使用 kubeadm 或者官方的 kube-up.sh 脚本部署 Kubernetes 集群的话，Aggregator 模式就是默认开启的；</li><li>如果是手动 DIY 搭建的话，你就需要在 kube-apiserver 的启动参数里加上一些配置。</li></ul><p>Aggregator 功能开启之后，你只需要将 Metrics Server 的 YAML 文件部署起来，就会看到 metrics.k8s.io 这个 API 出现在了你的 Kubernetes API 列表当中。</p><h2 id="USE-原则和-RED-原则"><a href="#USE-原则和-RED-原则" class="headerlink" title="USE 原则和 RED 原则"></a>USE 原则和 RED 原则</h2><p>Prometheus 项目在具体的监控指标规划上，我建议你<strong>遵循业界通用的 USE 原则和 RED 原则。</strong></p><p>其中，USE 原则指的是，按照如下三个维度来规划资源监控指标：</p><ol><li>利用率（Utilization），资源被有效利用起来提供服务的平均时间占比；</li><li>饱和度（Saturation），资源拥挤的程度，比如工作队列的长度；</li><li>错误率（Errors），错误的数量。</li></ol><p>而 RED 原则指的是，按照如下三个维度来规划服务监控指标：</p><ol><li>每秒请求数量（Rate）；</li><li>每秒错误数量（Errors）；</li><li>服务响应时间（Duration）。</li></ol><p>不难发现， USE 原则主要关注的是“资源”，比如节点和容器的资源使用情况，而 RED 原则主要关注的是“服务”，比如 kube-apiserver 或者某个应用的工作情况。这两种指标，在我今天为你讲解的 Kubernetes + Prometheus 组成的监控体系中，都是可以完全覆盖到的。</p><h1 id="49-Custom-Metrics-让Auto-Scaling不再“食之无味”"><a href="#49-Custom-Metrics-让Auto-Scaling不再“食之无味”" class="headerlink" title="49 | Custom Metrics: 让Auto Scaling不再“食之无味”"></a>49 | Custom Metrics: 让Auto Scaling不再“食之无味”</h1><h2 id="Custom-Metrics：自定义监控指标"><a href="#Custom-Metrics：自定义监控指标" class="headerlink" title="Custom Metrics：自定义监控指标"></a>Custom Metrics：自定义监控指标</h2><p>借助Kubernetes 里的核心监控体系，Kubernetes 为你提供了 Custom Metrics，自定义监控指标。</p><p>在过去的很多 PaaS 项目中，其实都有一种叫作 Auto Scaling，即自动水平扩展的功能。只不过，这个功能往往只能依据某种指定的资源类型执行水平扩展，比如 CPU 或者 Memory 的使用值。</p><p>而在真实的场景中，用户需要进行 Auto Scaling 的依据往往是自定义的监控指标。比如，某个应用的等待队列的长度，或者某种应用相关资源的使用情况。这些复杂多变的需求，在传统 PaaS 项目和其他容器编排项目里，几乎是不可能轻松支持的。</p><p>而凭借强大的 API 扩展机制，Custom Metrics 已经成为了 Kubernetes 的一项标准能力。并且，Kubernetes 的自动扩展器组件 Horizontal Pod Autoscaler （HPA）， 也可以直接使用 Custom Metrics 来执行用户指定的扩展策略，这里的整个过程都是非常灵活和可定制的。</p><p>不难想到，Kubernetes 里的 Custom Metrics 机制，也是借助 Aggregator APIServer 扩展机制来实现的。这里的具体原理是，当你把 Custom Metrics APIServer 启动之后，Kubernetes 里就会出现一个叫作custom.metrics.k8s.io的 API。而当你访问这个 URL 时，Aggregator 就会把你的请求转发给 Custom Metrics APIServer 。</p><h2 id="Custom-Metrics-APIServer"><a href="#Custom-Metrics-APIServer" class="headerlink" title="Custom Metrics APIServer"></a>Custom Metrics APIServer</h2><p>而 Custom Metrics APIServer 的实现，其实就是一个 Prometheus 项目的 Adaptor。</p><p>比如，现在我们要实现一个根据指定 Pod 收到的 HTTP 请求数量来进行 Auto Scaling 的 Custom Metrics，这个 Metrics 就可以通过访问如下所示的自定义监控 URL 获取到：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;&lt;apiserver_ip&gt;&#x2F;apis&#x2F;custom-metrics.metrics.k8s.io&#x2F;v1beta1&#x2F;namespaces&#x2F;default&#x2F;pods&#x2F;sample-metrics-app&#x2F;http_requests</span><br></pre></td></tr></table></figure><p>这里的工作原理是，当你访问这个 URL 的时候，Custom Metrics APIServer 就会去 Prometheus 里查询名叫 sample-metrics-app 这个 Pod 的 http_requests 指标的值，然后按照固定的格式返回给访问者。当然，http_requests 指标的值，就需要由以 Prometheus 为核心的监控体系，从目标 Pod 上采集。</p><p>这里具体的做法有很多种，最普遍的做法，就是让 Pod 里的应用本身暴露出一个 /metrics API，然后在这个 API 里返回自己收到的 HTTP 的请求的数量。所以说，接下来 HPA 只需要定时访问前面提到的自定义监控 URL，然后根据这些值计算是否要执行 Scaling 即可。</p><h1 id="50-让日志无处可逃：容器日志收集与管理"><a href="#50-让日志无处可逃：容器日志收集与管理" class="headerlink" title="50 | 让日志无处可逃：容器日志收集与管理"></a>50 | 让日志无处可逃：容器日志收集与管理</h1><p>首先需要明确的是，Kubernetes 里面对容器日志的处理方式，都叫作 cluster-level-logging，即：这个日志处理系统，与容器、Pod 以及 Node 的生命周期都是完全无关的。这种设计当然是为了保证，无论是容器挂了、Pod 被删除，甚至节点宕机的时候，应用的日志依然可以被正常获取到。</p><p>而对于一个容器来说，当应用把日志输出到 stdout 和 stderr 之后，容器项目在默认情况下就会把这些日志输出到宿主机上的一个 JSON 文件里。这样，你通过 kubectl logs 命令就可以看到这些容器的日志了。</p><p>而 Kubernetes 本身，实际上是不会为你做容器日志收集工作的，所以为了实现上述 cluster-level-logging，你需要在部署集群的时候，提前对具体的日志方案进行规划。而 Kubernetes 项目本身，主要为你推荐了三种日志方案。</p><h2 id="第一种方案：部署-logging-agent收集日志"><a href="#第一种方案：部署-logging-agent收集日志" class="headerlink" title="第一种方案：部署 logging agent收集日志"></a>第一种方案：部署 logging agent收集日志</h2><p>在 Node 上部署 logging agent，将日志文件转发到后端存储里保存起来。这个方案的架构图如下所示。</p><img src="/img/body/jike/b5515aed076aa6af63ace55b62d36243.jpg" alt="img" style="zoom: 50%; max-width: 50%;" /><p>这里的核心就在于 logging agent ，它一般都会以 DaemonSet 的方式运行在节点上，然后将宿主机上的容器日志目录挂载进去，最后由 logging-agent 把日志转发出去。</p><p>举个例子，我们可以通过 Fluentd 项目作为宿主机上的 logging-agent，然后把日志转发到远端的 ElasticSearch 里保存起来供将来进行检索。另外，在很多 Kubernetes 的部署里，会自动为你启用 logrotate，在日志文件超过 10MB 的时候自动对日志文件进行 rotate 操作。</p><p>在 Node 上部署 logging agent 优缺点：</p><ul><li><p>优点：一个节点只需要部署一个 agent，并且不会对应用和 Pod 有任何侵入性。所以，这个方案，在社区里是最常用的一种。</p></li><li><p>缺点：它要求应用输出的日志，都必须是直接输出到容器的 stdout 和 stderr 里。</p></li></ul><h2 id="第二种方案：通过sidecar-容器输出日志"><a href="#第二种方案：通过sidecar-容器输出日志" class="headerlink" title="第二种方案：通过sidecar 容器输出日志"></a>第二种方案：通过sidecar 容器输出日志</h2><p>第二种容器日志方案，就是对这种特殊情况的一个处理，即：当容器的日志只能输出到某些文件里的时候，我们可以通过一个 sidecar 容器把这些日志文件重新输出到 sidecar 的 stdout 和 stderr 上，这样就能够继续使用第一种方案了。这个方案的具体工作原理，如下所示。</p><img src="/img/body/jike/4863e3d7d1ef02a5a44e431369ac4120.jpg" alt="img" style="zoom: 50%; max-width: 50%;" /><h2 id="第三种方案：通过sidecar-容器收集日志"><a href="#第三种方案：通过sidecar-容器收集日志" class="headerlink" title="第三种方案：通过sidecar 容器收集日志"></a>第三种方案：通过sidecar 容器收集日志</h2><p>通过一个 sidecar 容器，直接把应用的日志文件发送到远程存储里面去。也就是相当于把方案一里的 logging agent，放在了应用 Pod 里。这种方案的架构如下所示：</p><img src="/img/body/jike/d464401baec60c11f96dfeea3ae3a9c7.jpg" alt="img" style="zoom: 50%; max-width: 50%;" /><p>在这种方案里，你的应用还可以直接把日志输出到固定的文件里而不是 stdout，你的 logging-agent 还可以使用 fluentd，后端存储还可以是 ElasticSearch。只不过， fluentd 的输入源，变成了应用的日志文件。通常我们会把 fluentd 的输入源配置保存在一个 ConfigMap 里。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 深入剖析Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 深入剖析Kuernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>45-47.容器运行时</title>
      <link href="/2020/10/04/45-47-%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6/"/>
      <url>/2020/10/04/45-47-%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6/</url>
      
        <content type="html"><![CDATA[<p>极客时间课程张磊的 <a href="https://time.geekbang.org/column/intro/116" target="_blank" rel="noopener">深入剖析Kuernetes</a> </p><h1 id="45-幕后英雄：SIG-Node与CRI"><a href="#45-幕后英雄：SIG-Node与CRI" class="headerlink" title="45 | 幕后英雄：SIG-Node与CRI"></a>45 | 幕后英雄：SIG-Node与CRI</h1><p>Kubernetes在调度和资源管理度这一步完成后，就需要负责将这个调度成功的 Pod，在宿主机上创建出来，并把它所定义的各个容器启动起来。这些，都是 kubelet 这个核心组件的主要功能。</p><p>与 kubelet 以及容器运行时管理相关的内容，都属于 SIG-Node 的范畴。</p><p>kubelet 这个组件，是 Kubernetes 里面第二个不可被替代的组件（第一个不可被替代的组件是 kube-apiserver）。也就是说，<strong>无论如何，我都不建议你对 kubelet 的代码进行大量的改动。保持 kubelet 跟上游基本一致的重要性，就跟保持 kube-apiserver 跟上游一致是一个道理。</strong></p><h2 id="kubelet-的工作核心SyncLoop"><a href="#kubelet-的工作核心SyncLoop" class="headerlink" title="kubelet 的工作核心SyncLoop"></a>kubelet 的工作核心SyncLoop</h2><p>kubelet 也是按照“控制器”模式来工作的。它实际的工作原理，可以用如下所示的一幅示意图来表示清楚。</p><img src="/img/body/jike/914e097aed10b9ff39b509759f8b1d03.png" alt="Kubelet的工作原理" style="zoom:67%;max-width: 70%" /><p>可以看到，kubelet 的工作核心，就是一个控制循环，即：SyncLoop（图中的大圆圈）。而驱动这个控制循环运行的事件，包括四种：</p><ol><li>Pod 更新事件；</li><li>Pod 生命周期变化；</li><li>kubelet 本身设置的执行周期；</li><li>定时的清理事件。</li></ol><p>所以，跟其他控制器类似，kubelet 启动的时候，要做的第一件事情，就是设置 Listers，也就是注册它所关心的各种事件的 Informer。这些 Informer，就是 SyncLoop 需要处理的数据的来源。</p><p>此外，kubelet 还负责维护着很多其他的子控制循环（也就是图中的小圆圈）。这些控制循环的名字，一般被称作某某 Manager，比如 Volume Manager、Image Manager、Node Status Manager 等等。</p><p>这些控制循环的责任，就是通过控制器模式，完成 kubelet 的某项具体职责。比如 Node Status Manager，就负责响应 Node 的状态变化，然后将 Node 的状态收集起来，并通过 Heartbeat 的方式上报给 APIServer。再比如 CPU Manager，就负责维护该 Node 的 CPU 核的信息，以便在 Pod 通过 cpuset 的方式请求 CPU 核的时候，能够正确地管理 CPU 核的使用量和可用量。</p><p>那么这个 <strong>SyncLoop，又是如何根据 Pod 对象的变化，来进行容器操作的呢？</strong></p><p>实际上，kubelet 也是通过 Watch 机制，监听了与自己相关的 Pod 对象的变化。当然，这个 Watch 的过滤条件是该 Pod 的 nodeName 字段与自己相同。kubelet 会把这些 Pod 的信息缓存在自己的内存里。</p><p>而当一个 Pod 完成调度、与一个 Node 绑定起来之后， 这个 Pod 的变化就会触发 kubelet 在控制循环里注册的 Handler，也就是上图中的 HandlePods 部分。此时，通过检查该 Pod 在 kubelet 内存里的状态，kubelet 就能够判断出这是一个新调度过来的 Pod，从而触发 Handler 里 ADD 事件对应的处理逻辑。</p><p>在具体的处理过程当中，kubelet 会启动一个名叫 Pod Update Worker 的、单独的 Goroutine 来完成对 Pod 的处理工作。</p><p>在这里需要注意的是，<strong>kubelet 调用下层容器运行时的执行过程，并不会直接调用 Docker 的 API，而是通过一组叫作 CRI（Container Runtime Interface，容器运行时接口）的 gRPC 接口来间接执行的。</strong></p><h2 id="为什么引入CRI？"><a href="#为什么引入CRI？" class="headerlink" title="为什么引入CRI？"></a>为什么引入CRI？</h2><p>Kubernetes 项目之所以要在 kubelet 中引入这样一层单独的抽象，当然是为了对 Kubernetes 屏蔽下层容器运行时的差异。实际上，对于 1.6 版本之前的 Kubernetes 来说，它就是直接调用 Docker 的 API 来创建和管理容器的。</p><p>Kata Containers 项目，这种基于虚拟化技术的强隔离容器，与 Kubernetes 和 Linux 容器项目之间具有良好的互补关系。所以，在 Kubernetes 上，对虚拟化容器的支持很快就被提上了日程。</p><p>在 2016 年，SIG-Node 决定开始动手解决上述问题。而解决办法就是把 kubelet 对容器的操作，统一地抽象成一个接口。这样，kubelet 就只需要跟这个接口打交道了。而作为具体的容器项目，比如 Docker、 rkt、runV，它们就只需要自己提供一个该接口的实现，然后对 kubelet 暴露出 gRPC 服务即可。这一层统一的容器操作接口，就是 CRI 。</p><h2 id="CRI的作用"><a href="#CRI的作用" class="headerlink" title="CRI的作用"></a>CRI的作用</h2><p>而在有了 CRI 之后，Kubernetes 以及 kubelet 本身的架构，就可以用如下所示的一幅示意图来描述。</p><img src="/img/body/jike/5161bd6201942f7a1ed6d70d7d55acfe.png" alt="带CRI的kubelet的架构" style="zoom:80%;" /><p>可以看到，当 Kubernetes 通过编排能力创建了一个 Pod 之后，调度器会为这个 Pod 选择一个具体的节点来运行。这时候，kubelet 就会通过 SyncLoop 来判断需要执行的具体操作，比如创建一个 Pod。那么此时，kubelet 实际上就会调用一个叫作 GenericRuntime 的通用组件来发起创建 Pod 的 CRI 请求。</p><p><strong>这个 CRI 请求，又该由谁来响应呢？</strong></p><ul><li><p>如果你使用的容器项目是 Docker 的话，那么负责响应这个请求的就是一个叫作 dockershim 的组件。它会把 CRI 请求里的内容拿出来，然后组装成 Docker API 请求发给 Docker Daemon。现在dockershim 依然是 kubelet 代码的一部分，但将来dockershim 肯定会被从 kubelet 里移出来，甚至废弃。</p></li><li><p>而更普遍的场景，就是你需要在每台宿主机上单独安装一个负责响应 CRI 的组件，这个组件，一般被称作 CRI shim。顾名思义，CRI shim 的工作，就是扮演 kubelet 与容器项目之间的“垫片”（shim）。所以它的作用非常单一，那就是实现 CRI 规定的每个接口，然后把具体的 CRI 请求“翻译”成对后端容器项目的请求或者操作。</p></li></ul><p>在这个过程中，kubelet 的 SyncLoop 和 CRI 的设计，是其中最重要的两个关键点。也正是基于以上设计，SyncLoop 本身就要求这个控制循环是绝对不可以被阻塞的。所以，凡是在 kubelet 里有可能会耗费大量时间的操作，比如准备 Pod 的 Volume、拉取镜像等，SyncLoop 都会开启单独的 Goroutine 来进行操作。</p><h1 id="46-解读-CRI-与-容器运行时"><a href="#46-解读-CRI-与-容器运行时" class="headerlink" title="46 | 解读 CRI 与 容器运行时"></a>46 | 解读 CRI 与 容器运行时</h1><h2 id="CRI-shim简述"><a href="#CRI-shim简述" class="headerlink" title="CRI shim简述"></a>CRI shim简述</h2><p>CRI 机制能够发挥作用的核心，就在于每一种容器项目现在都可以自己实现一个 CRI shim，自行对 CRI 请求进行处理。这样，Kubernetes 就有了一个统一的容器抽象层，使得下层容器运行时可以自由地对接进入 Kubernetes 当中。</p><p>所以说，这里的 CRI shim，就是容器项目的维护者们自行维护。而除了 dockershim 之外，其他容器运行时的 CRI shim，都是需要额外部署在宿主机上的。</p><p>举个例子。CNCF 里的 containerd 项目，就可以提供一个典型的 CRI shim 的能力，即：将 Kubernetes 发出的 CRI 请求，转换成对 containerd 的调用，然后创建出 runC 容器。而 runC 项目，才是负责执行设置容器 Namespace、Cgroups 和 chroot 等基础操作的组件。所以，这几层的组合关系，可以用如下所示的示意图来描述。</p><img src="/img/body/jike/62c591c4d832d44fed6f76f60be88e3d.png" alt="containerd 项目层级" style="zoom:60%;max-width: 50%" /><h2 id="CRI-shim对-CRI-的具体实现"><a href="#CRI-shim对-CRI-的具体实现" class="headerlink" title="CRI shim对 CRI 的具体实现"></a>CRI shim对 CRI 的具体实现</h2><p>而作为一个 CRI shim，containerd 对 CRI 的具体实现，又是怎样的呢？</p><p>我们先来看一下 CRI 这个接口的定义。下面这幅示意图，就展示了 CRI 里主要的待实现接口。</p><img src="/img/body/jike/f7e86505c09239b80ad05aecfb032e16.png" alt="CRI代码示例" style="zoom:60%;max-width: 60%;" /><p>具体地说，我们可以把 CRI 分为两组：</p><ul><li>第一组，是 RuntimeService。它提供的接口，主要是跟容器相关的操作。比如，创建和启动容器、删除容器、执行 exec 命令等。</li><li>第二组，则是 ImageService。它提供的接口，主要是容器镜像相关的操作，比如拉取镜像、删除镜像等。</li></ul><h2 id="RuntimeService-部分"><a href="#RuntimeService-部分" class="headerlink" title="RuntimeService 部分"></a>RuntimeService 部分</h2><p>CRI 设计的一个重要原则，就是确保这个接口本身，只关注容器，不关注 Pod:</p><ul><li>第一，Pod 是 Kubernetes 的编排概念，而不是容器运行时的概念。所以，我们就不能假设所有下层容器项目，都能够暴露出可以直接映射为 Pod 的 API。</li><li>第二，如果 CRI 里引入了关于 Pod 的概念，那么接下来只要 Pod API 对象的字段发生变化，那么 CRI 就很有可能需要变更。而在 Kubernetes 开发的前期，Pod 对象的变化还是比较频繁的，但对于 CRI 这样的标准接口来说，这个变更频率太快。</li></ul><p>所以，在 CRI 的设计里，并没有一个直接创建 Pod 或者启动 Pod 的接口，而是通过RunPodSandbox 的接口。</p><p>这个 PodSandbox，对应的并不是 Kubernetes 里的 Pod API 对象，而只是抽取了 Pod 里的一部分与容器运行时相关的字段，比如 HostName、DnsConfig、CgroupParent 等。所以说，PodSandbox 这个接口描述的，其实是 Kubernetes 将 Pod 这个概念映射到容器运行时层面所需要的字段，或者说是一个 Pod 对象子集。</p><p>而作为具体的容器项目，你就需要自己决定如何使用这些字段来实现一个 Kubernetes 期望的 Pod 模型。这里的原理，可以用如下所示的示意图来表示清楚。</p><img src="/img/body/jike/d9fb7404c5dc9e0b5c902f74df9d7a61.png" alt="RuntimeService示例图" style="zoom:50%;max-width: 60%" /><p>比如，当我们执行 kubectl run 创建了一个名叫 foo 的、包括了 A、B 两个容器的 Pod 之后。这个 Pod 的信息最后来到 kubelet，kubelet 就会按照图中所示的顺序来调用 CRI 接口。</p><p>在具体的 CRI shim 中，这些接口的实现是可以完全不同的。比如：</p><ul><li>如果是 Docker 项目，dockershim 就会创建出一个名叫 foo 的 Infra 容器（pause 容器），用来创建整个 Pod 的 Network Namespace。</li><li>如果是基于虚拟化技术的容器，比如 Kata Containers 项目，它的 CRI 实现就会直接创建出一个轻量级虚拟机来充当 Pod。</li></ul><p>此外，需要注意的是，在 RunPodSandbox 这个接口的实现中，你还需要调用 networkPlugin.SetUpPod(…) 来为这个 Sandbox 设置网络。这个 SetUpPod(…) 方法，实际上就在执行 CNI 插件里的 add(…) 方法，也就是 CNI 插件为 Pod 创建网络，并且把 Infra 容器加入到网络中的操作。</p><p>接下来，kubelet 继续调用 CreateContainer 和 StartContainer 接口来创建和启动容器 A、B。对应到 dockershim 里，就是直接启动 A，B 两个 Docker 容器。所以最后，宿主机上会出现三个 Docker 容器组成这一个 Pod。</p><p>而如果是 Kata Containers 的话，CreateContainer 和 StartContainer 接口的实现，就只会在前面创建的轻量级虚拟机里创建两个 A、B 容器对应的 Mount Namespace。所以，最后在宿主机上，只会有一个叫作 foo 的轻量级虚拟机在运行。</p><p>除了上述对容器生命周期的实现之外，CRI shim 还有一个重要的工作，就是如何实现 exec、logs 等接口。这些接口跟前面的操作有一个很大的不同，就是这些 gRPC 接口调用期间，kubelet 需要跟容器项目维护一个长连接来传输数据。这种 API，我们就称之为 Streaming API。</p><h2 id="Streaming-API-的原理"><a href="#Streaming-API-的原理" class="headerlink" title="Streaming API 的原理"></a>Streaming API 的原理</h2><p>CRI shim 里对 Streaming API 的实现，依赖于一套独立的 Streaming Server 机制。这一部分原理，可以用如下所示的示意图来为你描述。</p><img src="/img/body/jike/a8e7ff6a6b0c9591a0a4f2b8e9e9bdef.png" alt="Streaming API 的原理示意图" style="zoom: 67%;max-width: 80%;" /><p>Kubectl exec 执行过程：</p><ol><li>当我们对一个容器执行 kubectl exec 命令的时候，这个请求首先交给 API Server</li><li>然后 API Server 就会调用 kubelet 的 Exec API。</li><li>这时，kubelet 就会调用 CRI 的 Exec 接口，而负责响应这个接口的，自然就是具体的 CRI shim。</li><li>在这一步，CRI shim 并不会直接去调用后端的容器项目（比如 Docker ）来进行处理，而只会返回一个 URL 给 kubelet。这个 URL，就是该 CRI shim 对应的 Streaming Server 的地址和端口。</li><li>而 kubelet 在拿到这个 URL 之后，就会把它以 Redirect 的方式返回给 API Server。</li><li>所以这时候，API Server 就会通过重定向来向 Streaming Server 发起真正的 /exec 请求，与它建立长连接。</li></ol><p>当然，这个 Streaming Server 本身，是需要通过使用 SIG-Node 为你维护的 Streaming API 库来实现的。并且，Streaming Server 会在 CRI shim 启动时就一起启动。此外，Stream Server 这一部分具体怎么实现，由 CRI shim 的维护者自行决定。比如，对于 Docker 项目来说，dockershim 就是直接调用 Docker 的 Exec API 来作为实现的。</p><p>以上，就是 CRI 的设计以及具体的工作原理了。</p><h1 id="47-绝不仅仅是安全：Kata-Containers-与-gVisor"><a href="#47-绝不仅仅是安全：Kata-Containers-与-gVisor" class="headerlink" title="47 | 绝不仅仅是安全：Kata Containers 与 gVisor"></a>47 | 绝不仅仅是安全：Kata Containers 与 gVisor</h1><h2 id="其他容器项目"><a href="#其他容器项目" class="headerlink" title="其他容器项目"></a>其他容器项目</h2><p>CRI 的设计，其中的一个重要推动力，就是基于虚拟化或者独立内核的安全容器项目的逐渐成熟。</p><p>使用虚拟化技术来做一个像 Docker 一样的容器项目：</p><ul><li><p>Kata Containers 项目：2017 年，借着 Kubernetes 的东风，Intel Clear Container 和 runV 项目合并，就成了现在大家耳熟能详的 Kata Containers 项目。由于 Kata Containers 的本质就是一个精简后的轻量级虚拟机，所以它的特点，就是“像虚拟机一样安全，像容器一样敏捷”。</p></li><li><p>gVisor 项目：2018 年，Google 公司则发布了一个名叫 gVisor 的项目。gVisor 项目给容器进程配置一个用 Go 语言实现的、运行在用户态的、极小的“独立内核”。这个内核对容器进程暴露 Linux 内核 ABI，扮演着“Guest Kernel”的角色，从而达到了将容器和宿主机隔离开的目的。</p></li></ul><p>不难看到，无论是 Kata Containers，还是 gVisor，它们实现安全容器的方法其实是殊途同归的。<strong>这两种容器实现的本质，都是给进程分配了一个独立的操作系统内核，从而避免了让容器共享宿主机的内核。</strong>这样，容器进程能够看到的信息，就从整个宿主机内核变成了一个极小的、独立的、以容器为单位的内核，从而有效解决了容器进程发生“逃逸”或者夺取整个宿主机的控制权的问题。这个原理，可以用如下所示的示意图来表示清楚。</p><img src="/img/body/jike/959c4c40c767acb6a3ffe6e144202e1d.png" alt="容器实现示意图" style="zoom:50%;max-width: 40%" /><p>Kata Containers 和 gVisor的区别：</p><ul><li><p>Kata Containers 使用的是传统的虚拟化技术，通过虚拟硬件模拟出了一台“小虚拟机”，然后在这个小虚拟机里安装了一个裁剪后的 Linux 内核来实现强隔离。</p></li><li><p>而 gVisor 的做法则更加激进，Google 的工程师直接用 Go 语言“模拟”出了一个运行在用户态的操作系统内核，然后通过这个模拟的内核来代替容器进程向宿主机发起有限的、可控的系统调用。</p></li></ul><h2 id="KataContainers-容器"><a href="#KataContainers-容器" class="headerlink" title="KataContainers 容器"></a>KataContainers 容器</h2><p>它的工作原理可以用如下所示的示意图来描述。</p><img src="/img/body/jike/8d7bbc8acaf27adff890f0be637df889.png" alt="KataContainers设计原理" style="zoom:67%;" /><p>Kata Containers 的本质，就是一个轻量化虚拟机。所以当你启动一个 Kata Containers 之后，你其实就会看到一个正常的虚拟机在运行。这也就意味着，一个标准的虚拟机管理程序（Virtual Machine Manager, VMM）是运行 Kata Containers 必备的一个组件。在我们上面图中，使用的 VMM 就是 Qemu。</p><p>而使用了虚拟机作为进程的隔离环境之后，Kata Containers 原生就带有了 Pod 的概念。即：这个 Kata Containers 启动的虚拟机，就是一个 Pod；而用户定义的容器，就是运行在这个轻量级虚拟机里的进程。在具体实现上，Kata Containers 的虚拟机里会有一个特殊的 Init 进程负责管理虚拟机里面的用户容器，并且只为这些容器开启 Mount Namespace。所以，这些用户容器之间，原生就是共享 Network 以及其他 Namespace 的。</p><p>此外，为了跟上层编排框架比如 Kubernetes 进行对接，Kata Containers 项目会启动一系列跟用户容器对应的 shim 进程，来负责操作这些用户容器的生命周期。当然，这些操作，实际上还是要靠虚拟机里的 Init 进程来帮你做到。</p><p>而在具体的架构上，Kata Containers 的实现方式同一个正常的虚拟机其实也非常类似。这里的原理，可以用如下所示的一幅示意图来表示。</p><img src="/img/body/jike/1684d0d89c170c2f8e6d050919c883f3.jpg" alt="Kata Containers的类实现架构" style="zoom:60%;max-width: 55%" /><p>可以看到，当 Kata Containers 运行起来之后，虚拟机里的用户进程（容器），实际上只能看到虚拟机里的、被裁减过的 Guest Kernel，以及通过 Hypervisor 虚拟出来的硬件设备。</p><p>而为了能够对这个虚拟机的 I/O 性能进行优化，Kata Containers 也会通过 vhost 技术（比如：vhost-user）来实现 Guest 与 Host 之间的高效的网络通信，并且使用 PCI Passthrough （PCI 穿透）技术来让 Guest 里的进程直接访问到宿主机上的物理设备。这些架构设计与实现，其实跟常规虚拟机的优化手段是基本一致的。</p><h2 id="gVisor-容器"><a href="#gVisor-容器" class="headerlink" title="gVisor 容器"></a>gVisor 容器</h2><p>它的原理，可以用如下所示的示意图来表示清楚。</p><img src="/img/body/jike/2f7903a7c494ddf6989d00c794bd7a7b.png" alt="gVisor 的设计原理" style="zoom:67%;" /><p>gVisor 工作的核心，在于它为应用进程（用户容器），启动了一个名叫 Sentry 的进程。 而 Sentry 进程的主要职责，就是提供一个传统的操作系统内核的能力，即：运行用户程序，执行系统调用。所以说，Sentry 并不是使用 Go 语言重新实现了一个完整的 Linux 内核，而只是一个对应用进程“冒充”内核的系统组件。</p><p>在这种设计思想下，Sentry 其实需要自己实现一个完整的 Linux 内核网络栈，以便处理应用进程的通信请求。然后，把封装好的二层帧直接发送给 Kubernetes 设置的 Pod 的 Network Namespace 即可。</p><p>而在具体的实现上，gVisor 的 Sentry 进程，其实还分为两种不同的实现方式。这里的工作原理，可以用下面的示意图来描述清楚。</p><img src="/img/body/jike/5a1d6e0291306417864033b3f40f74b8.png" alt="Sentry 进程第一种实现方式" style="zoom:67%; max-width: 50%;" /><h3 id="Sentry-进程的第一种实现方式"><a href="#Sentry-进程的第一种实现方式" class="headerlink" title="Sentry 进程的第一种实现方式"></a>Sentry 进程的第一种实现方式</h3><p>使用 Ptrace 机制来拦截用户应用的系统调用（System Call），然后把这些系统调用交给 Sentry 来进行处理。</p><p>这个过程，对于应用进程来说，是完全透明的。而 Sentry 接下来，则会扮演操作系统的角色，在用户态执行用户程序，然后仅在需要的时候，才向宿主机发起 Sentry 自己所需要执行的系统调用。这，就是 gVisor 对用户应用进程进行强隔离的主要手段。不过， Ptrace 进行系统调用拦截的性能实在是太差，仅能供 Demo 时使用。</p><h3 id="Sentry-进程的第二种实现方式"><a href="#Sentry-进程的第二种实现方式" class="headerlink" title="Sentry 进程的第二种实现方式"></a>Sentry 进程的第二种实现方式</h3><p>这种方式则更加具有普适性。它的工作原理如下图所示。</p><img src="/img/body/jike/3faf90550425378be91eb8cd2f0c63bf.png" alt="Sentry 进程第二种实现方式" style="zoom:67%;max-width: 50%;" /><p>在这种实现里，Sentry 会使用 KVM 来进行系统调用的拦截，这个性能比 Ptrace 就要好很多了。</p><p>当然，为了能够做到这一点，Sentry 进程就必须扮演一个 Guest Kernel 的角色，负责执行用户程序，发起系统调用。而这些系统调用被 KVM 拦截下来，还是继续交给 Sentry 进行处理。只不过在这时候，Sentry 就切换成了一个普通的宿主机进程的角色，来向宿主机发起它所需要的系统调用。</p><p>可以看到，在这种实现里，Sentry 并不会真的像虚拟机那样去虚拟出硬件设备、安装 Guest 操作系统。它只是借助 KVM 进行系统调用的拦截，以及处理地址空间切换等细节</p><p>值得一提的是，在 Google 内部，他们也是使用的第二种基于 Hypervisor 的 gVisor 实现。只不过 Google 内部有自己研发的 Hypervisor，所以要比 KVM 实现的性能还要好。但到目前为止，gVisor 的实现依然不是非常完善。</p><h2 id="Kata-Containers-与-gVisor对比"><a href="#Kata-Containers-与-gVisor对比" class="headerlink" title="Kata Containers 与 gVisor对比"></a>Kata Containers 与 gVisor对比</h2><p>在性能上，KataContainers 和 KVM 实现的 gVisor 不分伯仲，在启动速度和占用资源上，基于用户态内核的 gVisor 还略胜一筹。但是，对于系统调用密集的应用，比如重 I/O 或者重网络的应用，gVisor 就会因为需要频繁拦截系统调用而出现性能急剧下降的情况。此外，gVisor 由于要自己使用 Sentry 去模拟一个 Linux 内核，所以它能支持的系统调用是有限的，只是 Linux 系统调用的一个子集。</p><p>不过，gVisor 虽然现在没有任何优势，但是这种<strong>通过在用户态运行一个操作系统内核，来为应用进程提供强隔离的思路，</strong>的确是未来安全容器进一步演化的一个非常有前途的方向。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 深入剖析Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 深入剖析Kuernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>40-44.作业调度和资源管理</title>
      <link href="/2020/10/02/40-44-%E4%BD%9C%E4%B8%9A%E8%B0%83%E5%BA%A6%E5%92%8C%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/"/>
      <url>/2020/10/02/40-44-%E4%BD%9C%E4%B8%9A%E8%B0%83%E5%BA%A6%E5%92%8C%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>极客时间课程张磊的 <a href="https://time.geekbang.org/column/intro/116" target="_blank" rel="noopener">深入剖析Kuernetes</a> </p><h1 id="40-Kubernetes的资源模型与资源管理"><a href="#40-Kubernetes的资源模型与资源管理" class="headerlink" title="40 | Kubernetes的资源模型与资源管理"></a>40 | Kubernetes的资源模型与资源管理</h1><h2 id="资源模型"><a href="#资源模型" class="headerlink" title="资源模型"></a>资源模型</h2><p>所有跟调度和资源管理相关的属性都应该是属于 Pod 对象的字段。而这其中最重要的部分，就是 Pod 的 CPU 和内存配置，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: frontend</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: db</span><br><span class="line">    image: mysql</span><br><span class="line">    env:</span><br><span class="line">    - name: MYSQL_ROOT_PASSWORD</span><br><span class="line">      value: &quot;password&quot;</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: &quot;64Mi&quot;</span><br><span class="line">        cpu: &quot;250m&quot;</span><br><span class="line">      limits:</span><br><span class="line">        memory: &quot;128Mi&quot;</span><br><span class="line">        cpu: &quot;500m&quot;</span><br><span class="line">  - name: wp</span><br><span class="line">    image: wordpress</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: &quot;64Mi&quot;</span><br><span class="line">        cpu: &quot;250m&quot;</span><br><span class="line">      limits:</span><br><span class="line">        memory: &quot;128Mi&quot;</span><br><span class="line">        cpu: &quot;500m&quot;</span><br></pre></td></tr></table></figure><p><strong>资源分类</strong></p><ul><li>可压缩资源：比如CPU；当可压缩资源不足时，Pod 只会“饥饿”，但不会退出。</li><li>不可压缩资源：比如内存；当不可压缩资源不足时，Pod 就会因为 OOM（Out-Of-Memory）被内核杀掉。</li></ul><p>Kubernetes 允许你将 CPU 限额设置为分数，比如在我们的例子里，CPU limits 的值就是 500m。所谓 500m，指的就是 500 millicpu，也就是 0.5 个 CPU 。</p><p>而对于内存资源来说，它的单位自然就是 bytes。Kubernetes 支持你使用 Ei、Pi、Ti、Gi、Mi、Ki（或者 E、P、T、G、M、K）的方式来作为 bytes 的值。</p><blockquote><p> 注意区分 MiB（mebibyte）和 MB（megabyte）的区别。1Mi=1024*1024；1M=1000*1000</p></blockquote><p>Kubernetes 里 Pod 的 CPU 和内存资源，实际上还要分为 limits 和 requests 两种情况，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spec.containers[].resources.limits.cpu</span><br><span class="line">spec.containers[].resources.limits.memory</span><br><span class="line">spec.containers[].resources.requests.cpu</span><br><span class="line">spec.containers[].resources.requests.memory</span><br></pre></td></tr></table></figure><p>在调度的时候，kube-scheduler 只会按照 requests 的值进行计算。而在真正设置 Cgroups 限制的时候，kubelet 则会按照 limits 的值来进行设置。</p><h2 id="QoS-模型"><a href="#QoS-模型" class="headerlink" title="QoS 模型"></a>QoS 模型</h2><ul><li><p>Guaranteed：当 Pod 里的每一个 Container 都同时设置了 requests 和 limits，并且 requests 和 limits 值相等的时候。</p><p>当这个 Pod 创建之后，它的 qosClass 字段就会被 Kubernetes 自动设置为 Guaranteed。需要注意的是，当 Pod 仅设置了 limits 没有设置 requests 的时候，Kubernetes 会自动为它设置与 limits 相同的 requests 值，所以，这也属于 Guaranteed 情况。</p></li><li><p>Burstable：当 Pod 不满足 Guaranteed 的条件，但至少有一个 Container 设置了 requests。</p></li><li><p>BestEffort：一个 Pod 既没有设置 requests，也没有设置 limits</p></li></ul><p>QoS 模型作用： QoS 划分的主要应用场景，是当宿主机资源紧张的时候，kubelet 对 Pod 进行 Eviction（即资源回收）时需要用到的。</p><p>具体地说，当 Kubernetes 所管理的宿主机上不可压缩资源短缺时，就有可能触发 Eviction。比如，可用内存（memory.available）、可用的宿主机磁盘空间（nodefs.available），以及容器运行时镜像存储空间（imagefs.available）等等。</p><p>目前，Kubernetes 为你设置的 Eviction 的默认阈值如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">memory.available&lt;100Mi</span><br><span class="line">nodefs.available&lt;10%</span><br><span class="line">nodefs.inodesFree&lt;5%</span><br><span class="line">imagefs.available&lt;15%</span><br></pre></td></tr></table></figure><p>当然，上述各个触发条件在 kubelet 里都是可配置的。比如下面这个例子：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubelet --eviction-hard&#x3D;imagefs.available&lt;10%,memory.available&lt;500Mi,nodefs.available&lt;5%,nodefs.inodesFree&lt;5% --eviction-soft&#x3D;imagefs.available&lt;30%,nodefs.available&lt;10% --eviction-soft-grace-period&#x3D;imagefs.available&#x3D;2m,nodefs.available&#x3D;2m --eviction-max-pod-grace-period&#x3D;600</span><br></pre></td></tr></table></figure><p>Eviction 在 Kubernetes 里其实分为 Soft 和 Hard 两种模式。</p><ul><li>Soft Eviction：允许你为 Eviction 过程设置一段“优雅时间”，比如上面例子里的 imagefs.available=2m，就意味着当 imagefs 不足的阈值达到 2 分钟之后，kubelet 才会开始 Eviction 的过程。</li><li>Hard Eviction：Eviction 过程就会在阈值达到之后立刻开始。</li></ul><p>当宿主机的 Eviction 阈值达到后，就会进入 MemoryPressure 或者 DiskPressure 状态（节点打上了污点），从而避免新的 Pod 被调度到这台宿主机上。</p><p>而当 Eviction 发生的时候，kubelet 具体会挑选哪些 Pod 进行删除操作，就需要参考这些 Pod 的 QoS 类别了。</p><p>BestEffort &gt; Burstable &gt; Guaranteed</p><ul><li>首当其冲的，自然是 BestEffort 类别的 Pod。</li><li>其次，是属于 Burstable 类别、并且发生“饥饿”的资源使用量已经超出了 requests 的 Pod。</li><li>最后，才是 Guaranteed 类别。并且，Kubernetes 会保证只有当 Guaranteed 类别的 Pod 的资源使用量超过了其 limits 的限制，或者宿主机本身正处于 Memory Pressure 状态时，Guaranteed 的 Pod 才可能被选中进行 Eviction 操作。</li></ul><p>当然，对于同 QoS 类别的 Pod 来说，Kubernetes 还会根据 Pod 的优先级来进行进一步地排序和选择。</p><h2 id="cpuset-的设置"><a href="#cpuset-的设置" class="headerlink" title="cpuset 的设置"></a>cpuset 的设置</h2><p>在使用容器的时候，你可以通过设置 cpuset 把容器绑定到某个 CPU 的核上，而不是像 cpushare 那样共享 CPU 的计算能力。</p><p>这种情况下，由于操作系统在 CPU 之间进行上下文切换的次数大大减少，容器里应用的性能会得到大幅提升。事实上，<strong>cpuset 方式，是生产环境里部署在线应用类型的 Pod 时，非常常用的一种方式。</strong></p><p><strong>条件：</strong></p><ul><li>首先，你的 Pod 必须是 Guaranteed 的 QoS 类型；</li><li>然后，你只需要将 Pod 的 CPU 资源的 requests 和 limits 设置为同一个相等的<strong>整数值</strong>即可。</li></ul><h1 id="41-十字路口上的Kubernetes默认调度器"><a href="#41-十字路口上的Kubernetes默认调度器" class="headerlink" title="41 | 十字路口上的Kubernetes默认调度器"></a>41 | 十字路口上的Kubernetes默认调度器</h1><p>在 Kubernetes 项目中，默认调度器的主要职责，就是为一个新创建出来的 Pod，寻找一个最合适的节点（Node）。</p><p>而这里“最合适”的含义，包括两层：</p><ol><li>从集群所有的节点中，根据调度算法挑选出所有可以运行该 Pod 的节点；</li><li>从第一步的结果中，再根据调度算法挑选一个最符合条件的节点作为最终结果。</li></ol><p>所以在具体的调度流程中，默认调度器会首先调用一组叫作 Predicate 的调度算法，来检查每个 Node。然后，再调用一组叫作 Priority 的调度算法，来给上一步得到的结果里的每个 Node 打分。最终的调度结果，就是得分最高的那个 Node。</p><p>调度器对一个 Pod 调度成功，实际上就是将它的 spec.nodeName 字段填上调度结果的节点名字。</p><p>上述调度机制的工作原理，可以用如下所示的一幅示意图来表示：</p><img src="/img/body/jike/bb95a7d4962c95d703f7c69caf53ca53.jpg" alt="调度机制的工作原理" style="zoom:67%;max-width: 70%" /><p>可以看到，Kubernetes 的调度器的核心，实际上就是两个相互独立的控制循环。</p><p>第一个控制循环，我们可以称之为 Informer Path。它的主要目的，是启动一系列 Informer，用来监听（Watch）Etcd 中 Pod、Node、Service 等与调度相关的 API 对象的变化。比如，当一个待调度 Pod（即：它的 nodeName 字段是空的）被创建出来之后，调度器就会通过 Pod Informer 的 Handler，将这个待调度 Pod 添加进调度队列。</p><p>在默认情况下，Kubernetes 的调度队列是一个 PriorityQueue（优先级队列），并且当某些集群信息发生变化的时候，调度器还会对调度队列里的内容进行一些特殊操作。这里的设计，主要是出于调度优先级和抢占的考虑，</p><p>此外，Kubernetes 的默认调度器还要负责对调度器缓存（即：scheduler cache）进行更新。事实上，Kubernetes 调度部分进行性能优化的一个最根本原则，就是尽最大可能将集群信息 Cache 化，以便从根本上提高 Predicate 和 Priority 调度算法的执行效率。</p><p>第二个控制循环，是调度器负责 Pod 调度的主循环，我们可以称之为 Scheduling Path。</p><p>Scheduling Path 的主要逻辑，就是不断地从调度队列里出队一个 Pod。然后，调用 Predicates 算法进行“过滤”。这一步“过滤”得到的一组 Node，就是所有可以运行这个 Pod 的宿主机列表。当然，Predicates 算法需要的 Node 信息，都是从 Scheduler Cache 里直接拿到的，这是调度器保证算法执行效率的主要手段之一。</p><p>接下来，调度器就会再调用 Priorities 算法为上述列表里的 Node 打分，分数从 0 到 10。得分最高的 Node，就会作为这次调度的结果。</p><p>调度算法执行完成后，调度器就需要将 Pod 对象的 nodeName 字段的值，修改为上述 Node 的名字。<strong>这个步骤在 Kubernetes 里面被称作 Bind。</strong></p><p>但是，为了不在关键调度路径里远程访问 APIServer，Kubernetes 的默认调度器在 Bind 阶段，只会更新 Scheduler Cache 里的 Pod 和 Node 的信息。<strong>这种基于“乐观”假设的 API 对象更新方式，在 Kubernetes 里被称作 Assume。</strong></p><p>Assume 之后，调度器才会创建一个 Goroutine 来异步地向 APIServer 发起更新 Pod 的请求，来真正完成 Bind 操作。如果这次异步的 Bind 过程失败了，其实也没有太大关系，等 Scheduler Cache 同步之后一切就会恢复正常。</p><p><strong>除了上述的“Cache 化”和“乐观绑定”，Kubernetes 默认调度器还有一个重要的设计，那就是“无锁化”。</strong></p><p>在 Scheduling Path 上，调度器会启动多个 Goroutine 以节点为粒度并发执行 Predicates 算法，从而提高这一阶段的执行效率。而与之类似的，Priorities 算法也会以 MapReduce 的方式并行计算然后再进行汇总。而在这些所有需要并发的路径上，调度器会避免设置任何全局的竞争资源，从而免去了使用锁进行同步带来的巨大的性能损耗。</p><p>所以，在这种思想的指导下，如果你再去查看一下前面的调度器原理图，你就会发现，Kubernetes 调度器只有对调度队列和 Scheduler Cache 进行操作时，才需要加锁。而这两部分操作，都不在 Scheduling Path 的算法执行路径上。</p><p>当然，Kubernetes 调度器的上述设计思想，也是在集群规模不断增长的演进过程中逐步实现的。尤其是 “Cache 化”，这个变化其实是最近几年 Kubernetes 调度器性能得以提升的一个关键演化。</p><p>而 Kubernetes 默认调度器的可扩展性设计，可以用如下所示的一幅示意图来描述：</p><img src="/img/body/jike/fd17097799fe17fcbc625bf178496acd.jpg" alt="默认调度器的可扩展性设计" style="zoom:67%;" /><p>可以看到，默认调度器的可扩展机制，在 Kubernetes 里面叫作 Scheduler Framework。顾名思义，这个设计的主要目的，就是在调度器生命周期的各个关键点上，为用户暴露出可以进行扩展和实现的接口，从而实现由用户自定义调度器的能力。</p><p>需要注意的是，上述这些可插拔式逻辑，都是标准的 Go 语言插件机制（Go plugin 机制），也就是说，你需要在编译的时候选择把哪些插件编译进去。</p><h1 id="42-Kubernetes默认调度器调度策略解析"><a href="#42-Kubernetes默认调度器调度策略解析" class="headerlink" title="42 | Kubernetes默认调度器调度策略解析"></a>42 | Kubernetes默认调度器调度策略解析</h1><h2 id="Predicates"><a href="#Predicates" class="headerlink" title="Predicates"></a>Predicates</h2><p><strong>Predicates 在调度过程中的作用，可以理解为 Filter，</strong>即：它按照调度策略，从当前集群的所有节点中，“过滤”出一系列符合条件的节点。这些节点，都是可以运行待调度 Pod 的宿主机。</p><p>而在 Kubernetes 中，默认的调度策略有如下三种。</p><h3 id="第一种类型，叫作-GeneralPredicates。"><a href="#第一种类型，叫作-GeneralPredicates。" class="headerlink" title="第一种类型，叫作 GeneralPredicates。"></a>第一种类型，叫作 GeneralPredicates。</h3><p>顾名思义，这一组过滤规则，负责的是最基础的调度策略。比如，PodFitsResources 计算的就是宿主机的 CPU 和内存资源等是否够用。</p><p>当然，我在前面已经提到过，PodFitsResources 检查的只是 Pod 的 requests 字段。需要注意的是，Kubernetes 的调度器并没有为 GPU 等硬件资源定义具体的资源类型，而是统一用一种名叫 Extended Resource 的、Key-Value 格式的扩展字段来描述的。</p><p>而 PodFitsHost 检查的是，宿主机的名字是否跟 Pod 的 spec.nodeName 一致。</p><p>PodFitsHostPorts 检查的是，Pod 申请的宿主机端口（spec.nodePort）是不是跟已经被使用的端口有冲突。</p><p>PodMatchNodeSelector 检查的是，Pod 的 nodeSelector 或者 nodeAffinity 指定的节点，是否与待考察节点匹配，等等。</p><p>像上面这样一组 GeneralPredicates，正是 Kubernetes 考察一个 Pod 能不能运行在一个 Node 上最基本的过滤条件。所以，GeneralPredicates 也会被其他组件（比如 kubelet）直接调用。</p><p>kubelet 在启动 Pod 前，会执行一个 Admit 操作来进行二次确认。这里二次确认的规则，就是执行一遍 GeneralPredicates。</p><h3 id="第二种类型，是与-Volume-相关的过滤规则。"><a href="#第二种类型，是与-Volume-相关的过滤规则。" class="headerlink" title="第二种类型，是与 Volume 相关的过滤规则。"></a>第二种类型，是与 Volume 相关的过滤规则。</h3><p>这一组过滤规则，负责的是跟容器持久化 Volume 相关的调度策略。</p><p>其中，NoDiskConflict 检查的条件，是多个 Pod 声明挂载的持久化 Volume 是否有冲突。</p><p>而 MaxPDVolumeCountPredicate 检查的条件，则是一个节点上某种类型的持久化 Volume 是不是已经超过了一定数目，如果是的话，那么声明使用该类型持久化 Volume 的 Pod 就不能再调度到这个节点了。</p><p>而 VolumeZonePredicate，则是检查持久化 Volume 的 Zone（高可用域）标签，是否与待考察节点的 Zone 标签相匹配。</p><p>此外，这里还有一个叫作 VolumeBindingPredicate 的规则。它负责检查的，是该 Pod 对应的 PV 的 nodeAffinity 字段，是否跟某个节点的标签相匹配。</p><p>在 Predicates 阶段，Kubernetes 就必须能够根据 Pod 的 Volume 属性来进行调度。此外，如果该 Pod 的 PVC 还没有跟具体的 PV 绑定的话，调度器还要负责检查所有待绑定 PV，当有可用的 PV 存在并且该 PV 的 nodeAffinity 与待考察节点一致时，这条规则才会返回“成功”。</p><h3 id="第三种类型，是宿主机相关的过滤规则。"><a href="#第三种类型，是宿主机相关的过滤规则。" class="headerlink" title="第三种类型，是宿主机相关的过滤规则。"></a>第三种类型，是宿主机相关的过滤规则。</h3><p>这一组规则，主要考察待调度 Pod 是否满足 Node 本身的某些条件。</p><p>比如，PodToleratesNodeTaints，负责检查的就是我们前面经常用到的 Node 的“污点”机制。只有当 Pod 的 Toleration 字段与 Node 的 Taint 字段能够匹配的时候，这个 Pod 才能被调度到该节点上。</p><p>而 NodeMemoryPressurePredicate，检查的是当前节点的内存是不是已经不够充足，如果是的话，那么待调度 Pod 就不能被调度到该节点上。</p><h3 id="第四种类型，是-Pod-相关的过滤规则。"><a href="#第四种类型，是-Pod-相关的过滤规则。" class="headerlink" title="第四种类型，是 Pod 相关的过滤规则。"></a>第四种类型，是 Pod 相关的过滤规则。</h3><p>这一组规则，跟 GeneralPredicates 大多数是重合的。而比较特殊的，是 PodAffinityPredicate。这个规则的作用，是检查待调度 Pod 与 Node 上的已有 Pod 之间的亲密（affinity）和反亲密（anti-affinity）关系。</p><p>上面这四种类型的 Predicates，就构成了调度器确定一个 Node 可以运行待调度 Pod 的基本策略。</p><p>在具体执行的时候， 当开始调度一个 Pod 时，Kubernetes 调度器会同时启动 16 个 Goroutine，来并发地为集群里的所有 Node 计算 Predicates，最后返回可以运行这个 Pod 的宿主机列表。</p><h2 id="Priorities"><a href="#Priorities" class="headerlink" title="Priorities"></a>Priorities</h2><p>​    </p><p>在 Predicates 阶段完成了节点的“过滤”之后，Priorities 阶段的工作就是为这些节点打分。这里打分的范围是 0-10 分，得分最高的节点就是最后被 Pod 绑定的最佳节点。</p><p>Priorities 里最常用到的一个打分规则，是 LeastRequestedPriority。它的计算方法，可以简单地总结为如下所示的公式：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">score &#x3D; (cpu((capacity-sum(requested))10&#x2F;capacity) + memory((capacity-sum(requested))10&#x2F;capacity))&#x2F;2</span><br></pre></td></tr></table></figure><p>可以看到，这个算法实际上就是在选择空闲资源（CPU 和 Memory）最多的宿主机。</p><p>而与 LeastRequestedPriority 一起发挥作用的，还有 BalancedResourceAllocation。它的计算公式如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">score &#x3D; 10 - variance(cpuFraction,memoryFraction,volumeFraction)*10</span><br></pre></td></tr></table></figure><p>其中，每种资源的 Fraction 的定义是 ：Pod 请求的资源 / 节点上的可用资源。而 variance 算法的作用，则是计算每两种资源 Fraction 之间的“距离”。而最后选择的，则是资源 Fraction 差距最小的节点。</p><p>所以说，BalancedResourceAllocation 选择的，其实是调度完成后，所有节点里各种资源分配最均衡的那个节点，从而避免一个节点上 CPU 被大量分配、而 Memory 大量剩余的情况。</p><p>此外，还有 NodeAffinityPriority、TaintTolerationPriority 和 InterPodAffinityPriority 这三种 Priority。顾名思义，它们与前面的 PodMatchNodeSelector、PodToleratesNodeTaints 和 PodAffinityPredicate 这三个 Predicate 的含义和计算方法是类似的。但是作为 Priority，一个 Node 满足上述规则的字段数目越多，它的得分就会越高。</p><p>在默认 Priorities 里，还有一个叫作 ImageLocalityPriority 的策略。它是在 Kubernetes v1.12 里新开启的调度规则，即：如果待调度 Pod 需要使用的镜像很大，并且已经存在于某些 Node 上，那么这些 Node 的得分就会比较高。</p><p>当然，为了避免这个算法引发调度堆叠，调度器在计算得分的时候还会根据镜像的分布进行优化，即：如果大镜像分布的节点数目很少，那么这些节点的权重就会被调低，从而“对冲”掉引起调度堆叠的风险。</p><p>以上，就是 Kubernetes 调度器的 Predicates 和 Priorities 里默认调度规则的主要工作原理了。</p><p><strong>在实际的执行过程中，调度器里关于集群和 Pod 的信息都已经缓存化，所以这些算法的执行过程还是比较快的。</strong></p><p>此外，对于比较复杂的调度算法来说，比如 PodAffinityPredicate，它们在计算的时候不只关注待调度 Pod 和待考察 Node，还需要关注整个集群的信息，比如，遍历所有节点，读取它们的 Labels。这时候，Kubernetes 调度器会在为每个待调度 Pod 执行该调度算法之前，先将算法需要的集群信息初步计算一遍，然后缓存起来。这样，在真正执行该算法的时候，调度器只需要读取缓存信息进行计算即可，从而避免了为每个 Node 计算 Predicates 的时候反复获取和计算整个集群的信息。</p><h1 id="43-Kubernetes默认调度器的优先级与抢占机制"><a href="#43-Kubernetes默认调度器的优先级与抢占机制" class="headerlink" title="43 | Kubernetes默认调度器的优先级与抢占机制"></a>43 | Kubernetes默认调度器的优先级与抢占机制</h1><p>优先级（Priority ）和抢占（Preemption）机制。</p><p>首先需要明确的是，优先级和抢占机制，解决的是 Pod 调度失败时该怎么办的问题。</p><h2 id="优先级"><a href="#优先级" class="headerlink" title="优先级"></a>优先级</h2><p>正常情况下，当一个 Pod 调度失败后，它就会被暂时“搁置”起来，直到 Pod 被更新，或者集群状态发生变化，调度器才会对这个 Pod 进行重新调度。</p><p>当一个高优先级的 Pod 调度失败后，该 Pod 并不会被“搁置”，而是会“挤走”某个 Node 上的一些低优先级的 Pod 。这样就可以保证这个高优先级 Pod 的调度成功。</p><p>要使用这个机制，你首先需要在 Kubernetes 里提交一个 PriorityClass 的定义，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: scheduling.k8s.io&#x2F;v1beta1</span><br><span class="line">kind: PriorityClass</span><br><span class="line">metadata:</span><br><span class="line">  name: high-priority</span><br><span class="line">value: 1000000</span><br><span class="line">globalDefault: false</span><br><span class="line">description: &quot;This priority class should be used for high priority service pods only.&quot;</span><br></pre></td></tr></table></figure><p>上面这个 YAML 文件，定义的是一个名叫 high-priority 的 PriorityClass，其中 value 的值是 1000000 （一百万）。</p><p>Kubernetes 规定，优先级是一个 32 bit 的整数，最大值不超过 1000000000（10 亿，1 billion），并且值越大代表优先级越高。而超出 10 亿的值，其实是被 Kubernetes 保留下来分配给系统 Pod 使用的。显然，这样做的目的，就是保证系统 Pod 不会被用户抢占掉。</p><p>而一旦上述 YAML 文件里的 globalDefault 被设置为 true 的话，那就意味着这个 PriorityClass 的值会成为系统的默认值。而如果这个值是 false，就表示我们只希望声明使用该 PriorityClass 的 Pod 拥有值为 1000000 的优先级，而对于没有声明 PriorityClass 的 Pod 来说，它们的优先级就是 0。</p><p>在创建了 PriorityClass 对象之后，Pod 就可以声明使用它了，如下所示</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  labels:</span><br><span class="line">    env: test</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">  priorityClassName: high-priority</span><br></pre></td></tr></table></figure><p>可以看到，这个 Pod 通过 priorityClassName 字段，声明了要使用名叫 high-priority 的 PriorityClass。当这个 Pod 被提交给 Kubernetes 之后，Kubernetes 的 PriorityAdmissionController 就会自动将这个 Pod 的 spec.priority 字段设置为 1000000。</p><p>调度器里维护着一个调度队列。所以，当 Pod 拥有了优先级之后，高优先级的 Pod 就可能会比低优先级的 Pod 提前出队，从而尽早完成调度过程。<strong>这个过程，就是“优先级”这个概念在 Kubernetes 里的主要体现。</strong></p><h2 id="抢占机制"><a href="#抢占机制" class="headerlink" title="抢占机制"></a>抢占机制</h2><p>而当一个高优先级的 Pod 调度失败的时候，调度器的抢占能力就会被触发。这时，调度器就会试图从当前集群里寻找一个节点，使得当这个节点上的一个或者多个低优先级 Pod 被删除后，待调度的高优先级 Pod 就可以被调度到这个节点上。<strong>这个过程，就是“抢占”这个概念在 Kubernetes 里的主要体现。</strong></p><p>当上述抢占过程发生时，抢占者并不会立刻被调度到被抢占的 Node 上。事实上，调度器只会将抢占者的 spec.nominatedNodeName 字段，设置为被抢占的 Node 的名字。然后，抢占者会重新进入下一个调度周期，然后在新的调度周期里来决定是不是要运行在被抢占的节点上。这当然也就意味着，即使在下一个调度周期，调度器也不会保证抢占者一定会运行在被抢占的节点上。</p><p>这样设计的一个重要原因是，调度器只会通过标准的 DELETE API 来删除被抢占的 Pod，所以，这些 Pod 必然是有一定的“优雅退出”时间（默认是 30s）的。而在这段时间里，其他的节点也是有可能变成可调度的，或者直接有新的节点被添加到这个集群中来。所以，鉴于优雅退出期间，集群的可调度性可能会发生的变化，<strong>把抢占者交给下一个调度周期再处理，是一个非常合理的选择。</strong></p><p>而在抢占者等待被调度的过程中，如果有其他更高优先级的 Pod 也要抢占同一个节点，那么调度器就会清空原抢占者的 spec.nominatedNodeName 字段，从而允许更高优先级的抢占者执行抢占，并且，这也就使得原抢占者本身，也有机会去重新抢占其他节点。这些，都是设置 nominatedNodeName 字段的主要目的。</p><h2 id="调度器里的抢占机制"><a href="#调度器里的抢占机制" class="headerlink" title="调度器里的抢占机制"></a>调度器里的抢占机制</h2><p>抢占发生的原因，一定是一个高优先级的 Pod 调度失败。这一次，我们还是称这个 Pod 为“抢占者”，称被抢占的 Pod 为“牺牲者”（victims）。</p><p>而 Kubernetes 调度器实现抢占算法的一个最重要的设计，就是在调度队列的实现里，使用了两个不同的队列。</p><p><strong>第一个队列，叫作 activeQ。</strong>凡是在 activeQ 里的 Pod，都是下一个调度周期需要调度的对象。所以，当你在 Kubernetes 集群里新创建一个 Pod 的时候，调度器会将这个 Pod 入队到 activeQ 里面。而我在前面提到过的、调度器不断从队列里出队（Pop）一个 Pod 进行调度，实际上都是从 activeQ 里出队的。</p><p><strong>第二个队列，叫作 unschedulableQ，</strong>专门用来存放调度失败的 Pod。</p><p>而这里的一个关键点就在于，当一个 unschedulableQ 里的 Pod 被更新之后，调度器会自动把这个 Pod 移动到 activeQ 里，从而给这些调度失败的 Pod “重新做人”的机会。</p><p>现在，回到我们的抢占者调度失败这个时间点上来。</p><p>调度失败之后，抢占者就会被放进 unschedulableQ 里面。</p><p>然后，这次失败事件就会触发调度器为抢占者寻找牺牲者的流程。</p><ol><li><p>第一步，调度器会检查这次失败事件的原因，来确认抢占是不是可以帮助抢占者找到一个新节点。这是因为有很多 Predicates 的失败是不能通过抢占来解决的。比如，PodFitsHost 算法（负责的是，检查 Pod 的 nodeSelector 与 Node 的名字是否匹配），这种情况下，除非 Node 的名字发生变化，否则你即使删除再多的 Pod，抢占者也不可能调度成功。</p></li><li><p>第二步，如果确定抢占可以发生，那么调度器就会把自己缓存的所有节点信息复制一份，然后使用这个副本来模拟抢占过程。</p></li></ol><p>这里的抢占过程很容易理解。调度器会检查缓存副本里的每一个节点，然后从该节点上最低优先级的 Pod 开始，逐一“删除”这些 Pod。而每删除一个低优先级 Pod，调度器都会检查一下抢占者是否能够运行在该 Node 上。一旦可以运行，调度器就记录下这个 Node 的名字和被删除 Pod 的列表，这就是一次抢占过程的结果了。</p><p>当遍历完所有的节点之后，调度器会在上述模拟产生的所有抢占结果里做一个选择，找出最佳结果。而这一步的判断原则，就是尽量减少抢占对整个系统的影响。比如，需要抢占的 Pod 越少越好，需要抢占的 Pod 的优先级越低越好，等等。</p><p>在得到了最佳的抢占结果之后，这个结果里的 Node，就是即将被抢占的 Node；被删除的 Pod 列表，就是牺牲者。所以接下来，调度器就可以真正开始抢占的操作了，这个过程，可以分为三步。</p><ol><li>第一步，调度器会检查牺牲者列表，清理这些 Pod 所携带的 nominatedNodeName 字段。</li><li>第二步，调度器会把抢占者的 nominatedNodeName，设置为被抢占的 Node 的名字。\</li><li>第三步，调度器会开启一个 Goroutine，同步地删除牺牲者。</li></ol><p>而第二步对抢占者 Pod 的更新操作，就会触发到我前面提到的“重新做人”的流程，从而让抢占者在下一个调度周期重新进入调度流程。</p><p>所以<strong>接下来，调度器就会通过正常的调度流程把抢占者调度成功。</strong>这也是为什么，我前面会说调度器并不保证抢占的结果：在这个正常的调度流程里，是一切皆有可能的。</p><p>不过，对于任意一个待调度 Pod 来说，因为有上述抢占者的存在，它的调度过程，其实是有一些特殊情况需要特殊处理的。</p><p>具体来说，在为某一对 Pod 和 Node 执行 Predicates 算法的时候，如果待检查的 Node 是一个即将被抢占的节点，即：调度队列里有 nominatedNodeName 字段值是该 Node 名字的 Pod 存在（可以称之为：“潜在的抢占者”）。那么，调度器就会对这个 Node ，将同样的 Predicates 算法运行两遍。</p><p>第一遍， 调度器会假设上述“潜在的抢占者”已经运行在这个节点上，然后执行 Predicates 算法；</p><p>第二遍， 调度器会正常执行 Predicates 算法，即：不考虑任何“潜在的抢占者”。</p><p>而只有这两遍 Predicates 算法都能通过时，这个 Pod 和 Node 才会被认为是可以绑定（bind）的。</p><p>不难想到，这里需要执行第一遍 Predicates 算法的原因，是由于 InterPodAntiAffinity 规则的存在。</p><p>由于 InterPodAntiAffinity 规则关心待考察节点上所有 Pod 之间的互斥关系，所以我们在执行调度算法时必须考虑，如果抢占者已经存在于待考察 Node 上时，待调度 Pod 还能不能调度成功。</p><p>当然，这也就意味着，我们在这一步只需要考虑那些优先级等于或者大于待调度 Pod 的抢占者。毕竟对于其他较低优先级 Pod 来说，待调度 Pod 总是可以通过抢占运行在待考察 Node 上。</p><p>而我们需要执行第二遍 Predicates 算法的原因，则是因为“潜在的抢占者”最后不一定会运行在待考察的 Node 上。关于这一点，我在前面已经讲解过了：Kubernetes 调度器并不保证抢占者一定会运行在当初选定的被抢占的 Node 上。</p><h1 id="44-Kubernetes-GPU管理与Device-Plugin机制"><a href="#44-Kubernetes-GPU管理与Device-Plugin机制" class="headerlink" title="44 | Kubernetes GPU管理与Device Plugin机制"></a>44 | Kubernetes GPU管理与Device Plugin机制</h1><p>首先需要解释CPU和GPU这两个缩写分别代表什么。CPU即中央处理器，GPU即图形处理器。其次，要解释两者的区别，要先明白两者的相同之处：两者都有总线和外界联系，有自己的缓存体系，以及数字和逻辑运算单元。一句话，两者都为了完成计算任务而设计。</p><p>CPU和GPU之所以大不相同，是由于其设计目标的不同，它们分别针对了两种不同的应用场景。CPU需要很强的通用性来处理各种不同的数据类型，同时又要逻辑判断又会引入大量的分支跳转和中断的处理。这些都使得CPU的内部结构异常复杂。而GPU面对的则是类型高度统一的、相互无依赖的大规模数据和不需要被打断的纯净的计算环境。</p><p>于是CPU和GPU就呈现出非常不同的架构（示意图）：</p><p><img src="/img/body/jike/918367f36e34c18dc1f92bd16760dae1_1440w.jpg" alt="img"></p><p>图片来自nVidia CUDA文档。其中绿色的是计算单元，橙红色的是存储单元，橙黄色的是控制单元。</p><p>GPU采用了数量众多的计算单元和超长的流水线，但只有非常简单的控制逻辑并省去了Cache。而CPU不仅被Cache占据了大量空间，而且还有有复杂的控制逻辑和诸多优化电路，相比之下计算能力只是CPU很小的一部分</p><p><strong>什么类型的程序适合在GPU上运行？</strong></p><p>　　（1）计算密集型的程序。所谓计算密集型(Compute-intensive)的程序，就是其大部分运行时间花在了寄存器运算上，寄存器的速度和处理器的速度相当，从寄存器读写数据几乎没有延时。可以做一下对比，读内存的延迟大概是几百个时钟周期；读硬盘的速度就不说了，即便是SSD, 也实在是太慢了。</p><p>　　（2）易于并行的程序。GPU其实是一种SIMD(Single Instruction Multiple Data)架构， 他有成百上千个核，每一个核在同一时间最好能做同样的事情。</p><p>GPU 支持对于 Kubernetes 项目来说，其实也有着超过技术本身的考虑。所以，尽管在硬件加速器这个领域里，Kubernetes 上游有着不少来自 NVIDIA 和 Intel 等芯片厂商的工程师，但这个特性本身，却从一开始就是以 Google Cloud 的需求为主导来推进的。</p><p>而对于云的用户来说，在 GPU 的支持上，他们最基本的诉求其实非常简单：我只要在 Pod 的 YAML 里面，声明某容器需要的 GPU 个数，那么 Kubernetes 为我创建的容器里就应该出现对应的 GPU 设备，以及它对应的驱动目录。</p><p>以 NVIDIA 的 GPU 设备为例，上面的需求就意味着当用户的容器被创建之后，这个容器里必须出现如下两部分设备和目录：</p><ol><li>GPU 设备，比如 /dev/nvidia0；</li><li>GPU 驱动目录，比如 /usr/local/nvidia/*。</li></ol><p>其中，GPU 设备路径，正是该容器启动时的 Devices 参数；而驱动目录，则是该容器启动时的 Volume 参数。所以，在 Kubernetes 的 GPU 支持的实现里，kubelet 实际上就是将上述两部分内容，设置在了创建该容器的 CRI （Container Runtime Interface）参数里面。这样，等到该容器启动之后，对应的容器里就会出现 GPU 设备和驱动的路径了。</p><p>不过，Kubernetes 在 Pod 的 API 对象里，并没有为 GPU 专门设置一个资源类型字段，而是使用了一种叫作 Extended Resource（ER）的特殊字段来负责传递 GPU 的信息。比如下面这个例子：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: cuda-vector-add</span><br><span class="line">spec:</span><br><span class="line">  restartPolicy: OnFailure</span><br><span class="line">  containers:</span><br><span class="line">    - name: cuda-vector-add</span><br><span class="line">      image: &quot;k8s.gcr.io&#x2F;cuda-vector-add:v0.1&quot;</span><br><span class="line">      resources:</span><br><span class="line">        limits:</span><br><span class="line">          nvidia.com&#x2F;gpu: 1</span><br></pre></td></tr></table></figure><p>可以看到，在上述 Pod 的 limits 字段里，这个资源的名称是nvidia.com/gpu，它的值是 1。也就是说，这个 Pod 声明了自己要使用一个 NVIDIA 类型的 GPU。</p><p>而在 kube-scheduler 里面，它其实并不关心这个字段的具体含义，只会在计算的时候，一律将调度器里保存的该类型资源的可用量，直接减去 Pod 声明的数值即可。所以说，Extended Resource，其实是 Kubernetes 为用户设置的一种对自定义资源的支持。</p><p>当然，为了能够让调度器知道这个自定义类型的资源在每台宿主机上的可用量，宿主机节点本身，就必须能够向 API Server 汇报该类型资源的可用数量。在 Kubernetes 里，各种类型的资源可用量，其实是 Node 对象 Status 字段的内容，比如下面这个例子：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Node</span><br><span class="line">metadata:</span><br><span class="line">  name: node-1</span><br><span class="line">...</span><br><span class="line">Status:</span><br><span class="line">  Capacity:</span><br><span class="line">   cpu:  2</span><br><span class="line">   memory:  2049008Ki</span><br></pre></td></tr></table></figure><p>而为了能够在上述 Status 字段里添加自定义资源的数据，你就必须使用 PATCH API 来对该 Node 对象进行更新，加上你的自定义资源的数量。这个 PATCH 操作，可以简单地使用 curl 命令来发起，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动 Kubernetes 的客户端 proxy，这样你就可以直接使用 curl 来跟 Kubernetes  的API Server 进行交互了</span><br><span class="line">$ kubectl proxy</span><br><span class="line"></span><br><span class="line"># 执行 PACTH 操作</span><br><span class="line">$ curl --header &quot;Content-Type: application&#x2F;json-patch+json&quot; \</span><br><span class="line">--request PATCH \</span><br><span class="line">--data &#39;[&#123;&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;&#x2F;status&#x2F;capacity&#x2F;nvidia.com&#x2F;gpu&quot;, &quot;value&quot;: &quot;1&quot;&#125;]&#39; \</span><br><span class="line">http:&#x2F;&#x2F;localhost:8001&#x2F;api&#x2F;v1&#x2F;nodes&#x2F;&lt;your-node-name&gt;&#x2F;status</span><br></pre></td></tr></table></figure><p>PATCH 操作完成后，你就可以看到 Node 的 Status 变成了如下所示的内容：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Node</span><br><span class="line">...</span><br><span class="line">Status:</span><br><span class="line">  Capacity:</span><br><span class="line">   cpu:  2</span><br><span class="line">   memory:  2049008Ki</span><br><span class="line">   nvidia.com&#x2F;gpu: 1</span><br></pre></td></tr></table></figure><p>这样在调度器里，它就能够在缓存里记录下 node-1 上的nvidia.com/gpu类型的资源的数量是 1。</p><p>当然，在 Kubernetes 的 GPU 支持方案里，你并不需要真正去做上述关于 Extended Resource 的这些操作。在 Kubernetes 中，对所有硬件加速设备进行管理的功能，都是由一种叫作 Device Plugin 的插件来负责的。这其中，当然也就包括了对该硬件的 Extended Resource 进行汇报的逻辑。</p><p>Kubernetes 的 Device Plugin 机制，我可以用如下所示的一幅示意图来和你解释清楚。</p><img src="/img/body/jike/10a472b64f9daf24f63df4e3ae24cd10.jpg" alt="img" style="zoom:67%;" /><p>首先，对于每一种硬件设备，都需要有它所对应的 Device Plugin 进行管理，这些 Device Plugin，都通过 gRPC 的方式，同 kubelet 连接起来。以 NVIDIA GPU 为例，它对应的插件叫作NVIDIA GPU device plugin。</p><p>这个 Device Plugin 会通过一个叫作 ListAndWatch 的 API，定期向 kubelet 汇报该 Node 上 GPU 的列表。比如，在我们的例子里，一共有三个 GPU（GPU0、GPU1 和 GPU2）。这样，kubelet 在拿到这个列表之后，就可以直接在它向 APIServer 发送的心跳里，以 Extended Resource 的方式，加上这些 GPU 的数量，比如nvidia.com/gpu=3。所以说，用户在这里是不需要关心 GPU 信息向上的汇报流程的。</p><p>需要注意的是，ListAndWatch 向上汇报的信息，只有本机上 GPU 的 ID 列表，而不会有任何关于 GPU 设备本身的信息。而且 kubelet 在向 API Server 汇报的时候，只会汇报该 GPU 对应的 Extended Resource 的数量。当然，kubelet 本身，会将这个 GPU 的 ID 列表保存在自己的内存里，并通过 ListAndWatch API 定时更新。</p><p>而当一个 Pod 想要使用一个 GPU 的时候，它只需要像我在本文一开始给出的例子一样，在 Pod 的 limits 字段声明nvidia.com/gpu: 1。那么接下来，Kubernetes 的调度器就会从它的缓存里，寻找 GPU 数量满足条件的 Node，然后将缓存里的 GPU 数量减 1，完成 Pod 与 Node 的绑定。</p><p>这个调度成功后的 Pod 信息，自然就会被对应的 kubelet 拿来进行容器操作。而当 kubelet 发现这个 Pod 的容器请求一个 GPU 的时候，kubelet 就会从自己持有的 GPU 列表里，为这个容器分配一个 GPU。此时，kubelet 就会向本机的 Device Plugin 发起一个 Allocate() 请求。这个请求携带的参数，正是即将分配给该容器的设备 ID 列表。</p><p>当 Device Plugin 收到 Allocate 请求之后，它就会根据 kubelet 传递过来的设备 ID，从 Device Plugin 里找到这些设备对应的设备路径和驱动目录。当然，这些信息，正是 Device Plugin 周期性的从本机查询到的。比如，在 NVIDIA Device Plugin 的实现里，它会定期访问 nvidia-docker 插件，从而获取到本机的 GPU 信息。</p><p>而被分配 GPU 对应的设备路径和驱动目录信息被返回给 kubelet 之后，kubelet 就完成了为一个容器分配 GPU 的操作。接下来，kubelet 会把这些信息追加在创建该容器所对应的 CRI 请求当中。这样，当这个 CRI 请求发给 Docker 之后，Docker 为你创建出来的容器里，就会出现这个 GPU 设备，并把它所需要的驱动目录挂载进去。</p><p>至此，Kubernetes 为一个 Pod 分配一个 GPU 的流程就完成了。</p><p>GPU 等硬件设备的调度工作，实际上是由 kubelet 完成的。即，kubelet 会负责从它所持有的硬件设备列表中，为容器挑选一个硬件设备，然后调用 Device Plugin 的 Allocate API 来完成这个分配操作。可以看到，在整条链路中，调度器扮演的角色，仅仅是为 Pod 寻找到可用的、支持这种硬件设备的节点而已。</p><p>这就使得，Kubernetes 里对硬件设备的管理，只能处理“设备个数”这唯一一种情况。一旦你的设备是异构的、不能简单地用“数目”去描述具体使用需求的时候，比如，“我的 Pod 想要运行在计算能力最强的那个 GPU 上”，Device Plugin 就完全不能处理了。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 深入剖析Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 深入剖析Kuernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux常见日志</title>
      <link href="/2020/09/30/linux%E5%B8%B8%E8%A7%81%E6%97%A5%E5%BF%97/"/>
      <url>/2020/09/30/linux%E5%B8%B8%E8%A7%81%E6%97%A5%E5%BF%97/</url>
      
        <content type="html"><![CDATA[<h1 id="常见日志"><a href="#常见日志" class="headerlink" title="常见日志"></a>常见日志</h1><h2 id="spingboot程序启动日志"><a href="#spingboot程序启动日志" class="headerlink" title="spingboot程序启动日志"></a>spingboot程序启动日志</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash-5.0# java -jar  -Xms1024M -Xmx1024M wos-service.jar --spring.profiles.active&#x3D;dev</span><br><span class="line">log4j:WARN No appenders could be found for logger (org.apache.dubbo.common.logger.LoggerFactory).</span><br><span class="line">log4j:WARN Please initialize the log4j system properly.</span><br><span class="line">log4j:WARN See http:&#x2F;&#x2F;logging.apache.org&#x2F;log4j&#x2F;1.2&#x2F;faq.html#noconfig for more info.</span><br><span class="line">2020-09-29 15:29:43.461 [TID: N&#x2F;A] [main] INFO  o.a.d.s.b.c.e.WelcomeLogoApplicationListener -</span><br><span class="line"></span><br><span class="line"> :: Dubbo Spring Boot (v2.7.8) : https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;dubbo-spring-boot-project</span><br><span class="line"> :: Dubbo (v2.7.8) : https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;dubbo</span><br><span class="line"> :: Discuss group : dev@dubbo.apache.org</span><br><span class="line"></span><br><span class="line">2020-09-29 15:29:43.521 [TID: N&#x2F;A] [main] INFO  o.a.d.s.b.c.e.OverrideDubboConfigApplicationListener -Dubbo Config was overridden by externalized configuration &#123;dubbo.application.logger&#x3D;slf4j, dubbo.application.name&#x3D;wos-service, dubbo.application.qos-accept-foreign-ip&#x3D;false, dubbo.application.qos-enable&#x3D;false, dubbo.config-center.address&#x3D;zookeeper:&#x2F;&#x2F;zk.dev.od.com:2181, dubbo.config.multiple&#x3D;true, dubbo.metadata-report.address&#x3D;zookeeper:&#x2F;&#x2F;zk.dev.od.com:2181, dubbo.provider.group&#x3D;wos, dubbo.provider.register&#x3D;false, dubbo.provider.version&#x3D;1.0.1, dubbo.registry.address&#x3D;zookeeper:&#x2F;&#x2F;zk.dev.od.com:2181, dubbo.registry.protocol.name&#x3D;dubbo, dubbo.registry.protocol.port&#x3D;20880&#125;</span><br><span class="line"></span><br><span class="line">  .   ____          _            __ _ _</span><br><span class="line"> &#x2F;\\ &#x2F; ___&#39;_ __ _ _(_)_ __  __ _ \ \ \ \</span><br><span class="line">( ( )\___ | &#39;_ | &#39;_| | &#39;_ \&#x2F; _&#96; | \ \ \ \</span><br><span class="line"> \\&#x2F;  ___)| |_)| | | | | || (_| |  ) ) ) )</span><br><span class="line">  &#39;  |____| .__|_| |_|_| |_\__, | &#x2F; &#x2F; &#x2F; &#x2F;</span><br><span class="line"> &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;|_|&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;|___&#x2F;&#x3D;&#x2F;_&#x2F;_&#x2F;_&#x2F;</span><br><span class="line"> :: Spring Boot ::        (v2.3.3.RELEASE)</span><br><span class="line"></span><br><span class="line">2020-09-29 15:29:44.556 [TID: N&#x2F;A] [main] INFO  c.i.wos.WosServiceApplication -Starting WosServiceApplication on wos-service-5c74b5fdcc-qg7vq with PID 209 (&#x2F;app&#x2F;wos-service.jar started by root in &#x2F;app)</span><br><span class="line">2020-09-29 15:29:44.557 [TID: N&#x2F;A] [main] INFO  c.i.wos.WosServiceApplication -The following profiles are active: dev</span><br><span class="line">2020-09-29 15:29:49.214 [TID: N&#x2F;A] [main] INFO  c.alibaba.spring.util.BeanRegistrar -The Infrastructure bean definition [Root bean: class [org.apache.dubbo.config.spring.beans.factory.annotation.ReferenceAnnotationBeanPostProcessor]; scope&#x3D;; abstract&#x3D;false; lazyInit&#x3D;null; autowireMode&#x3D;0; dependencyCheck&#x3D;0; autowireCandidate&#x3D;true; primary&#x3D;false; factoryBeanName&#x3D;null; factoryMethodName&#x3D;null; initMethodName&#x3D;null; destroyMethodName&#x3D;nullwith name [referenceAnnotationBeanPostProcessor] has been registered.</span><br></pre></td></tr></table></figure><h2 id="docker重启过程中，系统日志的输出"><a href="#docker重启过程中，系统日志的输出" class="headerlink" title="docker重启过程中，系统日志的输出"></a>docker重启过程中，系统日志的输出</h2><p>docker停止服务</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 ~]# docker stop nginx-2</span><br><span class="line">nginx-2</span><br></pre></td></tr></table></figure><p>系统日志的输出</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Sep 29 15:35:38 wang-18 containerd: time&#x3D;&quot;2020-09-29T15:35:38.720546541+08:00&quot; level&#x3D;info msg&#x3D;&quot;shim reaped&quot; id&#x3D;5705703d12cf9a8f18ce4d483b22a45e187ae2aa5072cbe50212379b5613db0e</span><br><span class="line">Sep 29 15:35:38 wang-18 dockerd: time&#x3D;&quot;2020-09-29T15:35:38.741125793+08:00&quot; level&#x3D;info msg&#x3D;&quot;ignoring event&quot; module&#x3D;libcontainerd namespace&#x3D;moby topic&#x3D;&#x2F;tasks&#x2F;delete type&#x3D;&quot;*events.TaskDelete&quot;</span><br><span class="line">Sep 29 15:35:38 wang-18 kernel: docker0: port 2(veth76a4bf6) entered disabled state</span><br><span class="line">Sep 29 15:35:38 wang-18 NetworkManager[6801]: &lt;info&gt;  [1601364938.8815] manager: (veth879fa45): new Veth device (&#x2F;org&#x2F;freedesktop&#x2F;NetworkManager&#x2F;Devices&#x2F;11)</span><br><span class="line">Sep 29 15:35:38 wang-18 kernel: docker0: port 2(veth76a4bf6) entered disabled state</span><br><span class="line">Sep 29 15:35:38 wang-18 kernel: device veth76a4bf6 left promiscuous mode</span><br><span class="line">Sep 29 15:35:38 wang-18 kernel: docker0: port 2(veth76a4bf6) entered disabled state</span><br><span class="line">Sep 29 15:35:38 wang-18 NetworkManager[6801]: &lt;info&gt;  [1601364938.9757] device (veth76a4bf6): released from master device docker0</span><br></pre></td></tr></table></figure><p>docker启动服务</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 ~]# docker start nginx-2</span><br><span class="line">nginx-2</span><br></pre></td></tr></table></figure><p>系统日志的输出</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Sep 29 15:36:04 wang-18 kernel: docker0: port 2(vethcc06d30) entered blocking state</span><br><span class="line">Sep 29 15:36:04 wang-18 kernel: docker0: port 2(vethcc06d30) entered disabled state</span><br><span class="line">Sep 29 15:36:04 wang-18 kernel: device vethcc06d30 entered promiscuous mode</span><br><span class="line">Sep 29 15:36:04 wang-18 kernel: IPv6: ADDRCONF(NETDEV_UP): vethcc06d30: link is not ready</span><br><span class="line">Sep 29 15:36:04 wang-18 kernel: docker0: port 2(vethcc06d30) entered blocking state</span><br><span class="line">Sep 29 15:36:04 wang-18 kernel: docker0: port 2(vethcc06d30) entered forwarding state</span><br><span class="line">Sep 29 15:36:04 wang-18 kernel: docker0: port 2(vethcc06d30) entered disabled state</span><br><span class="line">Sep 29 15:36:04 wang-18 NetworkManager[6801]: &lt;info&gt;  [1601364964.7324] manager: (veth5f132b0): new Veth device (&#x2F;org&#x2F;freedesktop&#x2F;NetworkManager&#x2F;Devices&#x2F;12)</span><br><span class="line">Sep 29 15:36:04 wang-18 NetworkManager[6801]: &lt;info&gt;  [1601364964.7497] manager: (vethcc06d30): new Veth device (&#x2F;org&#x2F;freedesktop&#x2F;NetworkManager&#x2F;Devices&#x2F;13)</span><br><span class="line">Sep 29 15:36:05 wang-18 containerd: time&#x3D;&quot;2020-09-29T15:36:05.136195267+08:00&quot; level&#x3D;info msg&#x3D;&quot;shim containerd-shim started&quot; address&#x3D;&#x2F;containerd-shim&#x2F;fbf0d289aad8c39e31e04d76aac5f649ef6a64992bb787db9ef4ecae621f71b3.sock debug&#x3D;false pid&#x3D;57791</span><br><span class="line">Sep 29 15:36:05 wang-18 kernel: IPv6: ADDRCONF(NETDEV_UP): eth0: link is not ready</span><br><span class="line">Sep 29 15:36:05 wang-18 kernel: IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready</span><br><span class="line">Sep 29 15:36:05 wang-18 kernel: IPv6: ADDRCONF(NETDEV_CHANGE): vethcc06d30: link becomes ready</span><br><span class="line">Sep 29 15:36:05 wang-18 kernel: docker0: port 2(vethcc06d30) entered blocking state</span><br><span class="line">Sep 29 15:36:05 wang-18 kernel: docker0: port 2(vethcc06d30) entered forwarding state</span><br><span class="line">Sep 29 15:36:05 wang-18 NetworkManager[6801]: &lt;info&gt;  [1601364965.9290] device (vethcc06d30): carrier: link connected</span><br></pre></td></tr></table></figure><h2 id="主机内存不足，主进程杀死服务系统日志"><a href="#主机内存不足，主进程杀死服务系统日志" class="headerlink" title="主机内存不足，主进程杀死服务系统日志"></a>主机内存不足，主进程杀死服务系统日志</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Sep 29 03:06:16 AliYunOS kernel: Out of memory: Kill process 9172 (java) score 55 or sacrifice child</span><br><span class="line">Sep 29 03:06:16 AliYunOS kernel: Killed process 9172, UID 496, (java) total-vm:6325920kB, anon-rss:893824kB, file-rss:10940kB</span><br><span class="line">Sep 29 14:00:38 AliYunOS rz[13760]: [hdfs] apipro-1.0-SNAPSHOT-jar-with-dependencies.jar&#x2F;ZMODEM: 17684313 Bytes, 1645623 BPS</span><br></pre></td></tr></table></figure><p>k8s中的pod内存超过限制，被主程序杀死</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Sep 29 15:54:24 k8s-node3 kernel: [13101]     0 13101    27242      173      10        0          -999 containerd-shim</span><br><span class="line">Sep 29 15:54:24 k8s-node3 kernel: [13118]     0 13118   810082    63384     177        0           952 java</span><br><span class="line">Sep 29 15:54:24 k8s-node3 kernel: Out of memory: Kill process 86425 (java) score 1022 or sacrifice child</span><br><span class="line">Sep 29 15:54:24 k8s-node3 kernel: Killed process 86425 (java) total-vm:6753608kB, anon-rss:620800kB, file-rss:0kB, shmem-rss:0kB</span><br><span class="line">Sep 29 15:54:24 k8s-node3 containerd: time&#x3D;&quot;2020-09-29T15:54:24.511077342+08:00&quot; level&#x3D;info msg&#x3D;&quot;shim reaped&quot; id&#x3D;5c30f79d5618787225a5dfb47b4b55279826d837fb2143c7d9fe1e3830aca1e6</span><br></pre></td></tr></table></figure><p><strong>一次阿里云k8s pod导致内存溢出日志示例</strong></p><img src="https://wangzhangtao.com/img/body/1.linux-tmp/image-20201023150218941.png" alt="image-20201023150218941" style="zoom:67%;" />]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> 日志 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 日志 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>36-39.网络控制、service和ingress</title>
      <link href="/2020/09/29/36-39-%E7%BD%91%E7%BB%9C%E6%8E%A7%E5%88%B6%E3%80%81service%E5%92%8Cingress/"/>
      <url>/2020/09/29/36-39-%E7%BD%91%E7%BB%9C%E6%8E%A7%E5%88%B6%E3%80%81service%E5%92%8Cingress/</url>
      
        <content type="html"><![CDATA[<p>极客时间课程张磊的 <a href="https://time.geekbang.org/column/intro/116" target="_blank" rel="noopener">深入剖析Kuernetes</a> </p><h1 id="36-为什么说Kubernetes只有soft-multi-tenancy？"><a href="#36-为什么说Kubernetes只有soft-multi-tenancy？" class="headerlink" title="36 | 为什么说Kubernetes只有soft multi-tenancy？"></a>36 | 为什么说Kubernetes只有soft multi-tenancy？</h1><p>soft multi-tenancy（弱租户能力）</p><p>Kubernetes 的网络模型，以及前面这些网络方案的实现，都只关注容器之间网络的“连通”，却并不关心容器之间网络的“隔离”。这跟传统的 IaaS 层的网络方案，区别非常明显。</p><p>在 Kubernetes 里，网络隔离能力的定义，是依靠一种专门的 API 对象来描述的，即：NetworkPolicy。</p><p>一个完整的 NetworkPolicy 对象的示例，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: networking.k8s.io&#x2F;v1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: test-network-policy</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      role: db</span><br><span class="line">  policyTypes:</span><br><span class="line">  - Ingress</span><br><span class="line">  - Egress</span><br><span class="line">  ingress:</span><br><span class="line">  - from:</span><br><span class="line">    - ipBlock:</span><br><span class="line">        cidr: 172.17.0.0&#x2F;16</span><br><span class="line">        except:</span><br><span class="line">        - 172.17.1.0&#x2F;24</span><br><span class="line">    - namespaceSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          project: myproject</span><br><span class="line">    - podSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          role: frontend</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 6379</span><br><span class="line">  egress:</span><br><span class="line">  - to:</span><br><span class="line">    - ipBlock:</span><br><span class="line">        cidr: 10.0.0.0&#x2F;24</span><br><span class="line">    ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 5978</span><br></pre></td></tr></table></figure><p>Kubernetes 里的 Pod 默认都是“允许所有”（Accept All）的，即：Pod 可以接收来自任何发送方的请求；或者，向任何接收方发送请求。而如果你要对这个情况作出限制，就必须通过 NetworkPolicy 对象来指定。</p><p>而如果你把 podSelector 字段留空：那么这个 NetworkPolicy 就会作用于当前 Namespace 下的所有 Pod。</p><p>而一旦 Pod 被 NetworkPolicy 选中，那么这个 Pod 就会进入“拒绝所有”（Deny All）的状态，即：这个 Pod 既不允许被外界访问，也不允许对外界发起访问。而 NetworkPolicy 定义的规则，其实就是“白名单”。</p><p>这个 NetworkPolicy 对象，指定的隔离规则如下所示：</p><ol><li>该隔离规则只对 default Namespace 下的，携带了 role=db 标签的 Pod 有效。限制的请求类型包括 ingress（流入）和 egress（流出）。</li><li>Kubernetes 会拒绝任何访问被隔离 Pod 的请求，除非这个请求来自于以下“白名单”里的对象，并且访问的是被隔离 Pod 的 6379 端口。这些“白名单”对象包括：a. default Namespace 里的，携带了 role=fronted 标签的 Pod；b. 携带了 project=myproject 标签的 Namespace 里的任何 Pod；c. 任何源地址属于 172.17.0.0/16 网段，且不属于 172.17.1.0/24 网段的请求。</li><li>Kubernetes 会拒绝被隔离 Pod 对外发起任何请求，除非请求的目的地址属于 10.0.0.0/24 网段，并且访问的是该网段地址的 5978 端口。</li></ol><p>如果要使上面定义的 NetworkPolicy 在 Kubernetes 集群里真正产生作用，你的 CNI 网络插件就必须是支持 Kubernetes 的 NetworkPolicy 的。</p><p>以比较简单的 NetworkPolicy 对象为例</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: test-network-policy</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  podSelector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      role: db</span><br><span class="line">  ingress:</span><br><span class="line">   - from:</span><br><span class="line">     - namespaceSelector:</span><br><span class="line">         matchLabels:</span><br><span class="line">           project: myproject</span><br><span class="line">     - podSelector:</span><br><span class="line">         matchLabels:</span><br><span class="line">           role: frontend</span><br><span class="line">     ports:</span><br><span class="line">       - protocol: tcp</span><br><span class="line">         port: 6379</span><br></pre></td></tr></table></figure><p>Kubernetes 网络插件对 Pod 进行隔离，其实是靠在宿主机上生成 NetworkPolicy 对应的 iptable 规则来实现的。</p><h2 id="iptable规则的具体实现"><a href="#iptable规则的具体实现" class="headerlink" title="iptable规则的具体实现"></a>iptable规则的具体实现</h2><p>后续进一步研究</p><h2 id="37-找到容器不容易：Service、DNS与服务发现"><a href="#37-找到容器不容易：Service、DNS与服务发现" class="headerlink" title="37 | 找到容器不容易：Service、DNS与服务发现"></a>37 | 找到容器不容易：Service、DNS与服务发现</h2><p>为什么需要 Service</p><ul><li>Pod 的 IP 不是固定的</li><li>一组 Pod 实例之间有负载均衡的需求。</li></ul><p>service通过label标签选择pod,  被 selector 选中的 Pod，就称为 Service 的 Endpoints，你可以使用 kubectl get ep 命令看到它们，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl get endpoints hostnames</span><br><span class="line">NAME        ENDPOINTS</span><br><span class="line">hostnames   10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376</span><br></pre></td></tr></table></figure><p>只有处于 Running 状态，且 readinessProbe 检查通过的 Pod，才会出现在 Service 的 Endpoints 列表里。并且，当某一个 Pod 出现问题时，Kubernetes 会自动把它从 Service 里摘除掉。</p><h2 id="Service-原理"><a href="#Service-原理" class="headerlink" title="Service 原理"></a>Service 原理</h2><p>service 是由 kube-proxy 组件，加上 iptables 来共同实现的。Service 提供的是 Round Robin （rr）方式的负载均衡。</p><p>一个 Service 对象被提交给 Kubernetes，那么 kube-proxy 就可以通过 Service 的 Informer 感知到这样一个 Service 对象的添加，这时它就会在宿主机上创建这样一条 iptables 规则（你可以通过 iptables-save 看到它），如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-A KUBE-SERVICES -d 10.0.1.175&#x2F;32 -p tcp -m comment --comment &quot;default&#x2F;hostnames: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3</span><br></pre></td></tr></table></figure><p>而KUBE-SVC-NWV5X2332I4OT4T3 规则，是一组随机模式（–mode random）的 iptables 链，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment &quot;default&#x2F;hostnames:&quot; -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ</span><br><span class="line">-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment &quot;default&#x2F;hostnames:&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3</span><br><span class="line">-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment &quot;default&#x2F;hostnames:&quot; -j KUBE-SEP-57KPRZ3JQVENLNBR</span><br></pre></td></tr></table></figure><p>访问 Service VIP 的 IP 包经过上述 iptables 处理之后，就已经变成了访问具体某一个后端 Pod 的 IP 包了。这些 Endpoints 对应的 iptables 规则，正是 kube-proxy 通过监听 Pod 的变化事件，在宿主机上生成并维护的。</p><h2 id="IPVS-的模式"><a href="#IPVS-的模式" class="headerlink" title="IPVS 的模式"></a>IPVS 的模式</h2><p>kube-proxy 通过 iptables 处理 Service 的过程，其实需要在宿主机上设置相当多的 iptables 规则。而且，kube-proxy 还需要在控制循环里不断地刷新这些规则来确保它们始终是正确的。</p><p>当你的宿主机上有大量 Pod 的时候，成百上千条 iptables 规则不断地被刷新，会大量占用该宿主机的 CPU 资源，甚至会让宿主机“卡”在这个过程中。所以说，一直以来，基于 iptables 的 Service 实现，都是制约 Kubernetes 项目承载更多量级的 Pod 的主要障碍。</p><p>IPVS 模式的工作原理，其实跟 iptables 模式类似。当我们创建了前面的 Service 之后，kube-proxy 首先会在宿主机上创建一个虚拟网卡（叫作：kube-ipvs0），并为它分配 Service VIP 作为 IP 地址</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ip addr</span><br><span class="line">  ...</span><br><span class="line">  73：kube-ipvs0：&lt;BROADCAST,NOARP&gt;  mtu 1500 qdisc noop state DOWN qlen 1000</span><br><span class="line">  link&#x2F;ether  1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff</span><br><span class="line">  inet 10.0.1.175&#x2F;32  scope global kube-ipvs0</span><br><span class="line">  valid_lft forever  preferred_lft forever</span><br></pre></td></tr></table></figure><p>而接下来，kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略。我们可以通过 ipvsadm 查看到这个设置，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># ipvsadm -ln</span><br><span class="line"> IP Virtual Server version 1.2.1 (size&#x3D;4096)</span><br><span class="line">  Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">    -&gt;  RemoteAddress:Port           Forward  Weight ActiveConn InActConn     </span><br><span class="line">  TCP  10.102.128.4:80 rr</span><br><span class="line">    -&gt;  10.244.3.6:9376    Masq    1       0          0         </span><br><span class="line">    -&gt;  10.244.1.7:9376    Masq    1       0          0</span><br><span class="line">    -&gt;  10.244.2.3:9376    Masq    1       0          0</span><br></pre></td></tr></table></figure><p>而相比于 iptables，IPVS 在内核中的实现其实也是基于 Netfilter 的 NAT 模式，所以在转发这一层上，理论上 IPVS 并没有显著的性能提升。但是，IPVS 并不需要在宿主机上为每个 Pod 设置 iptables 规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价。所以“将重要操作放入内核态”是提高性能的重要手段。</p><p>IPVS 模块只负责上述的负载均衡和代理功能。而一个完整的 Service 流程正常工作所需要的包过滤、SNAT 等操作，还是要靠 iptables 来实现。只不过，这些辅助性的 iptables 规则数量有限，也不会随着 Pod 数量的增加而增加。</p><p>Service 通过 DNS解析，域名格式为  <strong>jenkins.infra.svc.cluster.local</strong> </p><p>在 Kubernetes 里，/etc/hosts 文件是单独挂载的，这也是为什么 kubelet 能够对 hostname 进行修改并且 Pod 重建后依然有效的原因。这跟 Docker 的 Init 层是一个原理。</p><h1 id="38-从外界连通Service与Service调试“三板斧”"><a href="#38-从外界连通Service与Service调试“三板斧”" class="headerlink" title="38 | 从外界连通Service与Service调试“三板斧”"></a>38 | 从外界连通Service与Service调试“三板斧”</h1><h2 id="service的类型"><a href="#service的类型" class="headerlink" title="service的类型"></a>service的类型</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># kubectl explain service.spec.type</span><br><span class="line">Defaults to ClusterIP. Valid options are ExternalName, ClusterIP, NodePort, and LoadBalancer.</span><br><span class="line">默认类型为ClusterIP，有效选项包括ExternalName, ClusterIP, NodePort, and LoadBalancer.</span><br><span class="line"></span><br><span class="line">If clusterIP is &quot;None&quot;, no virtual IP is allocated and the endpoints are published as a set of endpoints rather than a stable IP.</span><br><span class="line">如果clusterIP类型是&quot;None&quot;，将没有虚拟IP地址被赋予， 端点被发布为一组端点，而不是一个稳定的IP。</span><br></pre></td></tr></table></figure><h2 id="NodePort类型的-Service"><a href="#NodePort类型的-Service" class="headerlink" title="NodePort类型的 Service"></a>NodePort类型的 Service</h2><p>示例如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: my-nginx</span><br><span class="line">  labels:</span><br><span class="line">    run: my-nginx</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - nodePort: 8080</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line">    name: http</span><br><span class="line">  - nodePort: 443</span><br><span class="line">    protocol: TCP</span><br><span class="line">    name: https</span><br><span class="line">  selector:</span><br><span class="line">    run: my-nginx</span><br></pre></td></tr></table></figure><p>在这个 Service 的定义里，我们声明它的类型是，type=NodePort。然后，我在 ports 字段里声明了 Service 的 8080 端口代理 Pod 的 80 端口，Service 的 443 端口代理 Pod 的 443 端口。</p><p>当然，如果你不显式地声明 nodePort 字段，Kubernetes 就会为你分配随机的可用端口来设置代理。这个端口的范围默认是 30000-32767，你可以通过 kube-apiserver 的–service-node-port-range 参数来修改它。</p><p>访问这个 Service，你只需要访问 <strong>&lt;任何一台宿主机的IP地址&gt;:8080</strong></p><p>kube-proxy 要做的，就是在每台宿主机上生成这样一条 iptables 规则：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default&#x2F;my-nginx: nodePort&quot; -m tcp --dport 8080 -j KUBE-SVC-67RL4FN6JRUPOJYM</span><br></pre></td></tr></table></figure><p>KUBE-SVC-67RL4FN6JRUPOJYM 其实就是一组随机模式的 iptables 规则。所以接下来的流程，就跟 ClusterIP 模式完全一样了。</p><h3 id="NodePort类型的源地址转换"><a href="#NodePort类型的源地址转换" class="headerlink" title="NodePort类型的源地址转换"></a>NodePort类型的源地址转换</h3><p>需要注意的是，在 NodePort 方式下，Kubernetes 会在 IP 包离开宿主机发往目的 Pod 时，对这个 IP 包做一次 SNAT 操作，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-A KUBE-POSTROUTING -m comment --comment &quot;kubernetes service traffic requiring SNAT&quot; -m mark --mark 0x4000&#x2F;0x4000 -j MASQUERADE</span><br></pre></td></tr></table></figure><p>它给即将离开这台主机的 IP 包，进行了一次 SNAT 操作，将这个 IP 包的源地址替换成了这台宿主机上的 CNI 网桥地址，或者宿主机本身的 IP 地址（如果 CNI 网桥不存在的话）。</p><p>当然，这个 SNAT 操作只需要对 Service 转发出来的 IP 包进行（否则普通的 IP 包就被影响了）。而 iptables 做这个判断的依据，就是查看该 IP 包是否有一个“0x4000”的“标志”。你应该还记得，这个标志正是在 IP 包被执行 DNAT 操作之前被打上去的。</p><p>为什么一定要对流出的包做 SNAT操作呢？client 可能有目的地址限制</p><p>所以这时候，你就可以将 Service 的 spec.externalTrafficPolicy 字段设置为 local，这就保证了所有 Pod 通过 Service 收到请求之后，一定可以看到真正的、外部 client 的源地址。</p><p>而这个机制的实现原理也非常简单：这时候，一台宿主机上的 iptables 规则，会设置为只将 IP 包转发给运行在这台宿主机上的 Pod。所以这时候，Pod 就可以直接使用源地址将回复包发出，不需要事先进行 SNAT 了。</p><p>当然，这也就意味着如果在一台宿主机上，没有任何一个被代理的 Pod 存在，那么你使用 node 2 的 IP 地址访问这个 Service，就是无效的。此时，你的请求会直接被 DROP 掉。</p><h2 id="LoadBalancer-类型的-Service"><a href="#LoadBalancer-类型的-Service" class="headerlink" title="LoadBalancer 类型的 Service"></a>LoadBalancer 类型的 Service</h2><p>适用于公有云上的 Kubernetes 服务。如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: example-service</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 8765</span><br><span class="line">    targetPort: 9376</span><br><span class="line">  selector:</span><br><span class="line">    app: example</span><br><span class="line">  type: LoadBalancer</span><br></pre></td></tr></table></figure><p>在公有云提供的 Kubernetes 服务里，都使用了一个叫作 CloudProvider 的转接层，来跟公有云本身的 API 进行对接。所以，在上述 LoadBalancer 类型的 Service 被提交后，Kubernetes 就会调用 CloudProvider 在公有云上为你创建一个负载均衡服务，并且把被代理的 Pod 的 IP 地址配置给负载均衡服务做后端。</p><h2 id="ExternalName-类型的-Service"><a href="#ExternalName-类型的-Service" class="headerlink" title="ExternalName 类型的 Service"></a>ExternalName 类型的 Service</h2><p>示例如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: my-service</span><br><span class="line">spec:</span><br><span class="line">  type: ExternalName</span><br><span class="line">  externalName: my.database.example.com</span><br></pre></td></tr></table></figure><p>当你通过 Service 的 DNS 名字访问它的时候，比如访问：my-service.default.svc.cluster.local。那么，Kubernetes 为你返回的就是my.database.example.com。所以说，ExternalName 类型的 Service，其实是在 kube-dns 里为你添加了一条 CNAME 记录。这时，访问 my-service.default.svc.cluster.local 就和访问 my.database.example.com 这个域名是一个效果了。</p><p>此外，Kubernetes 的 Service 还允许你为 Service 分配公有 IP 地址，比如下面这个例子：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: my-service</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: MyApp</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    protocol: TCP</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 9376</span><br><span class="line">  externalIPs:</span><br><span class="line">  - 80.11.12.10</span><br></pre></td></tr></table></figure><h2 id="Service问题解决思路"><a href="#Service问题解决思路" class="headerlink" title="Service问题解决思路"></a>Service问题解决思路</h2><ol><li>检查集群的 DNS是否正常。</li><li>这个 Service 是否有 Endpoints。如果你的 Pod 的 readniessProbe 没通过，它也不会出现在 Endpoints 列表里。 </li><li>确认 kube-proxy 是否在正确运行。可以查看 kube-proxy 输出的日志。</li><li>查看宿主机上的 iptables。</li></ol><h3 id="Pod-没办法通过-Service-访问到自己？"><a href="#Pod-没办法通过-Service-访问到自己？" class="headerlink" title="Pod 没办法通过 Service 访问到自己？"></a>Pod 没办法通过 Service 访问到自己？</h3><p>这往往就是因为 kubelet 的 hairpin-mode 没有被正确设置。关于 Hairpin 的原理我在前面已经介绍过，这里就不再赘述了。你只需要确保将 kubelet 的 hairpin-mode 设置为 hairpin-veth 或者 promiscuous-bridge 即可。（我没看懂这段话）</p><h1 id="39-谈谈Service与Ingress"><a href="#39-谈谈Service与Ingress" class="headerlink" title="39 | 谈谈Service与Ingress"></a>39 | 谈谈Service与Ingress</h1><p>这种全局的、为了代理不同后端 Service 而设置的负载均衡服务，就是 Kubernetes 里的 Ingress 服务。</p><p>所以，Ingress 的功能其实很容易理解：所谓 Ingress，就是 Service 的“Service”。</p><p>Ingress 对象，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: cafe-ingress</span><br><span class="line">spec:</span><br><span class="line">  tls:</span><br><span class="line">  - hosts:</span><br><span class="line">    - cafe.example.com</span><br><span class="line">    secretName: cafe-secret</span><br><span class="line">  rules:</span><br><span class="line">  - host: cafe.example.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;tea</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tea-svc</span><br><span class="line">          servicePort: 80</span><br><span class="line">      - path: &#x2F;coffee</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: coffee-svc</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></figure><p>通过上面的讲解，不难看到，所谓 Ingress 对象，其实就是 Kubernetes 项目对“反向代理”的一种抽象。</p><p>如果我的请求没有匹配到任何一条 IngressRule，会为你返回一个 Nginx 的 404 页面。</p><p>不过，Ingress Controller 也允许你通过 Pod 启动命令里的–default-backend-service 参数，设置一条默认规则，比如：–default-backend-service=nginx-default-backend。这样，任何匹配失败的请求，就都会被转发到这个名叫 nginx-default-backend 的 Service。所以，你就可以通过部署一个专门的 Pod，来为用户返回自定义的 404 页面了。</p><p>目前，Ingress 只能工作在七层，而 Service 只能工作在四层。所以当你想要在 Kubernetes 里为应用进行 TLS 配置等 HTTP 相关的操作时，都必须通过 Ingress 来进行。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 深入剖析Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 深入剖析Kuernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>32-35.网络模型和网络方案</title>
      <link href="/2020/09/26/32-35-%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%BD%91%E7%BB%9C%E6%96%B9%E6%A1%88/"/>
      <url>/2020/09/26/32-35-%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%BD%91%E7%BB%9C%E6%96%B9%E6%A1%88/</url>
      
        <content type="html"><![CDATA[<p>极客时间课程张磊的 <a href="https://time.geekbang.org/column/intro/116" target="_blank" rel="noopener">深入剖析Kuernetes</a> </p><h1 id="32-浅谈容器网络"><a href="#32-浅谈容器网络" class="headerlink" title="32 | 浅谈容器网络"></a>32 | 浅谈容器网络</h1><p>“网络栈”，包括了网卡（Network Interface）、回环设备（Loopback Device）、路由表（Routing Table）和 iptables 规则。这些要素，构成了一个进程发起和响应网络请求的基本环境。</p><p>在大多数情况下，我们希望容器进程能使用自己 Network Namespace 里的网络栈，即：拥有属于自己的 IP 地址和端口。</p><h2 id="宿主机上的容器间通信"><a href="#宿主机上的容器间通信" class="headerlink" title="宿主机上的容器间通信"></a>宿主机上的容器间通信</h2><p>在 Linux 中，能够起到虚拟交换机作用的网络设备，是网桥（Bridge）。它是一个工作在数据链路层（Data Link）的设备，主要功能是根据 MAC 地址学习来将数据包转发到网桥的不同端口（Port）上。</p><p>Docker 会默认在宿主机上创建一个名叫 docker0 的网桥，容器通过Veth Pair 的虚拟设备“连接”到 docker0 网桥上, 并通过 docker0 网桥进行通信。</p><p>Veth Pair 设备的特点是：它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里。</p><p>容器里的eth0 网卡，是一个 Veth Pair，它的一端在这个 nginx-1 容器的 Network Namespace 里，而另一端则位于宿主机上（Host Namespace），并且被“插”在了宿主机的 docker0 网桥上。</p><p>通过 brctl show 的输出，你可以看到这张网卡被“插”在了 docker0 上。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 ~]# brctl show </span><br><span class="line">bridge name     bridge id               STP enabled     interfaces</span><br><span class="line">docker0         8000.0242887e6be6       no              veth7999bd1</span><br></pre></td></tr></table></figure><p>一旦一张虚拟网卡被“插”在网桥上，它就会变成该网桥的“从设备”。从设备会被“剥夺”调用网络协议栈处理数据包的资格，从而“降级”成为网桥上的一个端口。而这个端口唯一的作用，就是接收流入的数据包，然后把这些数据包的处理全部交给对应的网桥。</p><p>路由规则的网关（Gateway）是 0.0.0.0，这就意味着这是一条直连规则，即：凡是匹配到这条规则的 IP 包，应该经过本机的 eth0 网卡，通过二层网络直接发往目的主机</p><p><strong>docker0 处理转发的过程，扮演二层交换机的角色。</strong></p><img src="/img/body/jike/e0d28e0371f93af619e91a86eda99a66.png" alt="img" style="zoom:67%;max-width:70%" /><p>被限制在 Network Namespace 里的容器进程，实际上是通过 Veth Pair 设备 + 宿主机网桥的方式，实现了跟同其他容器的数据交换。</p><p>在宿主机上访问它上面的容器，或者它上面的容器访问外网，都是通过docker0 网桥。当你遇到容器连不通“外网”的时候，</p><ul><li>a.先试试 docker0 网桥能不能 ping 通；</li><li>b.查看一下跟 docker0 和 Veth Pair 设备相关的 iptables 规则是不是有异常。</li></ul><h2 id="容器的跨主机通信"><a href="#容器的跨主机通信" class="headerlink" title="容器的跨主机通信"></a>容器的跨主机通信</h2><p>在Docker 的默认配置下，一台宿主机上的 docker0 网桥，和其他宿主机上的 docker0 网桥，没办法直接连通。</p><p><strong>Overlay Network（覆盖网络）</strong></p><img src="https://static001.geekbang.org/resource/image/b4/3d/b4387a992352109398a66d1dbe6e413d.png" alt="img" style="zoom:80%;max-width:70%" /><p>在已有的宿主机网络上，通过软件构建一个覆盖在已有宿主机网络之上的、可以把所有容器连通在一起的虚拟网络。这种技术就被称为：Overlay Network（覆盖网络）。</p><h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><p>Linux 内核中 Netfilter 是什么，都有哪些作用？ iptable防火墙主要通过Netfilter实现。</p><h1 id="33-深入解析容器跨主机网络"><a href="#33-深入解析容器跨主机网络" class="headerlink" title="33 | 深入解析容器跨主机网络"></a>33 | 深入解析容器跨主机网络</h1><p>通过Flannel 这个项目，讲解容器“跨主通信”的原理。</p><p>Flannel 项目本身只是一个框架，真正为我们提供容器网络功能的，是 Flannel 的后端实现。目前，Flannel 支持三种后端实现，分别是：</p><ol><li>VXLAN；</li><li>host-gw；</li><li>UDP。</li></ol><h2 id="UDP-模式"><a href="#UDP-模式" class="headerlink" title="UDP 模式"></a>UDP 模式</h2><p>UDP 模式，是 Flannel 最早支持的一种方式，却也是性能最差的一种方式。所以，这个模式目前已经被弃用。这种模式是最直接、也是最容易理解的容器跨主网络实现。</p><p>网卡flannel0 是一个 TUN 设备（Tunnel 设备）。在 Linux 中，TUN 设备是一种工作在三层（Network Layer）的虚拟网络设备。TUN 设备的功能非常简单，即：在操作系统内核和用户应用程序之间传递 IP 包。</p><p>当 IP 包从容器经过 docker0 出现在宿主机，然后又根据路由表进入 flannel0 设备后，宿主机上的 flanneld 进程（Flannel 项目在每个宿主机上的主进程），就会收到这个 IP 包。然后，flanneld 看到了这个 IP 包的目的地址，就把它发送给了 Node 2 宿主机flanneld监听的 8285 端口，这样包就到了另一台机器。</p><p>Flannel 项目里一个非常重要的概念：子网（Subnet）。</p><p>子网与宿主机的对应关系，正是保存在 Etcd 当中。示例图如下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ etcdctl ls &#x2F;coreos.com&#x2F;network&#x2F;subnets</span><br><span class="line">&#x2F;coreos.com&#x2F;network&#x2F;subnets&#x2F;100.96.1.0-24</span><br><span class="line">&#x2F;coreos.com&#x2F;network&#x2F;subnets&#x2F;100.96.2.0-24</span><br><span class="line">&#x2F;coreos.com&#x2F;network&#x2F;subnets&#x2F;100.96.3.0-24</span><br></pre></td></tr></table></figure><p>flanneld 进程从 Etcd 中找到这个子网对应的宿主机的 IP 地址是 10.168.0.3</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ etcdctl get &#x2F;coreos.com&#x2F;network&#x2F;subnets&#x2F;100.96.2.0-24</span><br><span class="line">&#123;&quot;PublicIP&quot;:&quot;10.168.0.3&quot;&#125;</span><br></pre></td></tr></table></figure><p><strong>Flannel UDP 模式的跨主通信流程图</strong></p><img src="/img/body/jike/8332564c0547bf46d1fbba2a1e0e166c.jpg" alt="基于Flannel UDP模式的跨主通信的基本原理" style="zoom:67%;max-width: 80%" /><p>Flannel UDP 模式提供的其实是一个三层的 Overlay 网络，即：它首先对发出端的 IP 包进行 UDP 封装，然后在接收端进行解封装拿到原始的 IP 包，进而把这个 IP 包转发给目标容器。这就好比，Flannel 在不同宿主机上的两个容器之间打通了一条“隧道”，使得这两个容器可以直接使用 IP 地址进行通信，而无需关心容器和宿主机的分布情况。</p><p>相比于两台宿主机之间的直接通信，基于 Flannel UDP 模式的容器通信多了一个额外的步骤，即 flanneld 的处理过程。而这个过程，由于使用到了 flannel0 这个 TUN 设备，仅在发出 IP 包的过程中，就需要经过三次用户态与内核态之间的数据拷贝，如下所示：</p><img src="/img/body/jike/84caa6dc3f9dcdf8b88b56bd2e22138d.png" alt="TUN设备示意图" style="zoom:80%;max-width: 40%" /><ul><li>第一次，用户态的容器进程发出的 IP 包经过 docker0 网桥进入内核态；</li><li>第二次，IP 包根据路由表进入 TUN（flannel0）设备，从而回到用户态的 flanneld 进程；</li><li>第三次，flanneld 进行 UDP 封包之后重新进入内核态，将 UDP 包通过宿主机的 eth0 发出去。</li></ul><p>Flannel 进行 UDP 封装（Encapsulation）和解封装（Decapsulation）的过程，也都是在用户态完成的。在 Linux 操作系统中，上述这些上下文切换和用户态操作的代价其实是比较高的，这也正是造成 Flannel UDP 模式性能不好的主要原因。</p><p>所以我们在进行系统级编程的时候，有一个非常重要的优化原则，就是要减少用户态到内核态的切换次数，并且把核心的处理逻辑都放在内核态进行。</p><h2 id="VXLAN模式"><a href="#VXLAN模式" class="headerlink" title="VXLAN模式"></a>VXLAN模式</h2><p>VXLAN，即 Virtual Extensible LAN（虚拟可扩展局域网），是 Linux 内核本身就支持的一种网络虚似化技术。所以说，VXLAN 可以完全在内核态实现上述封装和解封装的工作，从而通过与前面相似的“隧道”机制，构建出覆盖网络（Overlay Network）。</p><p>为何需要Vxlan：</p><ol><li>虚拟机规模受到网络规格的限制，大L2网络里，报文通过查询MAC地址转发，MAC表容量限制了虚拟机的数量。</li><li>网络隔离的限制，普通的vlan和VPN配置无法满足动态网络调整的需求，同时配置复杂</li><li>虚拟器搬迁受到限制，虚拟机启动后假如在业务不中断基础上将该虚拟机迁移到另外一台物理机上去，需要保持虚拟机的IP地址和MAC地址等参数保持不变，这就要求业务网络是一个二层的网络。</li></ol><p>VXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。</p><h2 id="VXLAN模式的跨主机通信过程"><a href="#VXLAN模式的跨主机通信过程" class="headerlink" title="VXLAN模式的跨主机通信过程"></a>VXLAN模式的跨主机通信过程</h2><p>为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。VTEP 设备的作用，其实跟前面的 flanneld 进程非常相似。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块）。</p><p>基于Flannel VXLAN模式的跨主通信的基本原理</p><img src="/img/body/jike/03185fab251a833fef7ed6665d5049f5.jpg" alt="基于Flannel VXLAN模式的跨主通信的基本原理" style="zoom:80%;max-width: 80%" /><p>当 container-1 发出请求之后，这个目的地址是 10.1.16.3 的 IP 包，会先出现在 docker0 网桥，然后被路由到本机 flannel.1 设备进行处理。</p><p>为了能够将“原始 IP 包”封装并且发送到正确的宿主机，VXLAN 就需要找到目的宿主机的 VTEP 设备。而这个设备的信息，正是每台宿主机上的 flanneld 进程负责维护的。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ route -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">...</span><br><span class="line">10.1.16.0       10.1.16.0       255.255.255.0   UG    0      0        0 flannel.1</span><br></pre></td></tr></table></figure><p>这条规则的意思是：凡是发往 10.1.16.0/24 网段的 IP 包，都需要经过 flannel.1 设备发出，并且，它最后被发往的网关地址是：10.1.16.0。</p><p>“源 VTEP 设备”收到“原始 IP 包”后，就要想办法把“原始 IP 包”加上一个目的 MAC 地址，封装成一个二层数据帧，然后发送给“目的 VTEP 设备”</p><p>ARP 记录，也是 flanneld 进程在 Node 2 节点启动时，自动添加在 Node 1 上的。我们可以通过 ip 命令看到它，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在Node 1上</span><br><span class="line">$ ip neigh show dev flannel.1</span><br><span class="line">10.1.16.0 lladdr 5e:f8:4f:00:e3:37 PERMANENT</span><br></pre></td></tr></table></figure><p>有了这个“目的 VTEP 设备”的 MAC 地址，Linux 内核就可以开始二层封包工作了。这个二层帧的格式，如下所示：</p><img src="https://static001.geekbang.org/resource/image/f2/55/f208fba66d2b58405864882342b23255.jpg" alt="img" style="zoom:80%;" /><p>Linux 内核会把“目的 VTEP 设备”的 MAC 地址，填写在图中的 Inner Ethernet Header 字段，得到一个二层数据帧。上述封包过程只是加一个二层头，不会改变“原始 IP 包”的内容。</p><p>上面提到的这些 VTEP 设备的 MAC 地址，对于宿主机网络来说并没有什么实际意义。所以上面封装出来的这个数据帧，并不能在我们的宿主机二层网络里传输。为了方便叙述，我们把它称为“内部数据帧”（Inner Ethernet Frame）。</p><p>所以接下来，Linux 内核还需要再把“内部数据帧”进一步封装成为宿主机网络里的一个普通的数据帧，好让它“载着”“内部数据帧”，通过宿主机的 eth0 网卡进行传输。我们把这次要封装出来的、宿主机对应的数据帧称为“外部数据帧”（Outer Ethernet Frame）。</p><p>Linux 内核会在“内部数据帧”前面，加上一个特殊的 VXLAN 头，而这个 VXLAN 头里有一个重要的标志叫作 VNI，它是 VTEP 设备识别某个数据帧是不是应该归自己处理的重要标识。而在 Flannel 中，VNI 的默认值是 1，这也是为何，宿主机上的 VTEP 设备都叫作 flannel.1 的原因，这里的“1”，其实就是 VNI 的值。</p><p>然后，Linux 内核会把这个数据帧封装进一个 UDP 包里发出去。</p><p>在宿主机看来，它会以为自己的 flannel.1 设备只是在向另外一台宿主机的 flannel.1 设备，发起了一次普通的 UDP 链接。这个 UDP 包里面，其实是一个完整的二层数据帧。</p><p>封装出来的“外部数据帧”的格式，如下所示：</p><img src="https://static001.geekbang.org/resource/image/8c/85/8cede8f74a57617494027ba137383f85.jpg" alt="图5 Flannel VXLAN模式的外部帧" style="zoom:80%;" /><p>Node 2 的内核网络栈会发现这个数据帧里有 VXLAN Header，并且 VNI=1。所以 Linux 内核会对它进行拆包，拿到里面的内部数据帧，然后根据 VNI 的值，把它交给 Node 2 上的 flannel.1 设备。</p><p>而 flannel.1 设备则会进一步拆包，取出“原始 IP 包”。接下来就回到了我在上一篇文章中分享的单机容器网络的处理流程。最终，IP 包就进入到了 container-2 容器的 Network Namespace 里。</p><h1 id="34-Kubernetes网络模型与CNI网络插件"><a href="#34-Kubernetes网络模型与CNI网络插件" class="headerlink" title="34 | Kubernetes网络模型与CNI网络插件"></a>34 | Kubernetes网络模型与CNI网络插件</h1><p>用户的容器都连接在 docker0 网桥上。而网络插件则在宿主机上创建了一个特殊的设备（UDP 模式创建的是 TUN 设备，VXLAN 模式创建的则是 VTEP 设备），docker0 与这个设备之间，通过 IP 转发（路由表）进行协作。</p><p>网络插件真正要做的事情，则是通过某种方法，把不同宿主机上的特殊设备连通，从而达到容器跨主机通信的目的。</p><p>实际上，上面这个流程，也正是 Kubernetes 对容器网络的主要处理方法。只不过，Kubernetes 是通过一个叫作 CNI 的接口，维护了一个单独的网桥来代替 docker0。这个网桥的名字就叫作：CNI 网桥，它在宿主机上的设备名称默认是：cni0。</p><p>以 Flannel 的 VXLAN 模式为例，在 Kubernetes 环境里，它的工作方式跟我们在上一篇文章中讲解的没有任何不同。只不过，docker0 网桥被替换成了 CNI 网桥而已，如下所示：</p><img src="/img/body/jike/9f11d8716f6d895ff6d1c813d460488c.jpg" alt="img" style="zoom:67%;max-width: 80%" /><p>在这里，Kubernetes 为 Flannel 分配的子网范围是 10.244.0.0/16。这个参数可以在部署的时候指定，比如：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubeadm init --pod-network-cidr&#x3D;10.244.0.0&#x2F;16</span><br></pre></td></tr></table></figure><p>也可以在部署完成后，通过修改 kube-controller-manager 的配置文件来指定。</p><h2 id="CNI-网桥"><a href="#CNI-网桥" class="headerlink" title="CNI 网桥"></a>CNI 网桥</h2><p>CNI 网桥只是接管所有 CNI 插件负责的、即 Kubernetes 创建的容器（Pod）。而此时，如果你用 docker run 单独启动一个容器，那么 Docker 项目还是会把这个容器连接到 docker0 网桥上。所以这个容器的 IP 地址，一定是属于 docker0 网桥的 172.17.0.0/16 网段。</p><p>Kubernetes 之所以要设置这样一个与 docker0 网桥功能几乎一样的 CNI 网桥，主要原因包括两个方面：</p><ul><li>一方面，Kubernetes 项目并没有使用 Docker 的网络模型（CNM），所以它并不希望、也不具备配置 docker0 网桥的能力；</li><li>另一方面，这还与 Kubernetes 如何配置 Pod，也就是 Infra 容器的 Network Namespace 密切相关。</li></ul><p>Kubernetes 创建一个 Pod 的第一步，就是创建并启动一个 Infra 容器，用来创建这个 Pod 的 Network Namespace。 CNI 的设计思想，就是：Kubernetes 在启动 Infra 容器之后，就可以直接调用 CNI 网络插件，为这个 Infra 容器的 Network Namespace，配置符合预期的网络栈。</p><p>CNI 的基础可执行文件，按照功能可以分为三类：</p><p>第一类，叫作 Main 插件，它是用来创建具体网络设备的二进制文件。比如，bridge（网桥设备）、ipvlan。</p><p>第二类，叫作 IPAM（IP Address Management）插件，它是负责分配 IP 地址的二进制文件。比如，dhcp，这个文件会向 DHCP 服务器发起请求；host-local，则会使用预先配置的 IP 地址段来进行分配。</p><p>第三类，是由 CNI 社区维护的内置 CNI 插件。比如：flannel，就是专门为 Flannel 项目提供的 CNI 插件。</p><h2 id="Kubernetes-的容器网络方案"><a href="#Kubernetes-的容器网络方案" class="headerlink" title="Kubernetes 的容器网络方案"></a>Kubernetes 的容器网络方案</h2><p>如果要实现一个给 Kubernetes 用的容器网络方案，其实需要做两部分工作，以 Flannel 项目为例：</p><p>首先，实现这个网络方案本身。主要就是 flanneld 进程里的主要逻辑。比如，创建和配置 flannel.1 设备、配置宿主机路由、配置 ARP 和 FDB 表里的信息等等。</p><p>然后，实现该网络方案对应的 CNI 插件。主要需要做的，就是配置 Infra 容器里面的网络栈，并把它连接在 CNI 网桥上。</p><p>需要注意的是，在 Kubernetes 中，处理容器网络相关的逻辑并不会在 kubelet 主干代码里执行，而是会在具体的 CRI（Container Runtime Interface，容器运行时接口）实现里完成。对于 Docker 项目来说，它的 CRI 实现叫作 dockershim，你可以在 kubelet 的代码里找到它。</p><p>所以，接下来 dockershim 会加载上述的 CNI 配置文件。需要注意，Kubernetes 目前不支持多个 CNI 插件混用。如果你在 CNI 配置目录（/etc/cni/net.d）里放置了多个 CNI 配置文件的话，dockershim 只会加载按字母顺序排序的第一个插件。</p><p>这时候，dockershim 会把这个 CNI 配置文件加载起来，并且把列表里的第一个插件、也就是 flannel 插件，设置为默认插件。而在后面的执行过程中，flannel 和 portmap 插件会按照定义顺序被调用，从而依次完成“配置容器网络”和“配置端口映射”这两步操作。</p><h2 id="CNI-插件的工作原理"><a href="#CNI-插件的工作原理" class="headerlink" title="CNI 插件的工作原理"></a>CNI 插件的工作原理</h2><p>当 kubelet 组件需要创建 Pod 的时候，它第一个创建的是 Infra 容器。所以在这一步，dockershim 就会先调用 Docker API 创建并启动 Infra 容器，紧接着执行一个叫作 SetUpPod 的方法。这个方法的作用就是：为 CNI 插件准备参数，然后调用 CNI 插件为 Infra 容器配置网络。</p><p>这里要调用的 CNI 插件，就是 /opt/cni/bin/flannel；而调用它所需要的参数，分为两部分。</p><p>第一部分，是由 dockershim 设置的一组 CNI 环境变量。</p><p>其中，最重要的环境变量参数叫作：CNI_COMMAND。它的取值只有两种：ADD 和 DEL。这个 ADD 和 DEL 操作，就是 CNI 插件唯一需要实现的两个方法。其中 ADD 操作的含义是：把容器添加到 CNI 网络里；DEL 操作的含义则是：把容器从 CNI 网络里移除掉。而对于网桥类型的 CNI 插件来说，这两个操作意味着把容器以 Veth Pair 的方式“插”到 CNI 网桥上，或者从网桥上“拔”掉。</p><p>第二部分，则是 dockershim 从 CNI 配置文件里加载到的、默认插件的配置信息。</p><p>而有了这两部分参数，Flannel CNI 插件实现 ADD 操作的过程就非常简单了。</p><h1 id="35-解读Kubernetes三层网络方案"><a href="#35-解读Kubernetes三层网络方案" class="headerlink" title="35 | 解读Kubernetes三层网络方案"></a>35 | 解读Kubernetes三层网络方案</h1><h2 id="host-gw-模式"><a href="#host-gw-模式" class="headerlink" title="host-gw 模式"></a>host-gw 模式</h2><p><strong>Flannel 的 host-gw 模式示例图</strong></p><img src="/img/body/jike/3d8b08411eeb49be2658eb4352206d25.png" alt="图1 Flannel host-gw示意图" style="zoom:67%;max-width: 80%" /><p>Node 1 上的 Infra-container-1，需要访问 Node 2 上的 Infra-container-2。</p><p>当你设置 Flannel 使用 host-gw 模式之后，flanneld 会在宿主机上创建这样一条规则，以 Node 1 为例：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ip route</span><br><span class="line">...</span><br><span class="line">10.244.1.0&#x2F;24 via 10.168.0.3 dev eth0</span><br></pre></td></tr></table></figure><p>这条路由规则的含义是：目的 IP 地址属于 10.244.1.0/24 网段的 IP 包，应该经过本机的 eth0 设备发出去（即：dev eth0）；并且，它下一跳地址（next-hop）是 10.168.0.3（即：via 10.168.0.3）。从 host-gw 示意图中可以看到，这个下一跳地址是我们的目的宿主机 Node 2。</p><p>配置了下一跳地址，那么接下来，当 IP 包从网络层进入链路层封装成帧的时候，eth0 设备就会使用下一跳地址对应的 MAC 地址，作为该数据帧的目的 MAC 地址。显然，这个 MAC 地址，正是 Node 2 的 MAC 地址。这样，这个数据帧就会从 Node 1 通过宿主机的二层网络顺利到达 Node 2 上。</p><p>而 Node 2 的内核网络栈从二层数据帧里拿到 IP 包后，会看到这个 IP 包的目的 IP 地址是 10.244.1.3，即 Infra-container-2 的 IP 地址。这时候，根据 Node 2 上的路由表，该目的地址会匹配到第二条路由规则（也就是 10.244.1.0 对应的路由规则），从而进入 cni0 网桥，进而进入到 Infra-container-2 当中。</p><blockquote><p>​    <strong>host-gw 模式的工作原理，其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。</strong></p></blockquote><p>Flannel host-gw 模式会使用路由表里的“下一跳”来设置目的 MAC 地址,所以必须要求集群宿主机之间是二层连通的。</p><p>Flannel 子网和主机的信息，都是保存在 Etcd 当中的。flanneld 只需要 WACTH 这些数据的变化，然后实时更新路由表即可。</p><p>Flannel host-gw 模式下，容器通信的过程就免除了额外的封包和解包带来的性能损耗。根据实际的测试，host-gw 的性能损失大约在 10% 左右，而其他所有基于 VXLAN“隧道”机制的网络方案，性能损失都在 20%~30% 左右。</p><h2 id="BGP协议"><a href="#BGP协议" class="headerlink" title="BGP协议"></a>BGP协议</h2><p>BGP 的全称是 Border Gateway Protocol，即：边界网关协议。它是一个 Linux 内核原生就支持的、专门用在大规模数据中心里维护不同的“自治系统”之间路由信息的、无中心的路由协议。</p><img src="https://static001.geekbang.org/resource/image/2e/9b/2e4b3bee1d924f4ae25e2c1fd115379b.jpg" alt="img" style="zoom:80%;max-width: 60%" /><p>在这个图中，我们有两个自治系统（Autonomous System，简称为 AS）：AS 1 和 AS 2。而所谓的一个自治系统，指的是一个组织管辖下的所有 IP 网络和路由器的全体。你可以把它想象成一个小公司里的所有主机和路由器。在正常情况下，自治系统之间不会有任何“来往”。</p><p>但是，如果这样两个自治系统里的主机，要通过 IP 地址直接进行通信，我们就必须使用路由器把这两个自治系统连接起来。</p><p>像上面这样负责把自治系统连接在一起的路由器，我们就把它形象地称为：边界网关。它跟普通路由器的不同之处在于，它的路由表里拥有其他自治系统里的主机路由信息。</p><p>在使用了 BGP 之后，你可以认为，在每个边界网关上都会运行着一个小程序，它们会将各自的路由表信息，通过 TCP 传输给其他的边界网关。而其他边界网关上的这个小程序，则会对收到的这些数据进行分析，然后将需要的信息添加到自己的路由表里。</p><p>所以 BGP，就是在大规模网络中实现节点路由信息共享的一种协议。</p><h2 id="Calico-项目"><a href="#Calico-项目" class="headerlink" title="Calico 项目"></a>Calico 项目</h2><p>Calico 项目提供的网络解决方案，与 Flannel 的 host-gw 模式，几乎是完全一样的。Calico 也会在每台宿主机上，添加一个格式如下所示的路由规则：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;目的容器IP地址段&gt; via &lt;网关的IP地址&gt; dev eth0</span><br></pre></td></tr></table></figure><p>不同于 Flannel 通过 Etcd 和宿主机上的 flanneld 来维护路由信息的做法，Calico 项目使用了BGP来自动地在整个集群中分发路由信息。BGP 取代了 Flannel 维护主机上路由表的功能。而且，BGP 这种原生就是为大规模网络环境而实现的协议，其可靠性和可扩展性，远非 Flannel 自己的方案可比。</p><p>Calico 项目的架构由三个部分组成：</p><ol><li>Calico 的 CNI 插件。这是 Calico 与 Kubernetes 对接的部分。</li><li>Felix。它是一个 DaemonSet，负责在宿主机上插入路由规则（即：写入 Linux 内核的 FIB 转发信息库），以及维护 Calico 所需的网络设备等工作。</li><li>BIRD。它就是 BGP 的客户端，专门负责在集群里分发路由规则信息。</li></ol><p>除了对路由信息的维护方式之外，Calico 项目与 Flannel 的 host-gw 模式的另一个不同之处，就是它不会在宿主机上创建任何网桥设备。这时候，Calico 的工作方式，可以用一幅示意图来描述，如下所示（下面我会统一用“BGP 示意图”来指代它）：</p><img src="https://static001.geekbang.org/resource/image/8d/1b/8db6dee96c4242738ae2878e58cecd1b.jpg" alt="图3 Calico工作原理" style="zoom:67%;max-width: 60%" /><p>Calico 的 CNI 插件会为每个容器设置一个 Veth Pair 设备，然后把其中的一端放置在宿主机上（它的名字以 cali 前缀开头）。</p><p>此外，由于 Calico 没有使用 CNI 的网桥模式，Calico 的 CNI 插件还需要在宿主机上为每个容器的 Veth Pair 设备配置一条路由规则，用于接收传入的 IP 包。比如，宿主机 Node 2 上的 Container 4 对应的路由规则，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">10.233.2.3 dev cali5863f3 scope link</span><br></pre></td></tr></table></figure><p>即：发往 10.233.2.3 的 IP 包，应该进入 cali5863f3 设备。</p><blockquote><p>基于上述原因，Calico  项目在宿主机上设置的路由规则，肯定要比 Flannel 项目多得多。不过，Flannel host-gw 模式使用 CNI  网桥的主要原因，其实是为了跟 VXLAN 模式保持一致。否则的话，Flannel 就需要维护两套 CNI 插件了。</p></blockquote><p>有了这样的 Veth Pair 设备之后，容器发出的 IP 包就会经过 Veth Pair 设备出现在宿主机上。然后，宿主机网络栈就会根据路由规则的下一跳 IP 地址，把它们转发给正确的网关。接下来的流程就跟 Flannel host-gw 模式完全一致了。</p><p>其中，这里最核心的“下一跳”路由规则，就是由 Calico 的 Felix 进程负责维护的。这些路由规则信息，则是通过 BGP Client 也就是 BIRD 组件，使用 BGP 协议传输而来的。</p><p>而这些通过 BGP 协议传输的消息，你可以简单地理解为如下格式：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[BGP消息]</span><br><span class="line">我是宿主机192.168.1.3</span><br><span class="line">10.233.2.0&#x2F;24网段的容器都在我这里</span><br><span class="line">这些容器的下一跳地址是我</span><br></pre></td></tr></table></figure><p>不难发现，Calico 项目实际上将集群里的所有节点，都当作是边界路由器来处理，它们一起组成了一个全连通的网络，互相之间通过 BGP 协议交换路由规则。这些节点，我们称为 BGP Peer。</p><h2 id="Route-Reflector-的模式"><a href="#Route-Reflector-的模式" class="headerlink" title="Route Reflector 的模式"></a>Route Reflector 的模式</h2><p>Calico 维护的网络在默认配置下，是一个被称为“Node-to-Node Mesh”的模式。这时候，每台宿主机上的 BGP Client 都需要跟其他所有节点的 BGP Client 进行通信以便交换路由信息。但是，随着节点数量 N 的增加，这些连接的数量就会以 N²的规模快速增长，从而给集群本身的网络带来巨大的压力。</p><p>所以，Node-to-Node Mesh 模式一般推荐用在少于 100 个节点的集群里。而在更大规模的集群中，你需要用到的是一个叫作 Route Reflector 的模式。</p><p>在这种模式下，Calico 会指定一个或者几个专门的节点，来负责跟所有节点建立 BGP 连接从而学习到全局的路由规则。而其他节点，只需要跟这几个专门的节点交换路由信息，就可以获得整个集群的路由规则信息了。</p><p>这些专门的节点，就是所谓的 Route Reflector 节点，它们实际上扮演了“中间代理”的角色，从而把 BGP 连接的规模控制在 N 的数量级上。</p><h2 id="Calico的IPIP-模式"><a href="#Calico的IPIP-模式" class="headerlink" title="Calico的IPIP 模式"></a>Calico的IPIP 模式</h2><p>Flannel host-gw 模式最主要的限制，就是要求集群宿主机之间是二层连通的。而这个限制对于 Calico 来说，也同样存在。</p><p>如果二层网络不通，你就需要为 Calico 打开 IPIP 模式。</p><p>这个模式下容器通信的原理，如图所示</p><img src="/img/body/jike/4dd9ad6415caf68da81562d9542049c9.jpg" alt="图4 Calico IPIP模式工作原理" style="zoom:67%; max-width: 60%" /><p>在 Calico 的 IPIP 模式下，Felix 进程在 Node 1 上添加的路由规则，会稍微不同，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">10.233.2.0&#x2F;24 via 192.168.2.2 tunl0</span><br></pre></td></tr></table></figure><p>可以看到，尽管这条规则的下一跳地址仍然是 Node 2 的 IP 地址，但这一次，要负责将 IP 包发出去的设备，变成了 tunl0。注意，是 T-U-N-L-0，而不是 Flannel UDP 模式使用的 T-U-N-0（tun0），这两种设备的功能是完全不一样的。Calico 使用的这个 tunl0 设备，是一个 IP 隧道（IP tunnel）设备。</p><p>IP 包进入 IP 隧道设备之后，就会被 Linux 内核的 IPIP 驱动接管。IPIP 驱动会将这个 IP 包直接封装在一个宿主机网络的 IP 包中，</p><img src="/img/body/jike/fc2b4173782b7a993f4a43a2cb966f90.jpg" alt="图 5 IPIP 封包方式" style="zoom:67%; max-width: 50%" /><p>而原 IP 包本身，则会被直接封装成新 IP 包的 Payload。这样，原先从容器到 Node 2 的 IP 包，就被伪装成了一个从 Node 1 到 Node 2 的 IP 包。</p><p>以上就是 Calico 项目主要的工作原理了。</p><h2 id="IPIP-模式替代方案"><a href="#IPIP-模式替代方案" class="headerlink" title="IPIP 模式替代方案"></a>IPIP 模式替代方案</h2><p>当 Calico 使用 IPIP 模式的时候，集群的网络性能会因为额外的封包和解包工作而下降。在实际测试中，Calico IPIP 模式与 Flannel VXLAN 模式的性能大致相当。所以，在实际使用时，如非硬性需求，我建议你将所有宿主机节点放在一个子网里，避免使用 IPIP。</p><p>在 Calico 项目中，它已经为你提供了两种将宿主机网关设置成 BGP Peer 的解决方案。</p><p>第一种方案，就是所有宿主机都跟宿主机网关建立 BGP Peer 关系。</p><p>第二种方案，是使用一个或多个独立组件负责搜集整个集群里的所有路由信息，然后通过 BGP 协议同步给网关。而我们前面提到，在大规模集群中，Calico 本身就推荐使用 Route Reflector 节点的方式进行组网。所以，这里负责跟宿主机网关进行沟通的独立组件，直接由 Route Reflector 兼任即可。</p><p>更重要的是，这种情况下网关的 BGP Peer 个数是有限并且固定的。所以我们就可以直接把这些独立组件配置成路由器的 BGP Peer，而无需 Dynamic Neighbors 的支持。</p><p>当然，这些独立组件的工作原理也很简单：它们只需要 WATCH Etcd 里的宿主机和对应网段的变化信息，然后把这些信息通过 BGP 协议分发给网关即可。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 深入剖析Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 深入剖析Kuernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>28-31.Kubernetes容器持久化存储</title>
      <link href="/2020/09/24/28-31-kubernetes%E5%AE%B9%E5%99%A8%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/"/>
      <url>/2020/09/24/28-31-kubernetes%E5%AE%B9%E5%99%A8%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/</url>
      
        <content type="html"><![CDATA[<p>极客时间课程张磊的 <a href="https://time.geekbang.org/column/intro/116" target="_blank" rel="noopener">深入剖析Kuernetes</a> </p><h1 id="28-PV、PVC、StorageClass，这些到底在说啥？"><a href="#28-PV、PVC、StorageClass，这些到底在说啥？" class="headerlink" title="28 | PV、PVC、StorageClass，这些到底在说啥？"></a>28 | PV、PVC、StorageClass，这些到底在说啥？</h1><p>PV 描述的，是持久化存储数据卷。这个 API 对象主要定义的是一个持久化存储在宿主机上的目录，比如一个 NFS 的挂载目录。</p><p>PVC 描述的，则是 Pod 所希望使用的持久化存储的属性。比如，Volume 存储的大小、可读写权限等等。</p><p>用户创建的 PVC 要真正被容器使用起来，就必须先和某个符合条件的 PV 进行绑定。这里要检查的条件，包括两部分：</p><ol><li>PV 和 PVC 的 spec 字段。比如，PV 的存储（storage）大小，就必须满足 PVC 的要求。</li><li>PV 和 PVC 的 storageClassName 字段必须一样。因为Kubernetes 只会将 StorageClass 相同的 PVC 和 PV 绑定起来</li></ol><p>PVC 和 PV 的设计，其实跟“面向对象”的思想完全一致。PVC 可以理解为持久化存储的“接口”，它提供了对某种持久化存储的描述，但不提供具体的实现；而这个持久化存储的实现部分则由 PV 负责完成。</p><p>在 Kubernetes 中，实际上存在着一个专门处理持久化存储的控制器，叫作 Volume Controller。这个 Volume Controller 维护着多个控制循环，其中有一个循环，就是在对 PV 和 PVC 进行“绑定”。它的名字叫作 PersistentVolumeController。</p><p>PersistentVolumeController 会不断地查看当前每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与这个“单身”的 PVC 进行绑定。这样，Kubernetes 就可以保证用户提交的每一个 PVC，只要有合适的 PV 出现，它就能够很快进入绑定状态。</p><p>PV 与 PVC 进行“绑定” ，其实就是将这个 PV 对象的名字，填在了 PVC 对象的 spec.volumeName 字段上。所以，接下来 Kubernetes 只要获取到这个 PVC 对象，就一定能够找到它所绑定的 PV。</p><h2 id="PV-对象如何变成容器里的一个持久化存储的呢？"><a href="#PV-对象如何变成容器里的一个持久化存储的呢？" class="headerlink" title="PV 对象如何变成容器里的一个持久化存储的呢？"></a>PV 对象如何变成容器里的一个持久化存储的呢？</h2><p>容器的 Volume，其实就是将一个宿主机上的目录，跟一个容器里的目录绑定挂载在了一起。</p><p>持久化 Volume，指的就是这个宿主机上的目录，具备“持久性”。即：这个目录里面的内容，既不会因为容器的删除而被清理掉，也不会跟当前的宿主机绑定。这样，当容器被重启或者在其他节点上重建出来之后，它仍然能够通过挂载这个 Volume，访问到这些内容。所以，大多数情况下，持久化 Volume 的实现，往往依赖于一个远程存储服务</p><p>而 Kubernetes 需要做的工作，就是使用这些存储服务，来为容器准备一个持久化的宿主机目录，以供将来进行绑定挂载时使用。</p><p>这个准备“持久化”宿主机目录的过程，我们可以形象地称为“两阶段处理”。</p><ol><li>第一阶段：kubelet 就需要先将磁盘 挂载到 Pod 所在的宿主机上。在 Kubernetes 中，我们把这个阶段称为 Attach。</li><li>第二个阶段：格式化这个磁盘设备，然后将它挂载到宿主机指定的挂载点上。我们一般称为：Mount。</li></ol><p>在这一步，kubelet 需要作为 client，将远端 NFS 服务器的目录（比如：“/”目录），挂载到 Volume 的宿主机目录上。通过这个挂载操作，Volume 的宿主机目录就成为了一个远程 NFS 目录的挂载点，后面你在这个目录里写入的所有文件，都会被保存在远程 NFS 服务器上。所以，我们也就完成了对这个 Volume 宿主机目录的“持久化”。所以，接下来，kubelet 只要把这个 Volume 目录通过 CRI 里的 Mounts 参数，传递给 Docker，然后就可以为 Pod 里的容器挂载这个“持久化”的 Volume 了。</p><h2 id="PV-的流程有单独的控制循环"><a href="#PV-的流程有单独的控制循环" class="headerlink" title="PV 的流程有单独的控制循环"></a>PV 的流程有单独的控制循环</h2><p>关于 PV 的“两阶段处理”流程，是靠独立于 kubelet 主控制循环（Kubelet Sync Loop）之外的两个控制循环来实现的。</p><p>其中，“第一阶段”的 Attach（以及 Dettach）操作，是由 Volume Controller 负责维护的，这个控制循环的名字叫作：AttachDetachController。而它的作用，就是不断地检查每一个 Pod 对应的 PV，和这个 Pod 所在宿主机之间挂载情况。从而决定，是否需要对这个 PV 进行 Attach（或者 Dettach）操作。</p><p>需要注意，作为一个 Kubernetes 内置的控制器，Volume Controller 自然是 kube-controller-manager 的一部分。所以，AttachDetachController 也一定是运行在 Master 节点上的。当然，Attach 操作只需要调用公有云或者具体存储项目的 API，并不需要在具体的宿主机上执行操作，所以这个设计没有任何问题。</p><p>而“第二阶段”的 Mount（以及 Unmount）操作，必须发生在 Pod 对应的宿主机上，所以它必须是 kubelet 组件的一部分。这个控制循环的名字，叫作：VolumeManagerReconciler，它运行起来之后，是一个独立于 kubelet 主循环的 Goroutine。</p><p>通过这样将 Volume 的处理同 kubelet 的主循环解耦，Kubernetes 就避免了这些耗时的远程挂载操作拖慢 kubelet 的主控制循环，进而导致 Pod 的创建效率大幅下降的问题。实际上，kubelet 的一个主要设计原则，就是它的主控制循环绝对不可以被 block。</p><h2 id="StorageClass"><a href="#StorageClass" class="headerlink" title="StorageClass"></a>StorageClass</h2><p>Kubernetes 为我们提供了一套可以自动创建 PV 的机制，即：Dynamic Provisioning。</p><p>Dynamic Provisioning 机制工作的核心，在于一个名叫 StorageClass 的 API 对象。</p><p>而 StorageClass  对象的作用，其实就是创建 PV 的模板。</p><p>具体地说，StorageClass 对象会定义如下两个部分内容：</p><ol><li><p>PV  的属性。比如，存储类型、Volume 的大小等等。</p></li><li><p>创建这种 PV 需要用到的存储插件。比如，Ceph 等等。</p></li></ol><p>有了这样两个信息之后，Kubernetes 就能够根据用户提交的 PVC，找到一个对应的 StorageClass 了。然后，Kubernetes 就会调用该 StorageClass 声明的存储插件，创建出需要的 PV。</p><p>实际上，如果你的集群已经开启了名叫  DefaultStorageClass 的 Admission Plugin，它就会为 PVC 和 PV 自动添加一个默认的  StorageClass；否则，PVC 的 storageClassName 的值就是“”，这也意味着它只能够跟  storageClassName 也是“”的 PV 进行绑定。</p><h2 id="PVC-和-PV-示意图"><a href="#PVC-和-PV-示意图" class="headerlink" title="PVC 和 PV 示意图"></a>PVC 和 PV 示意图</h2><img src="/img/body/jike/e8b2586e4e14eb54adf8ff95c5c18cd9.png" alt="PVC 和 PV 示意图" style="zoom:67%;max-width: 50%" /><p>从图中我们可以看到，在这个体系中：PVC 描述的，是 Pod 想要使用的持久化存储的属性，比如存储的大小、读写权限等。PV 描述的，则是一个具体的 Volume 的属性，比如 Volume 的类型、挂载目录、远程存储服务器地址等。而 StorageClass 的作用，则是充当 PV 的模板。并且，只有同属于一个 StorageClass 的 PV 和 PVC，才可以绑定在一起。</p><h1 id="29-PV、PVC体系是不是多此一举？从本地持久化卷谈起"><a href="#29-PV、PVC体系是不是多此一举？从本地持久化卷谈起" class="headerlink" title="29 | PV、PVC体系是不是多此一举？从本地持久化卷谈起"></a>29 | PV、PVC体系是不是多此一举？从本地持久化卷谈起</h1><p>“不能用”“不好用”“需要定制开发”，这才是落地开源基础设施项目的三大常态。</p><p>而在持久化存储领域，用户呼声最高的定制化需求，莫过于支持“本地”持久化存储了。</p><h2 id="Local-Persistent-Volume-局限性"><a href="#Local-Persistent-Volume-局限性" class="headerlink" title="Local Persistent Volume 局限性"></a>Local Persistent Volume 局限性</h2><p>事实上，它的适用范围非常固定，比如：高优先级的系统应用，需要在多个不同节点上存储数据，并且对 I/O 较为敏感。典型的应用包括：分布式数据存储比如 MongoDB、Cassandra 等，分布式文件系统比如 GlusterFS、Ceph 等，以及需要在本地磁盘上进行大量数据缓存的分布式应用。</p><p>其次，相比于正常的 PV，一旦这些节点宕机且不能恢复时，Local Persistent Volume 的数据就可能丢失。这就要求使用 Local Persistent Volume 的应用必须具备数据备份和恢复的能力，允许你把这些数据定时备份在其他位置。</p><h2 id="Local-Persistent-Volume-的设计难点"><a href="#Local-Persistent-Volume-的设计难点" class="headerlink" title="Local Persistent Volume 的设计难点"></a>Local Persistent Volume 的设计难点</h2><ol><li><p>如何把本地磁盘抽象成 PV</p><p>你绝不应该把一个宿主机上的目录当作 PV 使用。这是因为，这种本地目录的存储行为完全不可控，它所在的磁盘随时都可能被应用写满，甚至造成整个宿主机宕机。而且，不同的本地目录之间也缺乏哪怕最基础的 I/O 隔离机制。所以你需要在集群里配置好磁盘或者块设备。</p></li><li><p>调度器如何保证 Pod 始终能被正确地调度到它所请求的 Local Persistent Volume 所在的节点上呢？</p><p>所以，这时候，调度器就必须能够知道所有节点与 Local Persistent Volume 对应的磁盘的关联关系，然后根据这个信息来调度 Pod。这个原则，我们可以称为“在调度的时候考虑 Volume 分布”。在 Kubernetes 的调度器里，有一个叫作 VolumeBindingChecker 的过滤条件专门负责这个事情。</p></li></ol><h2 id="延迟绑定"><a href="#延迟绑定" class="headerlink" title="延迟绑定"></a>延迟绑定</h2><p> StorageClass 定义了 volumeBindingMode=WaitForFirstConsumer 的属性。它是 Local Persistent Volume 里一个非常重要的特性，即：延迟绑定。在使用 Local Persistent Volume 的时候，我们必须想办法推迟这个“绑定”操作。推迟到调度的时候。因为调度完成才能确定本地磁盘主机。</p><h2 id="架构的优势"><a href="#架构的优势" class="headerlink" title="架构的优势"></a>架构的优势</h2><p>这套存储体系带来了“解耦”的好处。其实，Kubernetes 很多看起来比较“繁琐”的设计（比如“声明式 API”，以及我今天讲解的“PV、PVC 体系”）的主要目的，都是希望为开发者提供更多的“可扩展性”，给使用者带来更多的“稳定性”和“安全感”。这两个能力的高低，是衡量开源基础设施项目水平的重要标准。      </p><h1 id="30-编写自己的存储插件：FlexVolume与CSI"><a href="#30-编写自己的存储插件：FlexVolume与CSI" class="headerlink" title="30 | 编写自己的存储插件：FlexVolume与CSI"></a>30 | 编写自己的存储插件：FlexVolume与CSI</h1><p>略：主要内容为开发</p><h1 id="31-容器存储实践：CSI插件编写指南"><a href="#31-容器存储实践：CSI插件编写指南" class="headerlink" title="31 | 容器存储实践：CSI插件编写指南"></a>31 | 容器存储实践：CSI插件编写指南</h1><p>略：主要内容为开发</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 深入剖析Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 深入剖析Kuernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>部署zookeeper3.6.1集群</title>
      <link href="/2020/09/23/%E9%83%A8%E7%BD%B2zookeeper3-6-1%E9%9B%86%E7%BE%A4/"/>
      <url>/2020/09/23/%E9%83%A8%E7%BD%B2zookeeper3-6-1%E9%9B%86%E7%BE%A4/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>因为公司架构升级，新版spring boot和zookeeper3.4.10兼容性不太好，应开发要求，需要将zookeeper升级到最新版本3.6.1.一开始想用k8s部署，困难重重，没有现成的适配于zookeeper3.6.1的镜像，官方的镜像不能直接使用，所以先部署docker集群让开发使用。</p><p>通过部署docker,对脚本和内部参数有了了解，然后通过对docker zookeeper3.6.1简单修改，和zookeeper3.4.10 statefulset脚本通用，完美。</p><h1 id="docker部署zookeeper3-6-1集群"><a href="#docker部署zookeeper3-6-1集群" class="headerlink" title="docker部署zookeeper3.6.1集群"></a>docker部署zookeeper3.6.1集群</h1><blockquote><p> <a href="https://archive.apache.org/dist/zookeeper/zookeeper-3.6.1/apache-zookeeper-3.6.1-bin.tar.gz" target="_blank" rel="noopener"> zookeeper-3.6.1下载 </a> </p></blockquote><h2 id="下载镜像到本地"><a href="#下载镜像到本地" class="headerlink" title="下载镜像到本地"></a>下载镜像到本地</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull zookeeper:3.6.1 </span><br><span class="line">docker tag 3bbbc7545ea8 harbor.od.com&#x2F;public&#x2F;zookeeper:3.6.1</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;zookeeper:3.6.1</span><br></pre></td></tr></table></figure><h2 id="使用docker启动服务"><a href="#使用docker启动服务" class="headerlink" title="使用docker启动服务"></a>使用docker启动服务</h2><p>Zookeeer3.6.1的docker默认启动命令为</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;docker-entrypoint.sh zkServer.sh start-foreground</span><br></pre></td></tr></table></figure><p>所以有关的变量，可以查看docker中的文件docker-entrypoint.sh</p><h3 id="创建本地存储路径"><a href="#创建本地存储路径" class="headerlink" title="创建本地存储路径"></a>创建本地存储路径</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;data&#x2F;zookeeper&#x2F;conf &#x2F;data&#x2F;zookeeper&#x2F;data</span><br></pre></td></tr></table></figure><h3 id="启动zookeeper服务"><a href="#启动zookeeper服务" class="headerlink" title="启动zookeeper服务"></a>启动zookeeper服务</h3><p>在wang-14节点上启动zookeeper服务，以本地文件为存储，主要添加了两个变量ZOO_MY_ID 和 ZOO_SERVERS</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -itd --name&#x3D;zk --rm --network host -v &#x2F;data&#x2F;zookeeper&#x2F;conf:&#x2F;conf -v &#x2F;data&#x2F;zookeeper&#x2F;data:&#x2F;data -e ZOO_SERVERS&#x3D;&quot;server.1&#x3D;zk1.od.com:2888:3888;2181 server.2&#x3D;zk2.od.com:2888:3888;2181 server.3&#x3D;zk3.od.com:2888:3888;2181&quot; -e ZOO_MY_ID&#x3D;1 harbor.od.com&#x2F;infra&#x2F;zookeeper:3.6.1</span><br></pre></td></tr></table></figure><p>主要修改的参数有</p><ul><li>–network host :使用宿主机的网络</li><li>-v /data/zookeeper/conf:/conf  :使用宿主机的/data/zookeeper/conf来存放配置文件</li><li>-v /data/zookeeper/data:/data  :使用宿主机的/data/zookeeper/date来存放数据</li><li>-e ZOO_SERVERS=”server.1=zk1.od.com:2888:3888;2181 server.2=zk2.od.com:2888:3888;2181 server.3=zk3.od.com:2888:3888;2181” , 设置集群，启动文件docker-entrypoint.sh会用到这个变量</li><li>-e ZOO_MY_ID=1 : 设置myid，启动文件docker-entrypoint.sh会用到这个变量</li></ul><p>另外两个节点类似，只是变量 ZOO_MY_ID 不一样，命令粘贴如下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 节点wang-15</span><br><span class="line">docker run -itd --name&#x3D;zk --rm --network host -v &#x2F;data&#x2F;zookeeper&#x2F;conf:&#x2F;conf -v &#x2F;data&#x2F;zookeeper&#x2F;data:&#x2F;data -e ZOO_SERVERS&#x3D;&quot;server.1&#x3D;zk1.od.com:2888:3888;2181 server.2&#x3D;zk2.od.com:2888:3888;2181 server.3&#x3D;zk3.od.com:2888:3888;2181&quot; -e ZOO_MY_ID&#x3D;2 harbor.od.com&#x2F;infra&#x2F;zookeeper:3.6.1 </span><br><span class="line"></span><br><span class="line"># 节点wang-16</span><br><span class="line">docker run -itd --name&#x3D;zk --rm --network host -v &#x2F;data&#x2F;zookeeper&#x2F;conf:&#x2F;conf -v &#x2F;data&#x2F;zookeeper&#x2F;data:&#x2F;data -e ZOO_SERVERS&#x3D;&quot;server.1&#x3D;zk1.od.com:2888:3888;2181 server.2&#x3D;zk2.od.com:2888:3888;2181 server.3&#x3D;zk3.od.com:2888:3888;2181&quot; -e ZOO_MY_ID&#x3D;3 harbor.od.com&#x2F;infra&#x2F;zookeeper:3.6.1</span><br></pre></td></tr></table></figure><h2 id="检测服务可用"><a href="#检测服务可用" class="headerlink" title="检测服务可用"></a>检测服务可用</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-14 bin]# .&#x2F;zkCli.sh -server zk1.od.com:2181</span><br><span class="line">[zk: 192.168.7.14:2181(CONNECTED) 0] create &#x2F;test &quot;test&quot; </span><br><span class="line">Created &#x2F;test</span><br><span class="line">[zk: 192.168.7.14:2181(CONNECTED) 0] ls &#x2F;</span><br><span class="line">[test, zookeeper]</span><br><span class="line"></span><br><span class="line">[root@wang-14 bin]# .&#x2F;zkCli.sh -server zk2.od.com:2181</span><br><span class="line">[zk: 192.168.7.15:2181(CONNECTED) 0] ls &#x2F;</span><br><span class="line">[test, zookeeper]</span><br></pre></td></tr></table></figure><p>新版的都有一个http服务，url为 <a href="http://192.168.7.14:8080/commands" target="_blank" rel="noopener">http://192.168.7.14:8080/commands</a> </p><h2 id="查看配置文件"><a href="#查看配置文件" class="headerlink" title="查看配置文件"></a>查看配置文件</h2><p>查看wang-14的配置文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-14 ~]# cat &#x2F;data&#x2F;zookeeper&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">dataDir&#x3D;&#x2F;data</span><br><span class="line">dataLogDir&#x3D;&#x2F;datalog</span><br><span class="line">tickTime&#x3D;2000</span><br><span class="line">initLimit&#x3D;5</span><br><span class="line">syncLimit&#x3D;2</span><br><span class="line">autopurge.snapRetainCount&#x3D;3</span><br><span class="line">autopurge.purgeInterval&#x3D;0</span><br><span class="line">maxClientCnxns&#x3D;60</span><br><span class="line">standaloneEnabled&#x3D;true</span><br><span class="line">admin.enableServer&#x3D;true</span><br><span class="line">server.1&#x3D;zk1.od.com:2888:3888;2181</span><br><span class="line">server.2&#x3D;zk2.od.com:2888:3888;2181</span><br><span class="line">server.3&#x3D;zk3.od.com:2888:3888;2181</span><br><span class="line">[root@wang-114 ~]# cat &#x2F;data&#x2F;zookeeper&#x2F;data&#x2F;myid </span><br><span class="line">1</span><br></pre></td></tr></table></figure><p>docker zookeeper:3.6.1 默认配置文件 /conf/zoo.cfg 如下，可以和我们的配置文件对比</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dataDir&#x3D;&#x2F;data  # 数据文件目录</span><br><span class="line">dataLogDir&#x3D;&#x2F;datalog  # 数据日志文件目录</span><br><span class="line">tickTime&#x3D;2000  # 心跳时间</span><br><span class="line">initLimit&#x3D;30  # 初始通信时间</span><br><span class="line">syncLimit&#x3D;10  # 同步通信时间</span><br><span class="line">autopurge.snapRetainCount&#x3D;3  # 保留文件的数量, 默认保留3个</span><br><span class="line">autopurge.purgeInterval&#x3D;0  #自动清理频率，单位是小时，0表示不开启</span><br><span class="line">maxClientCnxns&#x3D;60  # 单个客户端和单个服务器最大连接数，通过IP区分，默认60</span><br><span class="line">standaloneEnabled&#x3D;true  # 是否允许单机默认，默认允许。false代表分布式运行，即使只有一个zk</span><br><span class="line">admin.enableServer&#x3D;true  # HTTP接口，路径&#x2F;commands,默认开启 </span><br><span class="line">server.1&#x3D;localhost:2888:3888;2181  # 服务器编号&#x3D;服务器地址：leader服务器端口：选举端口；服务端口号</span><br></pre></td></tr></table></figure><p><strong>日志和数据文件示例</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test opt]# ls -l zookeeper-3.4.10&#x2F;data&#x2F;version-2</span><br><span class="line">-rw-r--r-- 1 root root 67108880 4月   3 17:00 log.117e2a</span><br><span class="line">-rw-r--r-- 1 root root 67108880 4月  24 14:53 log.126c7f</span><br><span class="line">-rw-r--r-- 1 root root  1725511 4月   3 17:00 snapshot.126c7e</span><br><span class="line">-rw-r--r-- 1 root root  1688328 4月  24 14:53 snapshot.13e331</span><br></pre></td></tr></table></figure><h1 id="k8s部署zookeeper3-6-1集群"><a href="#k8s部署zookeeper3-6-1集群" class="headerlink" title="k8s部署zookeeper3.6.1集群"></a>k8s部署zookeeper3.6.1集群</h1><h2 id="准备基础镜像"><a href="#准备基础镜像" class="headerlink" title="准备基础镜像"></a>准备基础镜像</h2><p>对官方镜像zookeeper:3.6.1进行初步优化</p><ol><li>zookeeper:3.6.1镜像默认配置文件地址为 /conf,为了规范化，我改为了/opt/zookeeper/conf 。</li><li>将k8s.gcr.io/kubernetes-zookeeper:1.0-3.4.10中的文件start-zookeeper和zookeeper-ready拷贝过来。start-zookeeper修改zookeeper配置文件，zookeeper-ready做探活使用。</li><li>修改apt源为阿里源，并添加一些常用命令 vim netstat ps telnet ping等。</li></ol><p><strong>dockerfile</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# cat Dockerfile </span><br><span class="line">FROM zookeeper:3.6.1</span><br><span class="line"></span><br><span class="line">RUN ln -s &#x2F;apache-zookeeper-3.6.1-bin &#x2F;opt&#x2F;zookeeper </span><br><span class="line">ENV ZOOCFGDIR&#x3D;&#x2F;opt&#x2F;zookeeper&#x2F;conf </span><br><span class="line"></span><br><span class="line">COPY start-zookeeper &#x2F;opt&#x2F;zookeeper&#x2F;bin</span><br><span class="line">COPY zookeeper-ready &#x2F;opt&#x2F;zookeeper&#x2F;bin</span><br><span class="line">COPY sources.list &#x2F;etc&#x2F;apt&#x2F;sources.list</span><br><span class="line"></span><br><span class="line">RUN chmod +x &#x2F;opt&#x2F;zookeeper&#x2F;bin&#x2F;start-zookeeper &#x2F;opt&#x2F;zookeeper&#x2F;bin&#x2F;zookeeper-ready &amp;&amp; \</span><br><span class="line">    chown zookeeper:zookeeper &#x2F;opt&#x2F;zookeeper&#x2F;bin&#x2F;start-zookeeper &#x2F;opt&#x2F;zookeeper&#x2F;bin&#x2F;zookeeper-ready &amp;&amp; \</span><br><span class="line">    apt-get update &amp;&amp;  apt-get -y install vim net-tools procps telnet inetutils-ping</span><br><span class="line"></span><br><span class="line">WORKDIR &#x2F;opt</span><br></pre></td></tr></table></figure><p><strong>apt源文件</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# cat sources.list </span><br><span class="line">deb https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian&#x2F; buster main contrib non-free</span><br><span class="line"># deb-src https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian&#x2F; buster main contrib non-free</span><br><span class="line">deb https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian&#x2F; buster-updates main contrib non-free</span><br><span class="line"># deb-src https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian&#x2F; buster-updates main contrib non-free</span><br><span class="line">deb https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian&#x2F; buster-backports main contrib non-free</span><br><span class="line"># deb-src https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian&#x2F; buster-backports main contrib non-free</span><br><span class="line">deb https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian-security buster&#x2F;updates main contrib non-free</span><br><span class="line"># deb-src https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian-security buster&#x2F;updates main contrib non-free</span><br></pre></td></tr></table></figure><p><strong>start-zookeeper文件</strong></p><p>在start-zookeeper的200行添加一行数据 [ 200aecho “4lw.commands.whitelist=*” &gt;&gt; $CONFIG_FILE ]。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed -i &#39;200aecho &quot;4lw.commands.whitelist&#x3D;*&quot; &gt;&gt; $CONFIG_FILE&#39; start-zookeeper</span><br></pre></td></tr></table></figure><p>这是为了修改 zookeeper的conf文件，添加配置4lw.commands.whitelist=*，是为了探活，否则示例如下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zookeeper@zk-0:&#x2F;opt$ echo ruok | nc 127.0.0.1 2181</span><br><span class="line">ruok is not executed because it is not in the whitelist.</span><br></pre></td></tr></table></figure><h3 id="通过dockerfile重新构建镜像"><a href="#通过dockerfile重新构建镜像" class="headerlink" title="通过dockerfile重新构建镜像"></a>通过dockerfile重新构建镜像</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker build -t harbor.od.com&#x2F;infra&#x2F;zookeeper:3.6.1 .</span><br><span class="line">docker push harbor.od.com&#x2F;infra&#x2F;zookeeper:3.6.1</span><br></pre></td></tr></table></figure><h2 id="准备资源配置清单"><a href="#准备资源配置清单" class="headerlink" title="准备资源配置清单"></a>准备资源配置清单</h2><p>zookeeper3.6.1的资源配置清单，除了镜像版本号，其他的和zookeeper3.4.10的完全一样。</p><h3 id="创建资源配置清单文件夹"><a href="#创建资源配置清单文件夹" class="headerlink" title="创建资源配置清单文件夹"></a>创建资源配置清单文件夹</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;zookeeper</span><br></pre></td></tr></table></figure><h3 id="svc-yaml"><a href="#svc-yaml" class="headerlink" title="svc.yaml"></a>svc.yaml</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;zookeeper&#x2F;svc.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: zk-hs</span><br><span class="line">  labels:</span><br><span class="line">    app: zk</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 2888</span><br><span class="line">    name: server</span><br><span class="line">  - port: 3888</span><br><span class="line">    name: leader-election</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: zk</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: zk-cs</span><br><span class="line">  labels:</span><br><span class="line">    app: zk</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 2181</span><br><span class="line">    name: client</span><br><span class="line">  selector:</span><br><span class="line">    app: zk</span><br><span class="line">---</span><br><span class="line">apiVersion: policy&#x2F;v1beta1</span><br><span class="line">kind: PodDisruptionBudget</span><br><span class="line">metadata:</span><br><span class="line">  name: zk-pdb</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: zk</span><br><span class="line">  maxUnavailable: 1</span><br><span class="line">---</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="sts-yaml"><a href="#sts-yaml" class="headerlink" title="sts.yaml"></a>sts.yaml</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;zookeeper&#x2F;sts.yaml </span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: zk</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: zk</span><br><span class="line">  serviceName: zk-hs</span><br><span class="line">  replicas: 3</span><br><span class="line">  updateStrategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">  podManagementPolicy: OrderedReady</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: zk</span><br><span class="line">    spec:</span><br><span class="line">      affinity:</span><br><span class="line">        podAntiAffinity:</span><br><span class="line">          requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">            - labelSelector:</span><br><span class="line">                matchExpressions:</span><br><span class="line">                  - key: &quot;app&quot;</span><br><span class="line">                    operator: In</span><br><span class="line">                    values:</span><br><span class="line">                    - zk</span><br><span class="line">              topologyKey: &quot;kubernetes.io&#x2F;hostname&quot;</span><br><span class="line">      containers:</span><br><span class="line">      - name: kubernetes-zookeeper</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        image: &quot;harbor.od.com&#x2F;infra&#x2F;zookeeper:3.6.1&quot;</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            memory: &quot;1Gi&quot;</span><br><span class="line">            cpu: &quot;0.5&quot;</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 2181</span><br><span class="line">          name: client</span><br><span class="line">        - containerPort: 2888</span><br><span class="line">          name: server</span><br><span class="line">        - containerPort: 3888</span><br><span class="line">          name: leader-election</span><br><span class="line">   </span><br><span class="line">        command:</span><br><span class="line">        - sh</span><br><span class="line">        - -c</span><br><span class="line">        - &quot;start-zookeeper \\</span><br><span class="line">          --servers&#x3D;3 \\</span><br><span class="line">          --data_dir&#x3D;&#x2F;var&#x2F;lib&#x2F;zookeeper&#x2F;data \\</span><br><span class="line">          --data_log_dir&#x3D;&#x2F;var&#x2F;lib&#x2F;zookeeper&#x2F;data&#x2F;log \\</span><br><span class="line">          --conf_dir&#x3D;&#x2F;opt&#x2F;zookeeper&#x2F;conf \\</span><br><span class="line">          --client_port&#x3D;2181 \\</span><br><span class="line">          --election_port&#x3D;3888 \\</span><br><span class="line">          --server_port&#x3D;2888 \\</span><br><span class="line">          --tick_time&#x3D;2000 \\</span><br><span class="line">          --init_limit&#x3D;10 \\</span><br><span class="line">          --sync_limit&#x3D;5 \\</span><br><span class="line">          --heap&#x3D;512M \\</span><br><span class="line">          --max_client_cnxns&#x3D;60 \\</span><br><span class="line">          --snap_retain_count&#x3D;3 \\</span><br><span class="line">          --purge_interval&#x3D;12 \\</span><br><span class="line">          --max_session_timeout&#x3D;40000 \\</span><br><span class="line">          --min_session_timeout&#x3D;4000 \\</span><br><span class="line">          --log_level&#x3D;INFO&quot;</span><br><span class="line">          </span><br><span class="line">        readinessProbe:</span><br><span class="line">          exec:</span><br><span class="line">            command:</span><br><span class="line">            - sh</span><br><span class="line">            - -c</span><br><span class="line">            - &quot;zookeeper-ready 2181&quot;</span><br><span class="line">          initialDelaySeconds: 10</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">        livenessProbe:</span><br><span class="line">          exec:</span><br><span class="line">            command:</span><br><span class="line">            - sh</span><br><span class="line">            - -c</span><br><span class="line">            - &quot;zookeeper-ready 2181&quot;</span><br><span class="line">          initialDelaySeconds: 10</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: datadir</span><br><span class="line">          mountPath: &#x2F;var&#x2F;lib&#x2F;zookeeper</span><br><span class="line">      securityContext:</span><br><span class="line">        runAsUser: 1000</span><br><span class="line">        fsGroup: 1000</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">  - metadata:</span><br><span class="line">      name: datadir</span><br><span class="line">      annotations:</span><br><span class="line">        volume.beta.kubernetes.io&#x2F;storage-class: nfs1</span><br><span class="line">    spec:</span><br><span class="line">      accessModes: [ &quot;ReadWriteOnce&quot; ]</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 10Gi</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h2 id="应用资源配置清单"><a href="#应用资源配置清单" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h2><h3 id="执行命令"><a href="#执行命令" class="headerlink" title="执行命令"></a>执行命令</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;zookeeper&#x2F;sts.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;zookeeper&#x2F;svc.yaml</span><br></pre></td></tr></table></figure><h3 id="查看执行结果"><a href="#查看执行结果" class="headerlink" title="查看执行结果"></a>查看执行结果</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pod </span><br><span class="line">NAME   READY   STATUS    RESTARTS   AGE</span><br><span class="line">zk-0   1&#x2F;1     Running   0          50s</span><br><span class="line">zk-1   1&#x2F;1     Running   0          29s</span><br><span class="line">zk-2   1&#x2F;1     Running   0          15s</span><br></pre></td></tr></table></figure><h2 id="查看结果"><a href="#查看结果" class="headerlink" title="查看结果"></a>查看结果</h2><p>我们查看zookeeper的配置文件 zoo.cfg</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[wangzt@daily ~]$ kubectl exec -it zk-0 cat &#x2F;opt&#x2F;zookeeper&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">#This file was autogenerated DO NOT EDIT</span><br><span class="line">clientPort&#x3D;2181</span><br><span class="line">dataDir&#x3D;&#x2F;var&#x2F;lib&#x2F;zookeeper&#x2F;data</span><br><span class="line">dataLogDir&#x3D;&#x2F;var&#x2F;lib&#x2F;zookeeper&#x2F;data&#x2F;log</span><br><span class="line">tickTime&#x3D;2000</span><br><span class="line">initLimit&#x3D;10</span><br><span class="line">syncLimit&#x3D;5</span><br><span class="line">maxClientCnxns&#x3D;60</span><br><span class="line">minSessionTimeout&#x3D;4000  # 默认分别为tickTime 的2倍</span><br><span class="line">maxSessionTimeout&#x3D;40000  # 默认分别为tickTime 的20倍</span><br><span class="line">autopurge.snapRetainCount&#x3D;3</span><br><span class="line">autopurge.purgeInteval&#x3D;12</span><br><span class="line">4lw.commands.whitelist&#x3D;*</span><br><span class="line">server.1&#x3D;zk-0.zk-hs.daily.svc.cluster.local:2888:3888</span><br><span class="line">server.2&#x3D;zk-1.zk-hs.daily.svc.cluster.local:2888:3888</span><br><span class="line">server.3&#x3D;zk-2.zk-hs.daily.svc.cluster.local:2888:3888</span><br></pre></td></tr></table></figure><p>下面是docker k8s-zookeeper:3.4.10默认配置文件 /opt/zookeeper/conf/zoo.cfg ,可以和我们自己的进行对比</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">clientPort&#x3D;2181</span><br><span class="line">dataDir&#x3D;&#x2F;var&#x2F;lib&#x2F;zookeeper&#x2F;data  # 数据文件目录</span><br><span class="line">dataLogDir&#x3D;&#x2F;var&#x2F;lib&#x2F;zookeeper&#x2F;data&#x2F;log  # 数据日志文件目录</span><br><span class="line">tickTime&#x3D;2000  # 心跳时间</span><br><span class="line">initLimit&#x3D;10  # 初始通信时间</span><br><span class="line">syncLimit&#x3D;5  # 同步通信时间</span><br><span class="line">maxClientCnxns&#x3D;60  # 单个客户端最大连接数，默认60</span><br><span class="line">minSessionTimeout&#x3D;4000  #  最小session超时时间</span><br><span class="line">maxSessionTimeout&#x3D;40000  # 最大session超时时间</span><br><span class="line">autopurge.snapRetainCount&#x3D;3  # 保留文件的数量，默认保留3个</span><br><span class="line">autopurge.purgeInteval&#x3D;12  # 自动清理频率，单位是小时，0表示不开启</span><br><span class="line">server.1&#x3D;zk-0.zk-hs.dev2.svc.cluster.local:2888:3888  # 服务器编号&#x3D;服务器地址：leader服务器端口：选举端口</span><br></pre></td></tr></table></figure><h1 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h1><p>我常用下面的命令，做制作镜像的调试</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -it --rm harbor.od.com&#x2F;public&#x2F;zookeeper:3.6.1 &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure><h2 id="常见报错"><a href="#常见报错" class="headerlink" title="常见报错"></a>常见报错</h2><h3 id="zookeeper启动报错：找不到或无法加载主类"><a href="#zookeeper启动报错：找不到或无法加载主类" class="headerlink" title="zookeeper启动报错：找不到或无法加载主类"></a>zookeeper启动报错：找不到或无法加载主类</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-14 zookeeper-3.6.1]# bin&#x2F;zkServer.sh start-foreground</span><br><span class="line">&#x2F;usr&#x2F;bin&#x2F;java</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;root&#x2F;test&#x2F;zookeeper-3.6.1&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">错误: 找不到或无法加载主类 org.apache.zookeeper.server.quorum.QuorumPeerMain</span><br></pre></td></tr></table></figure><p><strong>原因分析：</strong><br> 也即是下载的是未编译的 jar 包。<br> 注：zookeeper 好像从 3.5  版本以后，命名就发生了改变，如果是 apache-zookeeper-3.5.5.tar.gz 这般命名的，都是未编译的，而  apache-zookeeper-3.5.5-bin.tar.gz 这般命名的，才是已编译的包。</p><h3 id="zookeeper探活报错：IP不在白名单"><a href="#zookeeper探活报错：IP不在白名单" class="headerlink" title="zookeeper探活报错：IP不在白名单"></a>zookeeper探活报错：IP不在白名单</h3><p>如果在使用四字命令时出现下面这个提示，说明该命令不在zookeeper的白名单里。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zookeeper@zk-0:&#x2F;opt$ echo ruok | nc 127.0.0.1 2181</span><br><span class="line">ruok is not executed because it is not in the whitelist.</span><br></pre></td></tr></table></figure><p>找到conf文件夹下的zoo.cfg，在文件最后添加4lw.commands.whitelist=*</p><h4 id="知识扩展"><a href="#知识扩展" class="headerlink" title="知识扩展"></a>知识扩展</h4><p>zookeeper支持很多特定的命令来和它交互，主要是用来查询zookeeper的信息 。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zookeeper@zk-0:&#x2F;opt$ echo ruok | nc 127.0.0.1 2181</span><br><span class="line"></span><br><span class="line">-stat 查看状态信息</span><br><span class="line">-ruok 查看zookeeper是否启动</span><br><span class="line">-dump 列出没有处理的节点，临时节点</span><br><span class="line">-conf 查看服务器配置</span><br><span class="line">-cons 显示连接到服务端的信息</span><br><span class="line">-envi 显示环境变量信息</span><br><span class="line">-mntr 查看zk的健康信息</span><br><span class="line">-wchs 展示watch的信息</span><br><span class="line">-wchc和wchp 显示session的watch信息 path的watch信息</span><br></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><p>zookeeper参数文档</p><ul><li><p>zookeeper的配置参数详解（zoo.cfg）](<a href="https://www.cnblogs.com/xiohao/p/5541093.html" target="_blank" rel="noopener">https://www.cnblogs.com/xiohao/p/5541093.html</a>)</p></li><li><p>华为文档 <a href="https://support.huawei.com/enterprise/zh/doc/EDOC1100094182/c3a3653b" target="_blank" rel="noopener">https://support.huawei.com/enterprise/zh/doc/EDOC1100094182/c3a3653b</a></p></li><li><p>zookeeper使用简述  <a href="https://frameworks.readthedocs.io/en/latest/zookeeper/ZooKeeperNote.html" target="_blank" rel="noopener">https://frameworks.readthedocs.io/en/latest/zookeeper/ZooKeeperNote.html</a></p></li><li><p>知乎zk <a href="https://zhuanlan.zhihu.com/p/59313297" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/59313297</a></p></li><li><p><a href="https://segmentfault.com/a/1190000022995832" target="_blank" rel="noopener">https://segmentfault.com/a/1190000022995832</a></p></li><li><p><a href="https://blog.csdn.net/en_joker/article/details/79444600" target="_blank" rel="noopener">https://blog.csdn.net/en_joker/article/details/79444600</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> zookeeper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Esxi中CentOS7 扩展磁盘容量</title>
      <link href="/2020/09/22/esxi%E4%B8%ADcentos7-%E6%89%A9%E5%B1%95%E7%A3%81%E7%9B%98%E5%AE%B9%E9%87%8F/"/>
      <url>/2020/09/22/esxi%E4%B8%ADcentos7-%E6%89%A9%E5%B1%95%E7%A3%81%E7%9B%98%E5%AE%B9%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="Esxi中CentOS7-扩展磁盘容量"><a href="#Esxi中CentOS7-扩展磁盘容量" class="headerlink" title="Esxi中CentOS7 扩展磁盘容量"></a>Esxi中CentOS7 扩展磁盘容量</h1><p>磁盘容量使用了100%, 给磁盘空间扩充空间</p><h2 id="查看挂载点信息"><a href="#查看挂载点信息" class="headerlink" title="查看挂载点信息"></a>查看挂载点信息</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost]# df -h </span><br><span class="line">显示如下： </span><br><span class="line">文件系统 容量 已用 可用 已用% 挂载点 </span><br><span class="line">&#x2F;dev&#x2F;mapper&#x2F;centos-root 36G 36G 20K 100% &#x2F; </span><br><span class="line">……</span><br></pre></td></tr></table></figure><p>可见可用只剩20K，使用已经100%。</p><h2 id="2-扩展VMWare硬盘空间"><a href="#2-扩展VMWare硬盘空间" class="headerlink" title="2. 扩展VMWare硬盘空间"></a>2. 扩展VMWare硬盘空间</h2><p>首先需要关闭操作系统，然后在设置中将磁盘调整到需要的容量。 </p><p>如果选项是灰色的，说明虚拟机有快照，将其快照删除再操作。</p><h2 id="对新增加的硬盘进行分区、格式化"><a href="#对新增加的硬盘进行分区、格式化" class="headerlink" title="对新增加的硬盘进行分区、格式化"></a>对新增加的硬盘进行分区、格式化</h2><h3 id="1-查看一下调整后的磁盘状态"><a href="#1-查看一下调整后的磁盘状态" class="headerlink" title="1. 查看一下调整后的磁盘状态"></a>1. 查看一下调整后的磁盘状态</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost]# fdisk -l</span><br><span class="line"></span><br><span class="line">磁盘 &#x2F;dev&#x2F;sda：322.1 GB, 322122547200 字节，629145600 个扇区</span><br><span class="line">Units &#x3D; 扇区 of 1 * 512 &#x3D; 512 bytes</span><br><span class="line">扇区大小(逻辑&#x2F;物理)：512 字节 &#x2F; 512 字节</span><br><span class="line">I&#x2F;O 大小(最小&#x2F;最佳)：512 字节 &#x2F; 512 字节</span><br><span class="line">磁盘标签类型：dos</span><br><span class="line">磁盘标识符：0x000af019</span><br><span class="line"></span><br><span class="line">   设备 Boot      Start         End      Blocks   Id  System</span><br><span class="line">&#x2F;dev&#x2F;sda1   *        2048     2099199     1048576   83  Linux</span><br><span class="line">&#x2F;dev&#x2F;sda2         2099200   209715199   103808000   8e  Linux LVM</span><br></pre></td></tr></table></figure><p>显示信息： 磁盘 /dev/sda：322.1 GB, 322122547200 字节，629145600 个扇区</p><p>说明增加了空间的磁盘是/dev/sda </p><h3 id="2-磁盘分区"><a href="#2-磁盘分区" class="headerlink" title="2. 磁盘分区"></a>2. 磁盘分区</h3><p><strong>添加新的磁盘分区</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost]# fdisk &#x2F;dev&#x2F;sda</span><br><span class="line">分别键入以下参数：</span><br><span class="line">p       查看已分区数量（我看到有两个 &#x2F;dev&#x2F;sda1 和&#x2F;dev&#x2F;sda2） </span><br><span class="line">n       新增加一个分区 </span><br><span class="line">p       分区类型，选择主分区 </span><br><span class="line">        分区号选3（1和2已占用，见上） </span><br><span class="line">回车     默认选择（起始扇区） </span><br><span class="line">回车     默认选择（结束扇区） </span><br><span class="line">t       修改分区类型 </span><br><span class="line">       选分区3 </span><br><span class="line">8e     修改为LVM（8e就是LVM） </span><br><span class="line">w       写分区表，完成后退出fdisk命令</span><br><span class="line"></span><br><span class="line">使用partprobe重新读取分区表，或者重启机器。</span><br><span class="line">[root@localhost]# partprobe</span><br></pre></td></tr></table></figure><p><strong>实际操作过程</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost]# fdisk &#x2F;dev&#x2F;sda </span><br><span class="line">欢迎使用 fdisk (util-linux 2.23.2)。</span><br><span class="line"></span><br><span class="line">更改将停留在内存中，直到您决定将更改写入磁盘。</span><br><span class="line">使用写入命令前请三思。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">命令(输入 m 获取帮助)：p</span><br><span class="line"></span><br><span class="line">磁盘 &#x2F;dev&#x2F;sda：322.1 GB, 322122547200 字节，629145600 个扇区</span><br><span class="line">Units &#x3D; 扇区 of 1 * 512 &#x3D; 512 bytes</span><br><span class="line">扇区大小(逻辑&#x2F;物理)：512 字节 &#x2F; 512 字节</span><br><span class="line">I&#x2F;O 大小(最小&#x2F;最佳)：512 字节 &#x2F; 512 字节</span><br><span class="line">磁盘标签类型：dos</span><br><span class="line">磁盘标识符：0x000af019</span><br><span class="line"></span><br><span class="line">   设备 Boot      Start         End      Blocks   Id  System</span><br><span class="line">&#x2F;dev&#x2F;sda1   *        2048     2099199     1048576   83  Linux</span><br><span class="line">&#x2F;dev&#x2F;sda2         2099200   209715199   103808000   8e  Linux LVM</span><br><span class="line"></span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (2 primary, 0 extended, 2 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): p</span><br><span class="line">分区号 (3,4，默认 3)：</span><br><span class="line">起始 扇区 (209715200-629145599，默认为 209715200)：</span><br><span class="line">将使用默认值 209715200</span><br><span class="line">Last 扇区, +扇区 or +size&#123;K,M,G&#125; (209715200-629145599，默认为 629145599)：</span><br><span class="line">将使用默认值 629145599</span><br><span class="line">分区 3 已设置为 Linux 类型，大小设为 200 GiB</span><br><span class="line"></span><br><span class="line">命令(输入 m 获取帮助)：t</span><br><span class="line">分区号 (1-3，默认 3)：3</span><br><span class="line">Hex 代码(输入 L 列出所有代码)：8e</span><br><span class="line">已将分区“Linux”的类型更改为“Linux LVM”</span><br><span class="line"></span><br><span class="line">命令(输入 m 获取帮助)：w</span><br><span class="line">The partition table has been altered!</span><br><span class="line"></span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line"></span><br><span class="line">WARNING: Re-reading the partition table failed with error 16: 设备或资源忙.</span><br><span class="line">The kernel still uses the old table. The new table will be used at</span><br><span class="line">the next reboot or after you run partprobe(8) or kpartx(8)</span><br><span class="line">正在同步磁盘。</span><br></pre></td></tr></table></figure><h3 id="3-格式化分区"><a href="#3-格式化分区" class="headerlink" title="3. 格式化分区"></a>3. 格式化分区</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost]# mkfs.ext3 &#x2F;dev&#x2F;sda3</span><br></pre></td></tr></table></figure><h3 id="4-添加新LVM到已有的LVM组，实现扩容"><a href="#4-添加新LVM到已有的LVM组，实现扩容" class="headerlink" title="4. 添加新LVM到已有的LVM组，实现扩容"></a>4. 添加新LVM到已有的LVM组，实现扩容</h3><p>进入lvm管理</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost]# lvm</span><br></pre></td></tr></table></figure><p>关于lvm的一些基础知识可以参考这里：<a href="http://blog.csdn.net/wuweilong/article/details/7565530" target="_blank" rel="noopener">http://blog.csdn.net/wuweilong/article/details/7565530</a> </p><p><strong>以下命令都是在lvm&gt;下</strong> </p><p>（1）初始化刚才的分区</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pvcreate &#x2F;dev&#x2F;sda3</span><br></pre></td></tr></table></figure><p>（2）查看卷组名称</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vgdisplay</span><br></pre></td></tr></table></figure><p>记下”VG Name”，这里为centos</p><p>（3）将初始化过的分区加入到虚拟卷组</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vgextend centos &#x2F;dev&#x2F;sda3</span><br></pre></td></tr></table></figure><p>（4）扩展已有卷的容量。</p><p>查看卷组剩余空间。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vgdisplay</span><br></pre></td></tr></table></figure><p> 记下”Free PE/Size”的大小，例如这里为21513，然后扩展已有卷的容量</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lvextend -l +21513 &#x2F;dev&#x2F;mapper&#x2F;centos-root</span><br></pre></td></tr></table></figure><p>lvextend指令用于在线扩展逻辑卷的空间大小，而不中断应用程序对逻辑卷的访问。其后有两个选项</p><table><thead><tr><th>选项</th><th>说明</th></tr></thead><tbody><tr><td>-L</td><td>指定逻辑卷的大小，单位为“kKmMgGtT”字节，也就是Size</td></tr><tr><td>-l</td><td>指定逻辑卷的大小，单位为PE数</td></tr></tbody></table><p>其余两个参数：21513 是上边通过vgdisplay查看的free的大小，后边的目录参数/dev/mapper/centos-root可以通过df命令查看 </p><p>（5）查看卷容量</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pvdisplay</span><br></pre></td></tr></table></figure><p>（6）退出</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">quit</span><br></pre></td></tr></table></figure><p><strong>实际执行过程</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost]# lvm</span><br><span class="line">lvm&gt; pvcreate &#x2F;dev&#x2F;sda3</span><br><span class="line">WARNING: ext3 signature detected on &#x2F;dev&#x2F;sda3 at offset 1080. Wipe it? [y&#x2F;n]: y</span><br><span class="line">  Wiping ext3 signature on &#x2F;dev&#x2F;sda3.</span><br><span class="line">  Physical volume &quot;&#x2F;dev&#x2F;sda3&quot; successfully created.</span><br><span class="line">lvm&gt; vgdisplay</span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               centos</span><br><span class="line">  System ID             </span><br><span class="line">  Format                lvm2</span><br><span class="line">  Metadata Areas        1</span><br><span class="line">  Metadata Sequence No  6</span><br><span class="line">  VG Access             read&#x2F;write</span><br><span class="line">  VG Status             resizable</span><br><span class="line">  MAX LV                0</span><br><span class="line">  Cur LV                2</span><br><span class="line">  Open LV               2</span><br><span class="line">  Max PV                0</span><br><span class="line">  Cur PV                1</span><br><span class="line">  Act PV                1</span><br><span class="line">  VG Size               &lt;99.00 GiB</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              25343</span><br><span class="line">  Alloc PE &#x2F; Size       25343 &#x2F; &lt;99.00 GiB</span><br><span class="line">  Free  PE &#x2F; Size       0 &#x2F; 0   </span><br><span class="line">  VG UUID               1xe96x-uhNM-fO1d-xWgY-z0ol-KoD3-YvhHxL</span><br><span class="line">   </span><br><span class="line">lvm&gt; vgextend centos &#x2F;dev&#x2F;sda3</span><br><span class="line">  Volume group &quot;centos&quot; successfully extended</span><br><span class="line">lvm&gt; vgdisplay </span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               centos</span><br><span class="line">  System ID             </span><br><span class="line">  Format                lvm2</span><br><span class="line">  Metadata Areas        2</span><br><span class="line">  Metadata Sequence No  7</span><br><span class="line">  VG Access             read&#x2F;write</span><br><span class="line">  VG Status             resizable</span><br><span class="line">  MAX LV                0</span><br><span class="line">  Cur LV                2</span><br><span class="line">  Open LV               2</span><br><span class="line">  Max PV                0</span><br><span class="line">  Cur PV                2</span><br><span class="line">  Act PV                2</span><br><span class="line">  VG Size               298.99 GiB</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              76542</span><br><span class="line">  Alloc PE &#x2F; Size       25343 &#x2F; &lt;99.00 GiB</span><br><span class="line">  Free  PE &#x2F; Size       51199 &#x2F; &lt;200.00 GiB</span><br><span class="line">  VG UUID               1xe96x-uhNM-fO1d-xWgY-z0ol-KoD3-YvhHxL</span><br><span class="line">   </span><br><span class="line">lvm&gt; lvextend -l +51199 &#x2F;dev&#x2F;mapper&#x2F;centos-root</span><br><span class="line">  Size of logical volume centos&#x2F;root changed from &lt;91.25 GiB (23359 extents) to 291.24 GiB (74558 extents).</span><br><span class="line">  Logical volume centos&#x2F;root successfully resized.</span><br><span class="line">lvm&gt; pvdisplay </span><br><span class="line">  --- Physical volume ---</span><br><span class="line">  PV Name               &#x2F;dev&#x2F;sda2</span><br><span class="line">  VG Name               centos</span><br><span class="line">  PV Size               &lt;99.00 GiB &#x2F; not usable 3.00 MiB</span><br><span class="line">  Allocatable           yes (but full)</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              25343</span><br><span class="line">  Free PE               0</span><br><span class="line">  Allocated PE          25343</span><br><span class="line">  PV UUID               OJ7fYf-PFGE-0x89-nn1Q-MU4P-IMz2-CWdoCU</span><br><span class="line">   </span><br><span class="line">  --- Physical volume ---</span><br><span class="line">  PV Name               &#x2F;dev&#x2F;sda3</span><br><span class="line">  VG Name               centos</span><br><span class="line">  PV Size               200.00 GiB &#x2F; not usable 4.00 MiB</span><br><span class="line">  Allocatable           yes (but full)</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              51199</span><br><span class="line">  Free PE               0</span><br><span class="line">  Allocated PE          51199</span><br><span class="line">  PV UUID               oeXoGA-XA9o-evIn-i0bt-kWn8-Mnnw-5f105Z</span><br><span class="line">   </span><br><span class="line">lvm&gt; quit</span><br><span class="line">  Exiting.</span><br></pre></td></tr></table></figure><h3 id="5-文件系统扩容"><a href="#5-文件系统扩容" class="headerlink" title="5. 文件系统扩容"></a>5. 文件系统扩容</h3><p>以上只是卷扩容了，还要对文件系统实现真正扩容 </p><p><strong>CentOS 7 下面 由于使用的是 XFS，所以要用</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xfs_growfs &#x2F;dev&#x2F;mapper&#x2F;centos-root</span><br></pre></td></tr></table></figure><p><strong>CentOS 6 下面 要用</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">resize2fs &#x2F;dev&#x2F;mapper&#x2F;centos-root</span><br></pre></td></tr></table></figure><h3 id="6-查看新的磁盘空间"><a href="#6-查看新的磁盘空间" class="headerlink" title="6. 查看新的磁盘空间"></a>6. 查看新的磁盘空间</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">df -h</span><br></pre></td></tr></table></figure><h2 id="其他常用操作"><a href="#其他常用操作" class="headerlink" title="其他常用操作"></a>其他常用操作</h2><p>删除/home家目录，磁盘容量合并到 根/ 下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">umount &#x2F;home</span><br><span class="line">lvremove &#x2F;dev&#x2F;mapper&#x2F;centos-home</span><br><span class="line">lvextend -l +10559 &#x2F;dev&#x2F;mapper&#x2F;centos-root</span><br><span class="line">xfs_growfs &#x2F;dev&#x2F;mapper&#x2F;centos-root</span><br><span class="line">修改 &#x2F;etc&#x2F;fstab</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> lvm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> lvm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>23-27.声明式API、RBAC和Opertator</title>
      <link href="/2020/09/21/23-27-%E5%A3%B0%E6%98%8E%E5%BC%8Fapi%E3%80%81rbac%E5%92%8Copertator/"/>
      <url>/2020/09/21/23-27-%E5%A3%B0%E6%98%8E%E5%BC%8Fapi%E3%80%81rbac%E5%92%8Copertator/</url>
      
        <content type="html"><![CDATA[<p>极客时间课程张磊的 <a href="https://time.geekbang.org/column/intro/116" target="_blank" rel="noopener">深入剖析Kuernetes</a> </p><h1 id="23-声明式API与Kubernetes编程范式"><a href="#23-声明式API与Kubernetes编程范式" class="headerlink" title="23 | 声明式API与Kubernetes编程范式"></a>23 | 声明式API与Kubernetes编程范式</h1><p>通知命令直接执行的操作叫做命令式命令行操作。通过命令和配置文件执行的操作叫做命令式配置文件操作。</p><p>kubectl replace 的执行过程，是使用新的 YAML 文件中的 API 对象，替换原有的 API 对象；而 kubectl apply，则是执行了一个对原有 API 对象的 PATCH 操作。这意味着 kube-apiserver 在响应命令式请求（比如，kubectl replace）的时候，一次只能处理一个写请求，否则会有产生冲突的可能;  而对于声明式请求（比如，kubectl apply），一次能处理多个写操作，并且具备 Merge 能力。</p><h2 id="Dynamic-Admission-Control-动态准入控制"><a href="#Dynamic-Admission-Control-动态准入控制" class="headerlink" title="Dynamic Admission Control (动态准入控制)"></a>Dynamic Admission Control (动态准入控制)</h2><p>在 Kubernetes 项目中，当一个 Pod 或者任何一个 API 对象被提交给 APIServer 之后，有一些“初始化”性质的工作需要在它们被 Kubernetes 项目正式处理之前进行。比如，自动为所有 Pod 加上某些标签（Labels）。</p><p>而这个“初始化”操作的实现，借助的是一个叫作 Admission 的功能。它其实是 Kubernetes 项目里一组被称为 Admission Controller 的代码，可以选择性地被编译进 APIServer 中，在 API 对象创建之后会被立刻调用到。</p><p>但这就意味着，如果你现在想要添加一些自己的规则到 Admission Controller，就会比较困难。因为，这要求重新编译并重启 APIServer。显然，这种使用方法对 Istio 来说，影响太大了。</p><p>所以，Kubernetes 项目为我们额外提供了一种“热插拔”式的 Admission 机制，它就是 Dynamic Admission Control，也叫作：Initializer(初始值)。</p><h2 id="声明式-API-在实际使用-Istio"><a href="#声明式-API-在实际使用-Istio" class="headerlink" title="声明式 API 在实际使用:Istio"></a>声明式 API 在实际使用:Istio</h2><img src="/img/body/jike/d38daed2fedc90e20e9d2f27afbaec1b.jpg" alt="istio架构图" style="zoom:67%;max-width: 60%" /><p>在上面这个架构图中我们看到， Istio 项目架构的核心，是运行在每一个应用 Pod 里的 Envoy 容器。Envoy 项目成为 Service Mesh 体系的核心的优势: 编程友好的api，方便容器化，配置方便。</p><p>在 Istio 项目，把这个代理服务以 sidecar 容器的方式，运行在了每一个被治理的应用 Pod 中。Pod 里的所有容器都共享同一个 Network Namespace。所以，Envoy 容器就能够通过配置 Pod 里的 iptables 规则，把整个 Pod 的进出流量接管下来。这时候，Istio 的控制层（Control Plane）里的 Pilot 组件，就能够通过调用每个 Envoy 容器的 API，对这个 Envoy 代理进行配置，从而实现微服务治理。</p><p>Istio 项目需要在每个 Pod 里安装一个 Envoy 容器，通过k8s的Dynamic Admission Control（动态准入控制）功能，实现做到“无感”添加。被 Istio 处理后的这个 Pod 里，多出了一个叫作 envoy 的容器，它就是 Istio 要使用的 Envoy 代理。</p><p>Istio 要做的，就是编写一个用来为 Pod“自动注入”Envoy 容器的 Initializer。</p><ol><li><p>首先，Istio 会将这个 Envoy 容器本身的定义，以 ConfigMap 的方式保存在 Kubernetes 当中。</p></li><li><p>接下来，Istio 将一个编写好的 Initializer，作为一个 Pod 部署在 Kubernetes 中。</p></li><li><p>有了这个 TwoWayMergePatch 之后，Initializer 的代码就可以使用这个 patch 的数据，调用 Kubernetes 的 Client，发起一个 PATCH 请求。</p><p>这也就意味着，当你在 Initializer 里完成了要做的操作后，一定要记得将这个 metadata.initializers.pending 标志清除掉。这一点，你在编写 Initializer 代码的时候一定要非常注意。</p></li></ol><h2 id="声明式-API特点"><a href="#声明式-API特点" class="headerlink" title="声明式 API特点"></a>声明式 API特点</h2><p>Istio 项目的核心，就是由无数个运行在应用 Pod 中的 Envoy 容器组成的服务代理网格。这也正是 Service Mesh 的含义。</p><p>而这个机制得以实现的原理，正是借助了 Kubernetes 能够对 API 对象进行在线更新的能力，这也正是 Kubernetes“声明式 API”的独特之处：</p><ul><li>首先，所谓“声明式”，指的就是我只需要提交一个定义好的 API 对象来“声明”，我所期望的状态是什么样子。</li><li>其次，“声明式 API”允许有多个 API 写端，以 PATCH 的方式对 API 对象进行修改，而无需关心本地原始 YAML 文件的内容。</li><li>最后，也是最重要的，有了上述两个能力，Kubernetes 项目才可以基于对 API 对象的增、删、改、查，在完全无需外界干预的情况下，完成对“实际状态”和“期望状态”的调谐（Reconcile）过程。</li></ul><p>所以说，声明式 API，才是 Kubernetes 项目编排能力“赖以生存”的核心所在。</p><p>而在使用 Initializer 的流程中，最核心的步骤，莫过于 Initializer“自定义控制器”的编写过程。它遵循的，正是标准的“Kubernetes 编程范式”，即：<strong>如何使用控制器模式，同 Kubernetes 里 API 对象的“增、删、改、查”进行协作，进而完成用户业务逻辑的编写过程。</strong></p><h1 id="24-深入解析声明式API（一）：API对象的奥秘"><a href="#24-深入解析声明式API（一）：API对象的奥秘" class="headerlink" title="24 | 深入解析声明式API（一）：API对象的奥秘"></a>24 | 深入解析声明式API（一）：API对象的奥秘</h1><p>在 Kubernetes 项目中，一个 API 对象在 Etcd 里的完整资源路径，是由：Group（API 组）、Version（API 版本）和 Resource（API 资源类型）三个部分组成的。</p><p>通过这样的结构，整个 Kubernetes 里的所有 API 对象，实际上就可以用如下的树形结构表示出来：</p><img src="/img/body/jike/709700eea03075bed35c25b5b6cdefda.png" alt="API 对象在 Etcd 里的完整资源路径" style="zoom:67%;max-width: 70%" /><p>在这幅图中，你可以很清楚地看到 Kubernetes 里 API 对象的组织方式，其实是层层递进的。比如 “jobs”就是这个 API 对象的资源类型（Resource），“batch”就是它的组（Group），v1 就是它的版本（Version）。</p><p>当我们提交了这个 YAML 文件之后，Kubernetes 就会把这个 YAML 文件里描述的内容，转换成 Kubernetes 里的一个 Job 对象。</p><h2 id="在-Kubernetes-里匹配-CronJob-对象"><a href="#在-Kubernetes-里匹配-CronJob-对象" class="headerlink" title="在 Kubernetes 里匹配 CronJob 对象"></a>在 Kubernetes 里匹配 CronJob 对象</h2><h3 id="首先，Kubernetes-会匹配-API-对象的组。"><a href="#首先，Kubernetes-会匹配-API-对象的组。" class="headerlink" title="首先，Kubernetes 会匹配 API 对象的组。"></a>首先，Kubernetes 会匹配 API 对象的组。</h3><p>对于 Kubernetes 里的核心 API 对象，比如：Pod、Node 等，是不需要 Group 的（即：它们的 Group 是“”）。所以，对于这些 API 对象来说，Kubernetes 会直接在 /api 这个层级进行下一步的匹配过程。对于 Job 等非核心 API 对象来说，Kubernetes 就必须在 /apis 这个层级里查找它对应的 Group，进而根据“batch”这个 Group 的名字，找到 /apis/batch。</p><p>这些 API Group 的分类是以对象功能为依据的，比如 Job 和 CronJob 就都属于“batch” （离线业务）这个 Group。</p><h3 id="然后，Kubernetes-会进一步匹配到-API-对象的版本号。"><a href="#然后，Kubernetes-会进一步匹配到-API-对象的版本号。" class="headerlink" title="然后，Kubernetes 会进一步匹配到 API 对象的版本号。"></a>然后，Kubernetes 会进一步匹配到 API 对象的版本号。</h3><p>在 Kubernetes 中，同一种 API 对象可以有多个版本，对于会影响到用户的变更就可以通过升级新版本来处理，从而保证了向后兼容。</p><h3 id="最后，Kubernetes-会匹配-API-对象的资源类型。"><a href="#最后，Kubernetes-会匹配-API-对象的资源类型。" class="headerlink" title="最后，Kubernetes 会匹配 API 对象的资源类型。"></a>最后，Kubernetes 会匹配 API 对象的资源类型。</h3><p><strong>k8s的APi对象流程图</strong></p><img src="/img/body/jike/df6f1dda45e9a353a051d06c48f0286f.png" alt="k8s匹配流程图" style="zoom:67%;max-width: 80%" /><h2 id="APIServer-创建这个-CronJob-对象"><a href="#APIServer-创建这个-CronJob-对象" class="headerlink" title="APIServer 创建这个 CronJob 对象"></a>APIServer 创建这个 CronJob 对象</h2><ol><li><p>首先，当我们发起了创建 CronJob 的 POST 请求之后，我们编写的 YAML 的信息就被提交给了 APIServer。而 APIServer 的第一个功能，就是过滤这个请求，并完成一些前置性的工作，比如授权、超时处理、审计等。</p></li><li><p>然后，请求会进入 MUX 和 Routes 流程。如果你编写过 Web Server 的话就会知道，MUX 和 Routes 是 APIServer 完成 URL 和 Handler 绑定的场所。而 APIServer 的 Handler 要做的事情，就是按照我刚刚介绍的匹配过程，找到对应的 CronJob 类型定义。</p></li><li><p>接着，APIServer 最重要的职责就来了：根据这个 CronJob 类型定义，使用用户提交的 YAML 文件里的字段，创建一个 CronJob 对象。</p><p>而在这个过程中，APIServer 会进行一个 Convert 工作，即：把用户提交的 YAML 文件，转换成一个叫作 Super Version 的对象，它正是该 API 资源类型所有版本的字段全集。这样用户提交的不同版本的 YAML 文件，就都可以用这个 Super Version 对象来进行处理了。</p></li><li><p>接下来，APIServer 会先后进行 Admission() 和 Validation() 操作。比如，我在上一篇文章中提到的 Admission Controller 和 Initializer，就都属于 Admission 的内容。</p></li><li><p>最后，APIServer 会把验证过的 API 对象转换成用户最初提交的版本，进行序列化操作，并调用 Etcd 的 API 把它保存起来。</p></li></ol><p>由此可见，声明式 API 和 APIServer 对于 Kubernetes 来说非常重要。</p><p>CRD 的全称是 Custom Resource Definition (用户资源定义)。顾名思义，它指的就是，允许用户在 Kubernetes 中添加一个跟 Pod、Node 类似的、新的 API 资源类型，即：自定义 API 资源。</p><h3 id="为-Kubernetes-添加一个-API-资源类型示例"><a href="#为-Kubernetes-添加一个-API-资源类型示例" class="headerlink" title="为 Kubernetes 添加一个 API 资源类型示例"></a>为 Kubernetes 添加一个 API 资源类型示例</h3><p>主要为go开发相关知识，以后再看。</p><h1 id="25-深入解析声明式API（二）：编写自定义控制器"><a href="#25-深入解析声明式API（二）：编写自定义控制器" class="headerlink" title="25 | 深入解析声明式API（二）：编写自定义控制器"></a>25 | 深入解析声明式API（二）：编写自定义控制器</h1><p>“声明式 API”并不像“命令式 API”那样有着明显的执行逻辑。这就使得基于声明式 API 的业务功能实现，往往需要通过控制器模式来监控API 对象的变化（比如，创建或者删除 Network），然后以此来决定实际要执行的具体工作。</p><h2 id="编写自定义控制器示例"><a href="#编写自定义控制器示例" class="headerlink" title="编写自定义控制器示例"></a>编写自定义控制器示例</h2><p>主要为go开发相关知识，以后再看。</p><h1 id="26-基于角色的权限控制：RBAC"><a href="#26-基于角色的权限控制：RBAC" class="headerlink" title="26 | 基于角色的权限控制：RBAC"></a>26 | 基于角色的权限控制：RBAC</h1><p>Kubernetes 中所有的 API 对象，都保存在 Etcd 里。可是，对这些 API 对象的操作，却一定都是通过访问 kube-apiserver 实现的。其中一个非常重要的原因，就是你需要 APIServer 来帮助你做授权工作。</p><p>在 Kubernetes 项目中，负责完成授权（Authorization）工作的机制，就是 RBAC：基于角色的访问控制（Role-Based Access Control）。</p><h2 id="RBAC-的核心三个概念。"><a href="#RBAC-的核心三个概念。" class="headerlink" title="RBAC 的核心三个概念。"></a>RBAC 的核心三个概念。</h2><ol><li>Role：角色，它其实是一组规则，定义了一组对 Kubernetes API 对象的操作权限。</li><li>Subject：被作用者，既可以是“人”，也可以是“机器”，也可以是你在 Kubernetes 里定义的“用户”。</li><li>RoleBinding：定义了“被作用者”和“角色”的绑定关系。</li></ol><h2 id="Role和RoleBinding"><a href="#Role和RoleBinding" class="headerlink" title="Role和RoleBinding"></a>Role和RoleBinding</h2><p>Role 本身就是一个 Kubernetes 的 API 对象. Role 对象指定了它能产生作用的 Namepace。</p><p>Namespace 是 Kubernetes 项目里的一个逻辑管理单位。不同 Namespace 的 API 对象，在通过 kubectl 命令进行操作的时候，是互相隔离开的。当然，这仅限于逻辑上的“隔离”，Namespace 并不会提供任何实际的隔离或者多租户能力。默认 Namespace是 default。</p><p>Kubernetes 里的“User”，也就是“用户”，只是一个授权系统里的逻辑概念。它需要通过外部认证服务，比如 Keystone，来提供。</p><p>RoleBinding 对象就可以直接通过名字，来引用我们前面定义的 Role 对象（example-role），从而定义了“被作用者（Subject）”和“角色（Role）”之间的绑定关系。</p><p>Role 和 RoleBinding 对象都是 Namespaced 对象（Namespaced Object），它们对权限的限制规则仅在它们自己的 Namespace 内有效，roleRef 也只能引用当前 Namespace 里的 Role 对象。</p><p> verbs 字段的全集</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]</span><br></pre></td></tr></table></figure><p>Role 对象的 rules 字段也可以进一步细化。比如，你可以只针对某一个具体的对象进行权限设置，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;configmaps&quot;]</span><br><span class="line">  resourceNames: [&quot;my-config&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;]</span><br></pre></td></tr></table></figure><p>如果一个 Pod 没有声明 serviceAccountName，Kubernetes 会自动在它的 Namespace 下创建一个名叫 default 的默认 ServiceAccount，然后分配给这个 Pod。</p><p>在 Kubernetes 中已经内置了很多个为系统保留的 ClusterRole，它们的名字都以 system: 开头。你可以通过 kubectl get clusterroles 查看到它们。</p><blockquote><p> 所有的中间层都是为了解耦</p></blockquote><h1 id="27-聪明的微创新：Operator工作原理解读"><a href="#27-聪明的微创新：Operator工作原理解读" class="headerlink" title="27 | 聪明的微创新：Operator工作原理解读"></a>27 | 聪明的微创新：Operator工作原理解读</h1><p>而在 Kubernetes 生态中，还有一个相对更加灵活和编程友好的管理“有状态应用”的解决方案，它就是：Operator。</p><p>Operator 的工作原理，实际上是利用了 Kubernetes 的自定义 API 资源（CRD），来描述我们想要部署的“有状态应用”；然后在自定义控制器里，根据自定义 API 对象的变化，来完成具体的部署和运维工作。</p><p>以 Etcd Operator 为例，为你讲解一下 Operator 的工作原理和编写方法。</p><p>略……</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 深入剖析Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 深入剖析Kuernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>23-27.声明式API、RBAC和Opertator</title>
      <link href="/2020/09/21/23-27-%E5%A3%B0%E6%98%8E%E5%BC%8Fapi%E3%80%81rbac%E5%92%8Copertator/"/>
      <url>/2020/09/21/23-27-%E5%A3%B0%E6%98%8E%E5%BC%8Fapi%E3%80%81rbac%E5%92%8Copertator/</url>
      
        <content type="html"><![CDATA[<p>极客时间课程张磊的 <a href="https://time.geekbang.org/column/intro/116" target="_blank" rel="noopener">深入剖析Kuernetes</a> </p><h1 id="23-声明式API与Kubernetes编程范式"><a href="#23-声明式API与Kubernetes编程范式" class="headerlink" title="23 | 声明式API与Kubernetes编程范式"></a>23 | 声明式API与Kubernetes编程范式</h1><p>通知命令直接执行的操作叫做命令式命令行操作。</p><p>通过命令和配置文件执行的操作叫做命令式配置文件操作。</p><p>kubectl replace 的执行过程，是使用新的 YAML 文件中的 API 对象，替换原有的 API 对象；而 kubectl apply，则是执行了一个对原有 API 对象的 PATCH 操作。</p><p>更进一步地，这意味着 kube-apiserver 在响应命令式请求（比如，kubectl replace）的时候，一次只能处理一个写请求，否则会有产生冲突的可能。而对于声明式请求（比如，kubectl apply），一次能处理多个写操作，并且具备 Merge 能力。</p><h2 id="Dynamic-Admission-Control-动态准入控制"><a href="#Dynamic-Admission-Control-动态准入控制" class="headerlink" title="Dynamic Admission Control (动态准入控制)"></a>Dynamic Admission Control (动态准入控制)</h2><p>在 Kubernetes 项目中，当一个 Pod 或者任何一个 API 对象被提交给 APIServer 之后，有一些“初始化”性质的工作需要在它们被 Kubernetes 项目正式处理之前进行。比如，自动为所有 Pod 加上某些标签（Labels）。</p><p>而这个“初始化”操作的实现，借助的是一个叫作 Admission 的功能。它其实是 Kubernetes 项目里一组被称为 Admission Controller 的代码，可以选择性地被编译进 APIServer 中，在 API 对象创建之后会被立刻调用到。</p><p>但这就意味着，如果你现在想要添加一些自己的规则到 Admission Controller，就会比较困难。因为，这要求重新编译并重启 APIServer。显然，这种使用方法对 Istio 来说，影响太大了。</p><p>所以，Kubernetes 项目为我们额外提供了一种“热插拔”式的 Admission 机制，它就是 Dynamic Admission Control，也叫作：Initializer(初始值)。</p><h2 id="声明式-API-在实际使用-Istio"><a href="#声明式-API-在实际使用-Istio" class="headerlink" title="声明式 API 在实际使用:Istio"></a>声明式 API 在实际使用:Istio</h2><img src="http://wangzhangtao.com/img/body/jike/d38daed2fedc90e20e9d2f27afbaec1b.jpg" alt="istio架构图" style="zoom:67%;max-width: 60%" /><p>在上面这个架构图中我们看到， Istio 项目架构的核心，是运行在每一个应用 Pod 里的 Envoy 容器。Envoy 项目成为 Service Mesh 体系的核心的优势: 编程友好的api，方便容器化，配置方便。</p><p>在 Istio 项目，把这个代理服务以 sidecar 容器的方式，运行在了每一个被治理的应用 Pod 中。Pod 里的所有容器都共享同一个 Network Namespace。所以，Envoy 容器就能够通过配置 Pod 里的 iptables 规则，把整个 Pod 的进出流量接管下来。这时候，Istio 的控制层（Control Plane）里的 Pilot 组件，就能够通过调用每个 Envoy 容器的 API，对这个 Envoy 代理进行配置，从而实现微服务治理。</p><p>Istio 项目需要在每个 Pod 里安装一个 Envoy 容器，通过k8s的Dynamic Admission Control（动态准入控制）功能，实现做到“无感”添加。</p><p>被 Istio 处理后的这个 Pod 里，多出了一个叫作 envoy 的容器，它就是 Istio 要使用的 Envoy 代理。</p><p>Istio 要做的，就是编写一个用来为 Pod“自动注入”Envoy 容器的 Initializer。</p><ol><li><p>首先，Istio 会将这个 Envoy 容器本身的定义，以 ConfigMap 的方式保存在 Kubernetes 当中。</p></li><li><p>接下来，Istio 将一个编写好的 Initializer，作为一个 Pod 部署在 Kubernetes 中。</p></li><li><p>有了这个 TwoWayMergePatch 之后，Initializer 的代码就可以使用这个 patch 的数据，调用 Kubernetes 的 Client，发起一个 PATCH 请求。</p><p>这也就意味着，当你在 Initializer 里完成了要做的操作后，一定要记得将这个 metadata.initializers.pending 标志清除掉。这一点，你在编写 Initializer 代码的时候一定要非常注意。</p></li></ol><h2 id="声明式-API特点"><a href="#声明式-API特点" class="headerlink" title="声明式 API特点"></a>声明式 API特点</h2><p>Istio 项目的核心，就是由无数个运行在应用 Pod 中的 Envoy 容器组成的服务代理网格。这也正是 Service Mesh 的含义。</p><p>而这个机制得以实现的原理，正是借助了 Kubernetes 能够对 API 对象进行在线更新的能力，这也正是 Kubernetes“声明式 API”的独特之处：</p><ul><li>首先，所谓“声明式”，指的就是我只需要提交一个定义好的 API 对象来“声明”，我所期望的状态是什么样子。</li><li>其次，“声明式 API”允许有多个 API 写端，以 PATCH 的方式对 API 对象进行修改，而无需关心本地原始 YAML 文件的内容。</li><li>最后，也是最重要的，有了上述两个能力，Kubernetes 项目才可以基于对 API 对象的增、删、改、查，在完全无需外界干预的情况下，完成对“实际状态”和“期望状态”的调谐（Reconcile）过程。</li></ul><p>所以说，声明式 API，才是 Kubernetes 项目编排能力“赖以生存”的核心所在。</p><p>而在使用 Initializer 的流程中，最核心的步骤，莫过于 Initializer“自定义控制器”的编写过程。它遵循的，正是标准的“Kubernetes 编程范式”，即：<strong>如何使用控制器模式，同 Kubernetes 里 API 对象的“增、删、改、查”进行协作，进而完成用户业务逻辑的编写过程。</strong></p><h1 id="24-深入解析声明式API（一）：API对象的奥秘"><a href="#24-深入解析声明式API（一）：API对象的奥秘" class="headerlink" title="24 | 深入解析声明式API（一）：API对象的奥秘"></a>24 | 深入解析声明式API（一）：API对象的奥秘</h1><p>在 Kubernetes 项目中，一个 API 对象在 Etcd 里的完整资源路径，是由：Group（API 组）、Version（API 版本）和 Resource（API 资源类型）三个部分组成的。</p><p>通过这样的结构，整个 Kubernetes 里的所有 API 对象，实际上就可以用如下的树形结构表示出来：</p><img src="http://wangzhangtao.com/img/body/jike/709700eea03075bed35c25b5b6cdefda.png" alt="API 对象在 Etcd 里的完整资源路径" style="zoom:67%;max-width: 70%" /><p>在这幅图中，你可以很清楚地看到 Kubernetes 里 API 对象的组织方式，其实是层层递进的。</p><p>比如 “Job”就是这个 API 对象的资源类型（Resource），“batch”就是它的组（Group），v1 就是它的版本（Version）。</p><p>当我们提交了这个 YAML 文件之后，Kubernetes 就会把这个 YAML 文件里描述的内容，转换成 Kubernetes 里的一个 JOb 对象。</p><h2 id="在-Kubernetes-里找到-CronJob-对象"><a href="#在-Kubernetes-里找到-CronJob-对象" class="headerlink" title="在 Kubernetes 里找到 CronJob 对象"></a>在 Kubernetes 里找到 CronJob 对象</h2><h3 id="首先，Kubernetes-会匹配-API-对象的组。"><a href="#首先，Kubernetes-会匹配-API-对象的组。" class="headerlink" title="首先，Kubernetes 会匹配 API 对象的组。"></a>首先，Kubernetes 会匹配 API 对象的组。</h3><p>需要明确的是，对于 Kubernetes 里的核心 API 对象，比如：Pod、Node 等，是不需要 Group 的（即：它们的 Group 是“”）。所以，对于这些 API 对象来说，Kubernetes 会直接在 /api 这个层级进行下一步的匹配过程。</p><p>而对于 Job 等非核心 API 对象来说，Kubernetes 就必须在 /apis 这个层级里查找它对应的 Group，进而根据“batch”这个 Group 的名字，找到 /apis/batch。</p><p>不难发现，这些 API Group 的分类是以对象功能为依据的，比如 Job 和 CronJob 就都属于“batch” （离线业务）这个 Group。</p><h3 id="然后，Kubernetes-会进一步匹配到-API-对象的版本号。"><a href="#然后，Kubernetes-会进一步匹配到-API-对象的版本号。" class="headerlink" title="然后，Kubernetes 会进一步匹配到 API 对象的版本号。"></a>然后，Kubernetes 会进一步匹配到 API 对象的版本号。</h3><p>在 Kubernetes 中，同一种 API 对象可以有多个版本，对于会影响到用户的变更就可以通过升级新版本来处理，从而保证了向后兼容。</p><h3 id="最后，Kubernetes-会匹配-API-对象的资源类型。"><a href="#最后，Kubernetes-会匹配-API-对象的资源类型。" class="headerlink" title="最后，Kubernetes 会匹配 API 对象的资源类型。"></a>最后，Kubernetes 会匹配 API 对象的资源类型。</h3><p><strong>k8s的APi对象流程图</strong></p><img src="http://wangzhangtao.com/img/body/jike/df6f1dda45e9a353a051d06c48f0286f.png" alt="k8s匹配流程图" style="zoom:67%;max-width: 80%" /><h2 id="APIServer-创建这个-CronJob-对象"><a href="#APIServer-创建这个-CronJob-对象" class="headerlink" title="APIServer 创建这个 CronJob 对象"></a>APIServer 创建这个 CronJob 对象</h2><ol><li><p>首先，当我们发起了创建 CronJob 的 POST 请求之后，我们编写的 YAML 的信息就被提交给了 APIServer。而 APIServer 的第一个功能，就是过滤这个请求，并完成一些前置性的工作，比如授权、超时处理、审计等。</p></li><li><p>然后，请求会进入 MUX 和 Routes 流程。如果你编写过 Web Server 的话就会知道，MUX 和 Routes 是 APIServer 完成 URL 和 Handler 绑定的场所。而 APIServer 的 Handler 要做的事情，就是按照我刚刚介绍的匹配过程，找到对应的 CronJob 类型定义。</p></li><li><p>接着，APIServer 最重要的职责就来了：根据这个 CronJob 类型定义，使用用户提交的 YAML 文件里的字段，创建一个 CronJob 对象。</p><p>而在这个过程中，APIServer 会进行一个 Convert 工作，即：把用户提交的 YAML 文件，转换成一个叫作 Super Version 的对象，它正是该 API 资源类型所有版本的字段全集。这样用户提交的不同版本的 YAML 文件，就都可以用这个 Super Version 对象来进行处理了。</p></li><li><p>接下来，APIServer 会先后进行 Admission() 和 Validation() 操作。比如，我在上一篇文章中提到的 Admission Controller 和 Initializer，就都属于 Admission 的内容。</p></li><li><p>最后，APIServer 会把验证过的 API 对象转换成用户最初提交的版本，进行序列化操作，并调用 Etcd 的 API 把它保存起来。</p></li></ol><p>由此可见，声明式 API 和 APIServer 对于 Kubernetes 来说非常重要。</p><p>CRD 的全称是 Custom Resource Definition (用户资源定义)。顾名思义，它指的就是，允许用户在 Kubernetes 中添加一个跟 Pod、Node 类似的、新的 API 资源类型，即：自定义 API 资源。</p><h3 id="为-Kubernetes-添加一个-API-资源类型示例"><a href="#为-Kubernetes-添加一个-API-资源类型示例" class="headerlink" title="为 Kubernetes 添加一个 API 资源类型示例"></a>为 Kubernetes 添加一个 API 资源类型示例</h3><p>主要为go开发相关知识，以后再看。</p><h1 id="25-深入解析声明式API（二）：编写自定义控制器"><a href="#25-深入解析声明式API（二）：编写自定义控制器" class="headerlink" title="25 | 深入解析声明式API（二）：编写自定义控制器"></a>25 | 深入解析声明式API（二）：编写自定义控制器</h1><p>“声明式 API”并不像“命令式 API”那样有着明显的执行逻辑。这就使得基于声明式 API 的业务功能实现，往往需要通过控制器模式来监控API 对象的变化（比如，创建或者删除 Network），然后以此来决定实际要执行的具体工作。</p><h2 id="编写自定义控制器示例"><a href="#编写自定义控制器示例" class="headerlink" title="编写自定义控制器示例"></a>编写自定义控制器示例</h2><p>主要为go开发相关知识，以后再看。</p><h1 id="26-基于角色的权限控制：RBAC"><a href="#26-基于角色的权限控制：RBAC" class="headerlink" title="26 | 基于角色的权限控制：RBAC"></a>26 | 基于角色的权限控制：RBAC</h1><p>Kubernetes 中所有的 API 对象，都保存在 Etcd 里。可是，对这些 API 对象的操作，却一定都是通过访问 kube-apiserver 实现的。其中一个非常重要的原因，就是你需要 APIServer 来帮助你做授权工作。</p><p>在 Kubernetes 项目中，负责完成授权（Authorization）工作的机制，就是 RBAC：基于角色的访问控制（Role-Based Access Control）。</p><h2 id="RBAC-的核心三个概念。"><a href="#RBAC-的核心三个概念。" class="headerlink" title="RBAC 的核心三个概念。"></a>RBAC 的核心三个概念。</h2><ol><li>Role：角色，它其实是一组规则，定义了一组对 Kubernetes API 对象的操作权限。</li><li>Subject：被作用者，既可以是“人”，也可以是“机器”，也可以是你在 Kubernetes 里定义的“用户”。</li><li>RoleBinding：定义了“被作用者”和“角色”的绑定关系。</li></ol><h2 id="Role和RoleBinding"><a href="#Role和RoleBinding" class="headerlink" title="Role和RoleBinding"></a>Role和RoleBinding</h2><p>Role 本身就是一个 Kubernetes 的 API 对象. Role 对象指定了它能产生作用的 Namepace。</p><p>Namespace 是 Kubernetes 项目里的一个逻辑管理单位。不同 Namespace 的 API 对象，在通过 kubectl 命令进行操作的时候，是互相隔离开的。当然，这仅限于逻辑上的“隔离”，Namespace 并不会提供任何实际的隔离或者多租户能力。默认 Namespace是 default。</p><p>Kubernetes 里的“User”，也就是“用户”，只是一个授权系统里的逻辑概念。它需要通过外部认证服务，比如 Keystone，来提供。</p><p>RoleBinding 对象就可以直接通过名字，来引用我们前面定义的 Role 对象（example-role），从而定义了“被作用者（Subject）”和“角色（Role）”之间的绑定关系。</p><p>Role 和 RoleBinding 对象都是 Namespaced 对象（Namespaced Object），它们对权限的限制规则仅在它们自己的 Namespace 内有效，roleRef 也只能引用当前 Namespace 里的 Role 对象。</p><p> verbs 字段的全集</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]</span><br></pre></td></tr></table></figure><p>Role 对象的 rules 字段也可以进一步细化。比如，你可以只针对某一个具体的对象进行权限设置，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;configmaps&quot;]</span><br><span class="line">  resourceNames: [&quot;my-config&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;]</span><br></pre></td></tr></table></figure><p>如果一个 Pod 没有声明 serviceAccountName，Kubernetes 会自动在它的 Namespace 下创建一个名叫 default 的默认 ServiceAccount，然后分配给这个 Pod。</p><p>在 Kubernetes 中已经内置了很多个为系统保留的 ClusterRole，它们的名字都以 system: 开头。你可以通过 kubectl get clusterroles 查看到它们。</p><blockquote><p> 所有的中间层都是为了解耦</p></blockquote><h1 id="27-聪明的微创新：Operator工作原理解读"><a href="#27-聪明的微创新：Operator工作原理解读" class="headerlink" title="27 | 聪明的微创新：Operator工作原理解读"></a>27 | 聪明的微创新：Operator工作原理解读</h1><p>而在 Kubernetes 生态中，还有一个相对更加灵活和编程友好的管理“有状态应用”的解决方案，它就是：Operator。</p><p>Operator 的工作原理，实际上是利用了 Kubernetes 的自定义 API 资源（CRD），来描述我们想要部署的“有状态应用”；然后在自定义控制器里，根据自定义 API 对象的变化，来完成具体的部署和运维工作。</p><p>以 Etcd Operator 为例，为你讲解一下 Operator 的工作原理和编写方法。</p><p>略……</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>21-22.控制器DaemonSet和Job.md</title>
      <link href="/2020/09/20/21-22-%E6%8E%A7%E5%88%B6%E5%99%A8daemonset%E5%92%8Cjob-md/"/>
      <url>/2020/09/20/21-22-%E6%8E%A7%E5%88%B6%E5%99%A8daemonset%E5%92%8Cjob-md/</url>
      
        <content type="html"><![CDATA[<p>极客时间课程张磊的 <a href="https://time.geekbang.org/column/intro/116" target="_blank" rel="noopener">深入剖析Kuernetes</a> </p><h1 id="21-容器化守护进程的意义：DaemonSet"><a href="#21-容器化守护进程的意义：DaemonSet" class="headerlink" title="21 | 容器化守护进程的意义：DaemonSet"></a>21 | 容器化守护进程的意义：DaemonSet</h1><h2 id="DaemonSet特征"><a href="#DaemonSet特征" class="headerlink" title="DaemonSet特征"></a>DaemonSet特征</h2><ul><li><p>这个 Pod 运行在 Kubernetes  集群里的每一个节点（Node）上；</p></li><li><p>每个节点上只有一个这样的 Pod 实例；</p></li><li><p>当有新的节点加入 Kubernetes 集群后，该 Pod  会自动地在新节点上被创建出来；而当旧节点被删除后，它上面的 Pod 也相应地会被回收掉。</p></li></ul><p>更重要的是，跟其他编排对象不一样，DaemonSet 开始运行的时机，很多时候比整个 Kubernetes 集群出现的时机都要早。比如部署网络。</p><h2 id="DaemonSet工作机制"><a href="#DaemonSet工作机制" class="headerlink" title="DaemonSet工作机制"></a>DaemonSet工作机制</h2><p>DaemonSet 如何保证每个 Node 上有且只有一个被管理的 Pod。显然，这是一个典型的“控制器模型”能够处理的问题。</p><p>DaemonSet Controller 首先从 Etcd 里获取所有的 Node 列表，然后遍历所有的 Node。这时它就可以去检查，当前这个 Node 上是不是有一个携带了 name=fluentd-elasticsearch 标签的 Pod 在运行。</p><p>而检查的结果，可能有这么三种情况：</p><ol><li><p>没有这种 Pod，那么就意味着要在这个 Node 上创建这样一个 Pod；</p></li><li><p>有这种 Pod，但是数量大于 1，那就说明要把多余的 Pod 从这个 Node 上删除掉；</p></li><li><p>正好只有一个这种 Pod，那说明这个节点是正常的。</p></li></ol><p>用 nodeAffinity在指定的 Node 上创建新 Pod </p><p><strong>nodeAffinity使用示例</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: with-node-affinity</span><br><span class="line">spec:</span><br><span class="line">  affinity:</span><br><span class="line">    nodeAffinity:</span><br><span class="line">      requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">        nodeSelectorTerms:</span><br><span class="line">        - matchExpressions:</span><br><span class="line">          - key: metadata.name</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">            - node-geektime</span><br></pre></td></tr></table></figure><ol><li><p>requiredDuringSchedulingIgnoredDuringExecution：它的意思是说，这个 nodeAffinity 必须在每次调度的时候予以考虑。同时，这也意味着你可以设置在某些情况下不考虑这个 nodeAffinity；</p></li><li><p>这个 Pod，将来只允许运行在“metadata.name”是“node-geektime”的节点上。</p></li></ol><p>DaemonSet Controller 会在创建 Pod 的时候，自动在这个 Pod 的 API 对象里，加上这样一个 nodeAffinity 定义。</p><p>DaemonSet 还会给这个 Pod 自动加上另外一个与调度相关的字段，叫作 tolerations。这个字段意味着这个 Pod，会“容忍”（Toleration）某些 Node 的“污点”（Taint）。而通过这样一个 Toleration，调度器在调度这个 Pod 的时候，就会忽略当前节点上的“污点”，从而成功地将网络插件的 Agent 组件调度到这台机器上启动起来。</p><p>DaemonSet 其实是一个非常简单的控制器。在它的控制循环中，只需要遍历所有节点，然后根据节点上是否有被管理 Pod 的情况，来决定是否要创建或者删除一个 Pod。只不过，在创建每个 Pod 的时候，DaemonSet 会自动给这个 Pod 加上一个 nodeAffinity，从而保证这个 Pod 只会在指定节点上启动。同时，它还会自动给这个 Pod 加上一个 Toleration，从而忽略节点的 unschedulable“污点”。</p><h2 id="ControllerRevision-记录控制器对象的版本"><a href="#ControllerRevision-记录控制器对象的版本" class="headerlink" title="ControllerRevision:  记录控制器对象的版本"></a>ControllerRevision:  记录控制器对象的版本</h2><p>DaemonSet 控制器操作的直接就是 Pod，没有 ReplicaSet 这样的对象参与其中。所以 DaemonSet 使用 ControllerRevision，来保存和管理自己对应的“版本”。</p><p>kubectl rollout undo 操作，实际上相当于读取到了 Revision=1 的 ControllerRevision 对象保存的 Data 字段。</p><p><strong>查看ControllerRevision对象</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get controllerrevision -n kube-system -l name&#x3D;fluentd-elasticsearch</span><br></pre></td></tr></table></figure><p>ControllerRevision，专门用来记录某种 Controller 对象的版本。与此同时，这种“面向 API 对象”的设计思路，大大简化了控制器本身的逻辑，也正是 Kubernetes 项目“声明式 API”的优势所在。</p><p>在 Kubernetes 项目里，ControllerRevision 其实是一个通用的版本管理对象，比如StatefulSet也使用这个。这样，Kubernetes 项目就巧妙地避免了每种控制器都要维护一套冗余的代码和逻辑的问题。</p><h1 id="22-撬动离线业务：Job与CronJob"><a href="#22-撬动离线业务：Job与CronJob" class="headerlink" title="22 | 撬动离线业务：Job与CronJob"></a>22 | 撬动离线业务：Job与CronJob</h1><h2 id="在线业务和离线业务"><a href="#在线业务和离线业务" class="headerlink" title="在线业务和离线业务"></a>在线业务和离线业务</h2><p>Deployment、StatefulSet，以及 DaemonSet ，它们主要编排的对象，都是“在线业务”，即：Long Running Task（长作业）。这些应用一旦运行起来，除非出错或者停止，它的容器进程会一直保持在 Running 状态。</p><p>“离线业务”，或者叫作 Batch Job（计算业务）。这种业务在计算完成后就直接退出了，</p><h2 id="job任务"><a href="#job任务" class="headerlink" title="job任务"></a>job任务</h2><p><strong>job任务示例</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: batch&#x2F;v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: pi</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: pi</span><br><span class="line">        image: resouer&#x2F;ubuntu-bc </span><br><span class="line">        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;echo &#39;scale&#x3D;10000; 4*a(1)&#39; | bc -l &quot;]</span><br><span class="line">      restartPolicy: Never</span><br><span class="line">  backoffLimit: 4</span><br></pre></td></tr></table></figure><p>这个 Job 对象在创建后，它的 Pod 模板，被自动加上了一个 controller-uid=&lt; 一个随机字符串 &gt; 这样的 Label。而这个 Job 对象本身，则被自动加上了这个 Label 对应的 Selector，从而 保证了 Job 与它所管理的 Pod 之间的匹配关系。</p><p>而 Job Controller 之所以要使用这种携带了 UID 的 Label，就是为了避免不同 Job 对象所管理的 Pod 发生重合。需要注意的是，这种自动生成的 Label 对用户来说并不友好，所以不太适合推广到 Deployment 等长作业编排对象上。</p><p>在 Pod 模板中定义 restartPolicy=Never 的原因：离线计算的 Pod 永远都不应该被重启，否则它们会再重新计算一遍。</p><blockquote><p>restartPolicy 在 Job 对象里只允许被设置为 Never 和 OnFailure；而在 Deployment 对象里，restartPolicy 则只允许被设置为 Always。</p></blockquote><p><strong>离线作业失败了处理办法：</strong></p><ul><li><p>如果我们定义 restartPolicy=Never，那么离线作业失败后 Job Controller 就会不断地尝试创建一个新 Pod</p></li><li><p>如果我们定义 restartPolicy=OnFailure，那么离线作业失败后，Job Controller 就不会去尝试创建新的 Pod。但是，它会不断地尝试重启 Pod 里的容器。</p></li></ul><p>spec.backoffLimit 字段里定义了作业失败重试的最大次数为4（即，backoffLimit=4），而这个字段的默认值是 6。</p><p>spec.activeDeadlineSeconds 字段可以设置最长运行时间，被终止的 Pod 的状态里看到终止的原因是 reason: DeadlineExceeded。</p><h2 id="Job-Controller-的工作原理"><a href="#Job-Controller-的工作原理" class="headerlink" title="Job Controller 的工作原理"></a>Job Controller 的工作原理</h2><p><strong>Job 对象负责并行控制的参数：</strong></p><ol><li>spec.parallelism，它定义的是一个 Job 在任意时间最多可以启动多少个 Pod 同时运行；</li><li>spec.completions，它定义的是 Job 至少要完成的 Pod 数目，即 Job 的最小完成数。</li></ol><p>首先，Job Controller 控制的对象，直接就是 Pod。</p><p>其次，Job Controller 在控制循环中进行的调谐（Reconcile）操作，是根据实际在 Running 状态 Pod 的数目、已经成功退出的 Pod 的数目，以及 parallelism、completions 参数的值共同计算出在这个周期里，应该创建或者删除的 Pod 数目，然后调用 Kubernetes API 来执行这个操作。</p><p>综上所述，Job Controller 实际上控制了，作业执行的并行度，以及总共需要完成的任务数这两个重要参数。而在实际使用时，你需要根据作业的特性，来决定并行度（parallelism）和任务数（completions）的合理取值。</p><p><strong>常见的使用 Job 对象的方法：</strong></p><p>第一种用法，也是最简单粗暴的用法：外部管理器 +Job 模板。</p><p>第二种用法：拥有固定任务数目的并行 Job。</p><p>第三种用法，也是很常用的一个用法：指定并行度（parallelism），但不设置固定的 completions 的值。</p><h2 id="CronJob"><a href="#CronJob" class="headerlink" title="CronJob"></a>CronJob</h2><p><strong>crontab使用示例</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: batch&#x2F;v1beta1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: hello</span><br><span class="line">spec:</span><br><span class="line">  schedule: &quot;*&#x2F;1 * * * *&quot;</span><br><span class="line">  jobTemplate:</span><br><span class="line">    spec:</span><br><span class="line">      template:</span><br><span class="line">        spec:</span><br><span class="line">          containers:</span><br><span class="line">          - name: hello</span><br><span class="line">            image: busybox</span><br><span class="line">            args:</span><br><span class="line">            - &#x2F;bin&#x2F;sh</span><br><span class="line">            - -c</span><br><span class="line">            - date; echo Hello from the Kubernetes cluster</span><br><span class="line">          restartPolicy: OnFailure</span><br></pre></td></tr></table></figure><p>在这个 YAML 文件中，最重要的关键词就是 jobTemplate。CronJob 是一个专门用来管理 Job 对象的控制器。它创建和删除 Job 的依据，是 schedule 字段定义的、一个标准的Unix Cron格式的表达式。</p><p>由于定时任务的特殊性，很可能某个 Job 还没有执行完，另外一个新 Job 就产生了。这时候，你可以通过 spec.concurrencyPolicy 字段来定义具体的处理策略。比如：</p><ol><li>concurrencyPolicy=Allow，这也是默认情况，这意味着这些 Job 可以同时存在；</li><li>concurrencyPolicy=Forbid，这意味着不会创建新的 Pod，该创建周期被跳过；</li><li>concurrencyPolicy=Replace，这意味着新产生的 Job 会替换旧的、没有执行完的 Job。</li></ol><p>而如果某一次 Job Pod创建失败，这次创建就会被标记为“miss”。当在指定的时间窗口内，miss 的数目达到 100 时，那么 CronJob 会停止再创建这个 Job。</p><p>时间长度可以由 spec.startingDeadlineSeconds 字段指定。比如 startingDeadlineSeconds=200，意味着在 200 s 里，如果 miss 的数目达到了 100 次，那么这个 Job 就不会被创建执行了。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 深入剖析Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 深入剖析Kuernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>18-20.深入理解StatefulSet</title>
      <link href="/2020/09/18/18-20-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3statefulset/"/>
      <url>/2020/09/18/18-20-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3statefulset/</url>
      
        <content type="html"><![CDATA[<p>极客时间课程张磊的 <a href="https://time.geekbang.org/column/intro/116" target="_blank" rel="noopener">深入剖析Kuernetes</a> </p><h1 id="18-深入理解StatefulSet（一）：拓扑状态"><a href="#18-深入理解StatefulSet（一）：拓扑状态" class="headerlink" title="18 | 深入理解StatefulSet（一）：拓扑状态"></a>18 | 深入理解StatefulSet（一）：拓扑状态</h1><h2 id="有状态应用"><a href="#有状态应用" class="headerlink" title="有状态应用"></a>有状态应用</h2><p>实例之间有不对等关系，以及实例对外部数据有依赖关系的应用，就被称为“有状态应用”（Stateful Application）。</p><p>StatefulSet 的设计其实非常容易理解。它把真实世界里的应用状态，抽象为了两种情况：</p><p>​       拓扑状态。这种情况意味着，应用的多个实例之间不是完全对等的关系。这些应用实例，必须按照某些顺序启动,  并且新创建出来的 Pod，必须和原来 Pod 的网络标识一样，这样原先的访问者才能使用同样的方法，访问到这个新 Pod。</p><p>​       存储状态。这种情况意味着，应用的多个实例分别绑定了不同的存储数据。对于这些应用实例来说，Pod A 第一次读取到的数据，和重启之后再次读取到的数据，应该是同一份。比如一个数据库应用的多个存储实例。</p><p>StatefulSet 的核心功能，就是通过某种方式记录这些状态，然后在 Pod 被重新创建时，能够为新 Pod 恢复这些状态。</p><h2 id="Headless-Service"><a href="#Headless-Service" class="headerlink" title="Headless Service"></a>Headless Service</h2><p>Service访问方式</p><ol><li><p>以 Service 的 VIP（Virtual IP，即：虚拟 IP）方式</p></li><li><p>就是以 Service 的 DNS 方式。</p></li></ol><p>第一种处理方法，是 Normal Service。</p><p>第二种处理方法，正是 Headless Service。这种情况下，你访问“my-svc.my-namespace.svc.cluster.local”解析到的，直接就是 my-svc 代理的某一个 Pod 的 IP 地址。可以看到，这里的区别在于，Headless Service 不需要分配一个 VIP，而是可以直接以 DNS 记录的方式解析出被代理 Pod 的 IP 地址。</p><p>Service 被创建后并不会被分配一个 VIP，而是会以 DNS 记录的方式暴露出它所代理的 Pod。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    name: web</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br></pre></td></tr></table></figure><h2 id="StatefulSet如何维护拓扑状态"><a href="#StatefulSet如何维护拓扑状态" class="headerlink" title="StatefulSet如何维护拓扑状态"></a>StatefulSet如何维护拓扑状态</h2><p>StatefulSet 是如何使用这个 DNS 记录来维持 Pod 的拓扑状态的呢？</p><p><strong>StatefulSet案例</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">spec:</span><br><span class="line">  serviceName: &quot;nginx&quot;</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.9.1</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: web</span><br></pre></td></tr></table></figure><p>StatefulSet 给它所管理的所有 Pod 的名字，进行了编号，编号规则是：-，这些编号都是从 0 开始累加，与 StatefulSet 的每个 Pod 实例一一对应，绝不重复。</p><p>更重要的是，这些 Pod 的创建，也是严格按照编号顺序进行的。比如，在 web-0 进入到 Running 状态、并且细分状态（Conditions）成为 Ready 之前，web-1 会一直处于 Pending 状态。</p><p><strong>通过pod名称查看解析地址</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl run -i --tty --image busybox:1.28.4 dns-test --restart&#x3D;Never --rm &#x2F;bin&#x2F;sh</span><br><span class="line">nslookup web-0.nginx</span><br></pre></td></tr></table></figure><p>当我们把这两个 Pod 删除之后，Kubernetes 会按照原先编号的顺序，创建出了两个新的 Pod。并且，Kubernetes 依然为它们分配了与原来相同的域名：web-0.nginx 和 web-1.nginx。</p><p>通过这种方法，Kubernetes 就成功地将 Pod 的拓扑状态（比如：哪个节点先启动，哪个节点后启动），按照 Pod 的“名字 + 编号”的方式固定了下来，从而通过StatefulSet 就保证了 Pod 网络标识的稳定性。此外，Kubernetes 还为每一个 Pod 提供了一个固定并且唯一的访问入口，即：这个 Pod 对应的 DNS 记录。</p><p>不过，尽管 web-0.nginx 这条记录本身不会变，但它解析到的 Pod 的 IP 地址，并不是固定的。这就意味着，对于“有状态应用”实例的访问，你必须使用 DNS 记录或者 hostname 的方式，而绝不应该直接访问这些 Pod 的 IP 地址。</p><h1 id="19-深入理解StatefulSet（二）：存储状态"><a href="#19-深入理解StatefulSet（二）：存储状态" class="headerlink" title="19 | 深入理解StatefulSet（二）：存储状态"></a>19 | 深入理解StatefulSet（二）：存储状态</h1><p>StatefulSet 对存储状态的管理机制，主要使用的是一个叫作PVC (Persistent Volume Claim)  的功能。</p><h2 id="为什么引入pv和pvc"><a href="#为什么引入pv和pvc" class="headerlink" title="为什么引入pv和pvc"></a>为什么引入pv和pvc</h2><p>直接在pod中配置pv信息的缺点：</p><ul><li><p>其一， Pod 里 Volumes 字段，你不理解。</p></li><li><p>其二，存储信息泄密。比如存储服务器的地址、用户名、授权文件的位置等。</p></li></ul><p>所以Kubernetes 项目引入了一组叫作 Persistent Volume Claim（PVC）和 Persistent Volume（PV）的 API 对象，大大降低了用户声明和使用持久化 Volume 的门槛。</p><h2 id="如何使用pvc"><a href="#如何使用pvc" class="headerlink" title="如何使用pvc"></a>如何使用pvc</h2><p>第一步：定义一个 PVC，声明想要的 Volume 的属性：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: pv-claim</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br></pre></td></tr></table></figure><p>storage: 1Gi，表示我想要的 Volume 大小至少是 1 GiB；accessModes: ReadWriteOnce，表示这个 Volume 的挂载方式是可读写，并且只能被挂载在一个节点上而非被多个节点共享。</p><p>第二步：在应用的 Pod 中，声明使用这个 PVC</p><p>所以，Kubernetes 中 PVC 和 PV 的设计，实际上类似于“接口”和“实现”的思想。</p><p>PVC 其实就是一种特殊的 Volume。这些 PVC，都以“&lt;PVC 名字 &gt;-&lt;StatefulSet 名字 &gt;-&lt; 编号 &gt;”的方式命名，并且处于 Bound 状态。</p><h2 id="StatefulSet-的工作原理"><a href="#StatefulSet-的工作原理" class="headerlink" title="StatefulSet 的工作原理"></a>StatefulSet 的工作原理</h2><p>首先，StatefulSet  的控制器直接管理的是 Pod。这是因为，StatefulSet 里的不同 Pod 实例，不再像 ReplicaSet  中那样都是完全一样的，而是有了细微区别的。比如，每个 Pod 的 hostname、名字等都是不同的、携带了编号的。而 StatefulSet  区分这些实例的方式，就是通过在 Pod 的名字里加上事先约定好的编号。</p><p>其次，Kubernetes 通过  Headless Service，为这些有编号的 Pod，在 DNS 服务器中生成带有同样编号的 DNS 记录。只要 StatefulSet  能够保证这些 Pod 名字里的编号不变，那么 Service 里类似于  web-0.nginx.default.svc.cluster.local 这样的 DNS 记录也就不会变，而这条记录解析出来的 Pod 的  IP 地址，则会随着后端 Pod 的删除和再创建而自动更新。这当然是 Service 机制本身的能力，不需要 StatefulSet 操心。</p><p>最后，StatefulSet 还为每一个 Pod 分配并创建一个同样编号的 PVC。这样，Kubernetes 就可以通过 Persistent Volume 机制为这个 PVC 绑定上对应的 PV，从而保证了每一个 Pod 都拥有一个独立的 Volume。</p><h2 id="StatefulSet-是一种特殊的-Deployment"><a href="#StatefulSet-是一种特殊的-Deployment" class="headerlink" title="StatefulSet 是一种特殊的 Deployment"></a>StatefulSet 是一种特殊的 Deployment</h2><p>StatefulSet 其实是一种特殊的 Deployment，只不过这个“Deployment”的每个 Pod 实例的名字里，都携带了一个唯一并且固定的编号。这个编号的顺序，固定了 Pod 的拓扑关系；这个编号对应的 DNS 记录，固定了 Pod 的访问方式（即：在整个集群里唯一的、可被访问的身份）；这个编号对应的 PV，绑定了 Pod 与持久化存储的关系。所以，当 Pod 被删除重建时，这些“状态”都会保持不变。</p><p>StatefulSet 通过编号，使用 Kubernetes 里的两个标准功能：Headless Service 和 PV/PVC，实现了对 Pod 的拓扑状态和存储状态的维护。</p><p>而一旦你的应用没办法通过上述方式进行状态的管理，那就代表了 StatefulSet 已经不能解决它的部署问题了。这时候可以使用Operator。</p><h1 id="20-深入理解StatefulSet（三）：有状态应用实践"><a href="#20-深入理解StatefulSet（三）：有状态应用实践" class="headerlink" title="20 | 深入理解StatefulSet（三）：有状态应用实践"></a>20 | 深入理解StatefulSet（三）：有状态应用实践</h1><p>在常规环境里，部署这样一个主从模式的 MySQL 集群的主要难点在于：如何让从节点能够拥有主节点的数据，即：如何配置主（Master）从（Slave）节点的复制与同步。</p><h2 id="部署MySQL-集群基本步骤"><a href="#部署MySQL-集群基本步骤" class="headerlink" title="部署MySQL 集群基本步骤"></a>部署MySQL 集群基本步骤</h2><p>在安装好 MySQL 的 Master 节点之后，你需要做的</p><ol><li><p>通过 XtraBackup 将 Master 节点的数据备份到指定目录。</p></li><li><p>配置 Slave 节点。</p></li><li><p>启动 Slave 节点。</p></li><li><p>在这个集群中添加更多的 Slave 节点。</p></li></ol><h2 id="“容器化”MySQL-集群遇到的三个问题"><a href="#“容器化”MySQL-集群遇到的三个问题" class="headerlink" title="“容器化”MySQL 集群遇到的三个问题"></a>“容器化”MySQL 集群遇到的三个问题</h2><p>将部署 MySQL 集群的流程迁移到 Kubernetes 项目上，需要能够“容器化”地解决下面的三个问题：</p><ul><li><p>Master 节点和 Slave 节点需要有不同的配置文件（即：不同的 my.cnf）；</p></li><li><p>Master 节点和 Slave 节点需要能够传输备份信息文件；</p></li><li><p>在 Slave 节点第一次启动之前，需要执行一些初始化 SQL 操作；</p></li></ul><p>而由于 MySQL 本身同时拥有拓扑状态（主从节点的区别）和存储状态（MySQL 保存在本地的数据），我们要通过 StatefulSet 来解决这三个的问题。</p><p>其中，“第一个问题：Master 节点和 Slave 节点需要有不同的配置文件”。我们需要给主从节点分别准备两份不同的 MySQL 配置文件，然后根据 Pod 的序号（Index）挂载进去即可。</p><ul><li><p>master.cnf 开启了 log-bin，即：使用二进制日志文件的方式进行主从复制，这是一个标准的设置。</p></li><li><p>slave.cnf 的开启了 super-read-only，代表的是从节点会拒绝除了主节点的数据同步操作之外的所有写操作，即：它对用户是只读的。</p></li></ul><p>第一个名叫“mysql”的 Service 是一个 Headless Service（即：clusterIP= None）。所以它的作用，是通过为 Pod 分配 DNS 记录来固定它的拓扑状态，比如“mysql-0.mysql”和“mysql-1.mysql”这样的 DNS 名字。其中，编号为 0 的节点就是我们的主节点。</p><p>第二个名叫“mysql-read”的 Service，则是一个常规的 Service。</p><h2 id="Master-节点和-Slave-节点需要能够传输备份文件"><a href="#Master-节点和-Slave-节点需要能够传输备份文件" class="headerlink" title="Master 节点和 Slave 节点需要能够传输备份文件"></a>Master 节点和 Slave 节点需要能够传输备份文件</h2><p>解决这个问题的思路，我比较推荐的做法是：先搭建框架，再完善细节。其中，Pod 部分如何定义，是完善细节时的重点。</p><p>可以看到，StatefulSet 管理的“有状态应用”的多个实例，也都是通过同一份 Pod 模板创建出来的，使用的是同一个 Docker 镜像。这也就意味着：如果你的应用要求不同节点的镜像不一样，那就不能再使用 StatefulSet 了。对于这种情况，应该考虑我后面会讲解到的 Operator。</p><p>然后，我们来重点设计一下这个 StatefulSet 的 Pod 模板，也就是 template 字段。</p><p>第一步：从 ConfigMap 中，获取 MySQL 的 Pod 对应的配置文件。首先定义了一个 InitContainer ，修改配置文件</p><p><strong>InitContainer 示例图</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line"># template.spec</span><br><span class="line">initContainers:</span><br><span class="line">- name: init-mysql</span><br><span class="line">  image: mysql:5.7</span><br><span class="line">  command:</span><br><span class="line">  - bash</span><br><span class="line">  - &quot;-c&quot;</span><br><span class="line">  - |</span><br><span class="line">    set -ex</span><br><span class="line">    # 从Pod的序号，生成server-id</span><br><span class="line">    [[ &#96;hostname&#96; &#x3D;~ -([0-9]+)$ ]] || exit 1</span><br><span class="line">    ordinal&#x3D;$&#123;BASH_REMATCH[1]&#125;</span><br><span class="line">    echo [mysqld] &gt; &#x2F;mnt&#x2F;conf.d&#x2F;server-id.cnf</span><br><span class="line">    # 由于server-id&#x3D;0有特殊含义，我们给ID加一个100来避开它</span><br><span class="line">    echo server-id&#x3D;$((100 + $ordinal)) &gt;&gt; &#x2F;mnt&#x2F;conf.d&#x2F;server-id.cnf</span><br><span class="line">    # 如果Pod序号是0，说明它是Master节点，从ConfigMap里把Master的配置文件拷贝到&#x2F;mnt&#x2F;conf.d&#x2F;目录；</span><br><span class="line">    # 否则，拷贝Slave的配置文件</span><br><span class="line">    if [[ $ordinal -eq 0 ]]; then</span><br><span class="line">      cp &#x2F;mnt&#x2F;config-map&#x2F;master.cnf &#x2F;mnt&#x2F;conf.d&#x2F;</span><br><span class="line">    else</span><br><span class="line">      cp &#x2F;mnt&#x2F;config-map&#x2F;slave.cnf &#x2F;mnt&#x2F;conf.d&#x2F;</span><br><span class="line">    fi</span><br><span class="line">  volumeMounts:</span><br><span class="line">  - name: conf</span><br><span class="line">    mountPath: &#x2F;mnt&#x2F;conf.d</span><br><span class="line">  - name: config-map</span><br><span class="line">    mountPath: &#x2F;mnt&#x2F;config-map</span><br></pre></td></tr></table></figure><p>在完成 MySQL 节点的初始化后，这个 sidecar 容器的第二个工作，则是启动一个数据传输服务。</p><p>具体做法是：sidecar 容器会使用 ncat 命令启动一个工作在 3307 端口上的网络发送服务。一旦收到数据传输请求时，sidecar 容器就会调用 xtrabackup –backup 指令备份当前 MySQL 的数据，然后把这些备份数据返回给请求者。这就是为什么我们在 InitContainer 里定义数据拷贝的时候，访问的是“上一个 MySQL 节点”的 3307 端口。</p><p>第一个名叫“mysql”的 Service 是一个 Headless Service（即：clusterIP= None）。它的作用，是通过为 Pod 分配 DNS 记录来固定它的拓扑状态，比如“mysql-0.mysql”和“mysql-1.mysql”这样的 DNS 名字。其中，编号为 0 的节点就是我们的主节点。 </p><p>StatefulSet Controller 会按照与 Pod 编号相反的顺序，从最后一个 Pod 开始，逐一更新这个 StatefulSet 管理的每个 Pod。</p><p>此外，StatefulSet 的“滚动更新”还允许我们进行更精细的控制，比如金丝雀发布（Canary Deploy）或者灰度发布，这意味着应用的多个实例中被指定的一部分不会被更新到最新的版本。这个字段，正是 StatefulSet 的 spec.updateStrategy.rollingUpdate 的 partition 字段。把 partition 字段修改为 2。那么只有序号大于或者等于 2 的 Pod 会被更新。</p><h2 id="几个值得你注意和体会关键点"><a href="#几个值得你注意和体会关键点" class="headerlink" title="几个值得你注意和体会关键点"></a>几个值得你注意和体会关键点</h2><ul><li><p>“人格分裂”：在解决需求的过程中，一定要记得思考，该 Pod 在扮演不同角色时的不同操作。</p></li><li><p>“阅后即焚”：很多“有状态应用”的节点，只是在第一次启动的时候才需要做额外处理。所以，在编写 YAML 文件时，你一定要考虑“容器重启”的情况，不要让这一次的操作干扰到下一次的容器启动。</p></li><li><p>“容器之间平等无序”：除非是 InitContainer，否则一个 Pod 里的多个容器之间，是完全平等的。所以，你精心设计的 sidecar，绝不能对容器的顺序做出假设，否则就需要进行前置检查。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 深入剖析Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 深入剖析Kuernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>16-17.控制器和deployment</title>
      <link href="/2020/09/16/16-17-%E6%8E%A7%E5%88%B6%E5%99%A8%E5%92%8Cdeployment/"/>
      <url>/2020/09/16/16-17-%E6%8E%A7%E5%88%B6%E5%99%A8%E5%92%8Cdeployment/</url>
      
        <content type="html"><![CDATA[<p>极客时间课程张磊的 <a href="https://time.geekbang.org/column/intro/116" target="_blank" rel="noopener">深入剖析Kuernetes</a> </p><h1 id="16-编排其实很简单：谈谈“控制器”模型"><a href="#16-编排其实很简单：谈谈“控制器”模型" class="headerlink" title="16 | 编排其实很简单：谈谈“控制器”模型"></a>16 | 编排其实很简单：谈谈“控制器”模型</h1><p>Pod 实际上就是对容器的进一步抽象和封装。 Kubernetes 操作这些pod的逻辑，都由控制器（Controller）完成。</p><p>kube-controller-manager 的组件, 实际上就是一系列控制器的集合。</p><p>这些控制器之所以被统一放在 pkg/controller 目录下，就是因为它们都遵循 Kubernetes 项目中的一个通用编排模式，即：控制循环（control loop）。</p><p>在具体实现中，实际状态往往来自于 Kubernetes 集群本身。常见的实际状态的来源有 kubelet 通过心跳汇报的容器状态和节点状态，或者监控系统中保存的应用监控数据，或者控制器主动收集的它自己感兴趣的信息等等。</p><p>而期望状态，一般来自于用户提交的 YAML 文件。</p><p>一个 Kubernetes 对象的主要编排逻辑，实际上是在第三步的“对比”阶段完成的。这个操作，通常被叫作调谐（Reconcile）。这个调谐的过程，则被称作“Reconcile Loop”（调谐循环）或者“Sync Loop”（同步循环）。</p><p>比如，增加 Pod，删除已有的 Pod，或者更新 Pod 的某个字段。这也是 Kubernetes 项目“面向 API 对象编程”的一个直观体现。</p><p>像 Deployment 定义的 template 字段，在 Kubernetes 项目中有一个专有的名字，叫作 PodTemplate（Pod 模板）。</p><img src="/img/body/jike/72cc68d82237071898a1d149c8354b26.png" alt="img" style="zoom:67%;max-width: 70%" /><p>类似 Deployment 这样的一个控制器，实际上都是由上半部分的控制器定义（包括期望状态），加上下半部分的被控制对象的模板组成的。</p><p>这就是为什么，在所有 API 对象的 Metadata 里，都有一个字段叫作 ownerReference，用于保存当前这个 API 对象的拥有者（Owner）的信息。</p><p>命令式api和声明式api的区别</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">命令式开发：是关注计算机的执行步骤，告诉计算机一步一步怎么做，然后计算机再一步步做</span><br><span class="line">声明式开发：是告诉计算机做什么，但不告诉计算机怎么做</span><br></pre></td></tr></table></figure><h1 id="17-经典PaaS的记忆：作业副本与水平扩展"><a href="#17-经典PaaS的记忆：作业副本与水平扩展" class="headerlink" title="17 | 经典PaaS的记忆：作业副本与水平扩展"></a>17 | 经典PaaS的记忆：作业副本与水平扩展</h1><h2 id="Deployment，ReplicaSet-和pod"><a href="#Deployment，ReplicaSet-和pod" class="headerlink" title="Deployment，ReplicaSet 和pod"></a>Deployment，ReplicaSet 和pod</h2><p>Deployment 实现了 Kubernetes 项目中一个非常重要的功能：Pod 的“水平扩展 / 收缩”（horizontal scaling out/in）。这个功能，是从 PaaS 时代开始，一个平台级项目就必须具备的编排能力。</p><p>举个例子，如果你更新了 Deployment 的 Pod 模板（比如，修改了容器的镜像），那么 Deployment 就需要遵循一种叫作“滚动更新”（rolling update）的方式，来升级现有的容器。而这个能力的实现，依赖的是 Kubernetes 项目中的一个非常重要的概念（API 对象）：ReplicaSet。</p><p>一个 ReplicaSet 对象，其实就是由副本数目的定义和一个 Pod 模板组成的。它的定义其实是 Deployment 的一个子集。</p><p>更重要的是，Deployment 控制器实际操纵的，正是这样的 ReplicaSet 对象，而不是 Pod 对象。对于一个 Deployment 所管理的 Pod，它的 ownerReference 是 ReplicaSet。</p><p><strong>Deployment，ReplicaSet 和pod 关系图</strong></p><img src="/img/body/jike/711c07208358208e91fa7803ebc73058.jpg" alt="Deployment，ReplicaSet 和pod 关系图" style="zoom:67%;max-width: 50%" /><p>通过这张图，我们知道，一个定义了 replicas=3 的 Deployment，与它的 ReplicaSet，以及 Pod 的关系，实际上是一种“层层控制”的关系。</p><p>其中，ReplicaSet 负责通过“控制器模式”，保证系统中 Pod 的个数永远等于指定的个数（比如，3 个）。这也正是 Deployment 只允许容器的 restartPolicy=Always 的主要原因：<strong>只有在容器能保证自己始终是 Running 状态的前提下，ReplicaSet 调整 Pod 的个数才有意义</strong>。</p><p>而在此基础上，Deployment 同样通过“控制器模式”，来操作 ReplicaSet 的个数和属性，进而实现“水平扩展 / 收缩”和“滚动更新”这两个编排动作。</p><p>修改deploy副本数</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl scale deployment nginx-deployment --replicas&#x3D;4</span><br><span class="line">deployment.apps&#x2F;nginx-deployment scaled</span><br></pre></td></tr></table></figure><h2 id="deploy状态信息"><a href="#deploy状态信息" class="headerlink" title="deploy状态信息"></a>deploy状态信息</h2><p>我们来看下 nginx-deployment 创建后的状态信息</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl get deployments</span><br><span class="line">NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   3         0         0            0           1s</span><br></pre></td></tr></table></figure><p>在返回结果中，我们可以看到四个状态字段，它们的含义如下所示。</p><ol><li>DESIRED（desired）：用户期望的 Pod 副本个数（spec.replicas 的值）；</li><li>CURRENT：当前处于 Running 状态的 Pod 的个数；</li><li>UP-TO-DATE：当前处于最新版本的 Pod 的个数，所谓最新版本指的是 Pod 的 Spec 部分与 Deployment 里 Pod 模板里定义的完全一致；</li><li>AVAILABLE：当前已经可用的 Pod 的个数，即：既是 Running 状态，又是最新版本，并且已经处于 Ready（健康检查正确）状态的 Pod 的个数。</li></ol><p>kubectl rollout status 可以实时查看 Deployment 对象的状态变化。</p><p>在用户提交了一个 Deployment 对象后，Deployment Controller 就会立即创建一个 Pod 副本个数为 3 的 ReplicaSet。这个 ReplicaSet 的名字，则是由 Deployment 的名字和一个随机字符串共同组成。这个随机字符串叫作 pod-template-hash，</p><p>ReplicaSet 会把这个随机字符串加在它所控制的所有 Pod 的标签里，从而保证这些 Pod 不会与集群里的其他 Pod 混淆。</p><p>而 ReplicaSet 的 DESIRED、CURRENT 和 READY 字段的含义，和 Deployment 中是一致的。所以，相比之下，Deployment 只是在 ReplicaSet 的基础上，添加了 UP-TO-DATE 这个跟版本有关的状态字段。</p><h2 id="滚动更新"><a href="#滚动更新" class="headerlink" title="滚动更新"></a>滚动更新</h2><h3 id="“滚动更新”实现原理"><a href="#“滚动更新”实现原理" class="headerlink" title="“滚动更新”实现原理"></a>“滚动更新”实现原理</h3><p>修改 Deployment 有很多方法。比如，直接使用 kubectl edit 指令编辑 Etcd 里的 API 对象。kubectl edit  是把 API 对象的内容下载到了本地文件，让你修改完成后再提交上去。 kubectl edit 编辑完成后，Kubernetes 就会立刻触发“滚动更新”的过程。</p><p>首先，当你修改了 Deployment 里的 Pod 定义之后，Deployment Controller 会使用这个修改后的 Pod 模板，创建一个新的 ReplicaSet（hash=1764197365），这个新的 ReplicaSet 的初始 Pod 副本数是：0</p><p>接着交替进行，新 ReplicaSet 管理的 Pod 副本数，从 0 个变成 1 个，再变成 2 个，最后变成 3 个。而旧的 ReplicaSet 管理的 Pod 副本数则从 3 个变成 2 个，再变成 1 个，最后变成 0 个。这样，就完成了这一组 Pod 的版本升级过程。</p><p>将一个集群中正在运行的多个 Pod 版本，交替地逐一升级的过程，就是“滚动更新”。</p><h3 id="“滚动更新”的优点"><a href="#“滚动更新”的优点" class="headerlink" title="“滚动更新”的优点"></a>“滚动更新”的优点</h3><p>如果新版本 Pod 有问题那么“滚动更新”就会停止，从而允许开发和运维人员介入。而此时，由于应用本身还有旧版本的 Pod 在线，保证服务不受影响。</p><p>为了保证服务的连续性，Deployment Controller 还会确保，在任何时间窗口内，只有指定比例的 Pod 处于离线状态。同时，它也会确保，在任何时间窗口内，只有指定比例的新 Pod 被创建出来。这两个比例的值都是可以配置的，默认都是 DESIRED 值的 25%。</p><h3 id="“滚动更新”策略RollingUpdateStrategy"><a href="#“滚动更新”策略RollingUpdateStrategy" class="headerlink" title="“滚动更新”策略RollingUpdateStrategy"></a>“滚动更新”策略RollingUpdateStrategy</h3><p> RollingUpdateStrategy 的配置中，maxSurge 指定的是除了 DESIRED 数量之外，在一次“滚动”中，Deployment 控制器还可以创建多少个新 Pod；而 maxUnavailable 指的是，在一次“滚动”中，Deployment 控制器可以删除多少个旧 Pod。同时，这两个配置还可以用前面我们介绍的百分比形式来表示，比如：maxUnavailable=50%，指的是我们最多可以一次删除“50%*DESIRED 数量”个 Pod。</p><p><strong>Deployment、ReplicaSet 和 Pod 的关系图</strong></p><img src="/img/body/jike/bbc4560a053dee904e45ad66aac7145d.jpg" alt="Deployment、ReplicaSet 和 Pod 的关系图" style="zoom:67%;max-width: 50%" /><h2 id="Deployment-对应用进行版本控制的具体原理"><a href="#Deployment-对应用进行版本控制的具体原理" class="headerlink" title="Deployment 对应用进行版本控制的具体原理"></a>Deployment 对应用进行版本控制的具体原理</h2><p>通过kubectl set image 的指令，直接修改 nginx-deployment 所使用的镜像。</p><p> kubectl rollout undo 命令，把 Deployment 回滚到上一个版本：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl rollout undo deployment&#x2F;nginx-deployment</span><br><span class="line">deployment.extensions&#x2F;nginx-deployment</span><br></pre></td></tr></table></figure><p>首先，我需要使用 kubectl rollout history 命令，查看每次 Deployment 变更对应的版本。而由于我们在创建这个 Deployment 的时候，指定了–record 参数，所以我们创建这些版本时执行的 kubectl 命令，都会被记录下来。如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl rollout history deployment&#x2F;nginx-deployment</span><br><span class="line">deployments &quot;nginx-deployment&quot;</span><br><span class="line">REVISION    CHANGE-CAUSE</span><br><span class="line">1           kubectl create -f nginx-deployment.yaml --record</span><br><span class="line">2           kubectl edit deployment&#x2F;nginx-deployment</span><br><span class="line">3           kubectl set image deployment&#x2F;nginx-deployment nginx&#x3D;nginx:1.91</span><br></pre></td></tr></table></figure><p>通过这个 kubectl rollout history 指令，看到每个版本对应的 Deployment 的 API 对象的细节，具体命令如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl rollout history deployment&#x2F;nginx-deployment --revision&#x3D;2</span><br></pre></td></tr></table></figure><p>然后，我们就可以在 kubectl rollout undo 命令行最后，加上要回滚到的指定版本的版本号，就可以回滚到指定版本了</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl rollout undo deployment&#x2F;nginx-deployment --to-revision&#x3D;2</span><br><span class="line">deployment.extensions&#x2F;nginx-deployment</span><br></pre></td></tr></table></figure><p>所以，Kubernetes 项目还提供了一个指令，使得我们对 Deployment 的多次更新操作，最后 只生成一个 ReplicaSet。具体的做法是，在更新 Deployment 前，你要先执行一条 kubectl rollout pause 指令。它的用法如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl rollout pause deployment&#x2F;nginx-deployment</span><br><span class="line">deployment.extensions&#x2F;nginx-deployment paused</span><br></pre></td></tr></table></figure><p>这个 kubectl rollout pause 的作用，是让这个 Deployment 进入了一个“暂停”状态。</p><p>所以接下来，你可以使用 kubectl edit 或者 kubectl set image 指令，修改这个 Deployment 的内容。由于此时 Deployment 正处于“暂停”状态，所以我们对 Deployment 的所有修改，都不会触发新的“滚动更新”，也不会创建新的 ReplicaSet。</p><p>而等到我们对 Deployment 修改操作都完成之后，只需要再执行一条 kubectl rollout resume 指令，就可以把这个 Deployment“恢复”回来，如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl rollout resume deployment&#x2F;nginx-deployment</span><br><span class="line">deployment.extensions&#x2F;nginx-deployment resumed</span><br></pre></td></tr></table></figure><p>而在这个 kubectl rollout resume 指令执行之前，在 kubectl rollout pause 指令之后的这段时间里，我们对 Deployment 进行的所有修改，最后只会触发一次“滚动更新”。</p><p>通过 spec.revisionHistoryLimit 控制这些“历史”ReplicaSet 的数量，就是 Deployment 保留的“历史版本”个数。所以，如果把它设置为 0，你就再也不能做回滚操作了。</p><p>你听说过金丝雀发布（Canary Deployment）和蓝绿发布（Blue-Green Deployment）吗？你能说出它们是什么意思吗？</p><p>金丝雀发布：先发布一台机器或少量机器，做流量验证。如果新版没问题在把剩余机器全部更新。优点是影响范围小，不足的是要自己想办法如何控制自动更新。<br>蓝绿部署：事先准备好一组机器(绿组)全部更新，然后调整LB将流量全部引到绿组。优点是切换快捷回滚方便。不足的是有问题则影响全部用户。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 深入剖析Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 深入剖析Kuernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>13-15.深入解析pod对象</title>
      <link href="/2020/09/11/13-15-%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90pod%E5%AF%B9%E8%B1%A1/"/>
      <url>/2020/09/11/13-15-%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90pod%E5%AF%B9%E8%B1%A1/</url>
      
        <content type="html"><![CDATA[<p>极客时间课程张磊的 <a href="https://time.geekbang.org/column/intro/116" target="_blank" rel="noopener">深入剖析Kuernetes</a> </p><h1 id="13-为什么我们需要Pod？"><a href="#13-为什么我们需要Pod？" class="headerlink" title="13 | 为什么我们需要Pod？"></a>13 | 为什么我们需要Pod？</h1><h2 id="Kubernetes-为什么使用pod"><a href="#Kubernetes-为什么使用pod" class="headerlink" title="Kubernetes 为什么使用pod"></a>Kubernetes 为什么使用pod</h2><p>POD的直议是豆荚，豆荚中的一个或者多个豆 ( docker ) 属于同一个家庭，共享一个物理豆荚。</p><p>容器的本质是进程, Pod是 Kubernetes 项目的原子调度单位。</p><p>Pod 在 Kubernetes 项目里还有更重要的意义，那就是：容器设计模式。</p><p>Kubernetes 项目所做的，其实就是将“进程组”的概念映射到了容器技术中。pstree -g  展示当前系统中正在运行的进程的树状结构。</p><p>Mesos 中就有一个资源囤积（resource hoarding）的机制，会在所有设置了 Affinity 约束的任务都达到时，才开始对它们统一进行调度。而在 Google Omega 论文中，则提出了使用乐观调度处理冲突的方法，即：先不管这些冲突，而是通过精心设计的回滚机制在出现了冲突之后解决问题。</p><p>“超亲密关系”容器的典型特征包括但不限于：互相之间会发生直接的文件交换、使用 localhost 或者 Socket 文件进行本地通信、会发生非常频繁的远程调用、需要共享某些 Linux Namespace等等。</p><h2 id="Pod原理"><a href="#Pod原理" class="headerlink" title="Pod原理"></a>Pod原理</h2><p>首先，关于 Pod 最重要的一个事实是：它只是一个逻辑概念。</p><p>Kubernetes 真正处理的，还是宿主机操作系统上 Linux 容器的 Namespace 和 Cgroups，而并不存在一个所谓的 Pod 的边界或者隔离环境。</p><p>Pod，其实是一组共享了某些资源的容器。Pod 里的所有容器，共享的是同一个 Network Namespace，并且可以声明共享同一个 Volume。</p><h2 id="Pod网络"><a href="#Pod网络" class="headerlink" title="Pod网络"></a>Pod网络</h2><p>镜像 k8s.gcr.io/pause 是一个用汇编语言编写的、永远处于“暂停”状态的特殊容器，解压后的大小只有 100~200 KB 。</p><p>对于同一个 Pod 里面的所有用户容器来说，它们的进出流量，可以认为都是通过 Infra 容器完成的。这一点很重要，因为将来如果你要为 Kubernetes 开发一个网络插件时，应该重点考虑的是如何配置这个 Pod 的 Network Namespace，而不是每一个用户容器如何使用你的网络配置，这是没有意义的。</p><p>这就意味着，如果你的网络插件需要在容器里安装某些包或者配置才能完成的话，是不可取的：Infra 容器镜像的 rootfs 里几乎什么都没有，没有你随意发挥的空间。当然，这同时也意味着你的网络插件完全不必关心用户容器的启动与否，而只需要关注如何配置 Pod，也就是 Infra 容器的 Network Namespace 即可。</p><h2 id="Pod容器设计模式"><a href="#Pod容器设计模式" class="headerlink" title="Pod容器设计模式"></a>Pod容器设计模式</h2><p>在 Pod 中，所有 Init Container 定义的容器，都会比 spec.containers 定义的用户容器先启动。并且，<strong>Init Container 容器会按顺序逐一启动，而直到它们都启动并且退出了，用户容器才会启动</strong>。</p><p>实际上，这个所谓的“组合”操作，正是容器设计模式里最常用的一种模式 sidecar（边车）。顾名思义，sidecar 指的就是我们可以在一个 Pod 中，启动一个辅助容器，来完成一些独立于主进程（主容器）之外的工作。</p><p>Pod 的另一个重要特性是，它的所有容器都共享同一个 Network Namespace。这就使得很多与 Pod 网络相关的配置和管理，也都可以交给 sidecar 完成，而完全无须干涉用户容器。这里最典型的例子莫过于 Istio 这个微服务治理项目了。</p><h2 id="服务上云"><a href="#服务上云" class="headerlink" title="服务上云"></a>服务上云</h2><p>“服务上云”工作的完成，最终还是要靠深入理解容器的本质，即：进程。</p><p>实际上，一个运行在虚拟机里的应用，哪怕再简单，也是被管理在 systemd 或者 supervisord 之下的一组进程，而不是一个进程。这跟本地物理机上应用的运行方式其实是一样的。这也是为什么，从物理机到虚拟机之间的应用迁移，往往并不困难。</p><p>可是对于容器来说，一个容器永远只能管理一个进程。更确切地说，一个容器，就是一个进程。这是容器技术的“天性”，不可能被修改。所以，将一个原本运行在虚拟机里的应用，“无缝迁移”到容器中的想法，实际上跟容器的本质是相悖的。</p><p>Pod 这个概念，提供的是一种编排思想，而不是具体的技术方案。所以，如果愿意的话，你完全可以使用虚拟机来作为 Pod 的实现，然后把用户容器都运行在这个虚拟机里。</p><p> Pod  实际上是在扮演传统基础设施里”虚拟机”的角色；而容器则是这个虚拟机里的应用程序。要完成传统基于虚拟机的应用到微服务架构的迁移，核心思想是：分析应用组成（组件、进程），将其拆分成松耦合的容器（以容器镜像方式分发），利用 Init Container 来解决顺序和依赖关系。这样的设计，是为了使用户从传统环境（虚拟机环境）向 Kubernetes（容器环境）的迁移，更加平滑。</p><h1 id="14-深入解析Pod对象（一）：基本概念"><a href="#14-深入解析Pod对象（一）：基本概念" class="headerlink" title="14 | 深入解析Pod对象（一）：基本概念"></a>14 | 深入解析Pod对象（一）：基本概念</h1><p>进程的哪些属性属于 Pod 对象，而又有哪些属性属于 Container 呢？</p><p>凡是调度、网络、存储，以及安全相关的属性，基本上是 Pod 级别的。</p><p>这些属性的共同特征是，它们描述的是“机器”这个整体，而不是里面运行的“程序”。</p><h2 id="调度pod到指定的节点"><a href="#调度pod到指定的节点" class="headerlink" title="调度pod到指定的节点"></a>调度pod到指定的节点</h2><p>NodeSelector：是一个供用户将 Pod 与 Node 进行绑定的字段</p><p>NodeName：一旦 Pod 的这个字段被赋值，Kubernetes 项目就会被认为这个 Pod 已经经过了调度，调度的结果就是赋值的节点名字。所以，这个字段一般由调度器负责设置，但用户也可以设置它来“骗过”调度器，当然这个做法一般是在测试或者调试的时候才会用到。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spec: </span><br><span class="line">  nodeSelector:   </span><br><span class="line">    disktype: ssd</span><br><span class="line">  nodeName: wang-23.host.com</span><br></pre></td></tr></table></figure><p>HostAliases：定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容</p><p>除了上述跟“机器”相关的配置外，你可能也会发现，凡是跟容器的 Linux Namespace 相关的属性，也一定是 Pod 级别的。这个原因也很容易理解：Pod 的设计，就是要让它里面的容器尽可能多地共享 Linux Namespace，仅保留必要的隔离和限制能力。这样，Pod 模拟出的效果，就跟虚拟机里程序间的关系非常类似了。</p><p>凡是 Pod 中的容器要共享宿主机的 Namespace，也一定是 Pod 级别的定义</p><p>shareProcessNamespace=true： 这个 Pod 里的容器要共享 PID Namespace。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  shareProcessNamespace: true</span><br><span class="line">  hostNetwork: true </span><br><span class="line">  hostIPC: true </span><br><span class="line">  hostPID: true</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br><span class="line">  - name: shell</span><br><span class="line">    image: busybox</span><br><span class="line">    stdin: true</span><br><span class="line">    tty: true</span><br></pre></td></tr></table></figure><p>在 Pod 的 YAML 文件里声明开启stdin 和 tty ，其实等同于设置了 docker run 里的 -it（-i 即 stdin，-t 即 tty）参数。</p><h2 id="ImagePullPolicy-镜像拉取策略"><a href="#ImagePullPolicy-镜像拉取策略" class="headerlink" title="ImagePullPolicy 镜像拉取策略"></a>ImagePullPolicy 镜像拉取策略</h2><p>ImagePullPolicy 它定义了镜像拉取的策略。而它之所以是一个 Container 级别的属性，是因为容器镜像本来就是 Container 定义中的一部分。</p><ul><li>Always： 默认策略。即每次创建 Pod 都重新拉取一次镜像。另外，当容器的镜像是类似于 nginx 或者 nginx:latest  这样的名字时，ImagePullPolicy 也会被认为 Always。</li><li>IfNotPresent：只在宿主机上不存在这个镜像时才拉取。</li><li>Never：  Pod 永远不会主动拉取这个镜像</li></ul><h2 id="Lifecycle钩子"><a href="#Lifecycle钩子" class="headerlink" title="Lifecycle钩子"></a>Lifecycle钩子</h2><p> Lifecycle 定义的是 Container Lifecycle Hooks。顾名思义，Container Lifecycle Hooks 的作用，是在容器状态发生变化时触发一系列“钩子”。我们来看这样一个例子：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: lifecycle-demo</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: lifecycle-demo-container</span><br><span class="line">    image: nginx</span><br><span class="line">    lifecycle:</span><br><span class="line">      postStart:</span><br><span class="line">        exec:</span><br><span class="line">          command: [&quot;&#x2F;bin&#x2F;sh&quot;, &quot;-c&quot;, &quot;echo Hello from the postStart handler &gt; &#x2F;usr&#x2F;share&#x2F;message&quot;]</span><br><span class="line">      preStop:</span><br><span class="line">        exec:</span><br><span class="line">          command: [&quot;&#x2F;usr&#x2F;sbin&#x2F;nginx&quot;,&quot;-s&quot;,&quot;quit&quot;]</span><br></pre></td></tr></table></figure><ul><li><p>postStart : 在容器启动后，立刻执行一个指定的操作。需要明确的是，postStart 定义的操作，虽然是在 Docker 容器 ENTRYPOINT 执行之后，但它并不严格保证顺序。也就是说，在 postStart 启动时，ENTRYPOINT 有可能还没有结束。</p></li><li><p>preStop: 发生的时机，则是容器被杀死之前（比如，收到了 SIGKILL 信号）。而需要明确的是，preStop 操作的执行，是同步的。所以，它会阻塞当前的容器杀死流程，直到这个 Hook 定义操作完成之后，才允许容器被杀死，这跟 postStart 不一样。</p></li></ul><h2 id="Pod-对象在-Kubernetes-中的生命周期。"><a href="#Pod-对象在-Kubernetes-中的生命周期。" class="headerlink" title="Pod 对象在 Kubernetes 中的生命周期。"></a>Pod 对象在 Kubernetes 中的生命周期。</h2><p>Pod 生命周期的变化，主要体现在 Pod API 对象的 Status 部分，这是它除了 Metadata 和 Spec 之外的第三个重要字段。其中，pod.status.phase，就是 Pod 的当前状态，它有如下几种可能的情况：</p><ol><li><p>Pending。这个状态意味着，Pod 的  YAML 文件已经提交给了 Kubernetes，API 对象已经被创建并保存在 Etcd 当中。但是，这个 Pod  里有些容器因为某种原因而不能被顺利创建。比如，调度不成功。</p></li><li><p>Running。这个状态下，Pod  已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中。</p></li><li><p>Succeeded。这个状态意味着，Pod  里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次性任务时最为常见。</p></li><li><p>Failed。这个状态下，Pod  里至少有一个容器以不正常的状态（非 0 的返回码）退出。这个状态的出现，意味着你得想办法 Debug 这个容器的应用，比如查看 Pod 的  Events 和日志。</p></li><li><p>Unknown。这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给  kube-apiserver，这很有可能是主从节点（Master 和 Kubelet）间的通信出现了问题。</p></li></ol><p>更进一步地，Pod 对象的 Status 字段，还可以再细分出一组 Conditions。这些细分状态的值包括：PodScheduled、Ready、Initialized，以及 Unschedulable。它们主要用于描述造成当前 Status 的具体原因是什么。</p><p>Ready 这个细分状态非常值得我们关注：它意味着 Pod 不仅已经正常启动（Running 状态），而且已经可以对外提供服务了。注意两者之间（Running 和 Ready）的区别。</p><p>在学习完这篇文章后，我希望你能仔细阅读 $GOPATH/src/k8s.io/kubernetes/vendor/k8s.io/api/core/v1/types.go 里，type Pod struct ，尤其是 PodSpec 部分的内容。争取做到下次看到一个 Pod 的 YAML 文件时，不再需要查阅文档，就能做到把常用字段及其作用信手拈来。</p><h1 id="15-深入解析Pod对象（二）：使用进阶"><a href="#15-深入解析Pod对象（二）：使用进阶" class="headerlink" title="15 | 深入解析Pod对象（二）：使用进阶"></a>15 | 深入解析Pod对象（二）：使用进阶</h1><h2 id="投射数据卷"><a href="#投射数据卷" class="headerlink" title="投射数据卷"></a>投射数据卷</h2><p>我们就先从一种特殊的 Volume 开始，来帮助你更加深入地理解 Pod 对象各个重要字段的含义。这种特殊的 Volume，叫作 Projected Volume，你可以把它翻译为“投射数据卷”。</p><p>在 Kubernetes 中，有几种特殊的 Volume，它们存在的意义不是为了存放容器里的数据，也不是用来进行容器和宿主机之间的数据交换。这些特殊 Volume 的作用，是为容器提供预先定义好的数据。所以，从容器的角度来看，这些 Volume 里的信息就是仿佛是被 Kubernetes“投射”（Project）进入容器当中的。这正是 Projected Volume 的含义。</p><p>到目前为止，Kubernetes 支持的 Projected Volume 一共有四种：</p><ul><li>Secret；</li><li>ConfigMap；</li><li>Downward API；</li><li>ServiceAccountToken。</li></ul><h2 id="Secret"><a href="#Secret" class="headerlink" title="Secret"></a>Secret</h2><p>它的作用，是帮你把 Pod 想要访问的加密数据，存放到 Etcd 中。然后，你就可以通过在 Pod 的容器里挂载 Volume 的方式，访问到这些 Secret 里保存的信息了。</p><p>通过命令行创建</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl create secret generic user --from-file&#x3D;.&#x2F;username.txt</span><br></pre></td></tr></table></figure><p>通过yaml创建</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: mysecret</span><br><span class="line">type: Opaque</span><br><span class="line">data:</span><br><span class="line">  user: YWRtaW4&#x3D;</span><br><span class="line">  pass: MWYyZDFlMmU2N2Rm</span><br></pre></td></tr></table></figure><p>Secret 对象要求这些数据必须是经过 Base64 转码的</p><p>更重要的是，像这样通过挂载方式进入到容器里的 Secret，一旦其对应的 Etcd 里的数据被更新，这些 Volume 里的文件内容，同样也会被更新。其实，这是 kubelet 组件在定时维护这些 Volume。</p><p>需要注意的是，这个更新可能会有一定的延时。所以在编写应用程序时，在发起数据库连接的代码处写好重试和超时的逻辑，绝对是个好习惯。</p><h2 id="ConfigMap"><a href="#ConfigMap" class="headerlink" title="ConfigMap"></a>ConfigMap</h2><p>与 Secret 类似的是 ConfigMap，它与 Secret 的区别在于，ConfigMap 保存的是不需要加密的、应用所需的配置信息。而 ConfigMap 的用法几乎与 Secret 完全相同：你可以使用 kubectl create configmap 从文件或者目录创建 ConfigMap，也可以直接编写 ConfigMap 对象的 YAML 文件。</p><h2 id="Downward-API"><a href="#Downward-API" class="headerlink" title="Downward API"></a>Downward API</h2><p>它的作用是：让 Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">volumes:</span><br><span class="line">  - name: podinfo</span><br><span class="line">    projected:</span><br><span class="line">      sources:</span><br><span class="line">      - downwardAPI:</span><br><span class="line">          items:</span><br><span class="line">            - path: &quot;labels&quot;</span><br><span class="line">              fieldRef:</span><br><span class="line">                fieldPath: metadata.labels</span><br></pre></td></tr></table></figure><p>目前，Downward API 支持的字段已经非常丰富了，比如：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. 使用fieldRef可以声明使用:</span><br><span class="line">spec.nodeName - 宿主机名字</span><br><span class="line">status.hostIP - 宿主机IP</span><br><span class="line">metadata.name - Pod的名字</span><br><span class="line">metadata.namespace - Pod的Namespace</span><br><span class="line">status.podIP - Pod的IP</span><br><span class="line">spec.serviceAccountName - Pod的Service Account的名字</span><br><span class="line">metadata.uid - Pod的UID</span><br><span class="line">metadata.labels[&#39;&lt;KEY&gt;&#39;] - 指定&lt;KEY&gt;的Label值</span><br><span class="line">metadata.annotations[&#39;&lt;KEY&gt;&#39;] - 指定&lt;KEY&gt;的Annotation值</span><br><span class="line">metadata.labels - Pod的所有Label</span><br><span class="line">metadata.annotations - Pod的所有Annotation</span><br><span class="line"></span><br><span class="line">2. 使用resourceFieldRef可以声明使用:</span><br><span class="line">容器的CPU limit</span><br><span class="line">容器的CPU request</span><br><span class="line">容器的memory limit</span><br><span class="line">容器的memory request</span><br></pre></td></tr></table></figure><p>Downward API 能够获取到的信息，一定是 Pod 里的容器进程启动之前就能够确定下来的信息。而如果你想要获取 Pod 容器运行后才会出现的信息，比如，容器进程的 PID，那就肯定不能使用 Downward API 了，而应该考虑在 Pod 里定义一个 sidecar 容器。</p><p>Secret、ConfigMap，以及 Downward API 这三种 Projected Volume 定义的信息，大多还可以通过环境变量的方式出现在容器里。但是，通过环境变量获取这些信息的方式，不具备自动更新的能力。所以，一般情况下，我都建议你使用 Volume 文件的方式获取这些信息。</p><h2 id="Service-Account服务账号"><a href="#Service-Account服务账号" class="headerlink" title="Service Account服务账号"></a>Service Account服务账号</h2><p>Service Account 的授权信息和文件，实际上保存在它所绑定的一个特殊的 Secret 对象里的。这个特殊的 Secret 对象，就叫作 ServiceAccountToken。任何运行在 Kubernetes 集群上的应用，都必须使用这个 ServiceAccountToken 里保存的授权信息，也就是 Token，才可以合法地访问 API Server。</p><p>另外，为了方便使用，Kubernetes 已经为你提供了一个默认“服务账户”（default Service Account）。并且，任何一个运行在 Kubernetes 里的 Pod，都可以直接使用这个默认的 Service Account，而无需显示地声明挂载它。</p><p>这个 Secret 类型的 Volume，正是默认 Service Account 对应的 ServiceAccountToken。所以说，Kubernetes 其实在每个 Pod 创建的时候，自动在它的 spec.volumes 部分添加上了默认 ServiceAccountToken 的定义，然后自动给每个容器加上了对应的 volumeMounts 字段。这个过程对于用户来说是完全透明的。</p><p>这样，一旦 Pod 创建完成，容器里的应用就可以直接从这个默认 ServiceAccountToken 的挂载目录里访问到授权信息和文件。这个容器内的路径在 Kubernetes 里是固定的，即：/var/run/secrets/kubernetes.io/serviceaccount ，而这个 Secret 类型的 Volume 里面的内容如下所示：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ls &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount </span><br><span class="line">ca.crt namespace  token</span><br></pre></td></tr></table></figure><p>这种把 Kubernetes 客户端以容器的方式运行在集群里，然后使用 default Service Account 自动授权的方式，被称作“InClusterConfig”，也是我最推荐的进行 Kubernetes API 编程的授权方式。</p><h3 id="容器健康检查和恢复机制。"><a href="#容器健康检查和恢复机制。" class="headerlink" title="容器健康检查和恢复机制。"></a>容器健康检查和恢复机制。</h3><p>在 Kubernetes 中，你可以为 Pod 里的容器定义一个健康检查“探针”（Probe）。这样，kubelet 就会根据这个 Probe 的返回值决定这个容器的状态，而不是直接以容器进行是否运行（来自 Docker 返回的信息）作为依据。这种机制，是生产环境中保证应用健康存活的重要手段。</p><p>Kubernetes 中并没有 Docker 的 Stop 语义。所以虽然是 Restart（重启），但实际却是重新创建了容器。</p><p>这个功能就是 Kubernetes 里的 Pod 恢复机制，也叫 restartPolicy。它是 Pod 的 Spec 部分的一个标准字段（pod.spec.restartPolicy），默认值是 Always，即：任何时候这个容器发生了异常，它一定会被重新创建。</p><p>但一定要强调的是，Pod 的恢复过程，永远都是发生在当前节点上，而不会跑到别的节点上去。事实上，一旦一个 Pod 与一个节点（Node）绑定，除非这个绑定发生了变化（pod.spec.node 字段被修改），否则它永远都不会离开这个节点。这也就意味着，如果这个宿主机宕机了，这个 Pod 也不会主动迁移到其他节点上去。</p><p>而如果你想让 Pod 出现在其他的可用节点上，就必须使用 Deployment 这样的“控制器”来管理 Pod，</p><p>你还可以通过设置 restartPolicy，改变 Pod 的恢复策略：</p><ul><li>Always：在任何情况下，只要容器不在运行状态，就自动重启容器；</li><li>OnFailure: 只在容器 异常时才自动重启容器；</li><li>Never: 从来不重启容器。</li></ul><p>deployment所创建的pod restart策略只支持aways。</p><p>Kubernetes 的官方文档，把 restartPolicy 和 Pod 里容器的状态，以及 Pod 状态的对应关系</p><p>只要 Pod 的 restartPolicy 指定的策略允许重启异常的容器（比如：Always），那么这个 Pod 就会保持 Running 状态，并进行容器重启。否则，Pod 就会进入 Failed 状态 </p><p>对于包含多个容器的 Pod，只有它里面所有的容器都进入异常状态后，Pod 才会进入 Failed 状态。在此之前，Pod 都是 Running 状态。此时，Pod 的 READY 字段会显示正常容器的个数，比如：</p><p>Pod通过健康检查是指里面所有的Container都通过</p><p>service只会代理readiness检查返回正确的pod</p><h2 id="PodPreset"><a href="#PodPreset" class="headerlink" title="PodPreset"></a>PodPreset</h2><p>PodPreset 里定义的内容，只会在 Pod API 对象被创建之前追加在这个对象本身上，而不会影响任何 Pod 的控制器的定义。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: settings.k8s.io&#x2F;v1alpha1</span><br><span class="line">kind: PodPreset</span><br><span class="line">metadata:</span><br><span class="line">  name: allow-database</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      role: frontend</span><br><span class="line">  env:</span><br><span class="line">    - name: DB_PORT</span><br><span class="line">      value: &quot;6379&quot;</span><br><span class="line">  volumeMounts:</span><br><span class="line">    - mountPath: &#x2F;cache</span><br><span class="line">      name: cache-volume</span><br><span class="line">  volumes:</span><br><span class="line">    - name: cache-volume</span><br><span class="line">      emptyDir: &#123;&#125;</span><br></pre></td></tr></table></figure><p>比如，我们现在提交的是一个 nginx-deployment，那么这个 Deployment 对象本身是永远不会被 PodPreset 改变的，被修改的只是这个 Deployment 创建出来的所有 Pod。这一点请务必区分清楚。</p><p>实际上，Kubernetes 项目会帮你合并（Merge）这两个 PodPreset 要做的修改。而如果它们要做的修改有冲突的话，这些冲突字段就不会被修改。</p><p>体会 Kubernetes“一切皆对象”的设计思想。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 深入剖析Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 深入剖析Kuernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>监控DELL服务器的硬件信息</title>
      <link href="/2020/09/09/%E7%9B%91%E6%8E%A7dell%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E7%A1%AC%E4%BB%B6%E4%BF%A1%E6%81%AF/"/>
      <url>/2020/09/09/%E7%9B%91%E6%8E%A7dell%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E7%A1%AC%E4%BB%B6%E4%BF%A1%E6%81%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="DELL-iDRAC服务器远程控制设置"><a href="#DELL-iDRAC服务器远程控制设置" class="headerlink" title="DELL iDRAC服务器远程控制设置"></a>DELL iDRAC服务器远程控制设置</h1><blockquote><p> 原文地址<a href="https://blog.csdn.net/openbox2008/article/details/79636510" target="_blank" rel="noopener">DELL iDRAC服务器远程控制设置</a></p></blockquote><p> 什么是iDRAC呢？iDRAC又称为Integrated Dell Remote Access  Controller，也就是集成戴尔远程控制卡，这是戴尔服务器的独有功能，iDRAC卡相当于是附加在服务器上的一计算机，可以实现一对一的服务器远程管理与监控，通过与服务器主板上的管理芯片BMC进行通信，监控与管理服务器的硬件状态信息。它拥有自己的系统和IP地址，与服务器上的OS无关。是管理员进行远程访问和管理的利器，戴尔服务器集成了iDRAC控制卡，我们就可以扔掉价格昂贵的KVM设备了。在戴尔第12代服务器中，iDRAC的版本升级到了iDRAC 7，下面我们将以戴尔PowerEdge R820为例，为您图解iDRAC的一步步设置。</p><img src="http://wangzhangtao.com/img/body/2.监控DELL服务器硬件信息/67P09E8LK005_DSC_2067_500.JPG" alt="戴尔服务器的iDRAC远程控制接口，在图的右下角，有“iDRAC”字样" style="zoom:67%;" /><p>iDRAC的网口在服务器的背面，一般都标注iDRAC的字样。在戴尔第12代服务器中，这个网口使用了1Gbps的网口，速度更快。一般情况下，iDRAC功能默认都是关闭，我们需要在BIOS里面启用，首先我们先重启计算机，按F2 然后进入BIOS，选择iDRAC Setting。</p><img src="http://wangzhangtao.com/img/body/2.监控DELL服务器硬件信息/VYDMQ94DXL0U_bios_500.jpg" alt="在BIOS中选择iDRAC设置" style="zoom:67%;" /><img src="http://wangzhangtao.com/img/body/2.监控DELL服务器硬件信息/L4TNKM9Q5H51_bios2_500.jpg" alt="BIOS中iDRAC详细的设置列表" style="zoom:67%;" /><p>进入iDRAC Setting之后，我们看到很多详细的设置，一般情况下我们只要设置网络Network就可以了。</p><img src="http://wangzhangtao.com/img/body/2.监控DELL服务器硬件信息/5EMX84MIVT76_idrac1_500.jpg" alt="在BIOS中开启iDRAC功能" style="zoom:67%;" /><p>修改的参数会在推出时保存，并重启</p><h1 id="Zabbix监控DELL服务器的硬件信息"><a href="#Zabbix监控DELL服务器的硬件信息" class="headerlink" title="Zabbix监控DELL服务器的硬件信息"></a>Zabbix监控DELL服务器的硬件信息</h1><blockquote><p> 原文地址 <a href="https://blog.51cto.com/liqingbiao/2108554" target="_blank" rel="noopener">Zabbix通过SNMPv2监控DELL服务器的硬件信息</a></p></blockquote><p>Zabbix通过SNMPv2监控DELL服务器的硬件信息</p><h2 id="监控DELL服务器简述"><a href="#监控DELL服务器简述" class="headerlink" title="监控DELL服务器简述"></a>监控DELL服务器简述</h2><p>监控DELL服务器硬件一般有两种途径：<br>1、操作系统上安装OMSA，编写脚本调用omreport命令进行监控（需要在操作系统上安装比较麻烦）；<br>2、使用iDRAC（Integrated Dell Remote Access  Controller，是一款dell专门用于远程访问控制接口），可以不用在操作系统上安装OMSA，只需要在iDRAC上开启SNMP，zabbix通过SNMP进行监控。对于不支持OMSA的操作系统和要求不能安装额外软件的情况下，推荐使用SNMP监控，配置简单方便（推荐使用）。</p><h2 id="主要步骤"><a href="#主要步骤" class="headerlink" title="主要步骤"></a>主要步骤</h2><p><strong>1，登陆iDRAC，直接在浏览器输入访问的ip即可</strong></p><img src="http://wangzhangtao.com/img/body/2.监控DELL服务器硬件信息/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=.png" alt="Zabbix通过SNMPv2监控DELL服务器的硬件信息" style="zoom:67%;max-width: 70%;" /><p><strong>2,通过iDRAC开启snmp服务：iDRAC设置—&gt;网络—&gt;服务—&gt;snmp代理，启用snmp并设置团体名（community），不推荐使用默认的public。建议所有服务器设置统一的团体名。</strong></p><img src="http://wangzhangtao.com/img/body/2.监控DELL服务器硬件信息/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=-20200910143956239.png" alt="Zabbix通过SNMPv2监控DELL服务器的硬件信息" style="zoom:67%;max-width: 70%;" /><p>3，在zabbix_server或代理端通过snmpwalk（snmpwalk -v 2c -c  public IP）查看是否能获取到数据。如果获取不到数据，查看失败的原因。</p><img src="http://wangzhangtao.com/img/body/2.监控DELL服务器硬件信息/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=-20200910144013620.png" alt="Zabbix通过SNMPv2监控DELL服务器的硬件信息" style="zoom:67%;max-width: 70%;" /><p>4，在zabbix官网下下载改监控的相应的模板（<a href="https://share.zabbix.com/cat-server-hardware/dell/dell-idrac-chinese）" target="_blank" rel="noopener">https://share.zabbix.com/cat-server-hardware/dell/dell-idrac-chinese）</a>   <a href="https://share.zabbix.com/" target="_blank" rel="noopener">https://share.zabbix.com/</a></p><p>其他的模板都可以在这进行下载</p><img src="http://wangzhangtao.com/img/body/2.监控DELL服务器硬件信息/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=-20200910144037710.png" alt="Zabbix通过SNMPv2监控DELL服务器的硬件信息" style="zoom:67%;max-width: 70%;" /><p>5，把下载的模板导入到zabbix_server中。配置—&gt;模板—&gt;选择文件—&gt;导入</p><img src="http://wangzhangtao.com/img/body/2.监控DELL服务器硬件信息/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=-20200910144050016.png" alt="Zabbix通过SNMPv2监控DELL服务器的硬件信息" style="zoom:67%;max-width: 70%;" /><p>6，添加监控服务器。配置—&gt;主机—&gt;创建主机，填写相关信息。</p><img src="http://wangzhangtao.com/img/body/2.监控DELL服务器硬件信息/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=-20200910144102621.png" alt="Zabbix通过SNMPv2监控DELL服务器的硬件信息" style="zoom:67%;max-width: 70%;" /><img src="http://wangzhangtao.com/img/body/2.监控DELL服务器硬件信息/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=-20200910144110167.png" alt="Zabbix通过SNMPv2监控DELL服务器的硬件信息" style="zoom:67%;max-width: 70%;" /><img src="http://wangzhangtao.com/img/body/2.监控DELL服务器硬件信息/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=-20200910144116412.png" alt="Zabbix通过SNMPv2监控DELL服务器的硬件信息" style="zoom:67%;max-width: 70%;" /><p>7，修改收集数据的时间，查看获取的数据</p><img src="http://wangzhangtao.com/img/body/2.监控DELL服务器硬件信息/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=-20200910144135200.png" alt="Zabbix通过SNMPv2监控DELL服务器的硬件信息" style="zoom:67%;max-width: 70%;" /><img src="http://wangzhangtao.com/img/body/2.监控DELL服务器硬件信息/watermark,size_16,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_100,g_se,x_10,y_10,shadow_90,type_ZmFuZ3poZW5naGVpdGk=-20200910144142070.png" alt="Zabbix通过SNMPv2监控DELL服务器的硬件信息" style="zoom:67%;max-width: 70%;" /><p>最后可以查看下监控项看是否有不支持的，不支持的查看原因进行优化。</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
          <category> 服务器 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 其他 </tag>
            
            <tag> 服务器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10-12.Kubernetest集群搭建与实践</title>
      <link href="/2020/09/09/10-12-kubernetest%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/"/>
      <url>/2020/09/09/10-12-kubernetest%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<p>极客时间课程张磊的 <a href="https://time.geekbang.org/column/intro/116" target="_blank" rel="noopener">深入剖析Kuernetes</a> </p><h1 id="10-Kubernetes一键部署利器：kubeadm"><a href="#10-Kubernetes一键部署利器：kubeadm" class="headerlink" title="10 | Kubernetes一键部署利器：kubeadm"></a>10 | Kubernetes一键部署利器：kubeadm</h1><p>要真正发挥容器技术的实力，你就不能仅仅局限于对 Linux 容器本身的钻研和使用。</p><h2 id="kubeadm-init-的工作流程"><a href="#kubeadm-init-的工作流程" class="headerlink" title="kubeadm init 的工作流程"></a>kubeadm init 的工作流程</h2><ol><li><p>kubeadm 首先要做的，是一系列的检查工作，以确定这台机器可以用来部署 Kubernetes。这一步检查，我们称为“Preflight Checks”，它可以为你省掉很多后续的麻烦。</p></li><li><p>在通过了 Preflight Checks 之后，kubeadm 要为你做的，是生成 Kubernetes 对外提供服务所需的各种证书和对应的目录。</p><p>Kubernetes 对外提供服务时，除非专门开启“不安全模式”，否则都要通过 HTTPS 才能访问 kube-apiserver。这就需要为 Kubernetes 集群配置好证书文件。</p><p>kubeadm 为 Kubernetes 项目生成的证书文件都放在 Master 节点的 /etc/kubernetes/pki 目录下。在这个目录下，最主要的证书文件是 ca.crt 和对应的私钥 ca.key。</p><p>此外，用户使用 kubectl 获取容器日志等 streaming 操作时，需要通过 kube-apiserver 向 kubelet 发起请求，这个连接也必须是安全的。kubeadm 为这一步生成的是 apiserver-kubelet-client.crt 文件，对应的私钥是 apiserver-kubelet-client.key。</p></li></ol><ol start="3"><li><p>证书生成后，kubeadm 接下来会为其他组件生成访问 kube-apiserver 所需的配置文件。这些文件的路径是：/etc/kubernetes/xxx.conf：</p></li><li><p>kubeadm 会为 Master 组件生成 Pod 配置文件。</p><p>Kubernetes 有三个 Master 组件 kube-apiserver、kube-controller-manager、kube-scheduler，而它们都会被使用 Pod 的方式部署起来。</p><p>在 Kubernetes  中，有一种特殊的容器启动方法叫做“Static Pod”。它允许你把要部署的 Pod 的 YAML  文件放在一个指定的目录里。这样，当这台机器上的 kubelet 启动时，它会自动检查这个目录，加载所有的 Pod YAML  文件，然后在这台机器上启动它们。</p><p>从这一点也可以看出，kubelet 在 Kubernetes 项目中的地位非常高，在设计上它就是一个完全独立的组件，而其他 Master 组件，则更像是辅助性的系统容器。</p><p>在 kubeadm 中，Master 组件的 YAML 文件会被生成在 /etc/kubernetes/manifests 路径下。</p></li></ol><ol start="5"><li><p>kubeadm 还会再生成一个 Etcd 的 Pod YAML 文件，通过同样的 Static Pod 的方式启动 Etcd。</p></li><li><p>Master 容器启动后，kubeadm 会通过检查 localhost:6443/healthz 这个 Master 组件的健康检查 URL，等待 Master 组件完全运行起来。</p></li><li><p>然后，kubeadm 就会为集群生成一个 bootstrap token。</p><p>在后面，只要持有这个 token，任何一个安装了 kubelet 和 kubadm 的节点，都可以通过 kubeadm join 加入到这个集群当中。</p></li><li><p>在 token 生成之后，kubeadm 会将 ca.crt 等 Master 节点的重要信息，通过 ConfigMap 的方式保存在 Etcd 当中，供后续部署 Node 节点使用。这个 ConfigMap 的名字是 cluster-info。</p></li><li><p>kubeadm init 的最后一步，就是安装默认插件。Kubernetes 默认 kube-proxy 和 DNS 这两个插件是必须安装的。它们分别用来提供整个集群的服务发现和 DNS 功能。其实，这两个插件也只是两个容器镜像而已，所以 kubeadm 只要用 Kubernetes 客户端创建了两个 Pod 。</p></li></ol><h2 id="kubeadm-join-的工作流程"><a href="#kubeadm-join-的工作流程" class="headerlink" title="kubeadm join 的工作流程"></a>kubeadm join 的工作流程</h2><p>kubeadm 至少需要发起一次“不安全模式”的访问到 kube-apiserver，从而拿到保存在 ConfigMap 中的 cluster-info（它保存了 APIServer 的授权信息）。而 bootstrap token，扮演的就是这个过程中的安全验证的角色。</p><p>只要有了 cluster-info 里的 kube-apiserver 的地址、端口、证书，kubelet 就可以以“安全模式”连接到 apiserver 上，这样一个新的节点就部署完成了。接下来，你只要在其他节点上重复这个指令就可以了。</p><h2 id="配置-kubeadm-的部署参数"><a href="#配置-kubeadm-的部署参数" class="headerlink" title="配置 kubeadm 的部署参数"></a>配置 kubeadm 的部署参数</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm init --config kubeadm.yaml</span><br></pre></td></tr></table></figure><p>阿里云google镜像仓库 registry.cn-hangzhou.aliyuncs.com/google_containers </p><p>通过kubeadm部署生成的apiserver证书默认有效期是一年，官方是认为需要通过kubeadm upgrade 每年升级一次kubernetes，升级的时候也会更新证书， </p><h1 id="11-从0到1：搭建一个完整的Kubernetes集群"><a href="#11-从0到1：搭建一个完整的Kubernetes集群" class="headerlink" title="11 | 从0到1：搭建一个完整的Kubernetes集群"></a>11 | 从0到1：搭建一个完整的Kubernetes集群</h1><p>Kubernetes  支持容器网络插件，使用的是一个名叫 CNI 的通用接口，它也是当前容器网络的事实标准，市面上的所有容器网络开源项目都可以通过 CNI 接入  Kubernetes，比如 Flannel、Calico、Canal、Romana 等等.</p><p>容器最典型的特征之一：无状态。</p><h2 id="污点和污点容忍"><a href="#污点和污点容忍" class="headerlink" title="污点和污点容忍"></a>污点和污点容忍</h2><p>通过 Taint/Toleration 调整 Master 执行 Pod 的策略</p><p>一旦某个节点被加上了一个 Taint，即被“打上了污点”，那么所有 Pod 就都不能在这个节点上运行。除非，有个别的 Pod 声明自己能“容忍”这个“污点”，即声明了 Toleration，它才可以在这个节点上运行。</p><p>其中，为节点打上“污点”（Taint）的命令是：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl taint nodes node1 foo&#x3D;bar:NoSchedule</span><br></pre></td></tr></table></figure><p>这时，该 node1 节点上就会增加一个键值对格式的 Taint，即：foo=bar:NoSchedule。其中值里面的 NoSchedule，意味着这个 Taint 只会在调度新 Pod 时产生作用，而不会影响已经在 node1 上运行的 Pod，哪怕它们没有 Toleration。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">...</span><br><span class="line">spec:</span><br><span class="line">  tolerations:</span><br><span class="line">  - key: &quot;foo&quot;</span><br><span class="line">    operator: &quot;Equal&quot;</span><br><span class="line">    value: &quot;bar&quot;</span><br><span class="line">    effect: &quot;NoSchedule&quot;</span><br></pre></td></tr></table></figure><p>查看master节点的污点标示</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl describe node master</span><br><span class="line"></span><br><span class="line">Name:               master</span><br><span class="line">Roles:              master</span><br><span class="line">Taints:             node-role.kubernetes.io&#x2F;master:NoSchedule</span><br></pre></td></tr></table></figure><p>此时，你就需要像下面这样用“Exists”操作符（operator: “Exists”，“存在”即可）来说明，该 Pod 能够容忍所有以 foo 为键的 Taint，才能让这个 Pod 运行在该 Master 节点上：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl taint nodes --all node-role.kubernetes.io&#x2F;master-</span><br></pre></td></tr></table></figure><p>如上所示，我们在“node-role.kubernetes.io/master”这个键后面加上了一个短横线“-”，这个格式就意味着移除所有以“node-role.kubernetes.io/master”为键的 Taint。</p><h2 id="存储插件Rook-项目"><a href="#存储插件Rook-项目" class="headerlink" title="存储插件Rook 项目"></a>存储插件Rook 项目</h2><p>Rook 项目是一个基于 Ceph 的 Kubernetes 存储插件（它后期也在加入对更多存储实现的支持）。不过，不同于对 Ceph 的简单封装，Rook 在自己的实现中加入了水平扩展、迁移、灾难备份、监控等大量的企业级功能，使得这个项目变成了一个完整的、生产级别可用的容器存储插件。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;rook&#x2F;rook&#x2F;master&#x2F;cluster&#x2F;examples&#x2F;kubernetes&#x2F;ceph&#x2F;common.yaml</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;rook&#x2F;rook&#x2F;master&#x2F;cluster&#x2F;examples&#x2F;kubernetes&#x2F;ceph&#x2F;operator.yaml</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;rook&#x2F;rook&#x2F;master&#x2F;cluster&#x2F;examples&#x2F;kubernetes&#x2F;ceph&#x2F;cluster.yaml</span><br></pre></td></tr></table></figure><h1 id="12-牛刀小试：我的第一个容器化应用"><a href="#12-牛刀小试：我的第一个容器化应用" class="headerlink" title="12 | 牛刀小试：我的第一个容器化应用"></a>12 | 牛刀小试：我的第一个容器化应用</h1><p>Kubernetes 跟 Docker 等很多项目最大的不同，就在于它不推荐你使用命令行的方式直接运行容器（虽然 Kubernetes 项目也支持这种方式，比如：kubectl run），而是希望你用 YAML 文件的方式，即：把容器的定义、参数、配置，统统记录在一个 YAML 文件中，然后用这样一句指令把它运行起来。</p><h2 id="K8s的API-对象"><a href="#K8s的API-对象" class="headerlink" title="K8s的API 对象"></a>K8s的API 对象</h2><p>一个 YAML 文件，对应到  Kubernetes 中，就是一个 API Object（API 对象）。当你为这个对象的各个字段填好值并提交给 Kubernetes  之后，Kubernetes 就会负责创建出这些对象所定义的容器或者其他类型的 API 资源。</p><p>Pod 就是 Kubernetes 里的“应用”；而一个应用，可以由多个容器组成。</p><p>需要注意的是，像这样使用一种 API 对象（Deployment）管理另一种 API 对象（Pod）的方法，在 Kubernetes 中，叫作“控制器”模式（controller pattern）。</p><h2 id="API-对象的格式"><a href="#API-对象的格式" class="headerlink" title="API 对象的格式"></a>API 对象的格式</h2><p>每一个 API 对象都有一个叫作 Metadata 的字段，这个字段就是 API 对象的“标识”，即元数据，它也是我们从 Kubernetes 里找到这个对象的主要依据。这其中最主要使用到的字段是 Labels。顾名思义，Labels 就是一组 key-value 格式的标签。而像 Deployment 这样的控制器对象，就可以通过这个 Labels 字段从 Kubernetes 中过滤出它所关心的被控制对象。</p><p>另外，在 Metadata 中，还有一个与 Labels 格式、层级完全相同的字段叫 Annotations，它专门用来携带 key-value 格式的内部信息。所谓内部信息，指的是对这些信息感兴趣的，是 Kubernetes 组件本身，而不是用户。所以大多数 Annotations，都是在 Kubernetes 运行过程中，被自动加在这个 API 对象上。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get pods -l app&#x3D;nginx</span><br></pre></td></tr></table></figure><p>kubectl get 指令的作用，就是从 Kubernetes 里面获取（GET）指定的 API 对象。可以看到，在这里我还加上了一个 -l 参数，即获取所有匹配 app: nginx 标签的 Pod。需要注意的是，在命令行中，所有 key-value 格式的参数，都使用“=”而非“:”表示。</p><h2 id="对象的-事件Events"><a href="#对象的-事件Events" class="headerlink" title="对象的 事件Events"></a>对象的 事件Events</h2><p>可以使用 kubectl describe 命令，查看一个 API 对象的细节</p><p>在 Kubernetes 执行的过程中，对 API 对象的所有重要操作，都会被记录在这个对象的 Events 里，并且显示在 kubectl describe 指令返回的结果中。比如，对于这个 Pod，我们可以看到它被创建之后，被调度器调度（Successfully assigned）到了 node-1，拉取了指定的镜像（pulling image），然后启动了 Pod 里定义的容器（Started container）。所以，这个部分正是我们将来进行 Debug 的重要依据。如果有异常发生，你一定要第一时间查看这些 Events，往往可以看到非常详细的错误信息。</p><p>可以使用 kubectl replace 指令来完成更新；但推荐你使用 kubectl apply 命令，来统一进行 Kubernetes 对象的创建和更新操作</p><p>这样的操作方法，是 Kubernetes“声明式 API”所推荐的使用方法。也就是说，作为用户，你不必关心当前的操作是创建，还是更新，你执行的命令始终是 kubectl apply，而 Kubernetes 则会根据 YAML 文件的内容变化，自动进行具体的处理。</p><h2 id="存储Volume"><a href="#存储Volume" class="headerlink" title="存储Volume"></a>存储Volume</h2><p>在 Kubernetes 中，Volume 是属于 Pod 对象的一部分</p><p> emptyDir 类型。我们之前讲过的 Docker 的隐式 Volume 参数，即：不显式声明宿主机目录的 Volume。所以，Kubernetes 也会在宿主机上创建一个临时目录，这个目录将来就会被绑定挂载到容器所声明的 Volume 目录上。Kubernetes 的 emptyDir 类型，只是把 Kubernetes 创建的临时目录作为 Volume 的宿主机目录，交给了 Docker。这么做的原因，是 Kubernetes 不想依赖 Docker 自己创建的那个 _data 目录。</p><p>hostPath类型。Kubernetes 也提供了显式的 Volume 定义</p><p>yaml定义一个版本号的变量，当版本发生变更，我只需要修改版本号变量，或者外部传参就行了。不希望频繁修改yaml文件： placeholder，或者yaml模板jinja </p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 深入剖析Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 深入剖析Kuernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nacos的部署和使用</title>
      <link href="/2020/09/07/nacos%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
      <url>/2020/09/07/nacos%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="nacos简述"><a href="#nacos简述" class="headerlink" title="nacos简述"></a>nacos简述</h2><p>阿里baba于2018年7月份发布Nacos</p><p>Nacos 支持基于 DNS 和基于 RPC 的服务发现。服务提供者使用 原生SDK、OpenAPI、或一个独立的Agent TODO注册 Service 后，服务消费者可以使用DNS TODO 或HTTP&amp;API查找和发现服务。</p><p>Nacos 提供对服务的实时的健康检查，阻止向不健康的主机或服务实例发送请求。Nacos 支持传输层 (PING 或 TCP)和应用层 (如 HTTP、MySQL、用户自定义）的健康检查。 对于复杂的云环境和网络拓扑环境中（如 VPC、边缘网络等）服务的健康检查，Nacos 提供了 agent 上报模式和服务端主动检测2种健康检查模式。Nacos 还提供了统一的健康检查仪表盘，帮助您根据健康状态管理服务的可用性及流量。</p><p>Nacos 提供了一个简洁易用的UI (控制台样例 Demo) 帮助您管理所有的服务和应用的配置。Nacos 还提供包括配置版本跟踪、金丝雀发布、一键回滚配置以及客户端配置更新状态跟踪在内的一系列开箱即用的配置管理特性，帮助您更安全地在生产环境中管理配置变更和降低配置变更带来的风险。</p><p>Nacos 无缝支持一些主流的开源生态，例如</p><ul><li><p>Spring Cloud</p></li><li><p>Apache Dubbo and Dubbo Mesh TODO</p></li><li><p>Kubernetes and CNCF TODO</p></li></ul><h3 id="nacos简介以及作为注册-配置中心与Eureka、apollo的选型比较"><a href="#nacos简介以及作为注册-配置中心与Eureka、apollo的选型比较" class="headerlink" title="nacos简介以及作为注册/配置中心与Eureka、apollo的选型比较"></a><a href="https://www.jianshu.com/p/afd7776a64c6" target="_blank" rel="noopener">nacos简介以及作为注册/配置中心与Eureka、apollo的选型比较</a></h3><p>结论：使用Nacos代替Eureka和apollo，主要理由为：</p><h4 id="相比与Eureka："><a href="#相比与Eureka：" class="headerlink" title="相比与Eureka："></a><strong>相比与Eureka：</strong></h4><p> (1)Nacos具备服务优雅上下线和流量管理（API+后台管理页面），而Eureka的后台页面仅供展示，需要使用api操作上下线且不具备流量管理功能。<br> (2)从部署来看，Nacos整合了注册中心、配置中心功能，把原来两套集群整合成一套，简化了部署维护<br> (3)从长远来看，Eureka开源工作已停止，后续不再有更新和维护，而Nacos在以后的版本会支持SpringCLoud+Kubernetes的组合，填补 2 者的鸿沟，在两套体系下可以采用同一套服务发现和配置管理的解决方案，这将大大的简化使用和维护的成本。同时来说,Nacos 计划实现 Service Mesh，是未来微服务的趋势<br> (4)从伸缩性和扩展性来看Nacos支持跨注册中心同步，而Eureka不支持，且在伸缩扩容方面，Nacos比Eureka更优（nacos支持大数量级的集群）。<br> (5)Nacos具有分组隔离功能，一套Nacos集群可以支撑多项目、多环境。</p><h4 id="相比于apollo"><a href="#相比于apollo" class="headerlink" title="相比于apollo"></a><strong>相比于apollo</strong></h4><p> (1) Nacos部署简化，Nacos整合了配置中心与注册中心功能，且部署相比apollo简单，方便管理和监控。<br> (2) apollo容器化较困难，Nacos有官网的镜像可以直接部署，总体来说，Nacos比apollo更符合KISS原则<br> (3)性能方面，Nacos读写tps比apollo稍强一些</p><h2 id="部署k8s-nacos"><a href="#部署k8s-nacos" class="headerlink" title="部署k8s nacos"></a>部署k8s nacos</h2><p>我这里选择nfs作为后端存储，更多相关知识，请查看 <a href="https://nacos.io/zh-cn/docs/what-is-nacos.html" target="_blank" rel="noopener"> nacos官网 </a> 和 <a href="https://github.com/nacos-group/nacos-k8s" target="_blank" rel="noopener"> nacos-k8s </a> 。</p><h3 id="下载镜像到本地"><a href="#下载镜像到本地" class="headerlink" title="下载镜像到本地"></a>下载镜像到本地</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull nacos&#x2F;nacos-server:1.3.2  # 2020年8月</span><br><span class="line">docker tag f3549150bc00 harbor.od.com&#x2F;public&#x2F;nacos-server:v1.3.2</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;nacos-server:v1.3.2</span><br><span class="line"></span><br><span class="line">docker pull nacos&#x2F;nacos-peer-finder-plugin:1.0</span><br><span class="line">docker tag 06bdf39835b1 harbor.od.com&#x2F;public&#x2F;nacos-peer-finder-plugin:v1.0</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;nacos-peer-finder-plugin:v1.0</span><br></pre></td></tr></table></figure><h3 id="创建资源配置清单"><a href="#创建资源配置清单" class="headerlink" title="创建资源配置清单"></a>创建资源配置清单</h3><blockquote><p> 请参照官网 <a href="https://nacos.io/zh-cn/docs/use-nacos-with-kubernetes.html" target="_blank" rel="noopener">use-nacos-with-kubernetes</a>, 我的k8s集群里已有storageClass nfs1</p></blockquote><h4 id="nacos-rbac-yaml"><a href="#nacos-rbac-yaml" class="headerlink" title="nacos-rbac.yaml"></a>nacos-rbac.yaml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; nacos&#x2F;rbac.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;persistentvolumes&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;persistentvolumeclaims&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;endpoints&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">- apiGroups: [&quot;storage.k8s.io&quot;]</span><br><span class="line">  resources: [&quot;storageclasses&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;events&quot;]</span><br><span class="line">  verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: run-nfs-client-provisioner</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line">  namespace: default</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">---</span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;endpoints&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">---</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line">  # replace with namespace where provisioner is deployed</span><br><span class="line">  namespace: default</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">NS&#x3D;$(kubectl config get-contexts|grep -e &quot;^\*&quot; |awk &#39;&#123;print $5&#125;&#39;)</span><br><span class="line">NAMESPACE&#x3D;$&#123;NS:-default&#125;</span><br><span class="line">sed -i&#39;&#39; &quot;s&#x2F;namespace:.*&#x2F;namespace: $NAMESPACE&#x2F;g&quot; nacos&#x2F;nacos-rbac.yaml</span><br></pre></td></tr></table></figure><h4 id="nacos-headless-yaml"><a href="#nacos-headless-yaml" class="headerlink" title="nacos-headless.yaml"></a>nacos-headless.yaml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; nacos&#x2F;headless.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nacos-headless</span><br><span class="line">  labels:</span><br><span class="line">    app: nacos</span><br><span class="line">  annotations:</span><br><span class="line">    service.alpha.kubernetes.io&#x2F;tolerate-unready-endpoints: &quot;true&quot;</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">    - port: 8848</span><br><span class="line">      name: server</span><br><span class="line">      targetPort: 8848</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: nacos</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="nacos-cm-yaml"><a href="#nacos-cm-yaml" class="headerlink" title="nacos-cm.yaml"></a>nacos-cm.yaml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; nacos&#x2F;cm.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: nacos-cm</span><br><span class="line">data:</span><br><span class="line">  mysql.host: &quot;mysql-service&quot;</span><br><span class="line">  mysql.port: &quot;3306&quot;</span><br><span class="line">  mysql.user: &quot;btc_user&quot;</span><br><span class="line">  mysql.password: &quot;PpEZqboiBp&quot;</span><br><span class="line">  mysql.db.name: &quot;nacos&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="nacos-ss-yaml"><a href="#nacos-ss-yaml" class="headerlink" title="nacos-ss.yaml"></a>nacos-ss.yaml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; nacos&#x2F;ss.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: nacos-ss</span><br><span class="line">spec:</span><br><span class="line">  serviceName: nacos-headless</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nacos</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nacos</span><br><span class="line">      annotations:</span><br><span class="line">        pod.alpha.kubernetes.io&#x2F;initialized: &quot;true&quot;</span><br><span class="line">    spec:</span><br><span class="line">      affinity:</span><br><span class="line">        podAntiAffinity:</span><br><span class="line">          requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">            - labelSelector:</span><br><span class="line">                matchExpressions:</span><br><span class="line">                  - key: &quot;app&quot;</span><br><span class="line">                    operator: In</span><br><span class="line">                    values:</span><br><span class="line">                      - nacos</span><br><span class="line">              topologyKey: &quot;kubernetes.io&#x2F;hostname&quot;</span><br><span class="line">      # serviceAccountName: nfs-client-provisioner</span><br><span class="line">      initContainers:</span><br><span class="line">        - name: peer-finder-plugin-install</span><br><span class="line">          image: harbor.od.com&#x2F;public&#x2F;nacos-peer-finder-plugin:v1.0</span><br><span class="line">          imagePullPolicy: Always</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - name: plugindir</span><br><span class="line">              mountPath: &quot;&#x2F;home&#x2F;nacos&#x2F;plugins&#x2F;peer-finder&quot;</span><br><span class="line">      containers:</span><br><span class="line">        - name: nacos</span><br><span class="line">          imagePullPolicy: Always</span><br><span class="line">          image: harbor.od.com&#x2F;public&#x2F;nacos-server:v1.3.2</span><br><span class="line">          resources:</span><br><span class="line">            requests:</span><br><span class="line">              memory: &quot;2Gi&quot;</span><br><span class="line">              cpu: &quot;500m&quot;</span><br><span class="line">            limits:</span><br><span class="line">              memory: &quot;2Gi&quot;</span><br><span class="line">              cpu: &quot;500m&quot;</span><br><span class="line">          ports:</span><br><span class="line">            - containerPort: 8848</span><br><span class="line">              name: client-port</span><br><span class="line">          env:</span><br><span class="line">            - name: NACOS_REPLICAS</span><br><span class="line">              value: &quot;3&quot;</span><br><span class="line">            - name: SERVICE_NAME</span><br><span class="line">              value: &quot;nacos-headless&quot;</span><br><span class="line">            - name: DOMAIN_NAME</span><br><span class="line">              value: &quot;cluster.local&quot;</span><br><span class="line">            - name: POD_NAMESPACE</span><br><span class="line">              valueFrom:</span><br><span class="line">                fieldRef:</span><br><span class="line">                  apiVersion: v1</span><br><span class="line">                  fieldPath: metadata.namespace</span><br><span class="line">            - name: MYSQL_SERVICE_DB_NAME</span><br><span class="line">              valueFrom:</span><br><span class="line">                configMapKeyRef:</span><br><span class="line">                  name: nacos-cm</span><br><span class="line">                  key: mysql.db.name</span><br><span class="line">            - name: MYSQL_SERVICE_HOST</span><br><span class="line">              valueFrom:</span><br><span class="line">                configMapKeyRef:</span><br><span class="line">                  name: nacos-cm</span><br><span class="line">                  key: mysql.host</span><br><span class="line">            - name: MYSQL_SERVICE_PORT</span><br><span class="line">              valueFrom:</span><br><span class="line">                configMapKeyRef:</span><br><span class="line">                  name: nacos-cm</span><br><span class="line">                  key: mysql.port</span><br><span class="line">            - name: MYSQL_SERVICE_USER</span><br><span class="line">              valueFrom:</span><br><span class="line">                configMapKeyRef:</span><br><span class="line">                  name: nacos-cm</span><br><span class="line">                  key: mysql.user</span><br><span class="line">            - name: MYSQL_SERVICE_PASSWORD</span><br><span class="line">              valueFrom:</span><br><span class="line">                configMapKeyRef:</span><br><span class="line">                  name: nacos-cm</span><br><span class="line">                  key: mysql.password</span><br><span class="line">            - name: NACOS_SERVER_PORT</span><br><span class="line">              value: &quot;8848&quot;</span><br><span class="line">            - name: NACOS_APPLICATION_PORT</span><br><span class="line">              value: &quot;8848&quot;</span><br><span class="line">            - name: PREFER_HOST_MODE</span><br><span class="line">              value: &quot;hostname&quot;</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - name: plugindir</span><br><span class="line">              mountPath: &#x2F;home&#x2F;nacos&#x2F;plugins&#x2F;peer-finder</span><br><span class="line">            - name: datadir</span><br><span class="line">              mountPath: &#x2F;home&#x2F;nacos&#x2F;data</span><br><span class="line">            - name: logdir</span><br><span class="line">              mountPath: &#x2F;home&#x2F;nacos&#x2F;logs</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">    - metadata:</span><br><span class="line">        name: plugindir</span><br><span class="line">        annotations:</span><br><span class="line">          volume.beta.kubernetes.io&#x2F;storage-class: &quot;nfs1&quot;</span><br><span class="line">      spec:</span><br><span class="line">        accessModes: [ &quot;ReadWriteMany&quot; ]</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            storage: 5Gi</span><br><span class="line">    - metadata:</span><br><span class="line">        name: datadir</span><br><span class="line">        annotations:</span><br><span class="line">          volume.beta.kubernetes.io&#x2F;storage-class: &quot;nfs1&quot;</span><br><span class="line">      spec:</span><br><span class="line">        accessModes: [ &quot;ReadWriteMany&quot; ]</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            storage: 5Gi</span><br><span class="line">    - metadata:</span><br><span class="line">        name: logdir</span><br><span class="line">        annotations:</span><br><span class="line">          volume.beta.kubernetes.io&#x2F;storage-class: &quot;nfs1&quot;</span><br><span class="line">      spec:</span><br><span class="line">        accessModes: [ &quot;ReadWriteMany&quot; ]</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            storage: 5Gi</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="svc-yaml"><a href="#svc-yaml" class="headerlink" title="svc.yaml"></a>svc.yaml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; nacos&#x2F;svc.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nacos-service</span><br><span class="line">  annotations:</span><br><span class="line">    nginx.ingress.kubernetes.io&#x2F;affinity: &quot;true&quot;</span><br><span class="line">    nginx.ingress.kubernetes.io&#x2F;session-cookie-name: backend</span><br><span class="line">    nginx.ingress.kubernetes.io&#x2F;load-balancer-method: drr</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: nacos</span><br><span class="line">  ports:</span><br><span class="line">  - name: web</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 8848</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="Ingress-yaml"><a href="#Ingress-yaml" class="headerlink" title="Ingress.yaml"></a>Ingress.yaml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; nacos&#x2F;ingress.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: nacos-web</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: nacos.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: nacos-service</span><br><span class="line">          servicePort: web</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="执行资源配置清单"><a href="#执行资源配置清单" class="headerlink" title="执行资源配置清单"></a>执行资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f nacos&#x2F;rbac.yaml</span><br><span class="line">kubectl apply -f nacos&#x2F;headless.yaml</span><br><span class="line">kubectl apply -f nacos&#x2F;cm.yaml</span><br><span class="line">kubectl apply -f nacos&#x2F;ss.yaml</span><br><span class="line">kubectl apply -f nacos&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f nacos&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p>验证Nacos节点启动成功</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# kubectl get pod -l app&#x3D;nacos</span><br><span class="line"></span><br><span class="line">NAME      READY   STATUS    RESTARTS   AGE</span><br><span class="line">nacos-0   1&#x2F;1     Running   0          19h</span><br><span class="line">nacos-1   1&#x2F;1     Running   0          19h</span><br><span class="line">nacos-2   1&#x2F;1     Running   0          19h</span><br></pre></td></tr></table></figure><h3 id="通过控制台访问"><a href="#通过控制台访问" class="headerlink" title="通过控制台访问"></a>通过控制台访问</h3><p>访问链接：<a href="http://nacos.od.com/nacos/index.html" target="_blank" rel="noopener">http://nacos.od.com/nacos/index.html</a> ，默认用户名和密码：nacos : nacos</p><img src="/img/body/4.nacos%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BD%BF%E7%94%A8/image-20200908145249261.png" alt="image-20200908145249261" style="zoom:70%;max-width:70%" />]]></content>
      
      
      <categories>
          
          <category> kubernets </category>
          
          <category> nacos </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nacos </tag>
            
            <tag> kubernets </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>05-09.容器技术概念入门篇</title>
      <link href="/2020/09/06/05-09-%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8%E7%AF%87/"/>
      <url>/2020/09/06/05-09-%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8%E7%AF%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p> <a href="https://time.geekbang.org/column/article/14642" target="_blank" rel="noopener">深入剖析Kubernetes - 05</a></p></blockquote><h1 id="05-白话容器基础（一）：从进程说开去"><a href="#05-白话容器基础（一）：从进程说开去" class="headerlink" title="05|白话容器基础（一）：从进程说开去"></a>05|白话容器基础（一）：从进程说开去</h1><p>容器是一种沙盒，容器技术的核心功能，就是通过约束和修改进程的动态表现，为应用与应用之间创造出“边界”。</p><p>对于 Docker 等 Linux 容器来说，Cgroups 技术是用来制造约束的主要手段，而 Namespace 技术则是用来修改进程视图的主要方法。</p><p>Namespace 其实只是 Linux 创建新进程的一个可选参数。</p><p>Linux 操作系统提供了 PID（） ， Mount（文件系统）、UTS（时间）、IPC(进程间通信)、Network （网络）和 User （用户）这些 Namespace</p><p>进程包括计算机内存中的数据、寄存器里的值、堆栈中的指令、被打开的文件，以及各种设备的状态信息的一个集合，是运行起来后的计算机执行环境的总和，</p><p>进程是动态的程序，程序是静态的进程</p><p>容器是一个单进程。单进程意思不是只能运行一个进程，而是只有一个进程是可控的；控制指的是它们的回收和生命周期管理。</p><h2 id="虚拟机和容器"><a href="#虚拟机和容器" class="headerlink" title="虚拟机和容器"></a>虚拟机和容器</h2><img src="/img/body/jike/docker-hypervisor.png" alt="虚拟机和容器对比图" style="zoom:50%;max-width:60%" /><p>Hypervisor 是虚拟机通过硬件虚拟化功能，模拟出了运行一个操作系统需要的各种硬件，比如 CPU、内存、I/O 设备等等。然后，它在这些虚拟的硬件上安装了一个新的操作系统，即 Guest OS，但它带来了额外的资源消耗和占用。</p><p>使用 Docker 的时候，并没有一个真正的“Docker 容器”运行在宿主机里面。Docker 项目帮助用户启动的应用进程，只不过是在创建这些进程时， 为它们加上了各种各样的 Namespace 参数，真实进程是直接run在host os上。</p><p>“敏捷”和“高性能”是容器相较于虚拟机最大的优势，也是它能够在 PaaS 这种更细粒度的资源管理平台上大行其道的重要原因。</p><h1 id="06-白话容器基础（二）：隔离和限制"><a href="#06-白话容器基础（二）：隔离和限制" class="headerlink" title="06|白话容器基础（二）：隔离和限制"></a>06|白话容器基础（二）：隔离和限制</h1><p>容器是一个“单进程”模型，这是因为容器本身的设计，就是希望容器和应用能够同生命周期，方便对进程进行控制。</p><p>容器的资源隔离（进程视图）是通过linux  namespace实现（进程启动时指定namespace参数），容器的资源限制（cpu，memory大小等）是通过linux  cgroups实现（通过修改/proc目录下对应容器的文件信息）</p><p>容器的隔离只是相对于容器进程本身做了隔离，宿主机操作系统还是可以看到对应容器进程的真实的进程信息。容器的本质是一种特殊的进程，这些被“隔离”了的进程跟其他进程并没有太大区别。</p><p>相较于虚拟机，容器通过共享宿主机的os内核从而更加的轻量级化，性能更高，但同时也使容器的隔离变的不彻底。同时，对于系统时间是不可以通过namespace进行隔离的。</p><h2 id="隔离得不彻底"><a href="#隔离得不彻底" class="headerlink" title="隔离得不彻底"></a>隔离得不彻底</h2><p>首先，既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。</p><p>其次，在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：时间。</p><p>top 指令查看系统信息的主要数据来源是/proc目录，  /proc 目录存储的是记录当前内核运行状态的一系列特殊文件</p><h2 id="容器的“限制”问题。"><a href="#容器的“限制”问题。" class="headerlink" title="容器的“限制”问题。"></a>容器的“限制”问题。</h2><p>Linux Cgroups 的全称是 Linux Control Group。</p><p>Linux Cgroups 是 Linux 内核中用来为进程设置资源限制的，限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。</p><p>Linux Cgroups 的设计就是一个子系统目录加上一组资源限制文件的组合。</p><p>Cgroups  的每一个子系统都有其独有的资源限制能力，比如：</p><ul><li><p>cpu, 为进程分配cpu资源</p></li><li><p>blkio，为块设备设定I/O  限制，一般用于磁盘等设备；</p></li><li><p>cpuset，为进程分配单独的 CPU 核和对应的内存节点；</p></li><li><p>memory，为进程设定内存使用的限制。</p></li></ul><p>在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下。用mount -t cgroup 指令把它们展示出来</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ mount -t cgroup </span><br><span class="line">cpuset on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)</span><br><span class="line">cpu on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu type cgroup (rw,nosuid,nodev,noexec,relatime,cpu)</span><br><span class="line">cpuacct on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct)</span><br><span class="line">blkio on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)</span><br><span class="line">memory on &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>在 /sys/fs/cgroup 下面有很多 cpuset、cpu、 memory 等目录，而在子系统对应的资源种类下，你就可以看到该类资源具体可以被限制的方法。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ls &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu</span><br><span class="line">cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release</span><br><span class="line">cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks</span><br></pre></td></tr></table></figure><h2 id="利用cfs-period-和-cfs-quota控制CPU"><a href="#利用cfs-period-和-cfs-quota控制CPU" class="headerlink" title="利用cfs_period 和 cfs_quota控制CPU"></a>利用cfs_period 和 cfs_quota控制CPU</h2><p>cfs_period 和 cfs_quota组合使用，可以用来限制进程在长度为 cfs_period 的一段时间内，只能被分配到总量为 cfs_quota 的 CPU 时间。</p><p>在对应的子系统下面创建一个目录</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu$ mkdir container</span><br><span class="line">root@ubuntu:&#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu$ ls container&#x2F;</span><br><span class="line">cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release</span><br><span class="line">cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks</span><br></pre></td></tr></table></figure><p>操作系统会在你新创建的 container 目录下，自动生成该子系统对应的资源限制文件。</p><p>执行一个进程，它是一个死循环，可以把计算机的 CPU 吃到 100%，根据它的输出，我们可以看到这个脚本在后台运行的进程号（PID）是 226。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ while : ; do : ; done &amp;</span><br><span class="line">[1] 226</span><br></pre></td></tr></table></figure><p>我们可以用top确认CPU利用率</p><p>我们查看 container 目录下的文件，看到 container 控制组里的 CPU quota 还没有任何限制（即：-1），CPU period 则是默认的 100  ms（100000  us）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cat &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu&#x2F;container&#x2F;cpu.cfs_quota_us </span><br><span class="line">-1</span><br><span class="line">$ cat &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu&#x2F;container&#x2F;cpu.cfs_period_us </span><br><span class="line">100000</span><br></pre></td></tr></table></figure><p>通过修改这些文件的内容来设置限制。比如，向 container 组里的 cfs_quota 文件写入 20  ms（20000  us）：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo 20000 &gt; &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu&#x2F;container&#x2F;cpu.cfs_quota_us</span><br></pre></td></tr></table></figure><p>我们把被限制的进程的 PID 写入 container 组里的 tasks 文件，上面的设置就会对该进程生效了：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ echo 226 &gt; &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu&#x2F;container&#x2F;tasks</span><br></pre></td></tr></table></figure><p> top 指令查看查看结果</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ top</span><br><span class="line">%Cpu0 : 20.3 us, 0.0 sy, 0.0 ni, 79.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st</span><br></pre></td></tr></table></figure><h1 id="07-白话容器基础（三）：深入理解容器镜像"><a href="#07-白话容器基础（三）：深入理解容器镜像" class="headerlink" title="07 | 白话容器基础（三）：深入理解容器镜像"></a>07 | 白话容器基础（三）：深入理解容器镜像</h1><p>容器里的进程看到的文件系统是，Mount Namespace 修改的，容器进程对文件系统“挂载点”的认知。</p><p>Mount Namespace 跟其他 Namespace 的使用略有不同的地方：它对容器进程视图的改变，一定是伴随着挂载操作（mount）才能生效。</p><p>在 Linux 操作系统里，有一个名为 chroot（change root file system） 的命令可以改变进程的根目录到你指定的位置。</p><p>Mount Namespace 正是基于对 chroot 的不断改良才被发明出来的，它也是 Linux 操作系统里的第一个 Namespace。</p><p>挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”; 它的本质就是容器的根文件系统(rootfs)。</p><p>对 Docker 项目来说，它最核心的原理实际上就是为待创建的用户进程：</p><ol><li><p>启用 Linux Namespace 配置 : 视图隔离；</p></li><li><p>设置指定的 Cgroups 参数 : 资源限制 ;</p></li><li><p>切换进程的根目录（Change Root）;容器镜像生效，以实现环境一致性。</p></li></ol><p>rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。</p><p>由于 rootfs 里打包的不只是应用，而是整个操作系统的文件和目录，所以应用以及它运行所需要的所有依赖，都被封装在了一起。所以容器才有了一个重要特性：一致性。</p><p>对一个应用来说，操作系统本身才是它运行所需要的最完整的“依赖库”。</p><h2 id="容器镜像的分层"><a href="#容器镜像的分层" class="headerlink" title="容器镜像的分层"></a>容器镜像的分层</h2><p>Docker 在镜像的设计中，引入了层（layer）的概念。用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs。</p><p>UnionFS（Union File System）最主要的功能是将多个不同位置的目录联合挂载（union mount）到同一个目录下。</p><p>现在docker默认用 overlay2 作为 storage driver 主要是性能更好。而且 OverlayFS 已经进入 Linux 内核主线，而 AUFS 被拒。</p><p>“镜像”的三层：第一部分，只读层；第二部分，可读写层；第三部分，Init 层。</p><p>Init 层是一个以“-init”结尾的层，夹在只读层和读写层之间。Init 层是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等初始化时产生的信息。用户执行 docker commit 只会提交可读写层，不包含init层</p><h2 id="写时复制"><a href="#写时复制" class="headerlink" title="写时复制"></a>写时复制</h2><p>Copy-On-Write机制简称COW，是程序开发中比较常用的一种优化策略。在复制一个对象时,并不是真正的把原先的对象复制到内存的另外一个位置上，而是在新对象的内存映射表中设置一个指针，指向源对象的位置，并把那块内存的Copy-On-Write位设置为1，这是一种延时懒惰策略。<br>对于容器，我们从宿主机单独挂载了一个文件到容器里，然后修改宿主机的文件，容器里的文件却没生效。原因是，编辑过程中将变更写入新文件， 保存时再将备份文件替换原文件，此时会导致文件的inode发生变化。而docker  挂载文件时，并不是挂载了某个文件的路径，而是实打实的挂载了对应的文件，即<strong>挂载了inode文件</strong>。</p><h2 id="小知识点"><a href="#小知识点" class="headerlink" title="小知识点"></a>小知识点</h2><p>overlay共享数据方式是通过硬连接，而overlay2是通过每层的 lower文件。</p><p>katacontainers 基于虚拟化的容器是有独立内核的。</p><p>scratch 是万能的基础镜像。</p><p>kubernetes打开GC功能会定时清理镜像（默认开启）。</p><h2 id="知识扩展"><a href="#知识扩展" class="headerlink" title="知识扩展"></a>知识扩展</h2><ol><li><p>linux系统启动过程详解-开机加电后发生了什么 –linux内核剖析（零） <a href="https://blog.csdn.net/gatieme/article/details/50914250" target="_blank" rel="noopener">https://blog.csdn.net/gatieme/article/details/50914250</a></p></li><li><p>左耳朵耗子大师的 DOCKER基础技术：LINUX NAMESPACE 上下两篇，再去看官档关于overlay2的运行原理，还有linux mount技术的相关文章   <a href="https://coolshell.cn/articles/17010.html" target="_blank" rel="noopener">https://coolshell.cn/articles/17010.html</a></p></li></ol><h1 id="08-白话容器基础（四）：重新认识Docker容器"><a href="#08-白话容器基础（四）：重新认识Docker容器" class="headerlink" title="08 | 白话容器基础（四）：重新认识Docker容器"></a>08 | 白话容器基础（四）：重新认识Docker容器</h1><p>因为其他地方关于docker的基础命令和dockerfile参数已经很详细，这里我就不做过多记录。</p><p>Dockerfile 的设计思想，是使用一些标准的原语，描述我们所要构建的 Docker 镜像。并且这些原语，都是按顺序处理的。</p><p>Dockerfile 中的每个原语执行后，都会生成一个对应的镜像层。即使原语本身并没有明显地修改文件的操作（比如，ENV 原语），它对应的层也会存在。只不过在外界看来，这个层是空的。</p><p>docker commit，实际上就是在容器运行起来后，把最上层的“可读写层”，加上原先容器镜像的只读层，打包组成了一个新的镜像。当然下面这些只读层在宿主机上是共享的，不会占用额外的空间。</p><h2 id="Volume挂载"><a href="#Volume挂载" class="headerlink" title="Volume挂载"></a>Volume挂载</h2><p>Volume挂载时机:  在 rootfs 准备好之后，在执行 chroot 之前，把 Volume 指定的宿主机目录（比如 /home 目录），挂载到指定的容器目录（比如 /test 目录）在宿主机上对应的目录（即 /var/lib/docker/aufs/mnt/[可读写层 ID]/test）上，这个 Volume 的挂载工作就完成了</p><p>执行这个挂载操作时，“容器进程”已经创建了，也就意味着此时 Mount Namespace 已经开启了。所以，这个挂载事件只在这个容器里可见。你在宿主机上，是看不见容器内部的这个挂载点的。这就保证了容器的隔离性不会被 Volume 打破。</p><p>注意：这里提到的”容器进程”，是 Docker 创建的一个容器初始化进程 (dockerinit)，而不是应用进程 (ENTRYPOINT + CMD)。dockerinit 会负责完成根目录的准备、挂载设备和目录、配置 hostname 等一系列需要在容器内进行的初始化操作。最后，它通过 execv() 系统调用，让应用进程取代自己，成为容器里的 PID=1 的进程。注意，容器里的其他进程都是1号进程的子进程，</p><p> Linux 的绑定挂载（bind mount）机制。它的主要作用就是，允许你将一个目录或者文件，而不是整个设备，挂载到一个指定的目录上。并且，这时你在该挂载点上进行的任何操作，只是发生在被挂载的目录或者文件上，而原挂载点的内容则会被隐藏起来且不受影响。</p><p>Linux 绑定挂载实际上是一个 inode 替换的过程。在 Linux 操作系统中，inode 可以理解为存放文件内容的“对象”，而 dentry，也叫目录项，就是访问这个 inode 所使用的“指针”。</p><img src="/img/body/jike/95c957b3c2813bb70eb784b8d1daedc6-20200905173354769.png" alt="img" style="zoom:67%;max-width:60%" /><p>所以，进行一次绑定挂载，Docker 就可以将一个宿主机上的目录或文件，不动声色地挂载到容器中。</p><p>容器的镜像操作，比如 docker commit，都是发生在宿主机空间的。而由于 Mount Namespace 的隔离作用，宿主机并不知道这个绑定挂载的存在。所以，在宿主机看来，容器中可读写层的 /test 目录（/var/lib/docker/aufs/mnt/[可读写层 ID]/test），始终是空的。所以容器 Volume 里的信息，并不会被 docker commit 提交掉；但这个挂载点目录 /test 本身，则会出现在新的镜像当中。</p><p>docker exec：i.e. spawn a new process in existing namespace。（在已经存在的命名空间生成一个新的进程） </p><p>所有的层都保存在diff目录下，包括可读写层修改的数据</p><h2 id="本章的主要问题（没答案）"><a href="#本章的主要问题（没答案）" class="headerlink" title="本章的主要问题（没答案）"></a>本章的主要问题（没答案）</h2><p><strong>从问题出发，根据问题理解答案</strong></p><p>一、docker镜像如何制作的两种方式是什么？<br>二、容器既然是一个封闭的进程，那么外接程序是如何进入容器这个进程的呢？<br>三、docker commit对挂载点volume内容修改的影响是什么？<br>四、容器与宿主机如何进行文件读写？或volume是为了解决什么题？<br>五、Docker的copyData功能是什么？解决了什么问题？<br>六、bind mount机制是什么？<br>七、cgroup Namespace的作用是什么？</p><h2 id="Linux-Namespace-工作原理"><a href="#Linux-Namespace-工作原理" class="headerlink" title="Linux Namespace 工作原理"></a>Linux Namespace 工作原理</h2><p><strong>模拟进去其他进程的网络空间理解Linux Namespace 工作原理</strong></p><p>通过操作系统进程相关的知识，逐步剖析 Docker 容器的方法，是理解容器的一个关键思路，希望你一定要掌握。</p><p>通过如下指令，你可以看到当前正在运行的 Docker 容器的进程号（PID）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 ~]# docker inspect --format &#39;&#123;&#123; .State.Pid &#125;&#125;&#39; d45c14ee5303</span><br><span class="line">18814</span><br><span class="line">[root@wang-18 ~]# ls -l &#x2F;proc&#x2F;18814&#x2F;ns</span><br><span class="line">总用量 0</span><br><span class="line">lrwxrwxrwx. 1 root root 0 9月   3 18:57 ipc -&gt; ipc:[4026532561]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 9月   3 18:57 mnt -&gt; mnt:[4026532559]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 9月   2 18:24 net -&gt; net:[4026532564]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 9月   3 18:57 pid -&gt; pid:[4026532562]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 9月   5 17:11 user -&gt; user:[4026531837]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 9月   3 18:57 uts -&gt; uts:[4026532560]</span><br></pre></td></tr></table></figure><p>一个进程，可以选择加入到某个进程已有的 Namespace 当中，从而达到“进入”这个进程所在容器的目的，这正是 docker exec 的实现原理。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 test]# cat &lt;&lt; EOF &gt; set_ns.c</span><br><span class="line"></span><br><span class="line">#define _GNU_SOURCE</span><br><span class="line">#include &lt;fcntl.h&gt;</span><br><span class="line">#include &lt;sched.h&gt;</span><br><span class="line">#include &lt;unistd.h&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">#define errExit(msg) do &#123; perror(msg); exit(EXIT_FAILURE);&#125; while (0)</span><br><span class="line"></span><br><span class="line">int main(int argc, char *argv[]) &#123;</span><br><span class="line">    int fd;</span><br><span class="line">    </span><br><span class="line">    fd &#x3D; open(argv[1], O_RDONLY);</span><br><span class="line">    if (setns(fd, 0) &#x3D;&#x3D; -1) &#123;</span><br><span class="line">        errExit(&quot;setns&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    execvp(argv[2], &amp;argv[2]); </span><br><span class="line">    errExit(&quot;execvp&quot;);</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">[root@wang-18 test]# gcc -o set_ns set_ns.c </span><br><span class="line">[root@wang-18 test]# .&#x2F;set_ns &#x2F;proc&#x2F;18814&#x2F;ns&#x2F;net &#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">[root@wang-18 test]# ifconfig</span><br><span class="line">eth0: flags&#x3D;4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 172.16.18.3  netmask 255.255.255.0  broadcast 172.16.18.255</span><br><span class="line">        ether 02:42:ac:10:12:03  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 8  bytes 656 (656.0 B)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 0  bytes 0 (0.0 B)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure><p>在 /proc/[PID]/ns/net 目录下，这个 PID=56900 进程，与我们前面的 Docker 容器进程（PID=18814）指向的 Network Namespace 文件完全一样。这说明这两个进程，共享了这个名叫 net:[4026532564]的 Network Namespace。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-18 test]# ps -ef | grep &#x2F;bin&#x2F;bash | grep -v grep</span><br><span class="line">root      18814  18795  0 9月02 pts&#x2F;0   00:00:00 &#x2F;bin&#x2F;bash</span><br><span class="line">root      52675  18795  0 9月03 ?       00:00:00 &#x2F;bin&#x2F;bash</span><br><span class="line">root      56900  56736  0 17:19 pts&#x2F;0    00:00:00 &#x2F;bin&#x2F;bash</span><br><span class="line"></span><br><span class="line">[root@wang-18 test]# ls -l &#x2F;proc&#x2F;18814&#x2F;ns&#x2F;net &#x2F;proc&#x2F;56900&#x2F;ns&#x2F;net</span><br><span class="line">lrwxrwxrwx. 1 root root 0 9月   2 18:24 &#x2F;proc&#x2F;18814&#x2F;ns&#x2F;net -&gt; net:[4026532564]</span><br><span class="line">lrwxrwxrwx. 1 root root 0 9月   5 17:21 &#x2F;proc&#x2F;56900&#x2F;ns&#x2F;net -&gt; net:[4026532564]</span><br></pre></td></tr></table></figure><h1 id="09-从容器到容器云：谈谈Kubernetes的本质"><a href="#09-从容器到容器云：谈谈Kubernetes的本质" class="headerlink" title="09 | 从容器到容器云：谈谈Kubernetes的本质"></a>09 | 从容器到容器云：谈谈Kubernetes的本质</h1><h2 id="容器概述"><a href="#容器概述" class="headerlink" title="容器概述"></a>容器概述</h2><p>“容器” 实际上是一个由 Linux Namespace、Linux Cgroups 和 rootfs 三种技术构建出来的进程的隔离环境。</p><p>一组联合挂载在 /var/lib/docker/aufs/mnt 上的 rootfs，这一部分我们称为“容器镜像”（Container Image），是容器的静态视图；</p><p>一个由 Namespace+Cgroups 构成的隔离环境，这一部分我们称为“容器运行时”（Container Runtime），是容器的动态视图。</p><p>首先，Kubernetes 项目要解决的问题是什么？编排, 调度, 容器云,  集群管理等等</p><h2 id="Kubernetes架构"><a href="#Kubernetes架构" class="headerlink" title="Kubernetes架构"></a>Kubernetes架构</h2><p>Kubernetes 项目的架构，由 Master控制节点 和 Node 计算节点两种节点组成。跟它的原型项目 Borg 类似。</p><p>Master  控制节点，由三个紧密协作的独立组件组合而成，它们分别是负责 API 服务的 kube-apiserver、负责调度的  kube-scheduler，以及负责容器编排的 kube-controller-manager。整个集群的持久化数据，则由  kube-apiserver 处理后保存在 Etcd 中。</p><p>Master 节点：编排、管理、调度用户提交的作业。</p><img src="/img/body/jike/8ee9f2fa987eccb490cfaa91c6484f67.png" alt="img" style="zoom:67%;max-width:70%" /><p>  CNI（Container Networking Interface），CRI（Container Runtime  Interface），CSI（Container Storage Interface）, OCI (Open Container Initiative)</p><p>node 计算节点上最核心的部分 kubelet 主要负责同容器运行时（比如 Docker 项目）打交道。而这个交互所依赖的，是一个称作 CRI 的远程调用接口，这个接口定义了容器运行时的各项核心操作，比如：启动一个容器需要的所有参数。只要你的这个容器运行时能够运行标准的容器镜像，它就可以通过实现 CRI 接入到 Kubernetes 项目当中。</p><p>而具体的容器运行时，比如 Docker 项目，则一般通过 OCI 这个容器运行时规范同底层的 Linux 操作系统进行交互，即：把 CRI 请求翻译成对 Linux 操作系统的调用（操作 Linux Namespace 和 Cgroups 等）</p><p>kubelet 还通过 gRPC 协议同一个叫作 Device Plugin 的插件进行交互。这个插件，是 Kubernetes 项目用来管理 GPU 等宿主机物理设备的主要组件，也是基于  Kubernetes 项目进行机器学习训练、高性能作业支持等工作必须关注的功能。</p><p>kubelet  的另一个重要功能，则是调用网络插件和存储插件为容器配置网络和持久化存储。这两个插件与 kubelet 进行交互的接口，分别是  CNI 和 CSI 。</p><h2 id="Kubernetes设计思想"><a href="#Kubernetes设计思想" class="headerlink" title="Kubernetes设计思想"></a>Kubernetes设计思想</h2><p>Kubernetes 项目就没有像同时期的各种“容器云”项目那样，把 Docker 作为整个架构的核心，而仅仅把它作为最底层的一个容器运行时实现。</p><p>k8s着重解决的问题: 运行在大规模集群中的各种任务之间，实际上存在着各种各样的关系。这些关系的处理，才是作业编排和管理系统最困难的地方。</p><p>那些原先拥挤在同一个虚拟机里的各个应用、组件、守护进程，都可以被分别做成镜像，然后运行在一个个专属的容器中。它们之间互不干涉，拥有各自的资源配额，可以被调度在整个集群里的任何一台机器上。而这，正是一个 PaaS 系统最理想的工作状态，也是所谓“微服务”思想得以落地的先决条件。</p><p>Kubernetes 项目最主要的设计思想是，从更宏观的角度，以统一的方式来定义任务之间的各种关系，并且为将来支持更多种类的关系留有余地。</p><img src="/img/body/jike/16c095d6efb8d8c226ad9b098689f306.png" alt="img" style="zoom:67%;max-width: 70%" /><p>按照这幅图的线索，我们从容器这个最基础的概念出发，首先遇到了容器间“紧密协作”关系的难题，于是就扩展到了 Pod；有了 Pod 之后，我们希望能一次启动多个应用的实例，这样就需要 Deployment 这个 Pod  的多实例管理器；而有了这样一组相同的 Pod 后，我们又需要通过一个固定的 IP 地址和端口以负载均衡的方式访问它，于是就有了 Service。</p><p>Service 服务: 作为 Pod 的代理入口（Portal），代替 Pod 对外暴露一个固定的网络地址。</p><h2 id="如何容器化应用"><a href="#如何容器化应用" class="headerlink" title="如何容器化应用"></a>如何容器化应用</h2><p>除了应用与应用之间的关系外，应用运行的形态是影响“如何容器化这个应用”的第二个重要因素。</p><p>在 Kubernetes 项目中，我们所推崇的使用方法是：</p><p>首先，通过一个“编排对象”，比如 Pod、Job、CronJob 等，来描述你试图管理的应用；</p><p>然后，再为它定义一些“服务对象”，比如 Service、Secret、Horizontal Pod Autoscaler（自动水平扩展器）等。这些对象，会负责具体的平台级功能。</p><p>这种使用方法，就是所谓的“声明式 API”。这种 API 对应的“编排对象”和“服务对象”，都是 Kubernetes 项目中的 API 对象（API Object）。</p><p>API（Application Programming Interface,应用程序编程接口）</p><h2 id="编排和调度区别"><a href="#编排和调度区别" class="headerlink" title="编排和调度区别"></a>编排和调度区别</h2><p>过去很多的集群管理项目（比如  Yarn、Mesos，以及 Swarm）所擅长的，都是把一个容器，按照某种规则，放置在某个最佳节点上运行起来。这种功能，我们称为“调度”。</p><p>而  Kubernetes  项目所擅长的，是按照用户的意愿和整个系统的规则，完全自动化地处理好容器之间的各种关系。这种功能，就是我们经常听到的一个概念：编排。</p><p>所以说，Kubernetes 项目的本质，是为用户提供一个具有普遍意义的容器编排工具。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 深入剖析Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 深入剖析Kuernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>00-04.深入剖析k8s课前必读</title>
      <link href="/2020/09/01/00-04-%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90k8s%E8%AF%BE%E5%89%8D%E5%BF%85%E8%AF%BB/"/>
      <url>/2020/09/01/00-04-%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90k8s%E8%AF%BE%E5%89%8D%E5%BF%85%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="文章说明"><a href="#文章说明" class="headerlink" title="文章说明"></a>文章说明</h2><p>极客时间课程<a href="https://time.geekbang.org/column/intro/116" target="_blank" rel="noopener">深入剖析Kuernetes</a>是 张磊老师讲的，以前用我同事的账号看过一遍，觉得特别好。所以我自己又买了一次，进行二刷。</p><h2 id="开篇：打通“容器技术”的任督二脉"><a href="#开篇：打通“容器技术”的任督二脉" class="headerlink" title="开篇：打通“容器技术”的任督二脉"></a>开篇：打通“容器技术”的任督二脉</h2><p>开发运维环境从过去以物理机和虚拟机为主体，向以容器为核心的基础设施的转变过程，并不是一次温和（简单）的改革，而是涵盖了对网络、存储、调度、操作系统、分布式原理等各个方面的容器化理解和改造。</p><p>这些关于 Linux 内核、分布式系统、网络、存储等方方面面的积累，并不会在 Docker 或者 Kubernetes 的文档中交代清楚。可偏偏就是它们，才是真正掌握容器技术体系的精髓所在，是每一位技术从业者需要悉心修炼的“内功”。</p><p>Google 公司 Kubernetes 项目的首席布道师Kelsey的故事：<em>一个人的命运，当然要靠自我奋斗，但是也要考虑到历史的行程。</em>所以我们要追逐潮流，学习k8s。</p><h2 id="01-小鲸鱼大事件（一）"><a href="#01-小鲸鱼大事件（一）" class="headerlink" title="01|小鲸鱼大事件（一）"></a>01|小鲸鱼大事件（一）</h2><p>PaaS 项目，最核心的组件就是一套应用的打包和分发机制。</p><p>Docker 项目其实是提供了一种非常便利的打包机制-镜像，这种机制直接打包了应用运行所需要的整个操作系统，从而保证了本地环境和云端环境的高度一致！</p><h2 id="02-小鲸鱼大事件（二）"><a href="#02-小鲸鱼大事件（二）" class="headerlink" title="02|小鲸鱼大事件（二）"></a>02|小鲸鱼大事件（二）</h2><p>Docker 项目在短时间内迅速崛起的三个重要原因：</p><ol><li><p>PaaS 概念已经深入人心的完美契机。</p></li><li><p>Docker 镜像通过技术手段解决了 PaaS 的根本性问题（环境不一致）；</p></li><li><p>Docker 容器同开发者之间有着与生俱来的密切关系；</p></li></ol><p>docker公司战略：坚持把“开发者”群体放在至高无上的位置。</p><p>任何项目或者技术都应该是以用户为中心，找准目标人群，深挖用户痛点，通过用户最能接受的方式，去解决问题。</p><h2 id="03-小鲸鱼大事件（三）"><a href="#03-小鲸鱼大事件（三）" class="headerlink" title="03|小鲸鱼大事件（三）"></a>03|小鲸鱼大事件（三）</h2><p>Docker 公司借助PAAS热潮，通过并购构建自己的平台能力，但同时也和很多合作伙伴分道扬镳。</p><p>Swarm 的最大亮点，则是它完全使用 Docker 项目原本的容器管理 API 来完成集群管理。</p><p>Compose（Fig） 项目提出了“容器编排”概念。</p><h2 id="04-小鲸鱼大事件（四）"><a href="#04-小鲸鱼大事件（四）" class="headerlink" title="04|小鲸鱼大事件（四）"></a>04|小鲸鱼大事件（四）</h2><h3 id="CNCF基金会"><a href="#CNCF基金会" class="headerlink" title="CNCF基金会"></a>CNCF基金会</h3><p>2015 年 7 月,Google、RedHat 等开源基础设施领域玩家们，共同成立了名为 CNCF（Cloud Native Computing Foundation）的基金会。</p><p>它希望，以 Kubernetes 项目为基础，建立一个由开源基础设施领域厂商主导的、按照独立基金会方式运营的平台级社区，来对抗以 Docker 公司为核心的容器商业生态。</p><h3 id="如何解决-K8s-项目在编排领域的竞争力的问题。"><a href="#如何解决-K8s-项目在编排领域的竞争力的问题。" class="headerlink" title="如何解决 K8s 项目在编排领域的竞争力的问题。"></a>如何解决 K8s 项目在编排领域的竞争力的问题。</h3><p>Swarm 擅长的是跟 Docker 生态的无缝集成，而 Mesos 擅长的则是大规模集群的调度与管理。</p><p>核心竞争力google软件：Borg。Google 公司在容器化基础设施领域多年来实践经验的沉淀与升华。</p><p>RedHat能真正理解开源社区运作和项目研发真谛的，将k8s先进的思想通过技术手段在开源社区落地</p><h3 id="CNCF-社区必须以-K8s-项目为核心，覆盖足够多的场景。"><a href="#CNCF-社区必须以-K8s-项目为核心，覆盖足够多的场景。" class="headerlink" title="CNCF 社区必须以 K8s 项目为核心，覆盖足够多的场景。"></a>CNCF 社区必须以 K8s 项目为核心，覆盖足够多的场景。</h3><p>2016年，Docker 公司放弃现有的 Swarm 项目，将容器编排和集群管理功能全部内置到 Docker 项目当中，这其实违背了 Docker 项目与开发者保持亲密关系的初衷，提高了docker技术复杂度和维护难度。</p><p>Kubernetes 的应对策略则是反其道而行之，开始在整个社区推进“民主化”架构，即：从 API 到容器运行时的每一层，Kubernetes 项目都为开发者暴露出了可以扩展的插件机制，鼓励用户通过代码的方式介入 Kubernetes 项目的每一个阶段,形成了完全以 Kubernetes 项目为核心的“百家争鸣”。</p><h3 id="极客时间kubernets架构图"><a href="#极客时间kubernets架构图" class="headerlink" title="极客时间kubernets架构图"></a>极客时间kubernets架构图</h3><img src="https://wangzhangtao.com/img/body/a-temp/kubernets架构图.png" alt="kuberntes架构图" style="zoom:67%;max-width: 30%" />]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 深入剖析Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 深入剖析Kuernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>gitlab的使用</title>
      <link href="/2020/08/24/gitlab%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
      <url>/2020/08/24/gitlab%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h3 id="取消Auto-DevOps流水线"><a href="#取消Auto-DevOps流水线" class="headerlink" title="取消Auto DevOps流水线"></a>取消Auto DevOps流水线</h3><p>设置 - CI/CD - Auto DevOps - 默认为Auto DevOps流水线，取消选中✅</p><img src="http://wangzhangtao.com/img/body/4.gitlab%E7%9A%84%E4%BD%BF%E7%94%A8/image-20200814181650557.png" alt="image-20200814181650557" style="zoom:67%;max-width:70%" /><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> You won&#39;t be able to create new projects because you have reached your project limit.</span><br><span class="line">Don&#39;t show again | Remind later</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
          <category> gitlab </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 其他 </tag>
            
            <tag> gitlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx常见问题</title>
      <link href="/2020/08/21/nginx%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
      <url>/2020/08/21/nginx%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="nginx异常"><a href="#nginx异常" class="headerlink" title="nginx异常"></a>nginx异常</h1><h2 id="nginx报错-could-not-build-optimal-server-names-hash"><a href="#nginx报错-could-not-build-optimal-server-names-hash" class="headerlink" title="nginx报错: could not build optimal server_names_hash"></a>nginx报错: could not build optimal server_names_hash</h2><p>nginx报错具体内容：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test vhosts]# nginx -s reload</span><br><span class="line">nginx: [warn] could not build optimal server_names_hash, you should increase either server_names_hash_max_size: 512 or server_names_hash_bucket_size: 64; ignoring server_names_hash_bucket_size</span><br></pre></td></tr></table></figure><p>解决方法是在 nginx 配置文件的 http 段中增加如下配置：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    ...</span><br><span class="line">    server_names_hash_max_size 512;</span><br><span class="line">    server_names_hash_bucket_size 128;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="nginx报错-could-not-build-optimal-proxy-headers-hash"><a href="#nginx报错-could-not-build-optimal-proxy-headers-hash" class="headerlink" title="nginx报错:could not build optimal proxy_headers_hash"></a>nginx报错:could not build optimal proxy_headers_hash</h2><p>nginx报错具体内容：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test nginx]# nginx -t</span><br><span class="line">nginx: [warn] could not build optimal proxy_headers_hash, you should increase either proxy_headers_hash_max_size: 512 or proxy_headers_hash_bucket_size: 64; ignoring proxy_headers_hash_bucket_size</span><br><span class="line">nginx: the configuration file &#x2F;etc&#x2F;nginx&#x2F;nginx.conf syntax is ok</span><br><span class="line">nginx: configuration file &#x2F;etc&#x2F;nginx&#x2F;nginx.conf test is successful</span><br></pre></td></tr></table></figure><p>解决方法是在 nginx 配置文件的 http 段中增加如下配置：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">http&#123;</span><br><span class="line">   ......</span><br><span class="line">   proxy_headers_hash_max_size 51200;</span><br><span class="line">   proxy_headers_hash_bucket_size 6400;</span><br><span class="line">   ....</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="nginx报错-mmap-MAP-ANON-MAP-SHARED-1073741824-failed-12-Cannot-allocate-memory"><a href="#nginx报错-mmap-MAP-ANON-MAP-SHARED-1073741824-failed-12-Cannot-allocate-memory" class="headerlink" title="nginx报错: mmap(MAP_ANON|MAP_SHARED, 1073741824) failed (12: Cannot allocate memory)"></a>nginx报错: mmap(MAP_ANON|MAP_SHARED, 1073741824) failed (12: Cannot allocate memory)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx -t</span><br><span class="line">nginx: the configuration file &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf syntax is ok</span><br><span class="line">nginx: [alert] mmap(MAP_ANON|MAP_SHARED, 1073741824) failed (12: Cannot allocate memory)</span><br><span class="line">nginx: configuration file &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf test failed</span><br></pre></td></tr></table></figure><p>查看nignx的配置文件</p><p><code>proxy_cache_path /usr/local/nginx/proxy_cache_path levels=1:2 keys_zone=cache_one:1024m inactive=1d max_size=20g;</code> </p><p>内存缓存大小比服务器剩余内存还要大，所以内存空间不足，调小cache_one值即可。</p><h2 id="nginx配置https，重定向后https变成了http"><a href="#nginx配置https，重定向后https变成了http" class="headerlink" title="nginx配置https，重定向后https变成了http"></a>nginx配置https，重定向后https变成了http</h2><p>客户端发起https请求，通过nginx转发到后台服务器，结果后台服务器重定向到客户端的地址为 http，这是为什么呢？</p><p>因为nginx接受到https以后，转发到后台为http请求，后台服务地址重定向，并将地址直接返回给客户端，没有走nginx. 因为后台服务接收到的是http请求，所以重定向的也是http请求。如果想保留https，需要在配置文件中加上下面的参数</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">把proxy_redirect default ;改成 proxy_redirect http:&#x2F;&#x2F; https:&#x2F;&#x2F;；</span><br></pre></td></tr></table></figure><h3 id="参考文档："><a href="#参考文档：" class="headerlink" title="参考文档："></a>参考文档：</h3><ul><li><a href="https://www.cnblogs.com/52py/p/12374067.html" target="_blank" rel="noopener">nginx配置https，重定向后https变成了http</a></li></ul><h2 id="nginx报错：SSL-接收到一个超出最大准许长度的记录"><a href="#nginx报错：SSL-接收到一个超出最大准许长度的记录" class="headerlink" title="nginx报错：SSL 接收到一个超出最大准许长度的记录"></a>nginx报错：SSL 接收到一个超出最大准许长度的记录</h2><p>nginx报错，报错信息如下图：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">建立安全连接失败</span><br><span class="line"></span><br><span class="line">连接到 wangzhangtao.com 时发生错误。SSL 接收到一个超出最大准许长度的记录。</span><br><span class="line"></span><br><span class="line">错误代码：SSL_ERROR_RX_RECORD_TOO_LONG</span><br><span class="line"></span><br><span class="line">    由于不能验证所收到的数据是否可信，无法显示您想要查看的页面。</span><br><span class="line">    建议向此网站的管理员反馈这个问题。</span><br></pre></td></tr></table></figure><p><strong>ssl报错示例图</strong></p><img src="http://wangzhangtao.com/img/body/a-temp/image-20201021200104416.png" alt="ngin x ssl报错示例图" style="zoom:67%;max-width: 70%" /><p>解决方案：</p><p>如果提示“SSL 接收到一个超出最大准许长度的记录。错误代码：SSL_ERROR_RX_RECORD_TOO_LONG说明少了“ssl on”;</p><p>设置<code>listen 443 ssl;</code>或者 <code>ssl on;</code></p><h3 id="参考文档：-1"><a href="#参考文档：-1" class="headerlink" title="参考文档："></a>参考文档：</h3><ul><li><a href="https://blog.csdn.net/maxer025/article/details/84807770" target="_blank" rel="noopener">SSL 接收到一个超出最大准许长度的记录</a></li></ul><h1 id="nginx设置pc浏览器跳转到手机浏览器"><a href="#nginx设置pc浏览器跳转到手机浏览器" class="headerlink" title="nginx设置pc浏览器跳转到手机浏览器"></a>nginx设置pc浏览器跳转到手机浏览器</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#移动端跳转到 m.wang.com</span><br><span class="line">if ($http_user_agent ~* (mobile|nokia|iphone|ipad|android|samsung|htc|blackberry)) &#123;</span><br><span class="line">  rewrite  ^(.*)    http:&#x2F;&#x2F;m.wang.com$1 permanent;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#非移动端跳转到 www.wang.com</span><br><span class="line">if ($http_user_agent !~* (mobile|nokia|iphone|ipad|android|samsung|htc|blackberry)) &#123;</span><br><span class="line">    rewrite  ^(.*)    http:&#x2F;&#x2F;www.wang.com$1 permanent;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://blog.csdn.net/IT_townlet/article/details/87943533" target="_blank" rel="noopener">nginx设置pc浏览器跳转到手机浏览器</a></li></ul><h1 id="nginx根据二级域名做请求转发"><a href="#nginx根据二级域名做请求转发" class="headerlink" title="nginx根据二级域名做请求转发"></a>nginx根据二级域名做请求转发</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if ($http_host ~* &quot;^(.*?)\.wang\.com$&quot;) &#123;    #正则表达式</span><br><span class="line">     set $domain $1;                     #设置变量，获取二级域名</span><br><span class="line">&#125;</span><br><span class="line">        </span><br><span class="line">#移动端跳转到 m.wang.com</span><br><span class="line">if ($http_user_agent ~* (mobile|nokia|iphone|ipad|android|samsung|htc|blackberry)) &#123;</span><br><span class="line">    rewrite  ^(.*)    http:&#x2F;&#x2F;m.wang.com&#x2F;$&#123;domain&#125;$1 permanent;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="参考文档-1"><a href="#参考文档-1" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://blog.csdn.net/donghustone/article/details/88354008?utm_medium=distribute.pc_relevant_bbs_down.none-task-blog-baidujs-1.nonecase&depth_1-utm_source=distribute.pc_relevant_bbs_down.none-task-blog-baidujs-1.nonecase" target="_blank" rel="noopener">nginx根据二级域名做请求转发</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s部署dubbo-admin</title>
      <link href="/2020/08/17/k8s%E9%83%A8%E7%BD%B2dubbo-admin/"/>
      <url>/2020/08/17/k8s%E9%83%A8%E7%BD%B2dubbo-admin/</url>
      
        <content type="html"><![CDATA[<blockquote><p> <a href="http://wangzhangtao.com/2020/07/21/k8s部署zookeeper集群/" target="_blank" rel="noopener">k8s部署zookeeper集群</a></p></blockquote><h2 id="k8s部署dubbo-admin"><a href="#k8s部署dubbo-admin" class="headerlink" title="k8s部署dubbo-admin"></a>k8s部署dubbo-admin</h2><h3 id="自己根据文档创建镜像"><a href="#自己根据文档创建镜像" class="headerlink" title="自己根据文档创建镜像"></a>自己根据文档创建镜像</h3><ul><li><p><a href="https://github.com/apache/dubbo-admin/" target="_blank" rel="noopener">github仓库</a> </p></li><li><p><a href="https://github.com/apache/dubbo-admin/blob/develop/docker/latest/Dockerfile" target="_blank" rel="noopener">github仓库 dockerfile</a> 自己根据git创建镜像</p></li></ul><h3 id="下载dubbo-admin镜像"><a href="#下载dubbo-admin镜像" class="headerlink" title="下载dubbo-admin镜像"></a>下载dubbo-admin镜像</h3><p>下载rabbitmq镜像并推送到本地仓库</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull registry.cn-beijing.aliyuncs.com&#x2F;wangzt&#x2F;docker&#x2F;dubbo-admin:v0.1</span><br><span class="line">docker tag c30a8456f37c harbor.od.com&#x2F;public&#x2F;dubbo-admin:v0.2.0</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;dubbo-admin:v0.2.0</span><br></pre></td></tr></table></figure><h3 id="创建dubbo-admin资源配置清单"><a href="#创建dubbo-admin资源配置清单" class="headerlink" title="创建dubbo-admin资源配置清单"></a>创建dubbo-admin资源配置清单</h3><h4 id="deploy-yaml"><a href="#deploy-yaml" class="headerlink" title="deploy.yaml"></a>deploy.yaml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; dubbo-admin&#x2F;deploy.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: dubbo-admin</span><br><span class="line">  labels:</span><br><span class="line">    app: dubbo-admin</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: dubbo-admin</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: dubbo-admin</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - name: dubbo-admin</span><br><span class="line">          image: &#39;harbor.od.com&#x2F;public&#x2F;dubbo-admin:v0.2.0&#39;</span><br><span class="line">          imagePullPolicy: Always</span><br><span class="line">          command: [ &quot;&#x2F;bin&#x2F;bash&quot;, &quot;-ce&quot;, &quot;java -Dadmin.registry.address&#x3D;zookeeper:&#x2F;&#x2F;zk-cs:2181 -Dadmin.config-center&#x3D;zookeeper:&#x2F;&#x2F;zk-cs:2181 -Dadmin.metadata-report.address&#x3D;zookeeper:&#x2F;&#x2F;zk-cs:2181 -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -Djava.security.egd&#x3D;file:&#x2F;dev&#x2F;.&#x2F;urandom -jar &#x2F;app.jar&quot;]</span><br><span class="line">          readinessProbe:</span><br><span class="line">            tcpSocket:</span><br><span class="line">              port: 8080</span><br><span class="line">            initialDelaySeconds: 60 </span><br><span class="line">            periodSeconds: 20   </span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="svc-yaml"><a href="#svc-yaml" class="headerlink" title="svc.yaml"></a>svc.yaml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; dubbo-admin&#x2F;svc.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: dubbo-admin</span><br><span class="line">  labels:</span><br><span class="line">    app: dubbo-admin</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: dubbo-admin</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">    - name: dubbo-admin-80</span><br><span class="line">      port: 80</span><br><span class="line">      targetPort: 8080</span><br><span class="line">      nodePort: 28080</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="ingress-yaml"><a href="#ingress-yaml" class="headerlink" title="ingress.yaml"></a>ingress.yaml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; dubbo-admin&#x2F;ingress.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: dubbo-admin</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: dubbo.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: dubbo-admin</span><br><span class="line">          servicePort: dubbo-admin-80</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="执行dubbo-admin资源配置清单"><a href="#执行dubbo-admin资源配置清单" class="headerlink" title="执行dubbo-admin资源配置清单"></a>执行dubbo-admin资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f dubbo-admin&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f dubbo-admin&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f dubbo-admin&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> service </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> dubbo-admin </tag>
            
            <tag> service </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s部署dubbo-admin</title>
      <link href="/2020/08/17/k8s%E9%83%A8%E7%BD%B2dubbo-admin/"/>
      <url>/2020/08/17/k8s%E9%83%A8%E7%BD%B2dubbo-admin/</url>
      
        <content type="html"><![CDATA[<blockquote><p> <a href="http://wangzhangtao.com/2020/07/21/k8s部署zookeeper集群/" target="_blank" rel="noopener">k8s部署zookeeper集群</a></p></blockquote><h2 id="k8s部署dubbo-admin"><a href="#k8s部署dubbo-admin" class="headerlink" title="k8s部署dubbo-admin"></a>k8s部署dubbo-admin</h2><h3 id="下载dubbo-admin镜像"><a href="#下载dubbo-admin镜像" class="headerlink" title="下载dubbo-admin镜像"></a>下载dubbo-admin镜像</h3><blockquote><p> <a href="https://github.com/apache/dubbo-admin/" target="_blank" rel="noopener">github仓库</a> </p><p> <a href="https://github.com/apache/dubbo-admin/blob/develop/docker/latest/Dockerfile" target="_blank" rel="noopener">github仓库 dockerfile</a> 自己根据git创建镜像</p></blockquote><p>下载rabbitmq镜像并推送到本地仓库</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull registry.cn-beijing.aliyuncs.com&#x2F;wangzt&#x2F;docker&#x2F;dubbo-admin:v0.1</span><br><span class="line">docker tag c30a8456f37c harbor.od.com&#x2F;public&#x2F;dubbo-admin:v0.2.0</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;dubbo-admin:v0.2.0</span><br></pre></td></tr></table></figure><h3 id="创建dubbo-admin资源配置清单"><a href="#创建dubbo-admin资源配置清单" class="headerlink" title="创建dubbo-admin资源配置清单"></a>创建dubbo-admin资源配置清单</h3><h4 id="deploy-yaml"><a href="#deploy-yaml" class="headerlink" title="deploy.yaml"></a>deploy.yaml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; dubbo-admin&#x2F;deploy.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: dubbo-admin</span><br><span class="line">  labels:</span><br><span class="line">    app: dubbo-admin</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: dubbo-admin</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: dubbo-admin</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - name: dubbo-admin</span><br><span class="line">          image: &#39;harbor.od.com&#x2F;public&#x2F;dubbo-admin:v0.2.0&#39;</span><br><span class="line">          imagePullPolicy: Always</span><br><span class="line">          command: [ &quot;&#x2F;bin&#x2F;bash&quot;, &quot;-ce&quot;, &quot;java -Dadmin.registry.address&#x3D;zookeeper:&#x2F;&#x2F;zk-cs:2181 -Dadmin.config-center&#x3D;zookeeper:&#x2F;&#x2F;zk-cs:2181 -Dadmin.metadata-report.address&#x3D;zookeeper:&#x2F;&#x2F;zk-cs:2181 -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -Djava.security.egd&#x3D;file:&#x2F;dev&#x2F;.&#x2F;urandom -jar &#x2F;app.jar&quot;]</span><br><span class="line">          readinessProbe:</span><br><span class="line">            tcpSocket:</span><br><span class="line">              port: 8080</span><br><span class="line">            initialDelaySeconds: 60 </span><br><span class="line">            periodSeconds: 20   </span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="svc-yaml"><a href="#svc-yaml" class="headerlink" title="svc.yaml"></a>svc.yaml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; dubbo-admin&#x2F;svc.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: dubbo-admin</span><br><span class="line">  labels:</span><br><span class="line">    app: dubbo-admin</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: dubbo-admin</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">    - name: dubbo-admin-80</span><br><span class="line">      port: 80</span><br><span class="line">      targetPort: 8080</span><br><span class="line">      nodePort: 28080</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="ingress-yaml"><a href="#ingress-yaml" class="headerlink" title="ingress.yaml"></a>ingress.yaml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; dubbo-admin&#x2F;ingress.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: dubbo-admin</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: dubbo.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: dubbo-admin</span><br><span class="line">          servicePort: dubbo-admin-80</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="执行dubbo-admin资源配置清单"><a href="#执行dubbo-admin资源配置清单" class="headerlink" title="执行dubbo-admin资源配置清单"></a>执行dubbo-admin资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f dubbo-admin&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f dubbo-admin&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f dubbo-admin&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>linux常见问题1</title>
      <link href="/2020/08/16/linux%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%981/"/>
      <url>/2020/08/16/linux%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%981/</url>
      
        <content type="html"><![CDATA[<h2 id="linux常见问题1"><a href="#linux常见问题1" class="headerlink" title="linux常见问题1"></a>linux常见问题1</h2><h3 id="小问题"><a href="#小问题" class="headerlink" title="小问题"></a>小问题</h3><p><strong>ssh 根据指定端口登录远程服务器</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh -p 端口 用户@ip地址</span><br></pre></td></tr></table></figure><p><strong>管道的使用</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# echo &quot;world&quot; | xargs echo &quot;hello&quot;     </span><br><span class="line">hello world</span><br></pre></td></tr></table></figure><h3 id="sed的常见用法"><a href="#sed的常见用法" class="headerlink" title="sed的常见用法"></a>sed的常见用法</h3><h4 id="追加一个文件到另一个文件指定行"><a href="#追加一个文件到另一个文件指定行" class="headerlink" title="追加一个文件到另一个文件指定行"></a>追加一个文件到另一个文件指定行</h4><p>sed  ‘3r bbb.txt’ aaa.txt</p><h3 id="awk常见用法"><a href="#awk常见用法" class="headerlink" title="awk常见用法"></a>awk常见用法</h3><p>打印文本文件的总行数  <code>awk &#39;END{print NR}&#39; filename</code></p><p>打印文本第一行  <code>awk &#39;NR==1{print $0}&#39; filename</code></p><p>打印文本第二行第一列  <code>sed -n &quot;2, 1p&quot; filename | awk &#39;print $1&#39;</code></p><p>打印文本最后一行 <code>head -3 access.log | awk &#39;{print $(NF)}&#39;</code></p><p>打印文本倒数第二行 <code>head -3 access.log | awk &#39;{print $(NF-1)}&#39;</code></p><p>将第一列过滤重复列出每一项，每一项的出现次数，每一项的大小总和<br>  <code>awk &#39;{a[$1]++;b[$1]+=$2}END{for(i in a){print i,a[i],b[i]}}&#39; filename</code></p><h3 id="关闭SSH其他用户会话连接"><a href="#关闭SSH其他用户会话连接" class="headerlink" title="关闭SSH其他用户会话连接"></a>关闭SSH其他用户会话连接</h3><p><code>pkill -kill -t pts/1</code></p><h3 id="查看linux操作系统"><a href="#查看linux操作系统" class="headerlink" title="查看linux操作系统"></a>查看linux操作系统</h3><p>注:这个命令适用于所有的linux，包括<a href="http://www.linuxidc.com/topicnews.aspx?tid=10" target="_blank" rel="noopener">RedHat</a>、<a href="http://www.linuxidc.com/topicnews.aspx?tid=3" target="_blank" rel="noopener">SUSE</a>、Debian等发行版。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@nginx-test-c9db64556-dwmf2:&#x2F;# cat &#x2F;etc&#x2F;issue</span><br><span class="line">Debian GNU&#x2F;Linux 10 \n \l</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> 常见问题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 常见问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s部署消息队列rabbitmq</title>
      <link href="/2020/08/14/k8s%E9%83%A8%E7%BD%B2%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97rabbitmq/"/>
      <url>/2020/08/14/k8s%E9%83%A8%E7%BD%B2%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97rabbitmq/</url>
      
        <content type="html"><![CDATA[<h2 id="k8s部署rabbitmq"><a href="#k8s部署rabbitmq" class="headerlink" title="k8s部署rabbitmq"></a>k8s部署rabbitmq</h2><h3 id="下载rabbitmq镜像"><a href="#下载rabbitmq镜像" class="headerlink" title="下载rabbitmq镜像"></a>下载rabbitmq镜像</h3><p>下载rabbitmq镜像并推送到本地仓库</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull rabbitmq:3.8.6-management </span><br><span class="line">docker tag 64a1f920fb0d harbor.od.com&#x2F;public&#x2F;rabbitmq:v3.8.6-management</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;rabbitmq:v3.8.6-management</span><br></pre></td></tr></table></figure><ul><li>配置文件：/etc/rabbitmq/rabbitmq.conf</li></ul><h3 id="创建RabbitMQ资源配置清单"><a href="#创建RabbitMQ资源配置清单" class="headerlink" title="创建RabbitMQ资源配置清单"></a>创建RabbitMQ资源配置清单</h3><p>下面是<strong>RabbitMQ</strong>部署的定义代码，此代码由两部分组成，即<strong>RabbitMQ</strong>部署的部署以及其代理服务。</p><p>通过NodePort模式对外暴露了15672和5672端口，并通过nfs文件系统对<strong>RabbitMQ</strong>的数据进行持久化。</p><h4 id="deploy-yaml"><a href="#deploy-yaml" class="headerlink" title="deploy.yaml"></a>deploy.yaml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; rabbitmq&#x2F;deploy.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1  </span><br><span class="line">kind: Deployment  </span><br><span class="line">metadata:  </span><br><span class="line">  name: rabbitmq  </span><br><span class="line">spec:  </span><br><span class="line">  replicas: 1  </span><br><span class="line">  selector:  </span><br><span class="line">    matchLabels:  </span><br><span class="line">      app: rabbitmq  </span><br><span class="line">  template:  </span><br><span class="line">    metadata:  </span><br><span class="line">      labels:  </span><br><span class="line">        app: rabbitmq  </span><br><span class="line">    spec:  </span><br><span class="line">      containers:  </span><br><span class="line">      - name: rabbit</span><br><span class="line">        image: harbor.od.com&#x2F;public&#x2F;rabbitmq:v3.8.6-management</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        env:  # 设置变量</span><br><span class="line">        - name: RABBITMQ_DEFAULT_USER</span><br><span class="line">          value: admin</span><br><span class="line">        - name: RABBITMQ_DEFAULT_PASS</span><br><span class="line">          value: admin123</span><br><span class="line">        volumeMounts:  # 挂载磁盘</span><br><span class="line">        - name: rabbitmq-data</span><br><span class="line">          mountPath: &#x2F;var&#x2F;lib&#x2F;rabbitmq</span><br><span class="line">      volumes:</span><br><span class="line">      - name: rabbitmq-data </span><br><span class="line">        nfs:</span><br><span class="line">          server: 192.168.2.2</span><br><span class="line">          path: &#x2F;data&#x2F;nfs&#x2F;rabbitmq     </span><br><span class="line">          </span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="svc-yaml"><a href="#svc-yaml" class="headerlink" title="svc.yaml"></a>svc.yaml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; rabbitmq&#x2F;svc.yaml</span><br><span class="line">apiVersion: v1  </span><br><span class="line">kind: Service  </span><br><span class="line">metadata:  </span><br><span class="line">  name: rabbitmq-service  </span><br><span class="line">spec:  </span><br><span class="line">  ports:  </span><br><span class="line">  - name: rabbitmq15672  </span><br><span class="line">    protocol: TCP  </span><br><span class="line">    port: 15672  </span><br><span class="line">    targetPort: 15672 </span><br><span class="line">    nodePort: 30001</span><br><span class="line">  - name: rabbitmq5672   </span><br><span class="line">    protocol: TCP  </span><br><span class="line">    port: 5672   </span><br><span class="line">    targetPort: 5672</span><br><span class="line">    nodePort: 30002  </span><br><span class="line">  selector:  </span><br><span class="line">    app: rabbitmq  </span><br><span class="line">  type: NodePort</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="ingress-yaml"><a href="#ingress-yaml" class="headerlink" title="ingress.yaml"></a>ingress.yaml</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; rabbitmq&#x2F;ingress.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: rabbitmq</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: rabbitmq.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: rabbitmq-service</span><br><span class="line">          servicePort: rabbitmq15672 </span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单"><a href="#应用资源配置清单" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f rabbitmq&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f rabbitmq&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f rabbitmq&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p>kubectl delete -f rabbitmq/deploy.yaml -f rabbitmq/svc.yaml -f rabbitmq/ingress.yaml </p><h3 id="查看执行结果"><a href="#查看执行结果" class="headerlink" title="查看执行结果"></a>查看执行结果</h3><h4 id="查看pod运行状态"><a href="#查看pod运行状态" class="headerlink" title="查看pod运行状态"></a>查看pod运行状态</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ～]# kubectl get pod | grep rabbit</span><br><span class="line">rabbit-84d76d4bd6-txwqm           1&#x2F;1     Running   0          3m50s</span><br></pre></td></tr></table></figure><h4 id="通过url查看页面"><a href="#通过url查看页面" class="headerlink" title="通过url查看页面"></a>通过url查看页面</h4><p>我们通过变量设置的用户名admin 密码admin123, url: <a href="http://rabbitmq.od.com/" target="_blank" rel="noopener">http://rabbitmq.od.com/</a></p><img src="/img/body/9.k8s%E9%83%A8%E7%BD%B2%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97rabbitmq/image-20200814111217436.png" alt="image-20200814111217436" style="zoom:80%;max-width:60%" /><h2 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h2><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul><li><a href="https://www.cnblogs.com/lihanlin/p/12657669.html" target="_blank" rel="noopener">kubernetes搭建RabbitMQ</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> service </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> rabbitmq </tag>
            
            <tag> service </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>elk问题收集</title>
      <link href="/2020/08/05/elk%E9%97%AE%E9%A2%98%E6%94%B6%E9%9B%86/"/>
      <url>/2020/08/05/elk%E9%97%AE%E9%A2%98%E6%94%B6%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h2 id="elasticsearch启动报错1"><a href="#elasticsearch启动报错1" class="headerlink" title="elasticsearch启动报错1"></a>elasticsearch启动报错1</h2><h3 id="es报错"><a href="#es报错" class="headerlink" title="es报错"></a>es报错</h3><p>max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] </p><p>问题翻译过来就是：elasticsearch用户拥有的内存权限太小，至少需要262144；</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[2020-07-07T21:49:43,839][INFO ][o.e.p.PluginsService     ] [X-111.ecs] loaded module [x-pack-watcher] [2020-07-07T21:49:43,840][INFO ][o.e.p.PluginsService     ] [X-111.ecs] no plugins loaded [2020-07-07T21:49:50,525][INFO ][o.e.x.s.a.s.FileRolesStore] [X-111.ecs] parsed [0] roles from file [&#x2F;usr&#x2F;local&#x2F;elasticsearch&#x2F;config&#x2F;roles.yml] [2020-07-07T21:49:52,067][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [X-111.ecs] [controller&#x2F;1845] [Main.cc@110] controller (64 bit): Version 7.3.0 (Build ff2f774f78ce63) Copyright (c) 2019 Elasticsearch BV [2020-07-07T21:49:53,138][DEBUG][o.e.a.ActionModule       ] [X-111.ecs] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security [2020-07-07T21:49:53,626][INFO ][o.e.d.DiscoveryModule    ] [X-111.ecs] using discovery type [zen] and seed hosts providers [settings] [2020-07-07T21:49:54,865][INFO ][o.e.n.Node               ] [X-111.ecs] initialized [2020-07-07T21:49:54,869][INFO ][o.e.n.Node               ] [X-111.ecs] starting ... [2020-07-07T21:49:55,097][INFO ][o.e.t.TransportService   ] [X-111.ecs] publish_address &#123;172.16.1.2:9300&#125;, bound_addresses &#123;0.0.0.0:9300&#125; [2020-07-07T21:49:55,108][INFO ][o.e.b.BootstrapChecks    ] [X-111.ecs] bound or publishing to a non-loopback address, enforcing bootstrap checks ERROR: [1] bootstrap checks failed [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] [2020-07-07T21:49:55,138][INFO ][o.e.n.Node               ] [X-111.ecs] stopping ... [2020-07-07T21:49:55,164][INFO ][o.e.n.Node               ] [X-111.ecs] stopped [2020-07-07T21:49:55,165][INFO ][o.e.n.Node               ] [X-111.ecs] closing ... [2020-07-07T21:49:55,186][INFO ][o.e.n.Node               ] [X-111.ecs] closed</span><br></pre></td></tr></table></figure><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p><strong>临时生效</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 执行命令</span><br><span class="line">sysctl -w vm.max_map_count&#x3D;262144</span><br><span class="line"></span><br><span class="line"># 查看结果</span><br><span class="line">sysctl -a|grep vm.max_map_count</span><br><span class="line"></span><br><span class="line"># 显示 </span><br><span class="line">vm.max_map_count &#x3D; 262144</span><br></pre></td></tr></table></figure><p><strong>永久生效</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在&#x2F;etc&#x2F;sysctl.conf文件最后添加一行</span><br><span class="line">echo &quot;vm.max_map_count&#x3D;262144&quot; &gt;&gt; &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line"></span><br><span class="line"># 使文件生效</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure><h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h3><ul><li>每天进步一点 <a href="https://www.cnblogs.com/yidiandhappy/p/7714489.html" target="_blank" rel="noopener">max virtual memory areas vm.max_map_count  65530  is too low…</a></li></ul><h2 id="logstash启动报错"><a href="#logstash启动报错" class="headerlink" title="logstash启动报错"></a>logstash启动报错</h2><p>logstash启动报错<code>If you wish to run multiple instances, you must change the &quot;path.data&quot; setting.</code></p><p><strong>具体报错信息</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Thread.exclusive is deprecated, use Thread::Mutex</span><br><span class="line">Sending Logstash logs to &#x2F;usr&#x2F;local&#x2F;logstash&#x2F;logs which is now configured via log4j2.properties</span><br><span class="line">[2020-08-04T20:53:24,989][WARN ][logstash.config.source.multilocal] Ignoring the &#39;pipelines.yml&#39; file because modules or command line options are specified</span><br><span class="line">[2020-08-04T20:53:25,004][FATAL][logstash.runner          ] Logstash could not be started because there is already another instance using the configured data directory.  If you wish to run multiple instances, you must change the &quot;path.data&quot; setting.</span><br><span class="line">[2020-08-04T20:53:25,009][ERROR][org.logstash.Logstash    ] java.lang.IllegalStateException: Logstash stopped processing because of an error: (SystemExit) exit</span><br></pre></td></tr></table></figure><h3 id="解决办法："><a href="#解决办法：" class="headerlink" title="解决办法："></a>解决办法：</h3><p>原因：之前运行的instance有缓冲，在 logstash.yml 文件中找到 Data path 的路径(默认在安装目录的data目录下），里面有.lock文件，删除掉就可以。<br>默认路径程序安装目录下 data/.lock</p><h2 id="elasticsearch报错"><a href="#elasticsearch报错" class="headerlink" title="elasticsearch报错"></a>elasticsearch报错</h2><h3 id="报错信息"><a href="#报错信息" class="headerlink" title="报错信息"></a>报错信息</h3><p>在使用elasticsearch时出现了异常，显示<code>SearchPhaseExecutionException[Failed to execute phase [query], all shards failed]</code></p><p><strong>具体报错信息</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[2020-08-04T22:39:32,161][DEBUG][o.e.a.s.TransportSearchAction] [X-111.ecs] All shards failed for phase: [query]</span><br><span class="line">[2020-08-04T22:39:32,162][WARN ][r.suppressed             ] [X-111.ecs] path: &#x2F;.kibana_task_manager&#x2F;_search, params: &#123;ignore_unavailable&#x3D;true, index&#x3D;.kibana_task_manager&#125;</span><br><span class="line">org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed</span><br><span class="line">        at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:305) ~[elasticsearch-7.3.0.jar:7.3.0]</span><br><span class="line">        at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:139) ~[elasticsearch-7.3.0.jar:7.3.0]</span><br></pre></td></tr></table></figure><h3 id="1、查看索引信息"><a href="#1、查看索引信息" class="headerlink" title="1、查看索引信息"></a>1、查看索引信息</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -XGET &#39;http:&#x2F;&#x2F;localhost:9200&#x2F;_cluster&#x2F;health?pretty&#x3D;true&#39;</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;cluster_name&quot; : &quot;test&quot;,</span><br><span class="line">  &quot;status&quot; : &quot;yellow&quot;,</span><br><span class="line">  &quot;timed_out&quot; : false,</span><br><span class="line">  &quot;number_of_nodes&quot; : 1,</span><br><span class="line">  &quot;number_of_data_nodes&quot; : 1,</span><br><span class="line">  &quot;active_primary_shards&quot; : 134,</span><br><span class="line">  &quot;active_shards&quot; : 134,</span><br><span class="line">  &quot;relocating_shards&quot; : 0,</span><br><span class="line">  &quot;initializing_shards&quot; : 0,</span><br><span class="line">  &quot;unassigned_shards&quot; : 30,</span><br><span class="line">  &quot;delayed_unassigned_shards&quot; : 0,</span><br><span class="line">  &quot;number_of_pending_tasks&quot; : 0,</span><br><span class="line">  &quot;number_of_in_flight_fetch&quot; : 0,</span><br><span class="line">  &quot;task_max_waiting_in_queue_millis&quot; : 0,</span><br><span class="line">  &quot;active_shards_percent_as_number&quot; : 98.19711538461539  #数据的正常率，100表示一切ok</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2、查看所有所有的状态"><a href="#2、查看所有所有的状态" class="headerlink" title="2、查看所有所有的状态"></a>2、查看所有所有的状态</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@x-111 ~]# curl  &#39;localhost:9200&#x2F;_cat&#x2F;indices?v&#39;</span><br><span class="line">health status index                           uuid                   pri rep docs.count docs.deleted store.size pri.store.size</span><br><span class="line">yellow open   system-2020-08-04               Zu4PsRDQQBW6N3eb_3eeug   1   1      19454            0      5.8mb          5.8mb</span><br><span class="line">yellow open   system-2020-08-05               SCvqzJuwShq8TZCkzqVF2A   1   1         39            0    544.5kb        544.5kb</span><br><span class="line">green  open   .monitoring-es-7-2020.07.31     O-FIjvpwTXi_lnWtDp2YoA   1   0    1697483       463674    854.2mb        854.2mb</span><br></pre></td></tr></table></figure><ul><li>green：所有的主分片和副本分片都已分配。你的集群是 100% 可用的。</li><li>yellow：所有的主分片已经分片了，但至少还有一个副本是缺失的。不会有数据丢失，所以搜索结果依然是完整的。不过，你的高可用性在某种程度上被弱化。如果 更多的 分片消失，你就会丢数据了。把 yellow 想象成一个需要及时调查的警告。</li><li>red：至少一个主分片（以及它的全部副本）都在缺失中。这意味着你在缺少数据：搜索只能返回部分数据，而分配到这个分片上的写入请求会返回一个异常。</li></ul><h3 id="3、删除red状态的索引"><a href="#3、删除red状态的索引" class="headerlink" title="3、删除red状态的索引"></a>3、删除red状态的索引</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -XDELETE http:&#x2F;&#x2F;localhost:9200&#x2F;index_name</span><br></pre></td></tr></table></figure><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul><li><a href="https://www.cnblogs.com/zhangb8042/" target="_blank" rel="noopener">巽逸</a> 的 <a href="https://www.cnblogs.com/zhangb8042/articles/11346085.html" target="_blank" rel="noopener">删除索引损害的数据</a></li></ul><h2 id="elasticsearch报错：maximum-allowed-to-be-analyzed-for-highlighting"><a href="#elasticsearch报错：maximum-allowed-to-be-analyzed-for-highlighting" class="headerlink" title="elasticsearch报错：maximum allowed to be analyzed for highlighting"></a>elasticsearch报错：maximum allowed to be analyzed for highlighting</h2><h3 id="具体报错内容"><a href="#具体报错内容" class="headerlink" title="具体报错内容"></a>具体报错内容</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[2020-08-05T18:36:35,797][DEBUG][o.e.a.s.TransportSearchAction] [X-111.ecs] [113206] Failed to execute fetch phase</span><br><span class="line">org.elasticsearch.transport.RemoteTransportException: [X-111.ecs][172.16.16.111:9300][indices:data&#x2F;read&#x2F;search[phase&#x2F;fetch&#x2F;id]]</span><br><span class="line">Caused by: java.lang.IllegalArgumentException: The length of [message] field of [Erf6vXMBnlxAlybtik5H] doc of [uc-2020-08-05] index has exceeded [1000000] - maximum allowed to be analyzed for highlighting. This maximum can be set by changing the [index.highlight.max_analyzed_offset] index level setting. For large texts, indexing with offsets or term vectors is recommended!</span><br><span class="line">        at org.elasticsearch.search.fetch.subphase.highlight.UnifiedHighlighter.highlight(UnifiedHighlighter.java:89) ~[elasticsearch-7.3.0.jar:7.3.0]</span><br><span class="line">        at org.elasticsearch.search.fetch.subphase.highlight.HighlightPhase.hitExecute(HighlightPhase.java:107) ~[elasticsearch-7.3.0.jar:7.3.0]</span><br><span class="line">        at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:169) ~[elasticsearch-7.3.0.jar:7.3.0]</span><br><span class="line">        at org.elasticsearch.search.SearchService.lambda$executeFetchPhase$5(SearchService.java:507) ~[elasticsearch-7.3.0.jar:7.3.0]</span><br><span class="line">        at org.elasticsearch.search.SearchService$1.doRun(SearchService.java:347) ~[elasticsearch-7.3.0.jar:7.3.0]</span><br></pre></td></tr></table></figure><h3 id="解决方法-1"><a href="#解决方法-1" class="headerlink" title="解决方法"></a>解决方法</h3><p>错误原因：<strong>索引偏移量默认是100000，超过了最大值</strong></p><p>最大迁移索引不能配置在配置文件中，只能接口修改</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -XPUT &quot;http:&#x2F;&#x2F;127.0.0.1:9200&#x2F;_settings&quot; -H &#39;Content-Type: application&#x2F;json&#39; -d&#39; &#123;</span><br><span class="line">    &quot;index&quot; : &#123;</span><br><span class="line">        &quot;highlight.max_analyzed_offset&quot; : 100000000</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&#39;</span><br></pre></td></tr></table></figure><h3 id="参考文献-1"><a href="#参考文献-1" class="headerlink" title="参考文献"></a>参考文献</h3><ul><li><a href="https://www.cnblogs.com/zhanchenjin/p/11672900.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhanchenjin/p/11672900.html</a></li></ul><h2 id="filebeat-报错：Failed-to-publish-events"><a href="#filebeat-报错：Failed-to-publish-events" class="headerlink" title="filebeat 报错：Failed to publish events"></a>filebeat 报错：Failed to publish events</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2020-11-04T16:44:20.836+0800    ERROR   logstash&#x2F;async.go:256   Failed to publish events caused by: write tcp 172.16.18.22:45750-&gt;172.16.16.112:5044: write: connection reset by peer</span><br><span class="line">2020-11-04T16:44:22.018+0800    ERROR   pipeline&#x2F;output.go:121  Failed to publish events: write tcp 172.16.18.22:45750-&gt;172.16.16.112:5044: write: connection reset by peer</span><br><span class="line">2020-11-04T16:44:22.018+0800    INFO    pipeline&#x2F;output.go:95   Connecting to backoff(async(tcp:&#x2F;&#x2F;172.16.16.112:5044))</span><br><span class="line">2020-11-04T16:44:22.018+0800    INFO    pipeline&#x2F;output.go:105  Connection to backoff(async(tcp:&#x2F;&#x2F;172.16.16.112:5044)) established</span><br></pre></td></tr></table></figure><p><strong>原因：</strong></p><p> 据说是客户端操作不频繁导致Logstash可能由于不工作而尝试重置连接，在这种情况下会引起connection reset by peer</p><p><strong>解决方法：</strong></p><p> 在logstash的配置文件添加<strong>client_inactivity_timeout</strong>参数：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">input &#123;</span><br><span class="line"> beats &#123;</span><br><span class="line">port  &#x3D;&gt; &quot;5044&quot;</span><br><span class="line">codec &#x3D;&gt; json</span><br><span class="line">    client_inactivity_timeout &#x3D;&gt; 36000</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="参考文档-1"><a href="#参考文档-1" class="headerlink" title="参考文档"></a>参考文档</h3><ul><li><a href="https://blog.csdn.net/qq_25646191/article/details/108663974" target="_blank" rel="noopener">filebeat 报错：Failed to publish events</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> ELK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s资源存储pv和pvc</title>
      <link href="/2020/08/04/k8s%E8%B5%84%E6%BA%90%E5%AD%98%E5%82%A8pv%E5%92%8Cpvc/"/>
      <url>/2020/08/04/k8s%E8%B5%84%E6%BA%90%E5%AD%98%E5%82%A8pv%E5%92%8Cpvc/</url>
      
        <content type="html"><![CDATA[<h2 id="资源存储PV，PVC简介"><a href="#资源存储PV，PVC简介" class="headerlink" title="资源存储PV，PVC简介"></a>资源存储PV，PVC简介</h2><h3 id="中间件"><a href="#中间件" class="headerlink" title="中间件"></a>中间件</h3><p><strong>所有的中间件都是为了解耦</strong></p><p>中间件（英语：Middleware），又译中间件、中介层，是提供系统软件和应用软件之间连接的软件，以便于软件各部件之间的沟通。在现代信息技术应用框架如 Web 服务、面向服务的体系结构等项目中应用比较广泛。如数据库、Apache 的 Tomcat 等都属于中间件。</p><h3 id="PV和PVC介绍"><a href="#PV和PVC介绍" class="headerlink" title="PV和PVC介绍"></a>PV和PVC介绍</h3><p>　　管理存储是管理计算的一个明显问题。该PersistentVolume子系统为用户和管理员提供了一个API，用于抽象如何根据消费方式提供存储的详细信息。为此，我们引入了两个新的API资源：PersistentVolume（持久存储）和PersistentVolumeClaim（持久存储声明）<br>　　PersistentVolume（PV）是集群中由管理员配置的一段网络存储。 它是集群中的资源，就像节点是集群资源一样。 PV是容量插件，如Volumes，但其生命周期独立于使用PV的任何单个pod。 此API对象捕获存储实现的详细信息，包括NFS，iSCSI或特定于云提供程序的存储系统。<br>　　PersistentVolumeClaim（PVC）是由用户进行存储的请求。 它类似于pod。 Pod消耗节点资源，PVC消耗PV资源。Pod可以请求特定级别的资源（CPU和内存）, PVC声明可以请求特定存储的大小和访问模式（例如，可以一次读/写或多次只读）。<br>　　虽然PersistentVolumeClaims（PVC）允许用户使用抽象存储资源，但是PersistentVolumes对于不同的问题，用户通常需要具有不同属性（例如性能）。群集管理员需要能够提供各种PersistentVolumes不同的方式，比如存储大小和访问模式，但不会让用户了解这些卷的实现方式。对于这些需求，有StorageClass 资源。<br>　　StorageClass为管理员提供了一种描述他们提供的存储的“类”的方法。 不同的类可能映射到服务质量级别，或备份策略，或者由群集管理员确定的任意策略。 Kubernetes本身对于什么类别代表是不言而喻的。 这个概念有时在其他存储系统中称为“配置文件”。<br>　　<strong>PVC和PV是一一对应的</strong>。</p><h3 id="生命周期"><a href="#生命周期" class="headerlink" title="生命周期"></a>生命周期</h3><p>　　PV是集群中的资源。PVC是对这些资源的请求，并且还充当对资源的检查。PV和PVC之间的相互作用遵循以下生命周期：<br>Provisioning ——-&gt; Binding ——–&gt;Using——&gt;Releasing——&gt;Recycling</p><ol><li><p>供应准备Provisioning—通过集群外的存储系统或者云平台来提供存储持久化支持。</p><ul><li><p>静态提供Static：集群管理员创建多个PV。 它们携带可供集群用户使用的真实存储的详细信息。 它们存在于Kubernetes API中，可用于消费</p></li><li><p>动态提供Dynamic：当管理员创建的静态PV都不匹配用户的PersistentVolumeClaim时，集群可能会尝试为PVC创建动态配置卷。 此配置基于StorageClasses：PVC必须请求一个类，并且管理员必须已创建并配置该类才能进行动态配置。 要求该类的声明有效地为自己禁用动态配置。</p></li></ul></li><li><p>绑定Binding—用户创建pvc并指定需要的资源和访问模式。在找到可用pv之前，pvc会保持未绑定状态。</p></li><li><p>使用Using—用户可在pod中像volume一样使用pvc。</p></li><li><p>释放Releasing—用户删除pvc来回收存储资源，pv将变成“released”状态。由于还保留着之前的数据，这些数据需要根据不同的策略来处理，否则这些存储资源无法被其他pvc使用。</p></li><li><p>回收Recycling—pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。</p></li></ol><p><strong>回收策略</strong></p><ul><li>保留策略：允许人工处理保留的数据。</li><li>删除策略：将删除pv和外部关联的存储资源，需要插件支持。</li><li>回收策略：将执行清除操作，之后可以被新的pvc使用，需要插件支持。<br>注：目前（2018年10月）只有NFS和HostPath类型卷支持回收策略，AWS EBS,GCE PD,Azure Disk和Cinder支持删除(Delete)策略。</li></ul><h3 id="挂载磁盘分两步"><a href="#挂载磁盘分两步" class="headerlink" title="挂载磁盘分两步"></a>挂载磁盘分两步</h3><ol><li>Attach，为虚拟机挂载远程磁盘的操作。kubelet 为 Volume 创建目录，并绑定远程存储。</li><li>Mount，磁盘设备格式化并挂载到 Volume 宿主机目录的操作</li></ol><p>如果你的 Volume 类型是远程文件存储（比如 NFS）的话，kubelet跳过“第一阶段”（Attach）的操作</p><h3 id="PV卷阶段状态"><a href="#PV卷阶段状态" class="headerlink" title="PV卷阶段状态"></a>PV卷阶段状态</h3><ul><li>Available – 资源尚未被claim使用</li><li>Bound – 卷已经被绑定到claim了</li><li>Released – claim被删除，卷处于释放状态，但未被集群回收。</li><li>Failed – 卷自动回收失败</li></ul><h2 id="静态PV，PVC创建和使用"><a href="#静态PV，PVC创建和使用" class="headerlink" title="静态PV，PVC创建和使用"></a>静态PV，PVC创建和使用</h2><h3 id="创建pv存储，实际的存储资源"><a href="#创建pv存储，实际的存储资源" class="headerlink" title="创建pv存储，实际的存储资源"></a>创建pv存储，实际的存储资源</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# vim pv-nfs.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">name: pv-nfs</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: nfs1</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 1Gi</span><br><span class="line">    accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">      nfs:</span><br><span class="line">        server: 192.168.2.2</span><br><span class="line">        path: &quot;&#x2F;data&#x2F;nfs&#x2F;v1&quot;</span><br><span class="line"></span><br><span class="line">[root@wang-200 test]# kubectl apply -f pv-nfs.yaml  </span><br><span class="line">persistentvolume&#x2F;pv-nfs created</span><br><span class="line"></span><br><span class="line">[root@m3 test]# kubectl get pv</span><br><span class="line">NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE</span><br><span class="line">pv-nfs   1Gi        RWX            Retain           Available           nfs1                    41s</span><br></pre></td></tr></table></figure><h3 id="PVC-描述的，则是-Pod-所希望使用的持久化存储的属性。"><a href="#PVC-描述的，则是-Pod-所希望使用的持久化存储的属性。" class="headerlink" title="PVC 描述的，则是 Pod 所希望使用的持久化存储的属性。"></a>PVC 描述的，则是 Pod 所希望使用的持久化存储的属性。</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# vim pvc-nfs.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc-nfs</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        storage: 100M</span><br><span class="line">        storageClassName: nfs1</span><br><span class="line"></span><br><span class="line">[root@m3 test]# kubectl apply -f pvc-nfs.yaml </span><br><span class="line">persistentvolumeclaim&#x2F;pvc-nfs created</span><br></pre></td></tr></table></figure><h4 id="PV组件"><a href="#PV组件" class="headerlink" title="PV组件"></a>PV组件</h4><ul><li>Volume Controller：专门处理持久化存储的控制器</li><li>PersistentVolumeController ：处理pv和pvc<br>PersistentVolumeController 会不断地查看当前每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与这个“单身”的 PVC 进行绑定。这样，Kubernetes 就可以保证用户提交的每一个 PVC，只要有合适的 PV 出现，它就能够很快进入绑定状态</li></ul><h4 id="PVC和PV绑定条件："><a href="#PVC和PV绑定条件：" class="headerlink" title="PVC和PV绑定条件："></a>PVC和PV绑定条件：</h4><ul><li>storageClassName 字段一致</li><li>PV 满足 PVC 的 spec 字段</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 test]# kubectl get pvc</span><br><span class="line">NAME      STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc-nfs   Bound    pvc-nfs   1Gi        RWX            nfs1           2m34s</span><br><span class="line"></span><br><span class="line">[root@m3 test]# kubectl get pv</span><br><span class="line">NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   REASON   AGE</span><br><span class="line">pv-nfs   1Gi        RWX            Retain           Bound    default&#x2F;pvc-nfs   nfs1                    6m26s</span><br></pre></td></tr></table></figure><h3 id="pvc资源的使用"><a href="#pvc资源的使用" class="headerlink" title="pvc资源的使用"></a>pvc资源的使用</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 test]# vim pvc-nfs-pod.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc-nfs-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line"></span><br><span class="line">  - name: web</span><br><span class="line">    image: nginx:1.7.9</span><br><span class="line">    volumeMounts:</span><br><span class="line">      storageClassName</span><br><span class="line">      - name: nfs</span><br><span class="line">        mountPath: &quot;&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&quot;</span><br><span class="line">  volumes:</span><br><span class="line">  - name: nfs</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: nfs1</span><br><span class="line"></span><br><span class="line">[root@m3 test]# kubectl apply -f pvc-nfs-pod.yaml</span><br><span class="line">pod&#x2F;pvc-nfs-pod created</span><br><span class="line"></span><br><span class="line">访问nginx进行验证</span><br><span class="line">[root@m3 test]# kubectl get pod -o wide | grep pvc-nfs-pod</span><br><span class="line">pvc-nfs-pod                  1&#x2F;1     Running   0          3m13s   10.100.130.101   s3     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">[root@m3 test]# curl http:&#x2F;&#x2F;10.100.130.101</span><br><span class="line">hello nfs</span><br></pre></td></tr></table></figure><h2 id="动态持久化存储StorageClass"><a href="#动态持久化存储StorageClass" class="headerlink" title="动态持久化存储StorageClass"></a>动态持久化存储StorageClass</h2><p>我们学习了 PV 和 PVC 的使用方法，但是前面的 PV 都是静态的，什么意思？就是我要使用的一个 PVC 的话就必须手动去创建一个 PV，我们也说过这种方式在很大程度上并不能满足我们的需求，比如我们有一个应用需要对存储的并发度要求比较高，而另外一个应用对读写速度又要求比较高，特别是对于 StatefulSet 类型的应用简单的来使用静态的 PV 就很不合适了，这种情况下我们就需要用到动态 PV，也就是我们今天要讲解的 StorageClass。</p><p>而 StorageClass 对象的作用，其实就是创建 PV 的模板。具体地说，StorageClass 对象会定义如下两个部分内容：<br>第一，PV 的属性。比如，存储类型、Volume 的大小等等。<br>第二，创建这种 PV 需要用到的存储插件。比如，Ceph 等等。</p><p><strong>nfs创建pv并自动绑定pvc</strong></p><h3 id="1-创建ServiceAccount账号"><a href="#1-创建ServiceAccount账号" class="headerlink" title="1.创建ServiceAccount账号"></a>1.创建ServiceAccount账号</h3><p>创建账号，nfs-client-provisioner服务启动时需要以此账号的权限启动和运行</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 nfs]# cat nfs-client-provisioner.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: eip-nfs-client-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line">[root@m3 nfs]# kubectl apply -f nfs-client-provisioner.yaml </span><br><span class="line">serviceaccount&#x2F;nfs-client-provisioner created</span><br></pre></td></tr></table></figure><h3 id="2-创建集群role角色"><a href="#2-创建集群role角色" class="headerlink" title="2.创建集群role角色"></a>2.创建集群role角色</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 nfs]# cat nfs-client-provisioner-runner.yaml    </span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &#39;&#39;</span><br><span class="line">    resources:</span><br><span class="line">      - persistentvolumes</span><br><span class="line">    verbs: </span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">      - create</span><br><span class="line">      - delete</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &#39;&#39;</span><br><span class="line">    resources:</span><br><span class="line">      - persistentvolumeclaims</span><br><span class="line">    verbs: [ get, list, watch, update ]</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - storage.k8s.io</span><br><span class="line">    resources:</span><br><span class="line">      - storageclasses</span><br><span class="line">    verbs: [ get, list, watch ]</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &#39;&#39;</span><br><span class="line">    resources:</span><br><span class="line">      - events</span><br><span class="line">    verbs: [ create, update, patch ]</span><br><span class="line"></span><br><span class="line">[root@m3 nfs]# kubectl apply -f nfs-client-provisioner-runner.yaml </span><br><span class="line">clusterrole.rbac.authorization.k8s.io&#x2F;nfs-client-provisioner-runner created</span><br></pre></td></tr></table></figure><h3 id="3-使role角色和ServiceAccount账号绑定"><a href="#3-使role角色和ServiceAccount账号绑定" class="headerlink" title="3.使role角色和ServiceAccount账号绑定"></a>3.使role角色和ServiceAccount账号绑定</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 nfs]# cat run-nfs-client-provisioner.yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: run-nfs-client-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line">    namespace: kube-system</span><br><span class="line"></span><br><span class="line">[root@m3 nfs]# kubectl apply -f run-nfs-client-provisioner.yaml</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;run-nfs-client-provisioner created</span><br></pre></td></tr></table></figure><h3 id="4-创建role角色，赋予权限"><a href="#4-创建role角色，赋予权限" class="headerlink" title="4.创建role角色，赋予权限"></a>4.创建role角色，赋予权限</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 ~]# cat leader-locking-nfs-client-provisioner.yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">    - &#39;&#39;</span><br><span class="line">      resources:</span><br><span class="line">      - endpoints</span><br><span class="line">        verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">      - create</span><br><span class="line">      - update</span><br><span class="line">      - patch</span><br><span class="line"></span><br><span class="line">[root@m3 ~]# kubectl apply -f leader-locking-nfs-client-provisioner.yaml</span><br><span class="line">role.rbac.authorization.k8s.io&#x2F;leader-locking-nfs-client-provisioner created</span><br></pre></td></tr></table></figure><h3 id="5-将role角色和ServiceAccount绑定"><a href="#5-将role角色和ServiceAccount绑定" class="headerlink" title="5.将role角色和ServiceAccount绑定"></a>5.将role角色和ServiceAccount绑定</h3>   <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 ~]# cat leader-locking-nfs-client-provisioner-bind.yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line">    namespace: kube-system</span><br><span class="line"></span><br><span class="line">[root@m3 ~]# kubectl apply -f leader-locking-nfs-client-provisioner-bind.yaml </span><br><span class="line">rolebinding.rbac.authorization.k8s.io&#x2F;leader-locking-nfs-client-provisioner created</span><br></pre></td></tr></table></figure><p>​    </p><h3 id="6-创建nfs-工作端"><a href="#6-创建nfs-工作端" class="headerlink" title="6.创建nfs 工作端"></a>6.创建nfs 工作端</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 ~]# vim nfs-nfs3.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nfs-nfs3</span><br><span class="line">  name: nfs-nfs3</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nfs-nfs3</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nfs-nfs3</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - env:</span><br><span class="line">            - name: PROVISIONER_NAME</span><br><span class="line">              value: nfs-nfs3</span><br><span class="line">            - name: NFS_SERVER</span><br><span class="line">              value: 192.168.70.120</span><br><span class="line">            - name: NFS_PATH</span><br><span class="line">              value: &#x2F;data&#x2F;nfs&#x2F;v3</span><br><span class="line">          image: &#39;quay.io&#x2F;external_storage&#x2F;nfs-client-provisioner:v3.1.0-k8s1.11&#39;</span><br><span class="line">          name: nfs-client-provisioner</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - mountPath: &#x2F;persistentvolumes</span><br><span class="line">              name: nfs-client-root</span><br><span class="line">      serviceAccountName: eip-nfs-client-provisioner</span><br><span class="line">      volumes:</span><br><span class="line">        - name: nfs-client-root</span><br><span class="line">          nfs:</span><br><span class="line">            path: &#x2F;data&#x2F;nfs&#x2F;v3</span><br><span class="line">            server: 192.168.70.120</span><br><span class="line"></span><br><span class="line">[root@m3 ~]# kubectl apply -f nfs-nfs3.yaml </span><br><span class="line">deployment.apps&#x2F;nfs-nfs3 created</span><br></pre></td></tr></table></figure><h3 id="7-创建nfs-StorageClass"><a href="#7-创建nfs-StorageClass" class="headerlink" title="7.创建nfs StorageClass"></a>7.创建nfs StorageClass</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 nfs]# vim nfs-StorageClass.yaml</span><br><span class="line">apiVersion: storage.k8s.io&#x2F;v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    k8s.eip.work&#x2F;storageType: nfs_client_provisioner</span><br><span class="line">  name: nfs3</span><br><span class="line">parameters:</span><br><span class="line">  archiveOnDelete: &#39;false&#39;</span><br><span class="line">provisioner: nfs-nfs3</span><br><span class="line">reclaimPolicy: Delete</span><br><span class="line">volumeBindingMode: Immediate</span><br><span class="line"></span><br><span class="line">[root@m3 nfs]# kubectl apply -f nfs-StorageClass.yaml </span><br><span class="line">storageclass.storage.k8s.io&#x2F;nfs3 created</span><br><span class="line">[root@m3 nfs]# kubectl get pod -n kube-system | grep nfs3</span><br><span class="line">nfs-nfs3-66f4c9bcd5-5fd2x                 1&#x2F;1     Running   0          3m21s</span><br></pre></td></tr></table></figure><ul><li>reclaimPolicy回收策略: Delete立刻删除  Retain pvc删除后保留磁盘</li><li>volumeBindingMode绑定策略: Immediate立即绑定，WaitForFirstConsumer第一次使用时绑定</li></ul><h3 id="8-进行nfs测试"><a href="#8-进行nfs测试" class="headerlink" title="8.进行nfs测试"></a>8.进行nfs测试</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 nfs]# cat nfs-pvc.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc-nfs</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 2M</span><br><span class="line">  storageClassName: nfs3</span><br><span class="line">[root@m3 nfs]# kubectl apply -f nfs-pvc.yaml </span><br><span class="line">persistentvolumeclaim&#x2F;pvc-nfs created</span><br><span class="line"></span><br><span class="line">[root@m3 ~]# kubectl get pvc | grep pvc-nfs</span><br><span class="line">pvc-nfs              Bound    pvc-4773b235-aa7c-4c8e-bf81-85c97e0e28f6   2M         RWX            nfs3           78s</span><br><span class="line">[root@m3 ~]# kubectl get pv | grep pvc-nfs</span><br><span class="line">pvc-4773b235-aa7c-4c8e-bf81-85c97e0e28f6   2M         RWX            Delete           Bound    default&#x2F;pvc-nfs              nfs3                    88s</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>k8s资源存储pv和pvc</title>
      <link href="/2020/08/04/k8s%E8%B5%84%E6%BA%90%E5%AD%98%E5%82%A8pv%E5%92%8Cpvc/"/>
      <url>/2020/08/04/k8s%E8%B5%84%E6%BA%90%E5%AD%98%E5%82%A8pv%E5%92%8Cpvc/</url>
      
        <content type="html"><![CDATA[<h2 id="资源存储PV，PVC简介"><a href="#资源存储PV，PVC简介" class="headerlink" title="资源存储PV，PVC简介"></a>资源存储PV，PVC简介</h2><h3 id="中间件"><a href="#中间件" class="headerlink" title="中间件"></a>中间件</h3><p><strong>所有的中间件都是为了解耦</strong></p><p>中间件（英语：Middleware），又译中间件、中介层，是提供系统软件和应用软件之间连接的软件，以便于软件各部件之间的沟通。在现代信息技术应用框架如 Web 服务、面向服务的体系结构等项目中应用比较广泛。如数据库、Apache 的 Tomcat 等都属于中间件。</p><h3 id="PV和PVC介绍"><a href="#PV和PVC介绍" class="headerlink" title="PV和PVC介绍"></a>PV和PVC介绍</h3><p>　　管理存储是管理计算的一个明显问题。该PersistentVolume子系统为用户和管理员提供了一个API，用于抽象如何根据消费方式提供存储的详细信息。为此，我们引入了两个新的API资源：PersistentVolume（持久存储）和PersistentVolumeClaim（持久存储声明）<br>　　PersistentVolume（PV）是集群中由管理员配置的一段网络存储。 它是集群中的资源，就像节点是集群资源一样。 PV是容量插件，如Volumes，但其生命周期独立于使用PV的任何单个pod。 此API对象捕获存储实现的详细信息，包括NFS，iSCSI或特定于云提供程序的存储系统。<br>　　PersistentVolumeClaim（PVC）是由用户进行存储的请求。 它类似于pod。 Pod消耗节点资源，PVC消耗PV资源。Pod可以请求特定级别的资源（CPU和内存）, PVC声明可以请求特定存储的大小和访问模式（例如，可以一次读/写或多次只读）。<br>　　虽然PersistentVolumeClaims（PVC）允许用户使用抽象存储资源，但是PersistentVolumes对于不同的问题，用户通常需要具有不同属性（例如性能）。群集管理员需要能够提供各种PersistentVolumes不同的方式，比如存储大小和访问模式，但不会让用户了解这些卷的实现方式。对于这些需求，有StorageClass 资源。<br>　　StorageClass为管理员提供了一种描述他们提供的存储的“类”的方法。 不同的类可能映射到服务质量级别，或备份策略，或者由群集管理员确定的任意策略。 Kubernetes本身对于什么类别代表是不言而喻的。 这个概念有时在其他存储系统中称为“配置文件”。<br>　　<strong>PVC和PV是一一对应的</strong>。</p><h3 id="生命周期"><a href="#生命周期" class="headerlink" title="生命周期"></a>生命周期</h3><p>　　PV是集群中的资源。PVC是对这些资源的请求，并且还充当对资源的检查。PV和PVC之间的相互作用遵循以下生命周期：<br>Provisioning ——-&gt; Binding ——–&gt;Using——&gt;Releasing——&gt;Recycling</p><ol><li><p>供应准备Provisioning—通过集群外的存储系统或者云平台来提供存储持久化支持。</p><ul><li><p>静态提供Static：集群管理员创建多个PV。 它们携带可供集群用户使用的真实存储的详细信息。 它们存在于Kubernetes API中，可用于消费</p></li><li><p>动态提供Dynamic：当管理员创建的静态PV都不匹配用户的PersistentVolumeClaim时，集群可能会尝试为PVC创建动态配置卷。 此配置基于StorageClasses：PVC必须请求一个类，并且管理员必须已创建并配置该类才能进行动态配置。 要求该类的声明有效地为自己禁用动态配置。</p></li></ul></li><li><p>绑定Binding—用户创建pvc并指定需要的资源和访问模式。在找到可用pv之前，pvc会保持未绑定状态。</p></li><li><p>使用Using—用户可在pod中像volume一样使用pvc。</p></li><li><p>释放Releasing—用户删除pvc来回收存储资源，pv将变成“released”状态。由于还保留着之前的数据，这些数据需要根据不同的策略来处理，否则这些存储资源无法被其他pvc使用。</p></li><li><p>回收Recycling—pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。</p></li></ol><p><strong>回收策略</strong></p><ul><li>保留策略：允许人工处理保留的数据。</li><li>删除策略：将删除pv和外部关联的存储资源，需要插件支持。</li><li>回收策略：将执行清除操作，之后可以被新的pvc使用，需要插件支持。<br>注：目前（2018年10月）只有NFS和HostPath类型卷支持回收策略，AWS EBS,GCE PD,Azure Disk和Cinder支持删除(Delete)策略。</li></ul><h3 id="挂载磁盘分两步"><a href="#挂载磁盘分两步" class="headerlink" title="挂载磁盘分两步"></a>挂载磁盘分两步</h3><ol><li>Attach，为虚拟机挂载远程磁盘的操作。kubelet 为 Volume 创建目录，并绑定远程存储。</li><li>Mount，磁盘设备格式化并挂载到 Volume 宿主机目录的操作</li></ol><p>如果你的 Volume 类型是远程文件存储（比如 NFS）的话，kubelet跳过“第一阶段”（Attach）的操作</p><h3 id="PV卷阶段状态"><a href="#PV卷阶段状态" class="headerlink" title="PV卷阶段状态"></a>PV卷阶段状态</h3><ul><li>Available – 资源尚未被claim使用</li><li>Bound – 卷已经被绑定到claim了</li><li>Released – claim被删除，卷处于释放状态，但未被集群回收。</li><li>Failed – 卷自动回收失败</li></ul><h2 id="静态PV，PVC创建和使用"><a href="#静态PV，PVC创建和使用" class="headerlink" title="静态PV，PVC创建和使用"></a>静态PV，PVC创建和使用</h2><h3 id="创建pv存储，实际的存储资源"><a href="#创建pv存储，实际的存储资源" class="headerlink" title="创建pv存储，实际的存储资源"></a>创建pv存储，实际的存储资源</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# vim pv-nfs.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">name: pv-nfs</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: nfs1</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 1Gi</span><br><span class="line">    accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">      nfs:</span><br><span class="line">        server: 192.168.2.2</span><br><span class="line">        path: &quot;&#x2F;data&#x2F;nfs&#x2F;v1&quot;</span><br><span class="line"></span><br><span class="line">[root@wang-200 test]# kubectl apply -f pv-nfs.yaml  </span><br><span class="line">persistentvolume&#x2F;pv-nfs created</span><br><span class="line"></span><br><span class="line">[root@m3 test]# kubectl get pv</span><br><span class="line">NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE</span><br><span class="line">pv-nfs   1Gi        RWX            Retain           Available           nfs1                    41s</span><br></pre></td></tr></table></figure><h3 id="PVC-描述的，则是-Pod-所希望使用的持久化存储的属性。"><a href="#PVC-描述的，则是-Pod-所希望使用的持久化存储的属性。" class="headerlink" title="PVC 描述的，则是 Pod 所希望使用的持久化存储的属性。"></a>PVC 描述的，则是 Pod 所希望使用的持久化存储的属性。</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# vim pvc-nfs.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc-nfs</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        storage: 100M</span><br><span class="line">        storageClassName: nfs1</span><br><span class="line"></span><br><span class="line">[root@m3 test]# kubectl apply -f pvc-nfs.yaml </span><br><span class="line">persistentvolumeclaim&#x2F;pvc-nfs created</span><br></pre></td></tr></table></figure><h4 id="PV组件"><a href="#PV组件" class="headerlink" title="PV组件"></a>PV组件</h4><ul><li>Volume Controller：专门处理持久化存储的控制器</li><li>PersistentVolumeController ：处理pv和pvc<br>PersistentVolumeController 会不断地查看当前每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与这个“单身”的 PVC 进行绑定。这样，Kubernetes 就可以保证用户提交的每一个 PVC，只要有合适的 PV 出现，它就能够很快进入绑定状态</li></ul><h4 id="PVC和PV绑定条件："><a href="#PVC和PV绑定条件：" class="headerlink" title="PVC和PV绑定条件："></a>PVC和PV绑定条件：</h4><ul><li>storageClassName 字段一致</li><li>PV 满足 PVC 的 spec 字段</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 test]# kubectl get pvc</span><br><span class="line">NAME      STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc-nfs   Bound    pvc-nfs   1Gi        RWX            nfs1           2m34s</span><br><span class="line"></span><br><span class="line">[root@m3 test]# kubectl get pv</span><br><span class="line">NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   REASON   AGE</span><br><span class="line">pv-nfs   1Gi        RWX            Retain           Bound    default&#x2F;pvc-nfs   nfs1                    6m26s</span><br></pre></td></tr></table></figure><h3 id="pvc资源的使用"><a href="#pvc资源的使用" class="headerlink" title="pvc资源的使用"></a>pvc资源的使用</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 test]# vim pvc-nfs-pod.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc-nfs-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line"></span><br><span class="line">  - name: web</span><br><span class="line">    image: nginx:1.7.9</span><br><span class="line">    volumeMounts:</span><br><span class="line">      storageClassName</span><br><span class="line">      - name: nfs</span><br><span class="line">        mountPath: &quot;&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&quot;</span><br><span class="line">  volumes:</span><br><span class="line">  - name: nfs</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: nfs1</span><br><span class="line"></span><br><span class="line">[root@m3 test]# kubectl apply -f pvc-nfs-pod.yaml</span><br><span class="line">pod&#x2F;pvc-nfs-pod created</span><br><span class="line"></span><br><span class="line">访问nginx进行验证</span><br><span class="line">[root@m3 test]# kubectl get pod -o wide | grep pvc-nfs-pod</span><br><span class="line">pvc-nfs-pod                  1&#x2F;1     Running   0          3m13s   10.100.130.101   s3     &lt;none&gt;           &lt;none&gt;</span><br><span class="line">[root@m3 test]# curl http:&#x2F;&#x2F;10.100.130.101</span><br><span class="line">hello nfs</span><br></pre></td></tr></table></figure><h2 id="动态持久化存储StorageClass"><a href="#动态持久化存储StorageClass" class="headerlink" title="动态持久化存储StorageClass"></a>动态持久化存储StorageClass</h2><p>我们学习了 PV 和 PVC 的使用方法，但是前面的 PV 都是静态的，什么意思？就是我要使用的一个 PVC 的话就必须手动去创建一个 PV，我们也说过这种方式在很大程度上并不能满足我们的需求，比如我们有一个应用需要对存储的并发度要求比较高，而另外一个应用对读写速度又要求比较高，特别是对于 StatefulSet 类型的应用简单的来使用静态的 PV 就很不合适了，这种情况下我们就需要用到动态 PV，也就是我们今天要讲解的 StorageClass。</p><p>而 StorageClass 对象的作用，其实就是创建 PV 的模板。具体地说，StorageClass 对象会定义如下两个部分内容：<br>第一，PV 的属性。比如，存储类型、Volume 的大小等等。<br>第二，创建这种 PV 需要用到的存储插件。比如，Ceph 等等。</p><p><strong>nfs创建pv并自动绑定pvc</strong></p><h3 id="1-创建ServiceAccount账号"><a href="#1-创建ServiceAccount账号" class="headerlink" title="1.创建ServiceAccount账号"></a>1.创建ServiceAccount账号</h3><p>创建账号，nfs-client-provisioner服务启动时需要以此账号的权限启动和运行</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 nfs]# cat nfs-client-provisioner.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: eip-nfs-client-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line">[root@m3 nfs]# kubectl apply -f nfs-client-provisioner.yaml </span><br><span class="line">serviceaccount&#x2F;nfs-client-provisioner created</span><br></pre></td></tr></table></figure><h3 id="2-创建集群role角色"><a href="#2-创建集群role角色" class="headerlink" title="2.创建集群role角色"></a>2.创建集群role角色</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 nfs]# cat nfs-client-provisioner-runner.yaml    </span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &#39;&#39;</span><br><span class="line">    resources:</span><br><span class="line">      - persistentvolumes</span><br><span class="line">    verbs: </span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">      - create</span><br><span class="line">      - delete</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &#39;&#39;</span><br><span class="line">    resources:</span><br><span class="line">      - persistentvolumeclaims</span><br><span class="line">    verbs: [ get, list, watch, update ]</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - storage.k8s.io</span><br><span class="line">    resources:</span><br><span class="line">      - storageclasses</span><br><span class="line">    verbs: [ get, list, watch ]</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &#39;&#39;</span><br><span class="line">    resources:</span><br><span class="line">      - events</span><br><span class="line">    verbs: [ create, update, patch ]</span><br><span class="line"></span><br><span class="line">[root@m3 nfs]# kubectl apply -f nfs-client-provisioner-runner.yaml </span><br><span class="line">clusterrole.rbac.authorization.k8s.io&#x2F;nfs-client-provisioner-runner created</span><br></pre></td></tr></table></figure><h3 id="3-使role角色和ServiceAccount账号绑定"><a href="#3-使role角色和ServiceAccount账号绑定" class="headerlink" title="3.使role角色和ServiceAccount账号绑定"></a>3.使role角色和ServiceAccount账号绑定</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 nfs]# cat run-nfs-client-provisioner.yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: run-nfs-client-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line">    namespace: kube-system</span><br><span class="line"></span><br><span class="line">[root@m3 nfs]# kubectl apply -f run-nfs-client-provisioner.yaml</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;run-nfs-client-provisioner created</span><br></pre></td></tr></table></figure><h3 id="4-创建role角色，赋予权限"><a href="#4-创建role角色，赋予权限" class="headerlink" title="4.创建role角色，赋予权限"></a>4.创建role角色，赋予权限</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 ~]# cat leader-locking-nfs-client-provisioner.yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">    - &#39;&#39;</span><br><span class="line">      resources:</span><br><span class="line">      - endpoints</span><br><span class="line">        verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">      - create</span><br><span class="line">      - update</span><br><span class="line">      - patch</span><br><span class="line"></span><br><span class="line">[root@m3 ~]# kubectl apply -f leader-locking-nfs-client-provisioner.yaml</span><br><span class="line">role.rbac.authorization.k8s.io&#x2F;leader-locking-nfs-client-provisioner created</span><br></pre></td></tr></table></figure><h3 id="5-将role角色和ServiceAccount绑定"><a href="#5-将role角色和ServiceAccount绑定" class="headerlink" title="5.将role角色和ServiceAccount绑定"></a>5.将role角色和ServiceAccount绑定</h3>   <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 ~]# cat leader-locking-nfs-client-provisioner-bind.yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line">    namespace: kube-system</span><br><span class="line"></span><br><span class="line">[root@m3 ~]# kubectl apply -f leader-locking-nfs-client-provisioner-bind.yaml </span><br><span class="line">rolebinding.rbac.authorization.k8s.io&#x2F;leader-locking-nfs-client-provisioner created</span><br></pre></td></tr></table></figure><p>​    </p><h3 id="6-创建nfs-工作端"><a href="#6-创建nfs-工作端" class="headerlink" title="6.创建nfs 工作端"></a>6.创建nfs 工作端</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 ~]# vim nfs-nfs3.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nfs-nfs3</span><br><span class="line">  name: nfs-nfs3</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nfs-nfs3</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nfs-nfs3</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - env:</span><br><span class="line">            - name: PROVISIONER_NAME</span><br><span class="line">              value: nfs-nfs3</span><br><span class="line">            - name: NFS_SERVER</span><br><span class="line">              value: 192.168.70.200</span><br><span class="line">            - name: NFS_PATH</span><br><span class="line">              value: &#x2F;data&#x2F;nfs&#x2F;v3</span><br><span class="line">          image: &#39;quay.io&#x2F;external_storage&#x2F;nfs-client-provisioner:v3.1.0-k8s1.11&#39;</span><br><span class="line">          name: nfs-client-provisioner</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - mountPath: &#x2F;persistentvolumes</span><br><span class="line">              name: nfs-client-root</span><br><span class="line">      serviceAccountName: eip-nfs-client-provisioner</span><br><span class="line">      volumes:</span><br><span class="line">        - name: nfs-client-root</span><br><span class="line">          nfs:</span><br><span class="line">            path: &#x2F;data&#x2F;nfs&#x2F;test</span><br><span class="line">            server: 192.168.70.200</span><br><span class="line"></span><br><span class="line">[root@m3 ~]# kubectl apply -f nfs-nfs3.yaml </span><br><span class="line">deployment.apps&#x2F;nfs-nfs3 created</span><br></pre></td></tr></table></figure><h3 id="7-创建nfs-StorageClass"><a href="#7-创建nfs-StorageClass" class="headerlink" title="7.创建nfs StorageClass"></a>7.创建nfs StorageClass</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 nfs]# vim nfs-StorageClass.yaml</span><br><span class="line">apiVersion: storage.k8s.io&#x2F;v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    k8s.eip.work&#x2F;storageType: nfs_client_provisioner</span><br><span class="line">  name: nfs3</span><br><span class="line">parameters:</span><br><span class="line">  archiveOnDelete: &#39;false&#39;</span><br><span class="line">provisioner: nfs-nfs3</span><br><span class="line">reclaimPolicy: Delete</span><br><span class="line">volumeBindingMode: Immediate</span><br><span class="line"></span><br><span class="line">[root@m3 nfs]# kubectl apply -f nfs-StorageClass.yaml </span><br><span class="line">storageclass.storage.k8s.io&#x2F;nfs3 created</span><br><span class="line">[root@m3 nfs]# kubectl get pod -n kube-system | grep nfs3</span><br><span class="line">nfs-nfs3-66f4c9bcd5-5fd2x                 1&#x2F;1     Running   0          3m21s</span><br></pre></td></tr></table></figure><ul><li>reclaimPolicy回收策略: Delete立刻删除  Retain pvc删除后保留磁盘</li><li>volumeBindingMode绑定策略: Immediate立即绑定，WaitForFirstConsumer第一次使用时绑定</li></ul><h3 id="8-进行nfs测试"><a href="#8-进行nfs测试" class="headerlink" title="8.进行nfs测试"></a>8.进行nfs测试</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@m3 nfs]# cat nfs-pvc.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc-nfs</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 2M</span><br><span class="line">  storageClassName: nfs3</span><br><span class="line"></span><br><span class="line">[root@m3 nfs]# kubectl apply -f nfs-pvc.yaml </span><br><span class="line">persistentvolumeclaim&#x2F;pvc-nfs created</span><br><span class="line"></span><br><span class="line">[root@m3 ~]# kubectl get pvc | grep pvc-nfs</span><br><span class="line">pvc-nfs              Bound    pvc-4773b235-aa7c-4c8e-bf81-85c97e0e28f6   2M         RWX            nfs3           78s</span><br><span class="line">[root@m3 ~]# kubectl get pv | grep pvc-nfs</span><br><span class="line">pvc-4773b235-aa7c-4c8e-bf81-85c97e0e28f6   2M         RWX            Delete           Bound    default&#x2F;pvc-nfs              nfs3                    88s</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 知识点 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 知识点 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>centos7系统日志介绍</title>
      <link href="/2020/07/31/centos7%E7%B3%BB%E7%BB%9F%E6%97%A5%E5%BF%97%E4%BB%8B%E7%BB%8D/"/>
      <url>/2020/07/31/centos7%E7%B3%BB%E7%BB%9F%E6%97%A5%E5%BF%97%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<h2 id="Linux系统日志分类"><a href="#Linux系统日志分类" class="headerlink" title="Linux系统日志分类"></a>Linux系统日志分类</h2><p>CentOS系统中有两个日志服务，分别是传统的 rsyslog 和 systemd-journal</p><h2 id="rsyslog"><a href="#rsyslog" class="headerlink" title="rsyslog"></a>rsyslog</h2><p>rsyslog作为传统的系统日志服务，把所有收集到的日志都记录到/var/log/目录下的各个日志文件中。</p><h3 id="常见的日志文件"><a href="#常见的日志文件" class="headerlink" title="常见的日志文件"></a>常见的日志文件</h3><ul><li>/var/log/messages 绝大多数的系统日志都记录到该文件</li><li>/var/log/secure 所有跟安全和认证授权等日志都会记录到此文件</li><li>/var/log/maillog 邮件服务的日志</li><li>/var/log/cron crond计划任务的日志</li><li>/var/log/boot.log 系统启动的相关日志</li></ul><h3 id="rsyslog配置"><a href="#rsyslog配置" class="headerlink" title="rsyslog配置"></a>rsyslog配置</h3><p><strong>启动rsyslog服务</strong></p><p><code>systemctl restart rsyslog.service</code></p><p><strong>rsyslog配置文件</strong> </p><p>/etc/rsyslog.conf</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# grep -vE &quot;^$|^#&quot; &#x2F;etc&#x2F;rsyslog.conf </span><br><span class="line">$ModLoad imuxsock # provides support for local system logging (e.g. via logger command)</span><br><span class="line">$ModLoad imjournal # provides access to the systemd journal</span><br><span class="line">$WorkDirectory &#x2F;var&#x2F;lib&#x2F;rsyslog</span><br><span class="line">$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat</span><br><span class="line">$IncludeConfig &#x2F;etc&#x2F;rsyslog.d&#x2F;*.conf</span><br><span class="line">$OmitLocalLogging on</span><br><span class="line">$IMJournalStateFile imjournal.state</span><br><span class="line">*.info;mail.none;authpriv.none;cron.none                &#x2F;var&#x2F;log&#x2F;messages</span><br><span class="line">authpriv.*                                              &#x2F;var&#x2F;log&#x2F;secure</span><br><span class="line">mail.*                                                  -&#x2F;var&#x2F;log&#x2F;maillog</span><br><span class="line">cron.*                                                  &#x2F;var&#x2F;log&#x2F;cron</span><br><span class="line">*.emerg                                                 :omusrmsg:*</span><br><span class="line">uucp,news.crit                                          &#x2F;var&#x2F;log&#x2F;spooler</span><br><span class="line">local7.*                                                &#x2F;var&#x2F;log&#x2F;boot.log</span><br></pre></td></tr></table></figure><p><strong>日志输入规则：分类.级别　　存放的绝对路径。</strong></p><p>其中级别还有单独规则，如果.级别则是指记录高于等于某个级别的日志（严重性高于等于，编码低于等于）；如果.=级别则是指记录等于某个级别的日志；如果.!级别则是指除某个级别外全部记录；如果.none则是指排除某个类别。</p><p>　　注意：-/var/log/maillog这里的减号是由于以前的邮件比较多，先将数据存储到内存中，达到一定大小再全部写入硬盘，减少I/O的消耗。如果关机不当，数据会消失。</p><h3 id="日志的分类和级别"><a href="#日志的分类和级别" class="headerlink" title="日志的分类和级别"></a>日志的分类和级别</h3><h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><table><thead><tr><th>分类</th><th>说明</th></tr></thead><tbody><tr><td>daemon</td><td>后台进程相关的信息</td></tr><tr><td>kern</td><td>内核产生的信息</td></tr><tr><td>lpr</td><td>打印系统产生的信息</td></tr><tr><td>authpriv</td><td>安全认证信息</td></tr><tr><td>cron</td><td>定时计划任务相关的信息</td></tr><tr><td>mail</td><td>邮件相关的信息</td></tr><tr><td>syslog</td><td>日志服务本身的信息</td></tr><tr><td>news</td><td>新闻系统（过时）</td></tr><tr><td>local0~local7</td><td>8个系统保留的类，供其他程序使用或用户自定义</td></tr></tbody></table><h4 id="级别"><a href="#级别" class="headerlink" title="级别"></a>级别</h4><table><thead><tr><th>编码</th><th>优先级</th><th>严重性</th></tr></thead><tbody><tr><td>7</td><td>debug</td><td>信息对开发人员调试应用程序有用，在操作过程中没用</td></tr><tr><td>6</td><td>info</td><td>正常的操作信息，可以收集报告，测量吞吐量等</td></tr><tr><td>5</td><td>notice</td><td>注意，正常但重要的事件</td></tr><tr><td>4</td><td>warning</td><td>警告，如果不采取措施，将会发生错误，例如文件系统已使用90%</td></tr><tr><td>3</td><td>err</td><td>错误，阻止某个模块或程序的功能不能正常使用</td></tr><tr><td>2</td><td>crit</td><td>关键错误，已经影响到整个系统或软件不能正常工作</td></tr><tr><td>1</td><td>alert</td><td>警报，需要立即修改</td></tr><tr><td>0</td><td>emerg</td><td>紧急，内核崩溃等严重信息</td></tr></tbody></table><p>（编码越小，级别越高）</p><h3 id="自定义日志类型和存储位置"><a href="#自定义日志类型和存储位置" class="headerlink" title="自定义日志类型和存储位置"></a>自定义日志类型和存储位置</h3><p>　　这里需要用到local0~local7的自定义分类。我们以sshd为例。</p><p>　　首先我们需要使用vim打开sshd服务的配置文件，进行修改配置</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@xuexi ~]# vim &#x2F;etc&#x2F;ssh&#x2F;sshd_config</span><br><span class="line">#SyslogFacility AUTH</span><br><span class="line">SyslogFacility AUTHPRIV</span><br><span class="line">#LogLevel INFO</span><br></pre></td></tr></table></figure><p>　　然后找到如上位置，将其更改为我们需要的分类local0</p><p>　　修改后，如下所示</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#SyslogFacility AUTH</span><br><span class="line">SyslogFacility local0</span><br><span class="line">#SyslogFacility AUTHPRIV</span><br><span class="line">#LogLevel INFO</span><br></pre></td></tr></table></figure><p>　　接着我们需要使用vim打开rsyslog服务的配置文件，进行修改配置</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@xuexi ~]# vim &#x2F;etc&#x2F;rsyslog.conf</span><br><span class="line"></span><br><span class="line"># 在最后添加一行</span><br><span class="line">local0.*    &#x2F;var&#x2F;log&#x2F;sshd.log</span><br></pre></td></tr></table></figure><p>　　最后重启sshd和rsyslog两个服务</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@xuexi ~]# systemctl restart sshd</span><br><span class="line">[root@xuexi ~]# systemctl restart rsyslog.service</span><br></pre></td></tr></table></figure><p>　　注意：如果SELinux开着，请手动创建/var/log/sshd.log文件。</p><h2 id="systemd-journal"><a href="#systemd-journal" class="headerlink" title="systemd-journal"></a>systemd-journal</h2><p>systemd-journald是一个改进型日志管理服务，可以收集来自内核、系统早期启动阶段的日志、系统守护进程在启动和运行中的标准输出和错误信息，还有syslog的日志。</p><p>该日志服务仅仅把日志集中保存在单一结构的日志文件/run/log中，由于日志是经历过压缩和格式化的二进制数据，所以在查看和定位的时候很迅速。</p><p>默认情况下并不会持久化保存日志，只会保留一个月的日志。另外，一些rsyslog无法收集的日志也会被journal记录到。</p><p><strong>重启 journalctl 服务</strong></p><p>systemctl restart systemd-journald.service</p><p><strong>配置文件</strong> </p><p>/etc/systemd/journald.conf</p><p><strong>检查journal是否运行正常以及日志文件是否完整无损坏</strong></p><p>journalctl –verify</p><h2 id="日志常见问题"><a href="#日志常见问题" class="headerlink" title="日志常见问题"></a>日志常见问题</h2><h3 id="日志maildrop清理"><a href="#日志maildrop清理" class="headerlink" title="日志maildrop清理"></a>日志maildrop清理</h3><p>由于 Linux 在执行 cron 时，会将 cron 执行脚本中的 output 和 warning 信息，都会以邮件的形式发送 cron 所有者， 而由于客户环境中的 sendmail 和 postfix 没有正常运行，导致邮件发送不成功，全部小文件堆积在了 maildrop 目录下面，而且没有自动清理转换的机制，所以此目录堆积了大量的文件。查看 man cron 的信息，可以知道会发送给 cron owner.</p><p>/var/spool/postfix/maildrop 占用inode索引，导致服务器i节点数量不足</p><p>因为小文件太多，ls直接卡死等情况，文件无法直接删除，所以</p><blockquote><p> ⚠️：一定要切换到指定路径，否则后果很严重，当前数据被删除</p></blockquote><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;var&#x2F;spool&#x2F;postfix&#x2F;maildrop; </span><br><span class="line">find . -type f -exec rm -f &#123;&#125; \; # 方法一</span><br><span class="line">ls | xargs rm -f;   # 方法二</span><br></pre></td></tr></table></figure><h4 id="解决方法1"><a href="#解决方法1" class="headerlink" title="解决方法1"></a>解决方法1</h4><p>修改配置文件，取消邮件发送</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;crontab </span><br><span class="line">将 MAILTO&#x3D;root 替换成 MAILTO&#x3D;&quot;&quot;，</span><br><span class="line"></span><br><span class="line">service crond restart</span><br></pre></td></tr></table></figure><h4 id="解决方法2"><a href="#解决方法2" class="headerlink" title="解决方法2"></a>解决方法2</h4><p>​    如果是我们不关心的备注型等输出我们完全可以让其输出到 <code>/dev/null</code> ，这样就不会因为发送失败到导致在<code>/var/spool/postfix/maildrop</code>下面产出什么文件。</p><p>​    */10 * * * * /tmp/test.sh &gt;/dev/null 2&gt;&amp;1</p><h3 id="journalctl清理日志"><a href="#journalctl清理日志" class="headerlink" title="journalctl清理日志"></a>journalctl清理日志</h3><p><strong>查看垃圾文件的方法</strong></p><p>未清理前发现硬盘根分区空间告急，用 <code>du -t 100M /var</code>或 <code>journalctl --disk-usage</code>命令查看，发现/var/log/journal日志文件占用了近3G空间，每个日志文件体积高达8-128M，这些日志文件记录了很长时间以来的systemd情况，毫无价值，用<code>journalctl --vacuum-size=10M</code>命令将其清理之后，腾出了2.7G的空间。用<code>df</code>命令一查，／根分区果然宽敞了很多。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# journalctl --disk-usage </span><br><span class="line">Archived and active journals take up 192.0M on disk.</span><br></pre></td></tr></table></figure><h4 id="查看目录的文件大小并排序（单位为MB）"><a href="#查看目录的文件大小并排序（单位为MB）" class="headerlink" title="查看目录的文件大小并排序（单位为MB）"></a>查看目录的文件大小并排序（单位为MB）</h4><p>du -hm –max-depth=2 /var/ | sort -rn | head</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# du -hm --max-depth&#x3D;2 &#x2F;var&#x2F; | sort -rn | head</span><br><span class="line">4741    &#x2F;var&#x2F;</span><br><span class="line">4433    &#x2F;var&#x2F;log</span><br><span class="line">4105    &#x2F;var&#x2F;log&#x2F;journal</span><br><span class="line">195     &#x2F;var&#x2F;cache</span><br><span class="line">193     &#x2F;var&#x2F;cache&#x2F;yum</span><br><span class="line">112     &#x2F;var&#x2F;lib</span><br><span class="line">100     &#x2F;var&#x2F;lib&#x2F;rpm</span><br><span class="line">36      &#x2F;var&#x2F;log&#x2F;audit</span><br><span class="line">18      &#x2F;var&#x2F;log&#x2F;sa</span><br><span class="line">13      &#x2F;var&#x2F;lib&#x2F;yum</span><br></pre></td></tr></table></figure><h4 id="1、手动清理日志文件"><a href="#1、手动清理日志文件" class="headerlink" title="1、手动清理日志文件"></a>1、手动清理日志文件</h4><p>1）用echo命令，将空字符串内容重定向到指定文件中</p><p>echo “” &gt; system.journal</p><p>说明：此方法只会清空一次，一段时间后还要再次手动清空很麻烦，这里可以用以下命令让journalctl 自动维护空间</p><p> 2）直接删除 <strong>/var/log/journal/</strong> 目录下的日志文件</p><p>rm -rf /var/log/journal/f9d400c5e1e8c3a8209e990d887d4ac1</p><h4 id="2、journalctl-命令自动维护文件大小"><a href="#2、journalctl-命令自动维护文件大小" class="headerlink" title="2、journalctl 命令自动维护文件大小"></a>2、journalctl 命令自动维护文件大小</h4><p>1）只保留近一周的日志</p><p>journalctl –vacuum-time=1w</p><p> 2）只保留500MB的日志</p><p>journalctl –vacuum-size=500M</p><p> <strong>命令执行示例</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test journal]# journalctl --vacuum-time&#x3D;1w</span><br><span class="line">Deleted archived journal &#x2F;var&#x2F;log&#x2F;journal&#x2F;20190215172108590907433256076310&#x2F;user-1001@3485b7786e7b412c828c12638f75f7a9-0000000001eab2be-0005a853f7cd0f9f.journal (8.0M).</span><br><span class="line">Deleted archived journal &#x2F;var&#x2F;log&#x2F;journal&#x2F;20190215172108590907433256076310&#x2F;system@00000000000000000000000000000000-0000000001eb196c-0005a85a7d0fa36d.journal (56.0M).</span><br><span class="line">...</span><br><span class="line">&#x2F;var&#x2F;log&#x2F;journal&#x2F;20190215172108590907433256076310&#x2F;system@00000000000000000000000000000000-000000000218e63f-0005ab2389724c69.journal (56.0M).</span><br><span class="line">Vacuuming done, freed 3.3G of archived journals on disk.</span><br></pre></td></tr></table></figure><h2 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h2><h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h3><ul><li><a href="https://www.cnblogs.com/diantong/p/10706418.html" target="_blank" rel="noopener">CentOS日志的简单介绍</a></li><li><a href="https://blog.csdn.net/ithomer/article/details/89530790" target="_blank" rel="noopener">Linux 系统 /var/log/journal/ 垃圾日志清理</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> system </category>
          
      </categories>
      
      
        <tags>
            
            <tag> system </tag>
            
            <tag> log </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dockerfile的使用</title>
      <link href="/2020/07/30/dockerfile%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
      <url>/2020/07/30/dockerfile%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="dockerfile常用参数"><a href="#dockerfile常用参数" class="headerlink" title="dockerfile常用参数"></a>dockerfile常用参数</h2><h3 id="FROM-指定基础镜像"><a href="#FROM-指定基础镜像" class="headerlink" title="FROM 指定基础镜像"></a>FROM 指定基础镜像</h3><p>功能为指定基础镜像，并且必须是第一条指令。</p><p>如果不以任何镜像为基础，那么写法为：FROM scratch。</p><p>同时意味着接下来所写的指令将作为镜像的第一层开始</p><h3 id="LABEL-添加标签"><a href="#LABEL-添加标签" class="headerlink" title="LABEL 添加标签"></a>LABEL 添加标签</h3><p>添加标签来帮助组织镜像、记录许可信息、辅助自动化构建等。每个标签一行，由 LABEL 开头加上一个或多个标签对。</p><p>LABEL com.example.version=”0.0.1-beta”</p><h3 id="ARG-设置变量"><a href="#ARG-设置变量" class="headerlink" title="ARG 设置变量"></a>ARG 设置变量</h3><p>语法：<strong>ARG [=]</strong></p><p>设置变量命令，ARG命令定义了一个变量，在docker build创建镜像的时候，使用 –build-arg <varname>=<value>来指定参数</p><p>如果用户在build镜像时指定了一个参数没有定义在Dockerfile种，那么将有一个Warning</p><p>提示如下：</p><p><strong>[Warning] One or more build-args [foo] were not consumed.</strong></p><p>Dockerfile作者可以指定ARG一次定义一个变量，或者指定ARG多次定义多个变量。</p><p>Dockerfile作者也可以为这个变量指定一个默认值：</p><h3 id="ENV-设置环境变量"><a href="#ENV-设置环境变量" class="headerlink" title="ENV 设置环境变量"></a>ENV 设置环境变量</h3><p>设置环境变量，定义了环境变量，那么在后续的指令中，就可以使用这个环境变量。</p><p>格式：</p><ul><li>ENV <key> <value> ENV <key1>=<value1> <key2>=<value2>…</li></ul><p>ENV PROJECT_NAME=${PROJECT_NAME} </p><h3 id="WORKDIR-设置工作目录"><a href="#WORKDIR-设置工作目录" class="headerlink" title="WORKDIR 设置工作目录"></a>WORKDIR 设置工作目录</h3><p>语法： <strong>WORKDIR /path/to/workdir</strong></p><p>设置工作目录，对RUN,CMD,ENTRYPOINT,COPY,ADD生效。如果不存在则会创建，也可以设置多次。</p><h3 id="ADD-复制命令"><a href="#ADD-复制命令" class="headerlink" title="ADD 复制命令"></a>ADD 复制命令</h3><p> 一个复制命令，把文件复制到景象中。</p><p>如果把虚拟机与容器想象成两台linux服务器的话，那么这个命令就类似于scp，只是scp需要加用户名和密码的权限验证，而ADD不用。</p><p>语法如下：</p><ol><li><p>ADD … </p></li><li><p>ADD [“”,… “”]</p></li></ol><p><dest>路径的填写可以是容器内的绝对路径，也可以是相对于工作目录的相对路径</p><p><src>可以是一个本地文件或者是一个本地压缩文件，还可以是一个url</p><p>如果把<src>写成一个url，那么ADD就类似于wget命令</p><p>如以下写法都是可以的：</p><ul><li><strong>ADD test relativeDir/</strong> </li><li><strong>ADD test /relativeDir</strong></li><li><strong>ADD <a href="http://example.com/foobar" target="_blank" rel="noopener">http://example.com/foobar</a> /</strong></li></ul><p>尽量不要把<scr>写成一个文件夹，如果<src>是一个文件夹了，复制整个目录的内容,包括文件系统元数据</p><h3 id="COPY-复制命令"><a href="#COPY-复制命令" class="headerlink" title="COPY 复制命令"></a>COPY 复制命令</h3><p>看这个名字就知道，又是一个复制命令</p><p>语法如下：</p><ol><li><p>COPY … </p></li><li><p>COPY [“”,… “”]</p></li></ol><h4 id="COPY与ADD的区别"><a href="#COPY与ADD的区别" class="headerlink" title="COPY与ADD的区别"></a>COPY与ADD的区别</h4><p>COPY的<src>只能是本地文件，其他用法一致</p><h3 id="RUN-运行命令"><a href="#RUN-运行命令" class="headerlink" title="RUN 运行命令"></a>RUN 运行命令</h3><p> 功能为运行指定的命令</p><p>RUN命令有两种格式</p><ol><li><p>RUN </p></li><li><p>RUN [“executable”, “param1”, “param2”]</p></li></ol><p>注意：多行命令不要写多个RUN，原因是Dockerfile中每一个指令都会建立一层.</p><p> 多少个RUN就构建了多少层镜像，会造成镜像的臃肿、多层，不仅仅增加了构件部署的时间，还容易出错。</p><p>RUN书写时的换行符是\</p><h3 id="EXPOSE-声明端口"><a href="#EXPOSE-声明端口" class="headerlink" title="EXPOSE 声明端口"></a>EXPOSE 声明端口</h3><p>格式为 EXPOSE &lt;端口1&gt; [&lt;端口2&gt;…]。</p><p>EXPOSE 指令是声明运行时容器提供服务端口，这只是一个声明，在运行时并不会因为这个声明应用就会开启这个端口的服务。在 Dockerfile 中写入这样的声明有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。</p><p>要将 EXPOSE 和在运行时使用 -p &lt;宿主端口&gt;:&lt;容器端口&gt; 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。</p><h3 id="VOLUME-声名存储卷"><a href="#VOLUME-声名存储卷" class="headerlink" title="VOLUME 声名存储卷"></a>VOLUME 声名存储卷</h3><p>为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。</p><p>VOLUME /data</p><p>这里的 /data 目录就会在运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。</p><p>使用 <strong>dokcer inspect</strong> mycontainer 可以查看到具体的挂载情况</p><h3 id="CMD-容器启动时命令"><a href="#CMD-容器启动时命令" class="headerlink" title="CMD 容器启动时命令"></a>CMD 容器启动时命令</h3><p>功能为容器启动时要运行的命令</p><p>语法有三种写法</p><ol><li><p>CMD [“executable”,”param1”,”param2”]</p></li><li><p>CMD [“param1”,”param2”]</p></li><li><p>CMD command param1 param2</p></li></ol><p>第三种比较好理解了，就时shell这种执行方式和写法</p><p>第一种和第二种其实都是可执行文件加上参数的形式</p><h4 id="RUN-和-CMD比较"><a href="#RUN-和-CMD比较" class="headerlink" title="RUN 和 CMD比较"></a>RUN 和 CMD比较</h4><p>不要把RUN和CMD搞混了。</p><p>RUN是构件容器时就运行的命令以及提交运行结果</p><p>CMD是容器启动时执行的命令，在构件时并不运行，构件时紧紧指定了这个命令到底是个什么样子</p><h3 id="ENTRYPOINT-启动时默认命令"><a href="#ENTRYPOINT-启动时默认命令" class="headerlink" title="ENTRYPOINT 启动时默认命令"></a>ENTRYPOINT 启动时默认命令</h3><p>功能是启动时的默认命令</p><p>语法如下：</p><ol><li><p>ENTRYPOINT [“executable”, “param1”, “param2”]</p></li><li><p>ENTRYPOINT command param1 param2</p></li></ol><p>如果从上到下看到这里的话，那么你应该对这两种语法很熟悉啦。</p><p>第二种就是写shell</p><p>第一种就是可执行文件加参数</p><h4 id="ENTRYPOINT与CMD比较"><a href="#ENTRYPOINT与CMD比较" class="headerlink" title="ENTRYPOINT与CMD比较"></a>ENTRYPOINT与CMD比较</h4><p>（这俩命令太像了，而且还可以配合使用）：</p><ol><li>相同点：</li></ol><ul><li>只能写一条，如果写了多条，那么只有最后一条生效</li><li>容器启动时才运行，运行时机相同</li></ul><ol start="2"><li>不同点：</li></ol><ul><li>ENTRYPOINT不会被运行的command覆盖，而CMD则会被覆盖</li><li>如果我们在Dockerfile种同时写了ENTRYPOINT和CMD，并且CMD指令不是一个完整的可执行命令，那么CMD指定的内容将会作为ENTRYPOINT的参数</li></ul><p><strong>比如</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM ubuntu</span><br><span class="line">ENTRYPOINT [&quot;top&quot;, &quot;-b&quot;]</span><br><span class="line">CMD [&quot;-c&quot;]</span><br></pre></td></tr></table></figure><ul><li>如果我们在Dockerfile种同时写了ENTRYPOINT和CMD，并且CMD是一个完整的指令，那么它们两个会互相覆盖，谁在最后谁生效</li></ul><p><strong>比如</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM ubuntu</span><br><span class="line">ENTRYPOINT [&quot;top&quot;, &quot;-b&quot;]</span><br><span class="line">CMD ls -al</span><br></pre></td></tr></table></figure><p>那么将执行ls -al ,top -b不会执行。</p>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python操作Excel</title>
      <link href="/2020/07/30/python%E6%93%8D%E4%BD%9Cexcel/"/>
      <url>/2020/07/30/python%E6%93%8D%E4%BD%9Cexcel/</url>
      
        <content type="html"><![CDATA[<h2 id="Python操作Excel简介"><a href="#Python操作Excel简介" class="headerlink" title="Python操作Excel简介"></a>Python操作Excel简介</h2><h3 id="基础环境"><a href="#基础环境" class="headerlink" title="基础环境"></a>基础环境</h3><ul><li>Python版本号 3.6.8</li><li>pip版本号 20.1.1</li></ul><h3 id="python中与excel操作相关的模块"><a href="#python中与excel操作相关的模块" class="headerlink" title="python中与excel操作相关的模块"></a>python中与excel操作相关的模块</h3><ul><li><a href="http://pypi.python.org/pypi/xlrd" target="_blank" rel="noopener">xlrd库</a>：只能读excel，支持xls、xlsx</li><li><a href="http://pypi.python.org/pypi/xlwt" target="_blank" rel="noopener">xlwt库</a>：只能写excel，支持xls, 不支持对xlsx格式的修改，excel 的单格内容长度上限32767</li><li><a href="https://pypi.python.org/pypi/xlutils" target="_blank" rel="noopener">xlutils库</a>：结合xlrd，对一个已存在的文件进行修改，需要注意的是你必须同时安装这三个库, (xlutils提供方法帮助你把xlrd.Book对象复制到xlwt.Workbook对象)</li><li><a href="http://openpyxl.readthedocs.org/" target="_blank" rel="noopener">OpenPyXL</a>：主要针对xlsx格式的excel进行读取和编辑，不支持xls</li><li>xlsxwriter：可以写excel文件并加上图表，缺点是不能打开/修改已有文件，意味着使用 xlsxwriter 需要从零开始。</li></ul><h4 id="关于xlrd-xlwt和openpyxl的差别"><a href="#关于xlrd-xlwt和openpyxl的差别" class="headerlink" title="关于xlrd/xlwt和openpyxl的差别"></a>关于xlrd/xlwt和openpyxl的差别</h4><p>两者都是对于excel文件的操作插件，两者的主要区别在于写入操作，其中xlwt针对Ecxec2007之前的版本，即.xls文件，其要求单个sheet不超过65535行，而openpyxl则主要针对Excel2007之后的版本（.xlsx），它对文件大小没有限制。另外还有区别就是二者在读写速度上的差异，xlrd/xlwt在读写方面的速度都要优于openpyxl，但因为xlwt无法生成xlsx是个硬伤，所以想要尽量提高效率又不影响结果时，可以考虑用xlrd读取，用openpyxl写入。</p><h3 id="安装和excel相关python类"><a href="#安装和excel相关python类" class="headerlink" title="安装和excel相关python类"></a>安装和excel相关python类</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install xlrd  </span><br><span class="line">pip install xlwt </span><br><span class="line">pip install xlutils  </span><br><span class="line">pip install openpyxl</span><br></pre></td></tr></table></figure><h2 id="python-xlrd-xlwt使用"><a href="#python-xlrd-xlwt使用" class="headerlink" title="python xlrd/xlwt使用"></a>python xlrd/xlwt使用</h2><h3 id="写Excel"><a href="#写Excel" class="headerlink" title="写Excel"></a>写Excel</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import xlwt  #引入模块</span><br><span class="line"></span><br><span class="line">book1 &#x3D; xlwt.Workbook()   #创建 excel</span><br><span class="line">sheet &#x3D; book1.add_sheet(&#39;sheet1&#39;)   #创建sheet页</span><br><span class="line"></span><br><span class="line">sheet.write(0,0,&#39;名字&#39;)   #编辑表头</span><br><span class="line">sheet.write(1,0,&#39;王一&#39;)   #编辑内容</span><br><span class="line">sheet.write(2,0,&#39;王二&#39;)</span><br><span class="line"></span><br><span class="line">sheet.write(0,1,&#39;手机号&#39;)  #编辑表头</span><br><span class="line">sheet.write(1,1,&#39;119&#39;)    #编辑内容</span><br><span class="line">sheet.write(2,1,&#39;110&#39;)</span><br><span class="line"></span><br><span class="line">book1.save(&quot;students1.xls&quot;)  #保存下，xlsx也可以保存，但会打不开，使用wps可以打开，使用微软的会打不开</span><br></pre></td></tr></table></figure><p>保存的时候，如果是微软的office,后缀.xls<br>保存的时候，如果是wps .xls .xlsx都可以</p><h4 id="使用循环方式写入内容："><a href="#使用循环方式写入内容：" class="headerlink" title="使用循环方式写入内容："></a>使用循环方式写入内容：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#给定文件内容：</span><br><span class="line">stus&#x3D; [</span><br><span class="line">    [&#39;id&#39;, &#39;name&#39;, &#39;sex&#39;, &#39;age&#39;, &#39;addr&#39;, &#39;grade&#39;, &#39;phone&#39;, &#39;gold&#39;],</span><br><span class="line">    [314, &#39;矿泉水&#39;, &#39;男&#39;, 18, &#39;北京市昌平区&#39;, &#39;摩羯座&#39;, &#39;18317155663&#39;, 14405],</span><br><span class="line">    [315, &#39;矿泉水&#39;, &#39;女&#39;, 27, &#39;上海&#39;, &#39;摩羯座&#39;, &#39;18317155664&#39;, 100]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">#内容写入Excel</span><br><span class="line">book2&#x3D;xlwt.Workbook()  #新建一个Excel</span><br><span class="line">sheet2&#x3D;book2.add_sheet(&#39;sheet1&#39;)  #新建一个sheet页</span><br><span class="line"></span><br><span class="line">row &#x3D; 0  #行号</span><br><span class="line">for stu in stus:  #控制行</span><br><span class="line">    col &#x3D; 0#列号</span><br><span class="line">    for field in stu:  #控制列的</span><br><span class="line">        sheet2.write(row,col,field)</span><br><span class="line">        col+&#x3D;1  #列号</span><br><span class="line">    row+&#x3D;1</span><br><span class="line"></span><br><span class="line">book2.save(&#39;students2.xls&#39;)  #保存内容</span><br></pre></td></tr></table></figure><h3 id="读Excel"><a href="#读Excel" class="headerlink" title="读Excel"></a>读Excel</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import xlrd</span><br><span class="line"></span><br><span class="line">book3&#x3D;xlrd.open_workbook(&#39;students1.xls&#39;) #打开Excel</span><br><span class="line">sheet3&#x3D;book3.sheet_by_index(0) #根据编号获取sheet页</span><br><span class="line">#sheet&#x3D;book3.sheet_by_name(&#39;sheet1&#39;) #也可以根据sheet页名字获取sheet页</span><br><span class="line"></span><br><span class="line">print(sheet3.nrows) #Excel里有多少行</span><br><span class="line">print(sheet3.ncols)  #Excel里有多少列</span><br><span class="line"></span><br><span class="line">print(sheet.cell(0,0).value) #获取到指定单元格的内容</span><br><span class="line">print(sheet.cell(0,1).value) #获取到指定单元格的内容</span><br><span class="line"></span><br><span class="line">print(sheet.row_values(0))  #获取到整行的内容</span><br><span class="line">print(sheet.col_values(0))   #获取到整列的内容</span><br><span class="line"></span><br><span class="line">for i in range(sheet.nrows):  #循环获取每行的内容</span><br><span class="line">    print(sheet.row_values(i))</span><br></pre></td></tr></table></figure><h3 id="修改Excel"><a href="#修改Excel" class="headerlink" title="修改Excel"></a>修改Excel</h3><p>由于xlwt模块只能写一次，再重新打开Excel后会覆盖原来的内容；而xlrd模块只能读，因此修改Excel就要使用xlutils模块了</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import xlrd   #两个模块配合使用</span><br><span class="line">import xlutils</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">book4 &#x3D; xlrd.open_workbook(&#39;students1.xls&#39;)  #先用xlrd打开一个Excel</span><br><span class="line">new_book4 &#x3D; xlutils.copy.copy(book4)  #然后用xlutils里面的copy功能，复制一个Excel</span><br><span class="line"></span><br><span class="line">sheet4 &#x3D; new_book4.get_sheet(0)  #获取sheet页，注意这里的sheet 页是xlutils里的，只能用.get_sheet()的方法获取了</span><br><span class="line">sheet4.write(0,0,&#39;ID&#39;)  # 第一行，第一列</span><br><span class="line"></span><br><span class="line">os.rename(&#39;students1.xls&#39;,&#39;students1_bak.xls&#39;) #先把之前的Excel改下名字，之前的内容不至于丢失</span><br><span class="line">new_book4.save(&#39;students1.xls&#39;) #修改完内容后再保存成同名的Excel</span><br></pre></td></tr></table></figure><p>​    如果是单个模块：from xlutils import copy</p><h2 id="excel和数据库"><a href="#excel和数据库" class="headerlink" title="excel和数据库"></a>excel和数据库</h2><h3 id="在mysql中添加测试库"><a href="#在mysql中添加测试库" class="headerlink" title="在mysql中添加测试库"></a>在mysql中添加测试库</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create database od;</span><br><span class="line">use od;</span><br><span class="line"></span><br><span class="line">grant  all on od.* to &#39;oldboy&#39;@&#39;%&#39; identified by &#39;oldboy123&#39;;</span><br><span class="line"></span><br><span class="line">create table stu (</span><br><span class="line">    id int(10) NOT NULL AUTO_INCREMENT,</span><br><span class="line">    name varchar(100) ,</span><br><span class="line">    school varchar(100) ,</span><br><span class="line">    PRIMARY KEY (&#96;id&#96;) </span><br><span class="line">ENGINE&#x3D;InnoDB AUTO_INCREMENT&#x3D;1 DEFAULT CHARSET&#x3D;utf8 COLLATE&#x3D;utf8_unicode_ci;</span><br><span class="line"></span><br><span class="line">insert into stu(name, school) values(&quot;zhangsan&quot;, &quot;三中&quot;);</span><br><span class="line">insert into stu(name, school) values(&quot;lisi&quot;, &quot;四中&quot;);</span><br><span class="line">select * from stu;</span><br></pre></td></tr></table></figure><p># alter table stu add column school varchar(255) ;<br># alter table stu change column sex school varchar(255) ;</p><h3 id="从excel读取数据写入数据库"><a href="#从excel读取数据写入数据库" class="headerlink" title="从excel读取数据写入数据库"></a>从excel读取数据写入数据库</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import xlwt</span><br><span class="line">import xlrd</span><br><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line">#从excel读取数据写入mysql</span><br><span class="line">def excel_to_mysql(filename):</span><br><span class="line">    conn &#x3D; pymysql.connect(host&#x3D;&#39;mysql.od.com&#39;,port&#x3D;3306, user&#x3D;&#39;oldboy&#39;, password&#x3D;&#39;oldboy123&#39;,db&#x3D;&#39;od&#39;,charset&#x3D;&#39;utf8&#39;)</span><br><span class="line">    cur &#x3D; conn.cursor()     #连接数据库</span><br><span class="line">    book &#x3D; xlrd.open_workbook(filename)</span><br><span class="line">    sheet1 &#x3D; book.sheet_by_name(&#39;sheet1&#39;)</span><br><span class="line">    rows &#x3D; sheet1.nrows      #获取行数</span><br><span class="line">    for r in range(1,rows):  #将标题之外的其他行写入数据库</span><br><span class="line">        r_values &#x3D; sheet1.row_values(r)</span><br><span class="line">        sql &#x3D; &#39;insert into stu(name, school) values(%s,%s)&#39; #有几个字段需要几个%s</span><br><span class="line">        data &#x3D; cur.execute(sql,r_values)  #将每一行插入sql</span><br><span class="line">    conn.commit()           #插入所有数据后提交</span><br><span class="line">    cur.close()</span><br><span class="line">    conn.close()</span><br><span class="line"></span><br><span class="line">excel_to_mysql(&#39;students1.xls&#39;)</span><br></pre></td></tr></table></figure><h3 id="从数据库读入数据-写入xls"><a href="#从数据库读入数据-写入xls" class="headerlink" title="从数据库读入数据,写入xls"></a>从数据库读入数据,写入xls</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import xlwt</span><br><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line">def sql_connect(sql):</span><br><span class="line">    conn &#x3D; pymysql.connect(host&#x3D;&#39;mysql.od.com&#39;,port&#x3D;3306, user&#x3D;&#39;oldboy&#39;, password&#x3D;&#39;oldboy123&#39;,db&#x3D;&#39;od&#39;,charset&#x3D;&#39;utf8&#39;)</span><br><span class="line">    cur &#x3D; conn.cursor()</span><br><span class="line">    cur.execute(sql)</span><br><span class="line">    data &#x3D; cur.fetchall()</span><br><span class="line">    cur.close()</span><br><span class="line">    conn.close()</span><br><span class="line">    return data</span><br><span class="line"></span><br><span class="line">sql2 &#x3D; &quot;select * from stu limit 10;&quot;</span><br><span class="line">stus &#x3D; sql_connect(sql2)</span><br><span class="line"></span><br><span class="line">#内容写入Excel</span><br><span class="line">book2&#x3D;xlwt.Workbook()  #新建一个Excel</span><br><span class="line">sheet2&#x3D;book2.add_sheet(&#39;sheet1&#39;)  #新建一个sheet页</span><br><span class="line"></span><br><span class="line">row &#x3D; 0  #行号</span><br><span class="line">for stu in stus:  #控制行</span><br><span class="line">    col &#x3D; 0#列号</span><br><span class="line">    for field in stu:  #控制列的</span><br><span class="line">        sheet2.write(row,col,field)</span><br><span class="line">        col+&#x3D;1  #列号</span><br><span class="line">    row+&#x3D;1</span><br><span class="line"></span><br><span class="line">book2.save(&#39;students2.xls&#39;)  #保存内容</span><br></pre></td></tr></table></figure><h2 id="Python-Openpyxl使用"><a href="#Python-Openpyxl使用" class="headerlink" title="Python Openpyxl使用"></a>Python Openpyxl使用</h2><h3 id="openpyxl对Excel的操作命令"><a href="#openpyxl对Excel的操作命令" class="headerlink" title="openpyxl对Excel的操作命令"></a>openpyxl对Excel的操作命令</h3><ul><li>创建一个工作薄：wb = openpyxl.Workbook()</li><li>激活工作薄: wb.active</li><li>插入数据: ws[‘A1’] = 42， 或者 ws.append([1, 2, 3])</li><li>写入第一行、第三列的数据 value = ‘result’：ws.cell(row = 1,column = 3,value = ‘result’)</li><li>新增一个sheet表单：wb.create_sheet(‘Mysheet1’)</li><li>保存case.xlsx文件：wb.save(‘cases.xlsx’)</li><li>关闭工作薄：wb.close()</li></ul><ul><li>打开工作簿：wb = openpyxl.load_workbook(‘cases.xlsx’)</li><li>选取表单：sh = wb[‘sheet1’]</li><li>显示表名：print(wb.sheetnames)</li><li>读取第一行、第一列的数据：ce = ws(row = 1,column = 1)</li><li>按行读取数据：row_data = list(ws.rows)</li><li>按列读取数据：columns_data = list(ws.columns)</li></ul><ul><li><strong>写入数据之前，该文件一定要处于关闭状态</strong></li><li>获取最大行总数、最大列总数：print(ws.max_row, ws.max_column)</li><li>remove 删除表单的用法：sh = wb[‘sheet_name’] wb.remove(sh) </li></ul><h3 id="openpyxl-写入excel"><a href="#openpyxl-写入excel" class="headerlink" title="openpyxl 写入excel"></a>openpyxl 写入excel</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import openpyxl</span><br><span class="line">import datetime</span><br><span class="line"></span><br><span class="line">wb &#x3D; openpyxl.Workbook()  # 实例化</span><br><span class="line">ws &#x3D; wb.active  # 激活 worksheet</span><br><span class="line"></span><br><span class="line">ws[&#39;A1&#39;] &#x3D; 42  # 方式一：数据可以直接分配到单元格中(可以输入公式)</span><br><span class="line">ws[&#39;B1&#39;] &#x3D; datetime.datetime.now().strftime(&quot;%Y-%m-%d&quot;)  # Python 类型会被自动转换</span><br><span class="line">ws.cell(row &#x3D; 1,column &#x3D; 3,value &#x3D; &#39;result&#39;)  # 方式二：通过cell</span><br><span class="line">ws.append([1, 2, 3])  # 方式三：可以附加行，从第一列开始附加(从最下方空白处，最左开始)(可以输入多行)</span><br><span class="line">ws.append([&quot;wang&quot;, &quot;zhang&quot;, &quot;li&quot;])  </span><br><span class="line"></span><br><span class="line">ws1 &#x3D; wb.create_sheet(&quot;Mysheet1&quot;)  # 方式一：插入到最后(default) </span><br><span class="line">ws2 &#x3D; wb.create_sheet(&quot;Mysheet2&quot;, 1)  # 方式二：插入到最开始的位置</span><br><span class="line">ws2[&#39;A1&#39;] &#x3D; &quot;ws2&quot;</span><br><span class="line"></span><br><span class="line">wb.save(&#39;cases.xlsx&#39;)  # 保存为一个xlsx格式的文件</span><br><span class="line">wb.close()  # 关闭工作薄</span><br></pre></td></tr></table></figure><h3 id="openpyxl-读取excel"><a href="#openpyxl-读取excel" class="headerlink" title="openpyxl 读取excel"></a>openpyxl 读取excel</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import openpyxl</span><br><span class="line"></span><br><span class="line">wb &#x3D; openpyxl.load_workbook(&#39;cases.xlsx&#39;)  # 第一步：打开工作簿</span><br><span class="line">ws &#x3D; wb[&#39;Sheet&#39;]  # 第二步：选取表单</span><br><span class="line"></span><br><span class="line">print(wb.sheetnames)  # 显示所有表名</span><br><span class="line">for sheet in wb:  # 遍历所有表</span><br><span class="line">    print(sheet.title)</span><br><span class="line"></span><br><span class="line">ce &#x3D; ws.cell(row &#x3D; 1,column &#x3D; 1)   # 参数 row:行,column：列; 读取第一行，第一列的数据</span><br><span class="line">print(ce.value)</span><br><span class="line"></span><br><span class="line">print(list(ws.rows)[1:])     # 按行读取数据，去掉第一行的表头信息数据</span><br><span class="line">for cases in list(ws.rows)[1:]:</span><br><span class="line">    case_id &#x3D;  cases[0].value</span><br><span class="line">    case_excepted &#x3D; cases[1].value</span><br><span class="line">    case_data &#x3D; cases[2].value</span><br><span class="line">    print(case_id,case_excepted,case_data)</span><br><span class="line"></span><br><span class="line">print(list(ws.rows)[1:])  # 按列读取数据</span><br></pre></td></tr></table></figure><h3 id="访问单元格"><a href="#访问单元格" class="headerlink" title="访问单元格"></a>访问单元格</h3><h4 id="单一单元格访问"><a href="#单一单元格访问" class="headerlink" title="单一单元格访问"></a>单一单元格访问</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">c &#x3D; ws[&#39;A1&#39;]</span><br><span class="line">d &#x3D; ws.cell(row&#x3D;4, column&#x3D;2, value&#x3D;11)</span><br><span class="line">print(c.value, d.value)</span><br><span class="line"></span><br><span class="line">for i in range(1,5):</span><br><span class="line">    for j in range(1,5):</span><br><span class="line">        ws.cell(row&#x3D;i, column&#x3D;j).value</span><br></pre></td></tr></table></figure><h4 id="多单元格访问"><a href="#多单元格访问" class="headerlink" title="多单元格访问"></a>多单元格访问</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cell_range &#x3D; ws[&#39;A1&#39;:&#39;C2&#39;]  # 通过切片</span><br><span class="line">colC &#x3D; ws[&#39;C&#39;]  # 通过行(列)</span><br><span class="line">col_range &#x3D; ws[&#39;C:D&#39;]</span><br><span class="line">row10 &#x3D; ws[10]</span><br><span class="line">row_range &#x3D; ws[5:10]</span><br><span class="line"></span><br><span class="line"># 通过指定范围(行 → 行)</span><br><span class="line">for row in ws.iter_rows(min_row&#x3D;1, max_col&#x3D;3, max_row&#x3D;2):</span><br><span class="line">    for cell in row:</span><br><span class="line">        print(cell.value)</span><br><span class="line"></span><br><span class="line"># 通过指定范围(列 → 列)</span><br><span class="line">for col in ws.iter_cols(min_row&#x3D;1, max_col&#x3D;3, max_row&#x3D;2):</span><br><span class="line">    for cell in col:</span><br><span class="line">        print(cell.value)</span><br><span class="line"></span><br><span class="line">tuple(ws.rows)  # 遍历所有 方法一</span><br><span class="line">tuple(ws.columns)  # 遍历所有 方法二</span><br></pre></td></tr></table></figure><h3 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wb.save(&#39;cases.xlsx&#39;)  # 保存为一个xlsx格式的文件</span><br><span class="line">wb.close()  # 关闭工作薄</span><br></pre></td></tr></table></figure><h3 id="其他方法"><a href="#其他方法" class="headerlink" title="其他方法"></a>其他方法</h3><p>改变 sheet 标签按钮颜色</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ws.sheet_properties.tabColor &#x3D; &quot;1072BA&quot;</span><br></pre></td></tr></table></figure><p>获取最大行，最大列</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(ws.max_row)</span><br><span class="line">print(ws.max_column)</span><br></pre></td></tr></table></figure><p>获取每一行，每一列</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ws.rows为生成器, 里面是每一行的数据，每一行又由一个tuple包裹。</span><br><span class="line">ws.columns类似，不过里面是每个tuple是每一列的单元格。</span><br><span class="line"># 因为按行，所以返回A1, B1, C1这样的顺序</span><br><span class="line">for row in ws.rows:</span><br><span class="line">  for cell in row:</span><br><span class="line">    print(cell.value)</span><br><span class="line"> </span><br><span class="line"># A1, A2, A3这样的顺序</span><br><span class="line">for column in ws.columns:</span><br><span class="line">  for cell in column:</span><br><span class="line">    print(cell.value)</span><br></pre></td></tr></table></figure><p>根据数字得到字母，根据字母得到数字</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from openpyxl.utils import get_column_letter, column_index_from_string</span><br><span class="line"> </span><br><span class="line"># 根据列的数字返回字母</span><br><span class="line">print(get_column_letter(2)) # B</span><br><span class="line"># 根据字母返回列的数字</span><br><span class="line">print(column_index_from_string(&#39;D&#39;)) # 4</span><br></pre></td></tr></table></figure><p> 删除工作表（待验证）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wb.remove_sheet(&quot;Mysheet1&quot;)  # 方式一,我推荐</span><br><span class="line">del wb[sheet]  # 方式二</span><br></pre></td></tr></table></figure><p> 矩阵置换（行 → 列）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rows &#x3D; [</span><br><span class="line">  [&#39;Number&#39;, &#39;data1&#39;, &#39;data2&#39;],</span><br><span class="line">  [2, 40, 30],</span><br><span class="line">  [3, 40, 25],</span><br><span class="line">  [4, 50, 30],</span><br><span class="line">  [7, 50, 10]]</span><br><span class="line"> </span><br><span class="line">list(zip(*rows))</span><br><span class="line"> </span><br><span class="line"># out</span><br><span class="line">[(&#39;Number&#39;, 2, 3, 4, 7), (&#39;data1&#39;, 40, 40, 50, 50), (&#39;data2&#39;, 30, 25, 30, 10)]</span><br><span class="line"> </span><br><span class="line"># 注意 方法会舍弃缺少数据的列(行)</span><br><span class="line">rows &#x3D; [</span><br><span class="line">  [&#39;Number&#39;, &#39;data1&#39;, &#39;data2&#39;],</span><br><span class="line">  [2, 40  ], # 这里少一个数据</span><br><span class="line">  [3, 40, 25],</span><br><span class="line">  [4, 50, 30],</span><br><span class="line">  [7, 50, 10],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">list(zip(*rows))</span><br><span class="line"># out</span><br><span class="line">[(&#39;Number&#39;, 2, 3, 4, 7), (&#39;data1&#39;, 40, 40, 50, 50)]</span><br></pre></td></tr></table></figure><h3 id="设置单元格风格"><a href="#设置单元格风格" class="headerlink" title="设置单元格风格"></a>设置单元格风格</h3><p>① 需要导入的类</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from openpyxl.styles import Font, colors, Alignment</span><br></pre></td></tr></table></figure><p>② 字体</p><p>下面的代码指定了等线24号，加粗斜体，字体颜色红色。直接使用cell的font属性，将Font对象赋值给它。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bold_itatic_24_font &#x3D; Font(name&#x3D;&#39;等线&#39;, size&#x3D;24, italic&#x3D;True, color&#x3D;colors.BLUE, bold&#x3D;True)</span><br><span class="line"> </span><br><span class="line">ws[&#39;A1&#39;].font &#x3D; bold_itatic_24_font</span><br></pre></td></tr></table></figure><p>③ 对齐方式</p><p>也是直接使用cell的属性aligment，这里指定垂直居中和水平居中。除了center，还可以使用right、left等等参数。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 设置B1中的数据垂直居中和水平居中</span><br><span class="line">ws[&#39;B1&#39;].alignment &#x3D; Alignment(horizontal&#x3D;&#39;center&#39;, vertical&#x3D;&#39;center&#39;)</span><br></pre></td></tr></table></figure><p>④ 设置行高和列宽</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ws.row_dimensions[2].height &#x3D; 40  # 第2行行高</span><br><span class="line">ws.column_dimensions[&#39;C&#39;].width &#x3D; 30  # C列列宽</span><br></pre></td></tr></table></figure><p>⑤ 合并和拆分单元格</p><ul><li>所谓合并单元格，即以合并区域的左上角的那个单元格为基准，覆盖其他单元格使之称为一个大的单元格。</li><li>相反，拆分单元格后将这个大单元格的值返回到原来的左上角位置。</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 合并单元格， 往左上角写入数据即可</span><br><span class="line">ws.merge_cells(&#39;B1:G1&#39;) # 合并一行中的几个单元格</span><br><span class="line">ws.merge_cells(&#39;A1:C3&#39;) # 合并一个矩形区域中的单元格</span><br></pre></td></tr></table></figure><ul><li>合并后只可以往左上角写入数据，也就是区间中:左边的坐标。</li><li>如果这些要合并的单元格都有数据，只会保留左上角的数据，其他则丢弃。换句话说若合并前不是在左上角写入数据，合并后单元格中不会有数据。</li><li>以下是拆分单元格的代码。拆分后，值回到A1位置。</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sheet.unmerge_cells(&#39;A1:C3&#39;)</span><br></pre></td></tr></table></figure><p>最后举个例子</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import datetime</span><br><span class="line">from random import choice</span><br><span class="line">from time import time</span><br><span class="line">from openpyxl import load_workbook</span><br><span class="line">from openpyxl.utils import get_column_letter</span><br><span class="line"></span><br><span class="line">addr &#x3D; &quot;openpyxl.xlsx&quot;  # 设置文件 mingc</span><br><span class="line">wb &#x3D; load_workbook(addr)  # 打开文件</span><br><span class="line">ws &#x3D; wb.create_sheet()  # 创建一张新表</span><br><span class="line">ws.append([&#39;TIME&#39;, &#39;TITLE&#39;, &#39;A-Z&#39;])  # 第一行输入</span><br><span class="line"></span><br><span class="line"># 输入内容（50行数据）</span><br><span class="line">for i in range(50):</span><br><span class="line">  TIME &#x3D; datetime.datetime.now().strftime(&quot;%H:%M:%S&quot;)</span><br><span class="line">  TITLE &#x3D; str(time())</span><br><span class="line">  A_Z &#x3D; get_column_letter(choice(range(1, 10)))</span><br><span class="line">  ws.append([TIME, TITLE, A_Z])</span><br><span class="line"></span><br><span class="line">row_max &#x3D; ws.max_row  # 获取最大行</span><br><span class="line">con_max &#x3D; ws.max_column  # 获取最大列</span><br><span class="line"></span><br><span class="line"># 把上面写入内容打印在控制台</span><br><span class="line">for j in ws.rows: # we.rows 获取每一行数据</span><br><span class="line">  for n in j:</span><br><span class="line">    print(n.value, end&#x3D;&quot;\t&quot;)  # n.value 获取单元格的值</span><br><span class="line">  print()</span><br><span class="line"></span><br><span class="line">wb.save(addr)  # 保存，save（必须要写文件名（绝对地址）默认 py 同级目录下，只支持 xlsx 格式）</span><br></pre></td></tr></table></figure><h2 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h2><h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h3><ul><li>官网网址：<a href="http://www.python-exceel.org/" target="_blank" rel="noopener">http://www.python-exceel.org/</a></li><li><a href="https://www.cnblogs.com/brf-test/p/11748442.html" target="_blank" rel="noopener">Python之操作Excel</a> ， 内容：xlrd和xlwt的使用</li><li><a href="https://www.jianshu.com/p/d596ba8931ee" target="_blank" rel="noopener">Python 针对Excel操作</a>   ， 内容：xlrd和xlwt结合数据库进行读写</li><li><a href="http://www.52codes.net/develop/shell/58896.html" target="_blank" rel="noopener">浅谈Python_Openpyxl使用</a> ， 内容：Openpyxl的使用</li><li><a href="https://blog.gocalf.com/python-read-write-excel" target="_blank" rel="noopener">用 Python 读写 Excel 文件</a>  ， 内容：xlrd和xlwt，OpenPyXL 和 XlsxWriter 的优缺点（适应场景）</li></ul>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> excel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pod的使用</title>
      <link href="/2020/07/28/pod%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
      <url>/2020/07/28/pod%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h3 id="deploy示例"><a href="#deploy示例" class="headerlink" title="deploy示例"></a>deploy示例</h3><p>nginx-test.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-test</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.18.0  #stanleyws&#x2F;nginx-curl个人优化</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# kubectl logs -f nginx-test-bc75bcb6c-tjqpl</span><br><span class="line">192.168.2.2 - - [14&#x2F;Aug&#x2F;2020:09:16:19 +0000] &quot;GET &#x2F; HTTP&#x2F;1.1&quot; 200 612 &quot;-&quot; &quot;curl&#x2F;7.38.0&quot; &quot;-&quot;</span><br><span class="line">192.168.2.2 - - [14&#x2F;Aug&#x2F;2020:09:16:22 +0000] &quot;GET &#x2F; HTTP&#x2F;1.1&quot; 200 612 &quot;-&quot; &quot;curl&#x2F;7.38.0&quot; &quot;-&quot;</span><br></pre></td></tr></table></figure><h3 id="deploy主要配置参数"><a href="#deploy主要配置参数" class="headerlink" title="deploy主要配置参数"></a>deploy主要配置参数</h3><p>模版文件<code>model-service.yaml</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: model-service</span><br><span class="line">  namespace: test7</span><br><span class="line">  annotations:</span><br><span class="line">    k8s.eip.work&#x2F;service: model-service</span><br><span class="line">    k8s.eip.work&#x2F;ingress: &#39;false&#39;</span><br><span class="line">  labels:</span><br><span class="line">    k8s.eip.work&#x2F;layer: svc</span><br><span class="line">    k8s.eip.work&#x2F;name: model-service</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s.eip.work&#x2F;layer: svc</span><br><span class="line">      k8s.eip.work&#x2F;name: model-service</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s.eip.work&#x2F;layer: svc</span><br><span class="line">        k8s.eip.work&#x2F;name: model-service</span><br><span class="line">    spec:</span><br><span class="line">      hostAliases:  # 设置host解析</span><br><span class="line">      - ip: &quot;192.168.1.3&quot;</span><br><span class="line">        hostnames:</span><br><span class="line">        - &quot;rabbitmq.od.com&quot;</span><br><span class="line">      imagePullSecrets: # 设置harbor账号</span><br><span class="line">      - name: docker-user  </span><br><span class="line">      </span><br><span class="line">      affinity: # 设置pod亲和性</span><br><span class="line">        podAffinity:</span><br><span class="line">          preferredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">            - podAffinityTerm:</span><br><span class="line">                labelSelector:</span><br><span class="line">                  matchExpressions:</span><br><span class="line">                  - &#123; key: &quot;k8s.eip.work&#x2F;name&quot;, operator: In, values: [&quot;demo-service&quot;] &#125;</span><br><span class="line">                topologyKey: kubernetes.io&#x2F;hostname</span><br><span class="line">              weight: 30</span><br><span class="line"></span><br><span class="line">      initContainers: # 初始化镜像</span><br><span class="line">      - name: init-pkg</span><br><span class="line">        image: harbor.od.com&#x2F;test1&#x2F;model-jar:1.2.5</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        command: [&#39;sh&#39;, &#39;-c&#39;, &quot;cp -rf . &#x2F;app&#x2F; &quot;]</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: app-dir</span><br><span class="line">          mountPath: &#x2F;app</span><br><span class="line"></span><br><span class="line">      containers:</span><br><span class="line">        - name: design-service</span><br><span class="line">          </span><br><span class="line">          # 设置镜像版本</span><br><span class="line">          image: &#39;harbor.od.com&#x2F;test1&#x2F;model-service:v1.2.2&#39;</span><br><span class="line">          imagePullPolicy: Always</span><br><span class="line">          </span><br><span class="line">          # 修改启动命令 </span><br><span class="line">          # java -jar -Xms1024M -Xmx1024M model-service.jar --server.port&#x3D;8084 --spring.profiles.active&#x3D;test</span><br><span class="line">          # -javaagent:&#x2F;usr&#x2F;local&#x2F;file&#x2F;jmx_prometheus_javaagent-0.12.0.jar&#x3D;12345:&#x2F;usr&#x2F;local&#x2F;file&#x2F;tomcat.yml</span><br><span class="line">          command: [ &quot;&#x2F;bin&#x2F;bash&quot;, &quot;-ce&quot;, &quot;tail -f &#x2F;dev&#x2F;null&quot; ]</span><br><span class="line">          </span><br><span class="line">          env:  # 设置变量</span><br><span class="line">          - name: JAR_BALL</span><br><span class="line">            value: dubbo-server.jar</span><br><span class="line">          </span><br><span class="line">          readinessProbe:  # 配置健康检测</span><br><span class="line">            tcpSocket:  # tcp协议</span><br><span class="line">              port: 8080</span><br><span class="line">            httpGet:  # http协议</span><br><span class="line">              path: &#x2F;actuator&#x2F;health</span><br><span class="line">              port: 8080</span><br><span class="line">              httpHeaders: </span><br><span class="line">              - name: X-Custom-Header</span><br><span class="line">                value: Awesome </span><br><span class="line">            initialDelaySeconds: 60 </span><br><span class="line">            periodSeconds: 20</span><br><span class="line">          </span><br><span class="line">          resources:  # 配置资源限制 </span><br><span class="line">            requests:</span><br><span class="line">              cpu: 256m</span><br><span class="line">              memory: 1024Mi</span><br><span class="line">            limits:</span><br><span class="line">              cpu: 2000m</span><br><span class="line">              memory: 2048Mi</span><br><span class="line">              </span><br><span class="line">          volumeMounts:  # 挂载磁盘</span><br><span class="line">          - name: app-dir</span><br><span class="line">            mountPath: &#x2F;app</span><br><span class="line">      volumes:</span><br><span class="line">      - name: app-dir  # 挂载类型1: pod临时文件</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">      - name: app-dir  # 挂载类型2: 宿主机文件</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &#x2F;data</span><br><span class="line">      - name: app-dir  # 挂载类型3: nfs磁盘</span><br><span class="line">        nfs:</span><br><span class="line">          server: 192.168.1.2</span><br><span class="line">          path: &#x2F;data&#x2F;nfs&#x2F;od</span><br><span class="line">      - name: app-dir       </span><br><span class="line">        projected:      </span><br><span class="line">          sources:      </span><br><span class="line">          - secret:          </span><br><span class="line">              name: user</span><br><span class="line">     - name: nfs </span><br><span class="line">       persistentVolumeClaim: </span><br><span class="line">         claimName: nfs</span><br></pre></td></tr></table></figure><h3 id="svc主要配置参数"><a href="#svc主要配置参数" class="headerlink" title="svc主要配置参数"></a>svc主要配置参数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: demo-web</span><br><span class="line">  namespace: test1</span><br><span class="line">  annotations:</span><br><span class="line">    k8s.eip.work&#x2F;workload: demo-web</span><br><span class="line">    k8s.eip.work&#x2F;displayName: demo-web</span><br><span class="line">  labels:</span><br><span class="line">    k8s.eip.work&#x2F;layer: web</span><br><span class="line">    k8s.eip.work&#x2F;name: demo-web</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    k8s.eip.work&#x2F;layer: web</span><br><span class="line">    k8s.eip.work&#x2F;name: demo-web</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">    - name: demo-web-ng</span><br><span class="line">      port: 80</span><br><span class="line">      targetPort: 8080</span><br><span class="line">      nodePort: 28080</span><br></pre></td></tr></table></figure><h3 id="ingress主要配置参数"><a href="#ingress主要配置参数" class="headerlink" title="ingress主要配置参数"></a>ingress主要配置参数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: networking.k8s.io&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: demo-web</span><br><span class="line">  namespace: test1</span><br><span class="line">  annotations:</span><br><span class="line">    k8s.eip.work&#x2F;workload: demo-web</span><br><span class="line">    apps: demo-web</span><br><span class="line">  labels:</span><br><span class="line">    k8s.eip.work&#x2F;layer: web</span><br><span class="line">    apps: demo-web</span><br><span class="line">spec:</span><br><span class="line">  tls:</span><br><span class="line">    - hosts: </span><br><span class="line">        - demo.od.com</span><br><span class="line">      secretName: k8s-ssl</span><br><span class="line">  rules:</span><br><span class="line">    - host: demo.k8s.od.com</span><br><span class="line">      http:</span><br><span class="line">        paths:</span><br><span class="line">          - path: &#x2F;</span><br><span class="line">            backend:</span><br><span class="line">              serviceName: demo-web</span><br><span class="line">              servicePort: demo-web-ng</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 知识点 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 知识点 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx日志的使用和讲解</title>
      <link href="/2020/07/28/nginx%E6%97%A5%E5%BF%97%E7%9A%84%E4%BD%BF%E7%94%A8%E5%92%8C%E8%AE%B2%E8%A7%A3/"/>
      <url>/2020/07/28/nginx%E6%97%A5%E5%BF%97%E7%9A%84%E4%BD%BF%E7%94%A8%E5%92%8C%E8%AE%B2%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<blockquote><p> 参考文章 <a href="https://blog.51cto.com/13707680/2116001" target="_blank" rel="noopener">Nginx日志分析和参数详解</a></p></blockquote><h2 id="nginx日志记录格式"><a href="#nginx日志记录格式" class="headerlink" title="nginx日志记录格式"></a>nginx日志记录格式</h2><p>log_format用来设置日志的记录格式：</p><p>log_format name format</p><p>name表示格式名称，format表示等义的格式。</p><h3 id="nginx日志默认配置"><a href="#nginx日志默认配置" class="headerlink" title="nginx日志默认配置"></a>nginx日志默认配置</h3><pre><code>log_format  main  &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos;                  &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos;                  &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;;# 日志示例192.168.10.58 - - [22/Apr/2020:15:44:46 +0800] &quot;GET /login HTTP/1.1&quot; 200 4313 &quot;-&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:67.0) Gecko/20100101 Firefox/67.0&quot;</code></pre><h3 id="nginx日志优化配置"><a href="#nginx日志优化配置" class="headerlink" title="nginx日志优化配置"></a>nginx日志优化配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">log_format  main  &#39;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#39;</span><br><span class="line">                  &#39;$status $body_bytes_sent &quot;$http_referer&quot; &#39;</span><br><span class="line">                  &#39;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#39; </span><br><span class="line">                  &#39;&quot;$upstream_addr&quot; $request_time $upstream_response_time&#39;;  # 最后一行新加</span><br><span class="line"> </span><br><span class="line"> # 日志示例</span><br><span class="line"> 116.55.250.205 - - [28&#x2F;Jul&#x2F;2020:00:07:34 +0800] &quot;POST &#x2F;user&#x2F;getInfo HTTP&#x2F;1.1&quot; 200 709 &quot;-&quot; &quot;Mozilla&#x2F;4.0 (compatible; MSIE 6.0; Windows NT 5.0; Alexa Toolbar; Maxthon 2.0)&quot; &quot;60.205.221.21&quot; &quot;172.16.16.3:8080&quot; 0.099 0.032</span><br></pre></td></tr></table></figure><h2 id="nginx日志参数详解"><a href="#nginx日志参数详解" class="headerlink" title="nginx日志参数详解"></a>nginx日志参数详解</h2><p>$remote_addr：远程客户端的IP地址。</p><p>-：空白，用一个“-”占位符替代，历史原因导致还存在。</p><p>$remote_user：远程客户端用户名称，用于记录浏览者进行身份验证时提供的名字，如登录百度的用户名scq2099yt，如果没有登录就是空白。</p><p>[$time_local]：访问的时间与时区，比如18/Jul/2012:17:00:01 +0800，时间信息最后的”+0800”表示服务器所处时区位于UTC之后的8小时。</p><p>$request：请求的URI和HTTP协议，这是整个PV日志记录中最有用的信息，记录服务器收到一个什么样的请求</p><p>$status：记录请求返回的http状态码，比如成功是200。</p><p>$body_bytes_sent：发送给客户端的文件主体内容的大小，比如899，可以将日志每条记录中的这个值累加起来以粗略估计服务器吞吐量。</p><p>$http_referer：记录从哪个页面链接访问过来的。 </p><p>$http_user_agent：客户端浏览器信息</p><p>$http_x_forwarded_for：客户端的真实ip，通常web服务器放在反向代理的后面，这样就不能获取到客户的IP地址了，通过$remote_add拿到的IP地址是反向代理服务器的iP地址。反向代理服务器在转发请求的http头信息中，可以增加x_forwarded_for信息，用以记录原有客户端的IP地址和原来客户端的请求的服务器地址。</p><p>$upstream_addr：upstream的地址，即真正提供服务的主机地址。 </p><p>$request_time：整个请求的总时间。 </p><p>$upstream_response_time：请求过程中，upstream的响应时间。</p><h3 id="其他的字段"><a href="#其他的字段" class="headerlink" title="其他的字段"></a>其他的字段</h3><p>$http_host  #请求地址，即浏览器中你输入的地址（IP或域名）</p><p>$request_body  #记录POST数据</p><h2 id="Web服务流量名词介绍"><a href="#Web服务流量名词介绍" class="headerlink" title="Web服务流量名词介绍"></a>Web服务流量名词介绍</h2><h3 id="网站并发连接数"><a href="#网站并发连接数" class="headerlink" title="网站并发连接数"></a>网站并发连接数</h3><p>定义为网站服务器在单位时间内能够处理的最大连接数。示例：某网站的并发是5000.意味着单位时间内（理解为1秒或数秒内），正在处理的连接数，正在建立的连接数，加起来一共是5000个。</p><h3 id="IP"><a href="#IP" class="headerlink" title="IP"></a>IP</h3><p>即Internet Protocol，一般指独立IP数，独立IP数是指不同IP地址的计算机访问网站时被计的总次数。一般一天00:00-24:00内相同IP地址只被计算一次。</p><h3 id="PV"><a href="#PV" class="headerlink" title="PV"></a>PV</h3><p>即Page View，中文翻译为页面浏览，即页面浏览量或点击量，不管客户端是否相同，也不管IP和网站页面是否相同，用户只要访问网站页面就会计算PV，一次计为一个PV</p><h3 id="UV"><a href="#UV" class="headerlink" title="UV"></a>UV</h3><p>即Unique Visitor，同一个客户端（PC或移动端）访问网站被计为一个访客。一天（00:00-24:00）内相同的客户端访问同一个网站只统计一次UV。UV一般是以客户端Cookie等技术作为统计依据的，实际统计会有误差。</p><h3 id="IP，PV，UV的区别在哪？"><a href="#IP，PV，UV的区别在哪？" class="headerlink" title="IP，PV，UV的区别在哪？"></a>IP，PV，UV的区别在哪？</h3><p>   举例说明：假设某个公司有10个员工，都访问了<u><a href="http://www.taobao.com" target="_blank" rel="noopener">www.taobao.com</a></u>这个网站。每个人平均浏览了5个页面，但是公司的对外出口是一个公网IP。所以对于<u><a href="http://www.taobao.com" target="_blank" rel="noopener">www.taobao.com</a></u>这个网站而言，只会计算1个独立IP访问。但是因为有10个人在访问<u><a href="http://www.taobao.com" target="_blank" rel="noopener">www.taobao.com</a></u>这个网站，并且平均都访问了5次，因此，对于<u><a href="http://www.taobao.com" target="_blank" rel="noopener">www.taobao.com</a></u>这个网站而言，PV数就是10x5=50个PV，而因为有10个人访问，就是10个不同的客户端访问，因此，UV（独立访客）为10.   </p><p> 因此上例结果为IP数为1个，PV数为50个，UV为10个。通过这个结果，不难看出，一个网站的独立IP数量要比网站实际访问的PV数量小得多。通常情况下，网站的UV数也会大于独立IP数</p><h2 id="Nginx日志常用分析命令示范"><a href="#Nginx日志常用分析命令示范" class="headerlink" title="Nginx日志常用分析命令示范"></a>Nginx日志常用分析命令示范</h2><p>（注：日志的格式不同，awk取的项不同。下面命令针对上面日志格式执行）</p><p><strong>1）总请求数</strong></p><p><code>wc -l access.log | awk &#39;{print $1}&#39;</code></p><p><strong>2）独立IP数</strong></p><p><code>awk &#39;{print $1}&#39; access.log | sort | uniq | wc -l</code></p><p><strong>3）每秒客户端请求数 TOP5</strong></p><p><code>cat access.log | awk &#39;{print $4}&#39; | sort | uniq -c | sort -rn | head -5</code></p><p><strong>4）访问最频繁IP Top5</strong></p><p><code>head -100 access.log | awk &#39;{print $1}&#39; | sort | uniq -c | sort -nr | head -5</code></p><p><code>head -100 access.log | awk &#39;{ips[$1]++}END{for(ip in ips){print ips[ip], ip}}&#39; | sort -nr | head -5</code></p><p><strong>5）访问最频繁的URL TOP5</strong></p><p><code>awk &#39;{print $7}&#39; access.log | sort | uniq -c | sort -nr | head -5</code></p><p><strong>6)响应大于5秒的URL TOP5</strong></p><p><code>head -3 access.log | awk &#39;{if ( $(NF-1) &gt; 5 ){print $7 }}&#39; | sort | uniq -c | sort -rn | head -5</code></p><p><strong>7)HTTP状态码(非200)统计 Top5</strong></p><p><code>awk &#39;{if ($9 != 200){print $7}}&#39; access.log | sort | uniq -c | sort -rn | head -5</code></p><p><strong>8)分析请求数大于50000的源IP</strong></p><p><code>head access.log | awk &#39;{print $NF}&#39;|sort | uniq -c | sort -nr | awk &#39;{if ($1 &gt;50000){print $2}}&#39;</code></p>]]></content>
      
      
      <categories>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
            <tag> log </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nginx的详细用法</title>
      <link href="/2020/07/27/nginx%E7%9A%84%E8%AF%A6%E7%BB%86%E7%94%A8%E6%B3%95/"/>
      <url>/2020/07/27/nginx%E7%9A%84%E8%AF%A6%E7%BB%86%E7%94%A8%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="nginx的upstream的5种分配方式"><a href="#nginx的upstream的5种分配方式" class="headerlink" title="nginx的upstream的5种分配方式"></a>nginx的upstream的5种分配方式</h2><blockquote><p>原文地址 <a href="http://blog.chinaunix.net/uid-20662363-id-3049712.html" target="_blank" rel="noopener">nginx的upstream支持5种方式的分配</a></p></blockquote><h3 id="轮询（默认）"><a href="#轮询（默认）" class="headerlink" title="轮询（默认）"></a>轮询（默认）</h3><p>每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 </p><h3 id="weight"><a href="#weight" class="headerlink" title="weight"></a>weight</h3><p>指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 </p><p>例如：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">upstream bakend &#123;</span><br><span class="line">    server 192.168.0.2 weight&#x3D;10;</span><br><span class="line">    server 192.168.0.3 weight&#x3D;5;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ip-hash"><a href="#ip-hash" class="headerlink" title="ip_hash"></a><strong>ip_hash</strong></h3><p>每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 </p><p>例如： </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">upstream bakend &#123;</span><br><span class="line">    ip_hash;</span><br><span class="line">    server 192.168.0.2:8080;</span><br><span class="line">    server 192.168.0.3:8080;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="fair（第三方）"><a href="#fair（第三方）" class="headerlink" title="fair（第三方）"></a>fair（第三方）</h3><p>按后端服务器的响应时间来分配请求，响应时间短的优先分配。 </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">upstream backend &#123;</span><br><span class="line">    fair;</span><br><span class="line">    server server1;</span><br><span class="line">    server server2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="url-hash（第三方）"><a href="#url-hash（第三方）" class="headerlink" title="url_hash（第三方）"></a>url_hash（第三方）</h3><p>按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。</p><p>例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法 </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">upstream backend &#123;</span><br><span class="line">    server squid1:3128;</span><br><span class="line">    server squid2:3128;</span><br><span class="line">    hash $request_uri;</span><br><span class="line">    hash_method crc32;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="nginx优化配置"><a href="#nginx优化配置" class="headerlink" title="nginx优化配置"></a>nginx优化配置</h2><p>Nginx  1.1.14版本以前与后端upstream服务器建立的都是短链接，即通过HTTP/1.0向后端发起连接，并把请求的”Connection”  header设为”close”。这样nginx往upstream后端发请求时，也会消耗很多的时间与带宽(重复连接)，如果让nginx与upstream后端建立起长链接，从nginx发起的请求就可以挑选一个合适的长链接发往upstream后端服务器，这样即可以节省带宽，也可以提高响应速度。</p><p>其中keepalive指定最大保持的长连接数为1024.，同时要求HTTP协议版本指定为HTTP 1.1，以及清除掉HTTP头部Connection</p><p>针对后端的keepalive是通过nginx.conf配置文件来指定的，典型配置如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">upstream bakend &#123;</span><br><span class="line">    keepalive 16;</span><br><span class="line">    server 192.168.0.2:8080;</span><br><span class="line">    server 192.168.0.3:8080;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="nginx配置文件"><a href="#nginx配置文件" class="headerlink" title="nginx配置文件"></a>nginx配置文件</h2><h3 id="nginx重定义请求头"><a href="#nginx重定义请求头" class="headerlink" title="nginx重定义请求头"></a>nginx重定义请求头</h3><blockquote><p> 参考文章:<a href="https://www.cnblogs.com/liuxia912/p/10943970.html" target="_blank" rel="noopener">proxy_set_header常见用法</a></p></blockquote><p>proxy_set_header用来重定义发往后端服务器的请求头。</p><p>语法:    proxy_set_header field value;</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location &#x2F; &#123;</span><br><span class="line">    proxy_set_header Host $host;</span><br><span class="line">    proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">    proxy_set_header REMOTE-HOST $remote_addr;</span><br><span class="line">    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">    proxy_pass http:&#x2F;&#x2F;backend;</span><br><span class="line">    error_page 404 &#x3D; &#x2F;404.html;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="默认用法"><a href="#默认用法" class="headerlink" title="默认用法"></a>默认用法</h4><p>有两个请求头会被重新定义： </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">proxy_set_header Host $proxy_host; &#x2F;&#x2F;默认会将后端服务器的HOST填写进去</span><br><span class="line">proxy_set_header Connection close;</span><br></pre></td></tr></table></figure><h4 id="proxy-set-header-X-real-ip-remote-addr"><a href="#proxy-set-header-X-real-ip-remote-addr" class="headerlink" title="proxy_set_header X-real-ip $remote_addr;"></a>proxy_set_header X-real-ip $remote_addr;</h4><p>其中这个X-real-ip是一个自定义的变量名，名字可以随意取，这样做完之后，用户的真实ip就被放在X-real-ip这个变量里了，然后，在web端可以这样获取：</p><p>​        request.getHeader(“X-real-ip”) </p><h4 id="proxy-set-header-Host-http-host"><a href="#proxy-set-header-Host-http-host" class="headerlink" title="proxy_set_header Host $http_host;"></a>proxy_set_header Host $http_host;</h4><p>如果想获取客户端访问的头部，可以这样来设置。 </p><p>但是，如果客户端请求头中没有携带这个头部，那么传递到后端服务器的请求也不含这个头部。 </p><h4 id="proxy-set-header-Host-host"><a href="#proxy-set-header-Host-host" class="headerlink" title="proxy_set_header Host $host;"></a><strong>proxy_set_header Host $host;</strong></h4><p>这个配置相当于上面配置的增强。 </p><p>它的值在请求包含”Host”请求头时为”Host”字段的值，在请求未携带”Host”请求头时为虚拟主机的主域名。</p><h4 id="proxy-set-header-X-Forwarded-For-remote-addr"><a href="#proxy-set-header-X-Forwarded-For-remote-addr" class="headerlink" title="proxy_set_header X-Forwarded-For $remote_addr;"></a>proxy_set_header X-Forwarded-For $remote_addr;</h4><p>同上。 </p><p>真实的显示出客户端原始ip。（nginx更多使用这条配置，X-Forwarded-For为默认字段，以下介绍均为默认字段）</p><h4 id="proxy-set-header-lt-lt-lt-gt-gt-gt-“”"><a href="#proxy-set-header-lt-lt-lt-gt-gt-gt-“”" class="headerlink" title="proxy_set_header &lt;&lt;&lt;*&gt;&gt;&gt; “”;"></a>proxy_set_header &lt;&lt;&lt;*&gt;&gt;&gt; “”;</h4><p>请求头的值为空，请求头将不会传送给后端服务器。 </p><h4 id="proxy-set-header-X-Forwarded-For-proxy-add-x-forwarded-for"><a href="#proxy-set-header-X-Forwarded-For-proxy-add-x-forwarded-for" class="headerlink" title="proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;"></a>proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</h4><p>在默认情况下经过proxy转发的请求，在后端看来远程地址都是proxy端的ip 。</p><p>添加这条配置之后： </p><p>意思是增加一个$proxy_add_x_forwarded_for到X-Forwarded-For里去，注意是<strong>增加</strong>，而不是覆盖，当然由于默认的X-Forwarded-For值是空的，所以我们总感觉X-Forwarded-For的值就等于$proxy_add_x_forwarded_for的值，实际上当你搭建两台nginx在不同的ip上，并且都使用了这段配置，那你会发现在web服务器端通过request.getHeader(“X-Forwarded-For”)获得的将会是<strong><em>客户端***</em></strong>ip<strong>和</strong>第一台<strong><em>*nginx的ip</em></strong>。</p><h4 id="更改域名请求头-重定向"><a href="#更改域名请求头-重定向" class="headerlink" title="更改域名请求头(重定向)"></a>更改域名请求头(重定向)</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">proxy_set_header Host crmtest.aty.sohuno.com;</span><br></pre></td></tr></table></figure><h2 id="location匹配规则"><a href="#location匹配规则" class="headerlink" title="location匹配规则"></a>location匹配规则</h2><p>匹配规则示例：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location &#x3D; &#x2F; &#123;</span><br><span class="line">    #规则A</span><br><span class="line">&#125;</span><br><span class="line">location &#x3D; &#x2F;login &#123;</span><br><span class="line">    #规则B</span><br><span class="line">&#125;</span><br><span class="line">location ^~ &#x2F;static&#x2F; &#123;</span><br><span class="line">    #规则C</span><br><span class="line">&#125;</span><br><span class="line">location ~ \.(gif|jpg|png|js|css)$ &#123;</span><br><span class="line">    #规则D</span><br><span class="line">&#125;</span><br><span class="line">location ~* \.png$ &#123;</span><br><span class="line">    #规则E</span><br><span class="line">&#125;</span><br><span class="line">location !~ \.xhtml$ &#123;</span><br><span class="line">    #规则F</span><br><span class="line">&#125;</span><br><span class="line">location !~* \.xhtml$ &#123;</span><br><span class="line">    #规则G</span><br><span class="line">&#125;</span><br><span class="line">location &#x2F; &#123;</span><br><span class="line">    #规则H</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>linux常见问题2</title>
      <link href="/2020/07/25/linux%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%982/"/>
      <url>/2020/07/25/linux%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%982/</url>
      
        <content type="html"><![CDATA[<h2 id="linux常见问题2"><a href="#linux常见问题2" class="headerlink" title="linux常见问题2"></a>linux常见问题2</h2><h3 id="文件由windows格式转为unix"><a href="#文件由windows格式转为unix" class="headerlink" title="文件由windows格式转为unix"></a>文件由windows格式转为unix</h3><h4 id="查看文本文件格式"><a href="#查看文本文件格式" class="headerlink" title="查看文本文件格式"></a>查看文本文件格式</h4><p>:set ff  # 末行模式</p><img src="http://wangzhangtao.com/img/body/2.linux%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98-2/image-20200811145544737.png" alt="字体格式示意图" style="zoom:100%;" align="left" /><h4 id="修改文件文件格式"><a href="#修改文件文件格式" class="headerlink" title="修改文件文件格式"></a>修改文件文件格式</h4><p>:set ff=unix   </p><p>更改为unix格式，如果改为windows,则设置 ff=dos</p><h3 id="报错：libpcre-so-0-cannot-open-shared-object-file"><a href="#报错：libpcre-so-0-cannot-open-shared-object-file" class="headerlink" title="报错：libpcre.so.0: cannot open shared object file"></a>报错：libpcre.so.0: cannot open shared object file</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-41 etc]# &#x2F;data&#x2F;zabbix&#x2F;sbin&#x2F;zabbix_agentd </span><br><span class="line">&#x2F;data&#x2F;zabbix&#x2F;sbin&#x2F;zabbix_agentd: error while loading shared libraries: libpcre.so.0: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>缺少文件libpcre.so.0导致的，我们做一个软链接</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ln -s &#x2F;usr&#x2F;lib64&#x2F;libpcre.so.1 &#x2F;lib64&#x2F;libpcre.so.0</span><br></pre></td></tr></table></figure><h2 id="解决centos7-开机-etc-rc-local-不执行的问题"><a href="#解决centos7-开机-etc-rc-local-不执行的问题" class="headerlink" title="解决centos7 开机/etc/rc.local 不执行的问题"></a>解决centos7 开机/etc/rc.local 不执行的问题</h2><blockquote><p>原文地址：<a href="https://www.jb51.net/article/108874.htm" target="_blank" rel="noopener">解决centos7 开机/etc/rc.local 不执行的问题</a></p></blockquote><p>最近发现<strong>centos7</strong> 的<strong>/etc/rc.local</strong>不会开机执行，于是认真看了下<strong>/etc/rc.local</strong>文件内容的就发现了问题的原因了</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line"># THIS FILE IS ADDED FOR COMPATIBILITY PURPOSES</span><br><span class="line">#</span><br><span class="line"># It is highly advisable to create own systemd services or udev rules</span><br><span class="line"># to run scripts during boot instead of using this file.</span><br><span class="line">#</span><br><span class="line"># In constrast to previous versions due to parallel execution during boot</span><br><span class="line"># this script will NOT be run after all other services.</span><br><span class="line">#</span><br><span class="line"># Please note that you must run &#39;chmod +x &#x2F;etc&#x2F;rc.d&#x2F;rc.local&#39; to ensure</span><br><span class="line"># that this script will be executed during boot.</span><br></pre></td></tr></table></figure><p> <strong>翻译：</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#这个文件是为了兼容性的问题而添加的。</span><br><span class="line">#</span><br><span class="line">#强烈建议创建自己的systemd服务或udev规则来在开机时运行脚本而不是使用这个文件。</span><br><span class="line">#</span><br><span class="line">#与以前的版本引导时的并行执行相比较，这个脚本将不会在其他所有的服务后执行。</span><br><span class="line">#</span><br><span class="line">#请记住，你必须执行“chmod +x &#x2F;etc&#x2F;rc.d&#x2F;rc.local”来确保确保这个脚本在引导时执行。</span><br></pre></td></tr></table></figure><p>于是我有确认了下<strong>/etc/rc.local</strong>的权限</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# ll &#x2F;etc&#x2F;rc.local</span><br><span class="line">lrwxrwxrwx. 1 root root 13 8月 12 06:09 &#x2F;etc&#x2F;rc.local -&gt; rc.d&#x2F;rc.local</span><br><span class="line">[root@localhost ~]# ll &#x2F;etc&#x2F;rc.d&#x2F;rc.local</span><br><span class="line">-rw-r--r--. 1 root root 477 6月 10 13:35 &#x2F;etc&#x2F;rc.d&#x2F;rc.local</span><br></pre></td></tr></table></figure><p>/etc/rc.d/rc.local没有执行权限，于是按说明的内容执行</p><p><strong>chmod +x /etc/rc.d/rc.local</strong></p><p>重启后发现/etc/rc.local能够执行了。</p><p>看样子是版本的变迁，/etc/rc.local /etc/rc.d/rc.local正在弃用的路上。</p><h2 id="创建容器时报错"><a href="#创建容器时报错" class="headerlink" title="创建容器时报错"></a>创建容器时报错</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">IPv4 forwarding is disabled. Networking will not work.</span><br></pre></td></tr></table></figure><p>是因为宿主机没有开启网络路由转发功能，所以解决办法：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo &quot;net.ipv4.ip_forward&#x3D;1&quot; &gt;&gt; &#x2F;usr&#x2F;lib&#x2F;sysctl.d&#x2F;00-system.conf</span><br><span class="line">systemctl restart network</span><br></pre></td></tr></table></figure><h3 id="mac清除dns缓存"><a href="#mac清除dns缓存" class="headerlink" title="mac清除dns缓存"></a>mac清除dns缓存</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo dscacheutil -flushcache</span><br><span class="line">sudo killall -HUP mDNSResponder</span><br></pre></td></tr></table></figure><h3 id="dubbo服务连接zookeeper报错"><a href="#dubbo服务连接zookeeper报错" class="headerlink" title="dubbo服务连接zookeeper报错"></a>dubbo服务连接zookeeper报错</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Error starting ApplicationContext. To display the conditions report re-run your application with &#39;debug&#39; enabled.</span><br><span class="line">2020-09-28 18:43:22.443 [TID: N&#x2F;A] [main] ERROR o.s.boot.SpringApplication -Application run failed</span><br><span class="line">java.lang.IllegalStateException: Failed to receive INITIALIZED event from zookeeper, pls. check if url zookeeper:&#x2F;&#x2F;zk-cs:2181&#x2F;org.apache.dubbo.metadata.report.MetadataReport?dubbo.config-center.root-path&#x3D;&#x2F; is correct</span><br><span class="line">        at org.apache.dubbo.configcenter.support.zookeeper.ZookeeperDynamicConfiguration.&lt;init&gt;(ZookeeperDynamicConfiguration.java:63)</span><br><span class="line">        at org.apache.dubbo.configcenter.support.zookeeper.ZookeeperDynamicConfigurationFactory.createDynamicConfiguration(ZookeeperDynamicConfigurationFactory.java:37)</span><br><span class="line">        at org.apache.dubbo.common.config.configcenter.AbstractDynamicConfigurationFactory.lambda$getDynamicConfiguration$0(AbstractDynamicConfigurationFactory.java:39)</span><br><span class="line">        at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660)</span><br></pre></td></tr></table></figure><p>意思就是启动时，缓存将与服务器同步，在发现新节点时发布一系列NODE_ADDED事件。缓存完全同步后，将发布INITIALIZED事件。在此事件之后发布的所有事件都表示实际的服务器端变化。重新连接时，缓存将与服务器重新同步其内部状态，并在其内部状态完全刷新后再次触发此事件。</p><h4 id="解决办法1"><a href="#解决办法1" class="headerlink" title="解决办法1"></a>解决办法1</h4><p>将“init.timeout”的时间设置长一些，比如设置超时时间为10s：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dubbo.config-center.parameters[init.timeout]&#x3D;10000</span><br></pre></td></tr></table></figure><h4 id="解决办法2"><a href="#解决办法2" class="headerlink" title="解决办法2"></a>解决办法2</h4><p>修改zookeeper的minSessionTimeout</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;data&#x2F;zookeeper&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">minSessionTimeout&#x3D;20000</span><br></pre></td></tr></table></figure><h4 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h4><ul><li><a href="https://blog.csdn.net/weixin_38308374/article/details/105984022" target="_blank" rel="noopener">https://blog.csdn.net/weixin_38308374/article/details/105984022</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> 常见问题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 常见问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker的安装和使用</title>
      <link href="/2020/07/24/docker%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
      <url>/2020/07/24/docker%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h2><h3 id="编辑安装脚本"><a href="#编辑安装脚本" class="headerlink" title="编辑安装脚本"></a>编辑安装脚本</h3><p>vim /data/shell/install_docker.sh</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装docker服务</span><br><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager --add-repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo</span><br><span class="line"># yum remove docker  docker-common docker-selinux docker-engine</span><br><span class="line">yum install -y docker-ce-18.09.7 docker-ce-cli-18.09.7 containerd.io</span><br><span class="line">sleep 3</span><br><span class="line"></span><br><span class="line"># 配置docker参数，注意不同主机，docker bip不同</span><br><span class="line">mkdir &#x2F;etc&#x2F;docker&#x2F;</span><br><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;graph&quot;: &quot;&#x2F;data&#x2F;docker&quot;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay&quot;,</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;tpd9v3c5.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;insecure-registries&quot;: [&quot;registry.access.redhat.com&quot;,&quot;quay.io&quot;,&quot;harbor.od.com&quot;],</span><br><span class="line">  &quot;bip&quot;: &quot;172.16.200.1&#x2F;24&quot;,</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],</span><br><span class="line">  &quot;live-restore&quot;: true</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sed -i &quot;s&amp;172.16.200.1&#x2F;24&amp;172.16.$&#123;HOSTNUM&#125;.1&#x2F;24&amp;&quot; &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line"></span><br><span class="line"># 启动docker</span><br><span class="line">systemctl start docker</span><br><span class="line"># 添加docker开机自启</span><br><span class="line">systemctl enable docker</span><br><span class="line"></span><br><span class="line"># 查看docker版本号</span><br><span class="line">docker version</span><br></pre></td></tr></table></figure><h3 id="远程执行安装脚本"><a href="#远程执行安装脚本" class="headerlink" title="远程执行安装脚本"></a>远程执行安装脚本</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_docker.sh | sh</span><br></pre></td></tr></table></figure><h2 id="查看docker系统资源"><a href="#查看docker系统资源" class="headerlink" title="查看docker系统资源"></a>查看docker系统资源</h2><blockquote><p> 官网文档 <a href="https://docs.docker.com/engine/reference/commandline/system/" target="_blank" rel="noopener">docker system</a></p></blockquote><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# docker system --help</span><br><span class="line">Commands:</span><br><span class="line">  df          Show docker disk usage  # 显示docker磁盘使用情况</span><br><span class="line">  events      Get real time events from the server  # 从服务器获取实时事件</span><br><span class="line">  info        Display system-wide information  # 显示系统范围的信息</span><br><span class="line">  prune       Remove unused data  # 删除未使用的数据</span><br></pre></td></tr></table></figure><h3 id="df-查看docker磁盘使用情况"><a href="#df-查看docker磁盘使用情况" class="headerlink" title="df: 查看docker磁盘使用情况"></a>df: 查看docker磁盘使用情况</h3><p>提供Docker整体磁盘使用率的概况，包括镜像、容器和（本地）volume。所以我们现在随时都可以查看Docker使用了多少资源。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# docker system df</span><br><span class="line">TYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE(可回收的)</span><br><span class="line">Images              3                   1                   1.197GB             775.7MB (64%)</span><br><span class="line">Containers          1                   1                   90.34kB             0B (0%)</span><br><span class="line">Local Volumes       3                   3                   67.21MB             0B (0%)</span><br><span class="line">Build Cache         0                   0                   0B                  0B</span><br></pre></td></tr></table></figure><h3 id="info-查看系统信息"><a href="#info-查看系统信息" class="headerlink" title="info: 查看系统信息"></a>info: 查看系统信息</h3><p>查看整个docker系统的信息, 这个命令的缩写为<code>docker info</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# docker system info</span><br><span class="line">Client:</span><br><span class="line"> Debug Mode: false</span><br><span class="line"></span><br><span class="line">Server:</span><br><span class="line"> Containers: 1</span><br><span class="line">  Running: 1</span><br><span class="line">  Paused: 0</span><br><span class="line">  Stopped: 0</span><br><span class="line"> Images: 1</span><br><span class="line"> Server Version: 18.09.7</span><br><span class="line"> Storage Driver: overlay2</span><br><span class="line">  Backing Filesystem: xfs</span><br><span class="line">  Supports d_type: true</span><br><span class="line">  Native Overlay Diff: true</span><br></pre></td></tr></table></figure><h3 id="events-从服务器获取实时事件"><a href="#events-从服务器获取实时事件" class="headerlink" title="events: 从服务器获取实时事件"></a>events: 从服务器获取实时事件</h3><p>缩写命令 docker events</p><ul><li>-f “image”=”mysql:5.6” </li><li><strong>–since ：</strong>从指定的时间戳后显示所有事件;</li><li><strong>–until ：</strong>流水时间显示到指定的时间为止；</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# docker events  --since&#x3D;&quot;1467302400&quot;</span><br><span class="line">2016-07-08T19:44:54.501277677+08:00 network connect 66f958fd13dc4314ad20034e576d5c5eba72e0849dcc38ad9e8436314a4149d4 (container&#x3D;b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64, name&#x3D;bridge, type&#x3D;bridge)</span><br><span class="line">2016-07-08T19:44:54.723876221+08:00 container start b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64 (image&#x3D;nginx:latest, name&#x3D;elegant_albattani)</span><br><span class="line">2016-07-08T19:44:54.726110498+08:00 container resize b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64 (height&#x3D;39, image&#x3D;nginx:latest, name&#x3D;elegant_albattani, width&#x3D;167)</span><br><span class="line">2016-07-08T19:46:22.137250899+08:00 container die b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64 (exitCode&#x3D;0, image&#x3D;nginx:latest, name&#x3D;elegant_albattani)</span><br></pre></td></tr></table></figure><h3 id="prune-清理磁盘"><a href="#prune-清理磁盘" class="headerlink" title="prune: 清理磁盘"></a>prune: 清理磁盘</h3><blockquote><p> <strong>谨慎操作，镜像和容器将会被批量清理</strong></p></blockquote><ul><li>-f 强制删除，不再询问</li><li>-a 所有没有被使用的镜像都删除</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# docker system prune</span><br><span class="line">WARNING! This will remove:</span><br><span class="line"></span><br><span class="line">  - all stopped containers  # 所有停止的容器</span><br><span class="line">  - all networks not used by at least one container  # 所有没有被容器使用的网络</span><br><span class="line">  - all dangling images  # 所有废弃的镜像（没有镜像名称）</span><br><span class="line">  - all dangling build cache  # 所有废弃的构建缓存</span><br><span class="line"></span><br><span class="line">Are you sure you want to continue? [y&#x2F;N] y</span><br><span class="line">Total reclaimed space: 0B</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker的安装和使用</title>
      <link href="/2020/07/24/docker%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
      <url>/2020/07/24/docker%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h2><h3 id="编辑安装脚本"><a href="#编辑安装脚本" class="headerlink" title="编辑安装脚本"></a>编辑安装脚本</h3><p>vim /data/shell/install_docker.sh</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装docker服务</span><br><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager --add-repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo</span><br><span class="line">yum remove docker  docker-common docker-selinux docker-engine</span><br><span class="line">yum install docker-ce-18.06.0.ce -y</span><br><span class="line">sleep 3</span><br><span class="line"></span><br><span class="line"># 配置docker参数，注意不同主机，docker bip不同</span><br><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;graph&quot;: &quot;&#x2F;data&#x2F;docker&quot;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay&quot;,</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;tpd9v3c5.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;insecure-registries&quot;: [&quot;registry.access.redhat.com&quot;,&quot;quay.io&quot;,&quot;harbor.od.com&quot;],</span><br><span class="line">  &quot;bip&quot;: &quot;172.16.200.1&#x2F;24&quot;,</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],</span><br><span class="line">  &quot;live-restore&quot;: true</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sed -i &quot;s&amp;172.16.200.1&#x2F;24&amp;172.16.$&#123;HOSTNUM&#125;.1&#x2F;24&amp;&quot; &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line"></span><br><span class="line"># 启动docker</span><br><span class="line">systemctl start docker</span><br><span class="line"># 添加docker开机自启</span><br><span class="line">systemctl enable docker</span><br><span class="line"></span><br><span class="line"># 查看docker版本号</span><br><span class="line">docker version</span><br></pre></td></tr></table></figure><h3 id="远程执行安装脚本"><a href="#远程执行安装脚本" class="headerlink" title="远程执行安装脚本"></a>远程执行安装脚本</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_docker.sh | sh</span><br></pre></td></tr></table></figure><h2 id="查看docker系统资源"><a href="#查看docker系统资源" class="headerlink" title="查看docker系统资源"></a>查看docker系统资源</h2><blockquote><p> 官网文档 <a href="https://docs.docker.com/engine/reference/commandline/system/" target="_blank" rel="noopener">docker system</a></p></blockquote><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# docker system --help</span><br><span class="line">Commands:</span><br><span class="line">  df          Show docker disk usage  # 显示docker磁盘使用情况</span><br><span class="line">  events      Get real time events from the server  # 从服务器获取实时事件</span><br><span class="line">  info        Display system-wide information  # 显示系统范围的信息</span><br><span class="line">  prune       Remove unused data  # 删除未使用的数据</span><br></pre></td></tr></table></figure><h3 id="df-查看docker磁盘使用情况"><a href="#df-查看docker磁盘使用情况" class="headerlink" title="df: 查看docker磁盘使用情况"></a>df: 查看docker磁盘使用情况</h3><p>提供Docker整体磁盘使用率的概况，包括镜像、容器和（本地）volume。所以我们现在随时都可以查看Docker使用了多少资源。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# docker system df</span><br><span class="line">TYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE(可回收的)</span><br><span class="line">Images              3                   1                   1.197GB             775.7MB (64%)</span><br><span class="line">Containers          1                   1                   90.34kB             0B (0%)</span><br><span class="line">Local Volumes       3                   3                   67.21MB             0B (0%)</span><br><span class="line">Build Cache         0                   0                   0B                  0B</span><br></pre></td></tr></table></figure><h3 id="info-查看系统信息"><a href="#info-查看系统信息" class="headerlink" title="info: 查看系统信息"></a>info: 查看系统信息</h3><p>查看整个docker系统的信息, 这个命令的缩写为<code>docker info</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# docker system info</span><br><span class="line">Client:</span><br><span class="line"> Debug Mode: false</span><br><span class="line"></span><br><span class="line">Server:</span><br><span class="line"> Containers: 1</span><br><span class="line">  Running: 1</span><br><span class="line">  Paused: 0</span><br><span class="line">  Stopped: 0</span><br><span class="line"> Images: 1</span><br><span class="line"> Server Version: 18.09.7</span><br><span class="line"> Storage Driver: overlay2</span><br><span class="line">  Backing Filesystem: xfs</span><br><span class="line">  Supports d_type: true</span><br><span class="line">  Native Overlay Diff: true</span><br></pre></td></tr></table></figure><h3 id="events-从服务器获取实时事件"><a href="#events-从服务器获取实时事件" class="headerlink" title="events: 从服务器获取实时事件"></a>events: 从服务器获取实时事件</h3><p>缩写命令 docker events</p><ul><li>-f “image”=”mysql:5.6” </li><li><strong>–since ：</strong>从指定的时间戳后显示所有事件;</li><li><strong>–until ：</strong>流水时间显示到指定的时间为止；</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# docker events  --since&#x3D;&quot;1467302400&quot;</span><br><span class="line">2016-07-08T19:44:54.501277677+08:00 network connect 66f958fd13dc4314ad20034e576d5c5eba72e0849dcc38ad9e8436314a4149d4 (container&#x3D;b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64, name&#x3D;bridge, type&#x3D;bridge)</span><br><span class="line">2016-07-08T19:44:54.723876221+08:00 container start b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64 (image&#x3D;nginx:latest, name&#x3D;elegant_albattani)</span><br><span class="line">2016-07-08T19:44:54.726110498+08:00 container resize b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64 (height&#x3D;39, image&#x3D;nginx:latest, name&#x3D;elegant_albattani, width&#x3D;167)</span><br><span class="line">2016-07-08T19:46:22.137250899+08:00 container die b8573233d675705df8c89796a2c2687cd8e36e03646457a15fb51022db440e64 (exitCode&#x3D;0, image&#x3D;nginx:latest, name&#x3D;elegant_albattani)</span><br></pre></td></tr></table></figure><h3 id="prune-清理磁盘"><a href="#prune-清理磁盘" class="headerlink" title="prune: 清理磁盘"></a>prune: 清理磁盘</h3><blockquote><p> <strong>谨慎操作，镜像和容器将会被批量清理</strong></p></blockquote><ul><li>-f 强制删除，不再询问</li><li>-a 所有没有被使用的镜像都删除</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# docker system prune</span><br><span class="line">WARNING! This will remove:</span><br><span class="line"></span><br><span class="line">  - all stopped containers  # 所有停止的容器</span><br><span class="line">  - all networks not used by at least one container  # 所有没有被容器使用的网络</span><br><span class="line">  - all dangling images  # 所有废弃的镜像（没有镜像名称）</span><br><span class="line">  - all dangling build cache  # 所有废弃的构建缓存</span><br><span class="line"></span><br><span class="line">Are you sure you want to continue? [y&#x2F;N] y</span><br><span class="line">Total reclaimed space: 0B</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>nginx的安装和配置</title>
      <link href="/2020/07/24/nginx%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/"/>
      <url>/2020/07/24/nginx%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<h2 id="nginx域名模版"><a href="#nginx域名模版" class="headerlink" title="nginx域名模版"></a>nginx域名模版</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">upstream od-www &#123;</span><br><span class="line">    keepalive 16;</span><br><span class="line">    server 172.16.16.1:8080  max_fails&#x3D;2 fail_timeout&#x3D;30s;</span><br><span class="line">    server 172.16.16.2:8080  max_fails&#x3D;2 fail_timeout&#x3D;30s;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  www.od.com;</span><br><span class="line">    index index.html index.htm index.jsp;</span><br><span class="line">    access_log logs&#x2F;www.od.com.log main;  # 使用main自定义类型的日志模版</span><br><span class="line"></span><br><span class="line">    location &#x2F; &#123;  # nginx转发</span><br><span class="line">        proxy_set_header X-Forwarded-For $http_x_forwarded_for;</span><br><span class="line">        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">        proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">        proxy_ignore_client_abort on;  # 忽略客户端中止，499</span><br><span class="line">        proxy_set_header Host $Host;</span><br><span class="line">        proxy_pass http:&#x2F;&#x2F;od-www;</span><br><span class="line">        error_page 404 &#x3D; &#x2F;404.html;</span><br><span class="line">    &#125;</span><br><span class="line">      </span><br><span class="line">    rewrite ^(.*)$  https:&#x2F;&#x2F;$host$1 permanent;  # http协议永久转发https协议</span><br><span class="line"></span><br><span class="line">    if ($http_referer ~* &quot;www.google.com&quot;) &#123;  # 根据网站来源，进行转发</span><br><span class="line">        rewrite ^&#x2F;(.*)$ http:&#x2F;&#x2F;english.wang.com redirect;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if ($host &#x3D; &#39;wang.com&#39;) &#123;  # 默认使用www域名</span><br><span class="line">        rewrite ^&#x2F;(.*)$ http:&#x2F;&#x2F;www.wang.com&#x2F;$1 permanent;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="upsteam的使用"><a href="#upsteam的使用" class="headerlink" title="upsteam的使用"></a>upsteam的使用</h2><blockquote><p>本站关于<a href="/2020/07/27/nginx的详细用法/#nginx的upstream支持的5种分配方式">upsteam的5种分配方式</a>  <a href="http://wiki.nginx.org/NginxHttpUpstreamModule" target="_blank" rel="noopener">官网upstream模块</a></p></blockquote><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">upstream bakend&#123;  #定义负载均衡设备的Ip及设备状态</span><br><span class="line">    ip_hash;</span><br><span class="line">    server 127.0.0.1:6060 max_fails&#x3D;2 fail_timeout&#x3D;30s;</span><br><span class="line">    server 127.0.0.1:7070 weight&#x3D;2;</span><br><span class="line">    server 127.0.0.1:8080 backup;</span><br><span class="line">    server 127.0.0.1:9090 down;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在需要使用负载均衡的server中增加<br><code>proxy_pass http://bakend/;</code></p><h3 id="每个设备的状态"><a href="#每个设备的状态" class="headerlink" title="每个设备的状态"></a>每个设备的状态</h3><ol><li><p>max_fails ：当超过最大次数时，返回proxy_next_upstream 模块定义的错误 ;max_fails默认值为1.max_fails设为0则表示把这个检查取消</p></li><li><p>fail_timeout : max_fails次失败后，暂停的时间。fail_timeout的默认值是10s</p></li><li><p>weignt默认为1，weight越大，负载的权重就越大</p></li><li><p>backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻</p></li><li><p>down表示当前的server不可用，暂时不参于负载</p></li></ol><p>nginx支持同时设置多组的负载均衡，用来给不用的server来使用。 </p><p>client_body_in_file_only 设置为On 可以讲client post过来的数据记录到文件中用来做debug </p><p>client_body_temp_path 设置记录文件的目录 可以设置最多3层目录 </p><h3 id="负载均衡检查模块"><a href="#负载均衡检查模块" class="headerlink" title="负载均衡检查模块"></a>负载均衡检查模块</h3><blockquote><p> 参考文章 <a href="https://www.jianshu.com/p/171cacc346e7" target="_blank" rel="noopener">Nginx的负载均衡max_fails和fail_timeout设置</a></p></blockquote><p>​        对于负载均衡的节点可以配置如下可选参数参数：</p><p><code>max_fails=1 fail_timeout=10s</code></p><p>​        这个是Nginx在负载均衡功能中，用于判断后端节点状态，所用到两个参数。</p><p>​        Nginx基于连接探测，如果发现后端异常，在单位周期为fail_timeout设置的时间，中达到max_fails次数，这个周期次数内，如果后端同一个节点不可用，那么接下来将把节点标记为不可用，并等待下一个周期10秒以后（同样时常为fail_timeout）再一次去请求，判断是否连接是否成功。如果成功，将恢复之前的轮询方式，如果不可用将在下一个周期(fail_timeout)再试一次。</p><p>​        在两个节点都可用的情况下，突然有一个节点挂掉，客户端请求过来后哪怕请求到了不可用的节点，此次请求也不会失败，因为Nginx会把此次请求转发到另外一个可用节点，再把结果返回给客户端。</p><p>​        当一个节点挂掉，Nginx不知道节点是否恢复的时候，会把客户端的请求同时转发到两个节点，判断节点健康情况。</p><h2 id="nginx问题收集"><a href="#nginx问题收集" class="headerlink" title="nginx问题收集"></a>nginx问题收集</h2><h3 id="nginx中文乱码"><a href="#nginx中文乱码" class="headerlink" title="nginx中文乱码"></a>nginx中文乱码</h3><p>在conf中加入配置即可</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    charset utf-8;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="nginx报错Request-Entity-Too-Large"><a href="#nginx报错Request-Entity-Too-Large" class="headerlink" title="nginx报错Request Entity Too Large"></a>nginx报错Request Entity Too Large</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Return code is: 413, ReasonPhrase: Request Entity Too Large.</span><br></pre></td></tr></table></figure><h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><p>服务器限制了文件上传大小，上传文件超过了服务器限制！默认上传大小60m</p><p>修改服务器限制的文件大小</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">client_max_body_size 200m;</span><br></pre></td></tr></table></figure><h3 id="nginx强制下载txt等文件"><a href="#nginx强制下载txt等文件" class="headerlink" title="nginx强制下载txt等文件"></a>nginx强制下载txt等文件</h3><p>当前的浏览器能够识别文件格式，如果浏览器本身能够解析就会默认打开，如果不能解析就会下载该文件。比如txt文件就直接被解析，还有其他文件也是一样。</p><p>那么使用nginx做资源服务器的时候，如何强制下载文件呢？</p><p>添加配置文件 <code>add_header Content-Disposition: &#39;attachment;&#39;</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if ($request_filename ~* ^.*?\.(txt|pdf|doc|xls)$)&#123; </span><br><span class="line">    add_header Content-Disposition: &#39;attachment;&#39;; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="同一个用户访问时-转发到后台同一台机器"><a href="#同一个用户访问时-转发到后台同一台机器" class="headerlink" title="同一个用户访问时,转发到后台同一台机器"></a>同一个用户访问时,转发到后台同一台机器</h3><p>添加配置 <code>ip_hash</code>;</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">upstream backend&#123;</span><br><span class="line">    ip_hash;</span><br><span class="line">    server 192.168.128.1:8080 max_fails&#x3D;2 fail_timeout&#x3D;30s;</span><br><span class="line">    server 192.168.128.2:8080 max_fails&#x3D;2 fail_timeout&#x3D;30s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="nginx添加IP地址白名单"><a href="#nginx添加IP地址白名单" class="headerlink" title="nginx添加IP地址白名单"></a>nginx添加IP地址白名单</h3><blockquote><p>官网链接: <a href="http://nginx.org/en/docs/http/ngx_http_geo_module.html" target="_blank" rel="noopener">ngx_http_geo_module</a>  <a href="http://nginx.org/en/docs/http/ngx_http_map_module.html" target="_blank" rel="noopener">ngx_http_map_module</a></p></blockquote><p>在没有人为删除的情况下（–without-http_geo_module或–without-http_map_module），nginx默认加载了ngx-http-geo-module和ngx-http-map-module相关内容；</p><p>ngx-http-geo-module可以用来创建变量，变量值依赖于客户端 ip 地址;</p><p>修改nginx的配置文件nginx.conf</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    geo $remote_addr $ip_whitelist &#123;</span><br><span class="line">        default 0;  # 0表示禁止访问，1表示允许访问</span><br><span class="line">        192.168.0.2 1 ;</span><br><span class="line">        192.168.0.3 1 ;</span><br><span class="line">        include vhosts&#x2F;ip_white.config;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        if ( $ip_whitelist !&#x3D; 1 ) &#123; return 403; &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="nginx获取客户端真实IP"><a href="#nginx获取客户端真实IP" class="headerlink" title="nginx获取客户端真实IP"></a>nginx获取客户端真实IP</h3><blockquote><p>本站详细介绍<a href="/2020/07/27/nginx的详细用法/#nginx重定义请求头">proxy_set_header重定义请求头</a> </p><p>参考文章:<a href="https://www.cnblogs.com/liuxia912/p/10943970.html" target="_blank" rel="noopener">proxy_set_header常见用法</a></p></blockquote><p>proxy_set_header用来重定义发往后端服务器的请求头。</p><p>语法:    proxy_set_header field value;</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">location &#x2F; &#123;</span><br><span class="line">    proxy_set_header Host $host;</span><br><span class="line">    proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">    proxy_set_header REMOTE-HOST $remote_addr;</span><br><span class="line">    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">    proxy_pass http:&#x2F;&#x2F;backend;</span><br><span class="line">    error_page 404 &#x3D; &#x2F;404.html;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="nginx端口映射"><a href="#nginx端口映射" class="headerlink" title="nginx端口映射"></a>nginx端口映射</h2><p>需要加载nginx模块<code>ngx_stream_module.so</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">include &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;modules&#x2F;mod-stream.conf</span><br><span class="line"># load_module &quot;&#x2F;usr&#x2F;lib64&#x2F;nginx&#x2F;modules&#x2F;ngx_stream_module.so&quot;;</span><br><span class="line">include &#x2F;etc&#x2F;nginx&#x2F;port&#x2F;port.conf;</span><br></pre></td></tr></table></figure><p>查看文件 /etc/nginx/port/port.conf</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">stream &#123;</span><br><span class="line">    upstream wang-20880 &#123;</span><br><span class="line">        server 192.168.1.2:20880 max_fails&#x3D;3 fail_timeout&#x3D;30s;</span><br><span class="line">    &#125;</span><br><span class="line">    server &#123;</span><br><span class="line">        listen 20880;</span><br><span class="line">        proxy_connect_timeout 2s;</span><br><span class="line">        proxy_timeout 900s;</span><br><span class="line">        proxy_pass wang-20880;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s添加开发使用的ServiceAccount</title>
      <link href="/2020/07/22/k8s%E6%B7%BB%E5%8A%A0%E5%BC%80%E5%8F%91%E4%BD%BF%E7%94%A8%E7%9A%84serviceaccount/"/>
      <url>/2020/07/22/k8s%E6%B7%BB%E5%8A%A0%E5%BC%80%E5%8F%91%E4%BD%BF%E7%94%A8%E7%9A%84serviceaccount/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>  集群管理员 ClusterRole:cluster-admin</p><h3 id="创建资源配置清单"><a href="#创建资源配置清单" class="headerlink" title="创建资源配置清单"></a>创建资源配置清单</h3><p><strong>tech-user.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: tech-user</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: tech-user</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: view</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: tech-user</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: tech-user-node</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:node</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: tech-user</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: tech-user-pvp</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:persistent-volume-provisioner</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: tech-user</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure><h3 id="执行资源配置清单"><a href="#执行资源配置清单" class="headerlink" title="执行资源配置清单"></a>执行资源配置清单</h3><p>执行资源配置清单命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f tech-user.yaml</span><br></pre></td></tr></table></figure><p>执行资源配置清单过程</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test1 test]# kubectl apply -f tech-user.yaml </span><br><span class="line">serviceaccount&#x2F;tech-user created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;tech-user created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;tech-user-node created</span><br></pre></td></tr></table></figure><p>查看token </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo $(kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep tech-user | awk &#39;&#123;print $1&#125;&#39;) -o go-template&#x3D;&#39;&#123;&#123;.data.token&#125;&#125;&#39; | base64 -d)</span><br></pre></td></tr></table></figure><img src="http://wangzhangtao.com/img/body/5.k8s添加开发使用的ServiceAccount/image-20200729135251710.png" alt="token展示示例图" style="zoom:80%;max-width:90%" /><h2 id="添加某个namespace操作pod的权限"><a href="#添加某个namespace操作pod的权限" class="headerlink" title="添加某个namespace操作pod的权限"></a>添加某个namespace操作pod的权限</h2><p>创建成功以后，开发可以看到所有命名空间的pod和日志，但是没有进入pod的权限。</p><p>进入pod时报错：pods “uc-service-869cd47b8d-kgrrs” is forbidden: User “system:serviceaccount:kube-system:tech-user” cannot create resource “pods/exec” in API group “” in the namespace “test”</p><p>示例图：</p><img src="http://wangzhangtao.com/img/body/5.k8s添加开发使用的ServiceAccount/image-20200722161326884.png" alt="进入pod权限被拒示例图" style="zoom:67%;max-width:80%" /><h3 id="创建资源配置清单-1"><a href="#创建资源配置清单-1" class="headerlink" title="创建资源配置清单"></a>创建资源配置清单</h3><p>test4-user.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---    </span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: test4-user</span><br><span class="line">  namespace: test4</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &#39;&#39;</span><br><span class="line">  resources: [&quot;pods&quot;, &quot;pods&#x2F;log&quot;, &quot;pods&#x2F;status&quot;, &quot;pods&#x2F;exec&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;]</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: test4-user</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: test4-user</span><br><span class="line">subjects:</span><br><span class="line">- apiGroup: &quot;&quot;</span><br><span class="line">  kind: ServiceAccount</span><br><span class="line">  name: tech-user</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure><h3 id="执行资源配置清单-1"><a href="#执行资源配置清单-1" class="headerlink" title="执行资源配置清单"></a>执行资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f test4-user.yaml</span><br></pre></td></tr></table></figure><h2 id="创建config-通过kubectl进入"><a href="#创建config-通过kubectl进入" class="headerlink" title="创建config,通过kubectl进入"></a>创建config,通过kubectl进入</h2><p>在kubeconfig配置文件中设置，指定了一个已存在的名字，将合并新字段并覆盖旧字段。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">DASH_TOCKEN&#x3D;$(kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep tech-user | awk &#39;&#123;print $1&#125;&#39;) -o go-template&#x3D;&#39;&#123;&#123;.data.token&#125;&#125;&#39; | base64 -d)</span><br><span class="line"></span><br><span class="line">kubectl config set-cluster kubernetes --server&#x3D;https:&#x2F;&#x2F;api.k8s.od.com:8443 --insecure-skip-tls-verify&#x3D;true --kubeconfig&#x3D;&#x2F;temp&#x2F;config </span><br><span class="line"></span><br><span class="line">kubectl config set-credentials tech-user --token&#x3D;$DASH_TOCKEN --kubeconfig&#x3D;&#x2F;temp&#x2F;config</span><br><span class="line"></span><br><span class="line">kubectl config set-context tech@kubernetes --cluster&#x3D;kubernetes --user&#x3D;tech-user --namespace&#x3D;test4 --kubeconfig&#x3D;&#x2F;temp&#x2F;config</span><br><span class="line"></span><br><span class="line">kubectl config use-context tech@kubernetes --kubeconfig&#x3D;&#x2F;temp&#x2F;config</span><br></pre></td></tr></table></figure><h3 id="更换-kubectl配置文件路径"><a href="#更换-kubectl配置文件路径" class="headerlink" title="更换 kubectl配置文件路径"></a>更换 kubectl配置文件路径</h3><p>默认路径  $USER/.kube/config </p><h4 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export KUBECONFIG&#x3D;&#x2F;temp&#x2F;config</span><br><span class="line">kubectl get pod</span><br></pre></td></tr></table></figure><h2 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h2><h3 id="自制config证书不认证"><a href="#自制config证书不认证" class="headerlink" title="自制config证书不认证"></a>自制config证书不认证</h3><p>通过自制config访问k8s集群，报错证书不认证，因为我在设置集群时，没有添加证书</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test ~]# kubectl get pod               </span><br><span class="line">Unable to connect to the server: x509: certificate signed by unknown authority</span><br></pre></td></tr></table></figure><h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><p>不进行ca证书校验，只进行单向权限校验</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl config set-cluster kubernetes --server&#x3D;https:&#x2F;&#x2F;api.k8s.od.com:8443 --insecure-skip-tls-verify&#x3D;true --kubeconfig&#x3D;&#x2F;temp&#x2F;config</span><br></pre></td></tr></table></figure><h2 id="配置K8S的yum源"><a href="#配置K8S的yum源" class="headerlink" title="配置K8S的yum源"></a>配置K8S的yum源</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name&#x3D;Kubernetes</span><br><span class="line">baseurl&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;0</span><br><span class="line">repo_gpgcheck&#x3D;0</span><br><span class="line">gpgkey&#x3D;http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg</span><br><span class="line">       http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">yum install -y kubectl-1.16.2</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> service </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> service </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s部署zookeeper3.4.10集群</title>
      <link href="/2020/07/21/k8s%E9%83%A8%E7%BD%B2zookeeper3-4-10%E9%9B%86%E7%BE%A4/"/>
      <url>/2020/07/21/k8s%E9%83%A8%E7%BD%B2zookeeper3-4-10%E9%9B%86%E7%BE%A4/</url>
      
        <content type="html"><![CDATA[<p>K8s文档 <a href="https://kubernetes.io/zh/docs/tutorials/stateful-application/zookeeper/" target="_blank" rel="noopener">https://kubernetes.io/zh/docs/tutorials/stateful-application/zookeeper/</a></p><blockquote><p><a href="http://wangzhangtao.com/2020/09/23/部署zookeeper3-6-1集群/" target="_blank" rel="noopener">http://wangzhangtao.com/2020/09/23/部署zookeeper3-6-1集群/</a></p></blockquote><h2 id="准备开始"><a href="#准备开始" class="headerlink" title="准备开始"></a>准备开始</h2><p>你需要一个至少包含四个节点的集群，每个节点至少 2 CPUs 和 4 GiB 内存。</p><p>本教程假设你的集群配置为动态的<a href="http://wangzhangtao.com/2020/07/21/%E5%8A%A8%E6%80%81%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8storageclass/" target="_blank" rel="noopener">提供 PersistentVolumes</a>。如果你的集群没有配置成这样，在开始本教程前，你需要手动准备三个 20 GiB 的卷。</p><h2 id="准备基础镜像"><a href="#准备基础镜像" class="headerlink" title="准备基础镜像"></a>准备基础镜像</h2><p><code>[root@wang-200 ~]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># docker pull k8s.gcr.io&#x2F;kubernetes-zookeeper:1.0-3.4.10</span><br><span class="line">docker pull mirrorgooglecontainers&#x2F;kubernetes-zookeeper:1.0-3.4.10</span><br><span class="line">docker tag mirrorgooglecontainers&#x2F;kubernetes-zookeeper:1.0-3.4.10 harbor.od.com&#x2F;public&#x2F;zookeeper:3.4.10</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;zookeeper:3.4.10</span><br></pre></td></tr></table></figure><h2 id="准备资源配置清单"><a href="#准备资源配置清单" class="headerlink" title="准备资源配置清单"></a>准备资源配置清单</h2><p>创建资源配置清单文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;zookeeper</span><br></pre></td></tr></table></figure><h3 id="svc-yaml"><a href="#svc-yaml" class="headerlink" title="svc.yaml"></a>svc.yaml</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;zookeeper&#x2F;svc.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: zk-hs</span><br><span class="line">  labels:</span><br><span class="line">    app: zk</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 2888</span><br><span class="line">    name: server</span><br><span class="line">  - port: 3888</span><br><span class="line">    name: leader-election</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: zk</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: zk-cs</span><br><span class="line">  labels:</span><br><span class="line">    app: zk</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 2181</span><br><span class="line">    name: client</span><br><span class="line">  selector:</span><br><span class="line">    app: zk</span><br><span class="line">---</span><br><span class="line">apiVersion: policy&#x2F;v1beta1</span><br><span class="line">kind: PodDisruptionBudget</span><br><span class="line">metadata:</span><br><span class="line">  name: zk-pdb</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: zk</span><br><span class="line">  maxUnavailable: 1</span><br><span class="line">---</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="sts-yaml"><a href="#sts-yaml" class="headerlink" title="sts.yaml"></a>sts.yaml</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;zookeeper&#x2F;sts.yaml </span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: zk</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: zk</span><br><span class="line">  serviceName: zk-hs</span><br><span class="line">  replicas: 3</span><br><span class="line">  updateStrategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">  podManagementPolicy: OrderedReady</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: zk</span><br><span class="line">    spec:</span><br><span class="line">      affinity:</span><br><span class="line">        podAntiAffinity:</span><br><span class="line">          requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">            - labelSelector:</span><br><span class="line">                matchExpressions:</span><br><span class="line">                  - key: &quot;app&quot;</span><br><span class="line">                    operator: In</span><br><span class="line">                    values:</span><br><span class="line">                    - zk</span><br><span class="line">              topologyKey: &quot;kubernetes.io&#x2F;hostname&quot;</span><br><span class="line">      containers:</span><br><span class="line">      - name: kubernetes-zookeeper</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        image: &quot;harbor.od.com&#x2F;public&#x2F;zookeeper:3.4.10&quot;</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            memory: &quot;1Gi&quot;</span><br><span class="line">            cpu: &quot;0.5&quot;</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 2181</span><br><span class="line">          name: client</span><br><span class="line">        - containerPort: 2888</span><br><span class="line">          name: server</span><br><span class="line">        - containerPort: 3888</span><br><span class="line">          name: leader-election</span><br><span class="line">        command:</span><br><span class="line">        - sh</span><br><span class="line">        - -c</span><br><span class="line">        - &quot;start-zookeeper \\</span><br><span class="line">          --servers&#x3D;3 \\</span><br><span class="line">          --data_dir&#x3D;&#x2F;var&#x2F;lib&#x2F;zookeeper&#x2F;data \\</span><br><span class="line">          --data_log_dir&#x3D;&#x2F;var&#x2F;lib&#x2F;zookeeper&#x2F;data&#x2F;log \\</span><br><span class="line">          --conf_dir&#x3D;&#x2F;opt&#x2F;zookeeper&#x2F;conf \\</span><br><span class="line">          --client_port&#x3D;2181 \\</span><br><span class="line">          --election_port&#x3D;3888 \\</span><br><span class="line">          --server_port&#x3D;2888 \\</span><br><span class="line">          --tick_time&#x3D;2000 \\</span><br><span class="line">          --init_limit&#x3D;10 \\</span><br><span class="line">          --sync_limit&#x3D;5 \\</span><br><span class="line">          --heap&#x3D;512M \\</span><br><span class="line">          --max_client_cnxns&#x3D;60 \\</span><br><span class="line">          --snap_retain_count&#x3D;3 \\</span><br><span class="line">          --purge_interval&#x3D;12 \\</span><br><span class="line">          --max_session_timeout&#x3D;40000 \\</span><br><span class="line">          --min_session_timeout&#x3D;4000 \\</span><br><span class="line">          --log_level&#x3D;INFO&quot;</span><br><span class="line">        readinessProbe:</span><br><span class="line">          exec:</span><br><span class="line">            command:</span><br><span class="line">            - sh</span><br><span class="line">            - -c</span><br><span class="line">            - &quot;zookeeper-ready 2181&quot;</span><br><span class="line">          initialDelaySeconds: 10</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">        livenessProbe:</span><br><span class="line">          exec:</span><br><span class="line">            command:</span><br><span class="line">            - sh</span><br><span class="line">            - -c</span><br><span class="line">            - &quot;zookeeper-ready 2181&quot;</span><br><span class="line">          initialDelaySeconds: 10</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: datadir</span><br><span class="line">          mountPath: &#x2F;var&#x2F;lib&#x2F;zookeeper</span><br><span class="line">      securityContext:</span><br><span class="line">        runAsUser: 1000</span><br><span class="line">        fsGroup: 1000</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">  - metadata:</span><br><span class="line">      name: datadir</span><br><span class="line">      annotations:</span><br><span class="line">        volume.beta.kubernetes.io&#x2F;storage-class: nfs1</span><br><span class="line">    spec:</span><br><span class="line">      accessModes: [ &quot;ReadWriteOnce&quot; ]</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 10Gi</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h2 id="应用资源配置清单"><a href="#应用资源配置清单" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h2><h3 id="执行命令"><a href="#执行命令" class="headerlink" title="执行命令"></a>执行命令</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;zookeeper&#x2F;svc.yaml -n test4</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;zookeeper&#x2F;sts.yaml -n test4</span><br></pre></td></tr></table></figure><h3 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;zookeeper&#x2F;svc.yaml -n test4</span><br><span class="line">service&#x2F;zk-hs created</span><br><span class="line">service&#x2F;zk-cs created</span><br><span class="line">poddisruptionbudget.policy&#x2F;zk-pdb created</span><br><span class="line">[root@wang-200 ~]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;zookeeper&#x2F;sts.yaml -n test4</span><br><span class="line">statefulset.apps&#x2F;zk created</span><br></pre></td></tr></table></figure><h3 id="查看执行结果"><a href="#查看执行结果" class="headerlink" title="查看执行结果"></a>查看执行结果</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pod -n test4</span><br><span class="line">NAME   READY   STATUS    RESTARTS   AGE</span><br><span class="line">zk-0   1&#x2F;1     Running   0          50s</span><br><span class="line">zk-1   1&#x2F;1     Running   0          29s</span><br><span class="line">zk-2   1&#x2F;1     Running   0          15s</span><br></pre></td></tr></table></figure><h2 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h2><blockquote><p><a href="http://wangzhangtao.com/2020/08/17/k8s部署dubbo-admin/" target="_blank" rel="noopener">k8s部署dubbo-admin</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> service </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> service </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8s部署动态持久化存储StorageClass</title>
      <link href="/2020/07/21/k8s%E9%83%A8%E7%BD%B2%E5%8A%A8%E6%80%81%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8storageclass/"/>
      <url>/2020/07/21/k8s%E9%83%A8%E7%BD%B2%E5%8A%A8%E6%80%81%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8storageclass/</url>
      
        <content type="html"><![CDATA[<h2 id="动态持久化存储StorageClass说明"><a href="#动态持久化存储StorageClass说明" class="headerlink" title="动态持久化存储StorageClass说明"></a>动态持久化存储StorageClass说明</h2><p>我们学习了 PV 和 PVC 的使用方法，但是前面的 PV 都是静态的，什么意思？就是我要使用的一个 PVC 的话就必须手动去创建一个  PV，我们也说过这种方式在很大程度上并不能满足我们的需求，比如我们有一个应用需要对存储的并发度要求比较高，而另外一个应用对读写速度又要求比较高，特别是对于 StatefulSet 类型的应用简单的来使用静态的 PV 就很不合适了，这种情况下我们就需要用到动态 PV，也就是我们今天要讲解的  StorageClass。</p><p>而 StorageClass 对象的作用，其实就是创建 PV 的模板。具体地说，StorageClass 对象会定义如下两个部分内容：</p><p>第一，PV 的属性。比如，存储类型、Volume 的大小等等。</p><p>第二，创建这种 PV 需要用到的存储插件。比如，Ceph 等等。</p><p><strong>准备本地镜像</strong></p><p><code>[root@wang-200 ~]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull quay.io&#x2F;external_storage&#x2F;nfs-client-provisioner:v3.1.0-k8s1.11</span><br><span class="line">docker tag quay.io&#x2F;external_storage&#x2F;nfs-client-provisioner:v3.1.0-k8s1.11 harbor.od.com&#x2F;public&#x2F;nfs-client-provisioner:v3.1.0-k8s1.11</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;nfs-client-provisioner:v3.1.0-k8s1.11</span><br></pre></td></tr></table></figure><h2 id="创建资源配置清单"><a href="#创建资源配置清单" class="headerlink" title="创建资源配置清单"></a>创建资源配置清单</h2><p>创建资源配置清单文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;storageclass</span><br></pre></td></tr></table></figure><h3 id="创建pvc-供storageclass服务使用"><a href="#创建pvc-供storageclass服务使用" class="headerlink" title="创建pvc,供storageclass服务使用"></a>创建pvc,供storageclass服务使用</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;storageclass&#x2F;pvc.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  finalizers:</span><br><span class="line">    - kubernetes.io&#x2F;pv-protection</span><br><span class="line">  name: nfs-pv-nfs1</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: nfs-storageclass-provisioner</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 60Gi</span><br><span class="line">  nfs:</span><br><span class="line">    path: &#x2F;data&#x2F;nfs&#x2F;v2</span><br><span class="line">    server: 192.168.70.200</span><br><span class="line">  persistentVolumeReclaimPolicy: Retain</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  finalizers:</span><br><span class="line">    - kubernetes.io&#x2F;pvc-protection</span><br><span class="line">  name: nfs-pvc-nfs1</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 60Gi</span><br><span class="line">  storageClassName: nfs-storageclass-provisioner</span><br><span class="line">  volumeMode: Filesystem</span><br><span class="line">  volumeName: nfs-pv-nfs1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="创建storageclass"><a href="#创建storageclass" class="headerlink" title="创建storageclass"></a>创建storageclass</h3><h4 id="rbac-yaml"><a href="#rbac-yaml" class="headerlink" title="rbac.yaml"></a><strong>rbac.yaml</strong></h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;storageclass&#x2F;rbac.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: eip-nfs-client-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: eip-nfs-client-provisioner-runner</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &#39;&#39;</span><br><span class="line">    resources:</span><br><span class="line">      - persistentvolumes</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">      - create</span><br><span class="line">      - delete</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &#39;&#39;</span><br><span class="line">    resources:</span><br><span class="line">      - persistentvolumeclaims</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">      - update</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - storage.k8s.io</span><br><span class="line">    resources:</span><br><span class="line">      - storageclasses</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &#39;&#39;</span><br><span class="line">    resources:</span><br><span class="line">      - events</span><br><span class="line">    verbs:</span><br><span class="line">      - create</span><br><span class="line">      - update</span><br><span class="line">      - patch</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: eip-run-nfs-client-provisioner</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: eip-nfs-client-provisioner-runner</span><br><span class="line">  #  namespace: kube-system</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: eip-nfs-client-provisioner</span><br><span class="line">    namespace: kube-system</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: Role</span><br><span class="line">metadata:</span><br><span class="line">  name: eip-leader-locking-nfs-client-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &#39;&#39;</span><br><span class="line">    resources:</span><br><span class="line">      - endpoints</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">      - create</span><br><span class="line">      - update</span><br><span class="line">      - patch</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: eip-leader-locking-nfs-client-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: eip-leader-locking-nfs-client-provisioner</span><br><span class="line">  # namespace: kube-system</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: eip-nfs-client-provisioner</span><br><span class="line">    namespace: kube-system</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="deploy-yaml"><a href="#deploy-yaml" class="headerlink" title="deploy.yaml"></a><strong>deploy.yaml</strong></h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;storageclass&#x2F;deploy.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: eip-nfs-nfs1</span><br><span class="line">  name: eip-nfs-nfs1</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: eip-nfs-nfs1</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: eip-nfs-nfs1</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - env:</span><br><span class="line">            - name: PROVISIONER_NAME</span><br><span class="line">              value: nfs-nfs1</span><br><span class="line">            - name: NFS_SERVER</span><br><span class="line">              value: 192.168.70.200</span><br><span class="line">            - name: NFS_PATH</span><br><span class="line">              value: &#x2F;data&#x2F;nfs&#x2F;v2</span><br><span class="line">          image: &#39;harbor.od.com&#x2F;public&#x2F;nfs-client-provisioner:v3.1.0-k8s1.11&#39;</span><br><span class="line">          name: nfs-client-provisioner</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - mountPath: &#x2F;persistentvolumes</span><br><span class="line">              name: nfs-client-root</span><br><span class="line">      serviceAccountName: eip-nfs-client-provisioner</span><br><span class="line">      volumes:</span><br><span class="line">        - name: nfs-client-root</span><br><span class="line">          persistentVolumeClaim:</span><br><span class="line">            claimName: nfs-pvc-nfs1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="StorageClass-yaml"><a href="#StorageClass-yaml" class="headerlink" title="StorageClass.yaml"></a><strong>StorageClass.yaml</strong></h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;storageclass&#x2F;storageclass.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: storage.k8s.io&#x2F;v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    k8s.eip.work&#x2F;storageType: nfs_client_provisioner</span><br><span class="line">  name: nfs1</span><br><span class="line">parameters:</span><br><span class="line">  archiveOnDelete: &#39;false&#39;</span><br><span class="line">provisioner: nfs-nfs1</span><br><span class="line">reclaimPolicy: Delete</span><br><span class="line">volumeBindingMode: Immediate</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><em>reclaimPolicy回收策略: Delete立刻删除  Retain pvc删除后保留磁盘</em></p><p><em>volumeBindingMode绑定策略: Immediate立即绑定，WaitForFirstConsumer第一次使用时绑定</em></p><h2 id="应用资源配置清单"><a href="#应用资源配置清单" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;storageclass&#x2F;pvc.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;storageclass&#x2F;rbac.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;storageclass&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;storageclass&#x2F;storageclass.yaml</span><br></pre></td></tr></table></figure><h3 id="查看pod正常"><a href="#查看pod正常" class="headerlink" title="查看pod正常"></a>查看pod正常</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pv</span><br><span class="line">NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                      STORAGECLASS                   REASON   AGE</span><br><span class="line">nfs-pv-nfs1   60Gi       RWX            Retain           Bound    kube-system&#x2F;nfs-pvc-nfs1   nfs-storageclass-provisioner            49s</span><br><span class="line">[root@wang-200 ~]# kubectl get pod -n kube-system | grep nfs</span><br><span class="line">eip-nfs-nfs1-5fb77547cc-8cqc2           1&#x2F;1     Running   0          30s</span><br><span class="line">[root@wang-200 ~]# kubectl get storageclass -n kube-system</span><br><span class="line">NAME   PROVISIONER   AGE</span><br><span class="line">nfs1   nfs-nfs1      40s</span><br></pre></td></tr></table></figure><h2 id="验证nfs可用"><a href="#验证nfs可用" class="headerlink" title="验证nfs可用"></a>验证nfs可用</h2><p>nfs-pvc-test.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; nfs-pvc-test.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc-nfs-test</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 2M</span><br><span class="line">  storageClassName: nfs1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>执行nfs测试资源配置文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# kubectl apply -f nfs-pvc-test.yaml </span><br><span class="line">persistentvolumeclaim&#x2F;pvc-nfs-test created</span><br></pre></td></tr></table></figure><p>查看pvc状态</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# kubectl get pvc</span><br><span class="line">NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc-nfs-test   Bound    pvc-b63cc3f6-a71d-4dcb-9502-2c33ca5c1150   2M         RWX            nfs1           18s</span><br><span class="line">[root@wang-200 test]# kubectl get pv | grep &quot;pvc-b63cc3f6-a71d-4dcb-9502-2c33ca5c1150&quot;</span><br><span class="line">pvc-b63cc3f6-a71d-4dcb-9502-2c33ca5c1150   2M         RWX            Delete           Bound    default&#x2F;pvc-nfs-test       nfs1                                    2m12s</span><br></pre></td></tr></table></figure><h3 id="创建volumeClaimTemplates资源配置清单"><a href="#创建volumeClaimTemplates资源配置清单" class="headerlink" title="创建volumeClaimTemplates资源配置清单"></a>创建volumeClaimTemplates资源配置清单</h3><p>volumeClaimTemplates.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; volumeClaimTemplates.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1beta1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">spec:</span><br><span class="line">  serviceName: &quot;nginx&quot;</span><br><span class="line">  replicas: 3</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">  - metadata:</span><br><span class="line">      name: test </span><br><span class="line">      annotations:</span><br><span class="line">        volume.beta.kubernetes.io&#x2F;storage-class: nfs1</span><br><span class="line">    spec:</span><br><span class="line">      accessModes: [ &quot;ReadWriteOnce&quot; ]</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 5Gi </span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: magine1989</span><br><span class="line">        image: nginx:1.11.10</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: &quot;&#x2F;mnt&#x2F;rbd&quot;</span><br><span class="line">          name: test</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="执行资源配置清单"><a href="#执行资源配置清单" class="headerlink" title="执行资源配置清单"></a>执行资源配置清单</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# kubectl apply -f volumeClaimTemplates.yaml</span><br><span class="line">statefulset.apps&#x2F;web created</span><br></pre></td></tr></table></figure><p>查看执行结果，查看pod是正常的</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 test]# kubectl get pod</span><br><span class="line">NAME    READY   STATUS    RESTARTS   AGE</span><br><span class="line">web-0   1&#x2F;1     Running   0          103s</span><br><span class="line">web-1   1&#x2F;1     Running   0          60s</span><br><span class="line">web-2   1&#x2F;1     Running   0          25s</span><br></pre></td></tr></table></figure><p>发现创建了三个pvc：test-web-0，test-web-1，test-web-2</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-25 ~]# kubectl get pvc </span><br><span class="line">NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc-nfs-test   Bound    pvc-eef95322-a1a0-40a8-9d22-65118ea7176a   2M         RWX            nfs1           12h</span><br><span class="line">test-web-0     Bound    pvc-0dd83501-df66-41d1-af63-70160e40c1be   5Gi        RWO            nfs1           2m4s</span><br><span class="line">test-web-1     Bound    pvc-ff643989-5437-4f23-b36a-1eb3f417d5ee   5Gi        RWO            nfs1           60s</span><br><span class="line">test-web-2     Bound    pvc-9c75033e-faa7-43c4-98fb-fe0fa4d3dc47   5Gi        RWO            nfs1           52s</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> service </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> service </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nexus2升级到nexus3</title>
      <link href="/2020/07/21/nexus2%E5%8D%87%E7%BA%A7%E5%88%B0nexus3/"/>
      <url>/2020/07/21/nexus2%E5%8D%87%E7%BA%A7%E5%88%B0nexus3/</url>
      
        <content type="html"><![CDATA[<h2 id="Nexus升级简述"><a href="#Nexus升级简述" class="headerlink" title="Nexus升级简述"></a>Nexus升级简述</h2><h3 id="升级原因"><a href="#升级原因" class="headerlink" title="升级原因"></a>升级原因</h3><p>由于nexus版本太老，且不支持https镜像同步，所以准备从nexus2升级到nexus3</p><h3 id="升级版本规划"><a href="#升级版本规划" class="headerlink" title="升级版本规划"></a>升级版本规划</h3><p>Nexus当前版本2.8.1-01</p><p>Nexus最新版本3.24.0-02</p><p>官网nexus升级 <a href="https://support.sonatype.com/hc/en-us/articles/217967608?_ga=2.231284383.1615800229.1594781578-129423342.1591932545" target="_blank" rel="noopener">How to Upgrade to Nexus Repository Manager 3.0.x</a></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggylcm02k4j31lo0kqjwm.jpg" alt="image-20200721141414874" style="zoom:80%;max-width:60%" /><p>如果是<strong>Nexus</strong> 2.x ，需要升级到2.14.1的最新版本  2.14.18   <a href="https://help.sonatype.com/repomanager2/download/download-archives---repository-manager-oss" target="_blank" rel="noopener">Download Archives - Repository Manager OSS</a> （查看最新2.14的版本）</p><h3 id="升级方案"><a href="#升级方案" class="headerlink" title="升级方案"></a>升级方案</h3><p><strong>基础环境</strong>：<a href="/2020/05/28/%E4%B8%80%E3%80%81%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/#%E5%AE%89%E8%A3%85java">java8</a></p><p>版本: 2.8.1-01 –&gt; 2.14.18 –&gt; 3.24.1-02</p><blockquote><p>安装包下载地址： <a href="https://download.sonatype.com/nexus/oss/nexus-2.14.18-01-bundle.tar.gz" target="_blank" rel="noopener">nexus-2.14.18-01-bundle.tar.gz 链接</a>  <a href="http://download.sonatype.com/nexus/3/nexus-3.24.0-02-unix.tar.gz" target="_blank" rel="noopener">nexus-3.24.0-02-unix.tar.gz链接</a></p><p>docker镜像下载地址  sonatype/nexus:2.14.18  sonatype/nexus3:3.24.0</p></blockquote><h2 id="nexus版本2-8-1-01升级到2-14-18"><a href="#nexus版本2-8-1-01升级到2-14-18" class="headerlink" title="nexus版本2.8.1-01升级到2.14.18"></a>nexus版本2.8.1-01升级到2.14.18</h2><h3 id="官网文档升级步骤"><a href="#官网文档升级步骤" class="headerlink" title="官网文档升级步骤"></a>官网文档升级步骤</h3><ul><li>Extracting the new release bundle 解压下载的2的最新版</li><li>Replicating configuration changes  复制配置文件</li><li>Stopping 2.x instance 停止之前的2版本的进程</li><li>Replacing the application directory with the new instance  用新的版本作为新的应用程序，即和sonatype-work要在同一目录</li><li>Starting the new instance 启动新的进程</li></ul><h3 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h3><ol><li>解压新的nexus，新的nexu目录必须和sonatype-work在同一层级</li></ol><p><code>tar -xvf nexus-2.14.18-01-bundle.tar.gz</code> </p><ol start="2"><li><p>如果之前的nexus配置文件，即conf下的配置文件做过修改，那么需要覆盖到新的nexus</p></li><li><p>停止旧的nexus,正常启动新的nexus即可</p></li></ol><p><code>nexus-2.14.18-01/bin/nexus start</code></p><h4 id="失败提示："><a href="#失败提示：" class="headerlink" title="失败提示："></a>失败提示：</h4><p>If you insist running as root, then set the environment variable RUN_AS_USER=root before running this script.</p><p>需要你修改nexus-2.14.18-01/bin/nexus，在文件的开始添加RUN_AS_USER=root即可</p><h2 id="安装3-24-1-02"><a href="#安装3-24-1-02" class="headerlink" title="安装3.24.1-02"></a>安装3.24.1-02</h2><h3 id="解压安装包"><a href="#解压安装包" class="headerlink" title="解压安装包"></a>解压安装包</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar xf nexus-3.24.0-02-unix.tar.gz</span><br></pre></td></tr></table></figure><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim nexus-3.24.0-02&#x2F;bin&#x2F;nexus.vmoptions</span><br><span class="line">-Dkaraf.data&#x3D;..&#x2F;sonatype-work&#x2F;nexus3  \#此处修改数据存放目录为新目录,与之前区别开</span><br><span class="line"></span><br><span class="line">vim nexus-3.24.0-02&#x2F;etc&#x2F;nexus-default.properties </span><br><span class="line">application-port&#x3D;8082 # 修改端口号</span><br></pre></td></tr></table></figure><h3 id="启动nexus3"><a href="#启动nexus3" class="headerlink" title="启动nexus3"></a>启动nexus3</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nexus program]# nexus-3.24.0-02&#x2F;bin&#x2F;nexus start</span><br><span class="line">WARNING: ************************************************************</span><br><span class="line">WARNING: Detected execution as &quot;root&quot; user.  This is NOT recommended!</span><br><span class="line">WARNING: ************************************************************</span><br><span class="line">Starting nexus</span><br><span class="line">[root@nexus program]# nexus-3.24.0-02&#x2F;bin&#x2F;nexus status</span><br></pre></td></tr></table></figure><h3 id="浏览器登录nexus3"><a href="#浏览器登录nexus3" class="headerlink" title="浏览器登录nexus3"></a>浏览器登录nexus3</h3><p><a href="http://127.0.0.1:8082/" target="_blank" rel="noopener">http://127.0.0.1:8082/</a></p><p>默认admin密码在文件里</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nexus program]# cat sonatype-work&#x2F;nexus3&#x2F;admin.password</span><br><span class="line">bdbb4ce2-2771-4aff-ac88-3b7c91661463</span><br></pre></td></tr></table></figure><p>首次登陆后需要修改密码,比如设置为admin123</p><h2 id="nexus版本2-14-18升级到3-24-1-02"><a href="#nexus版本2-14-18升级到3-24-1-02" class="headerlink" title="nexus版本2.14.18升级到3.24.1-02"></a>nexus版本2.14.18升级到3.24.1-02</h2><h3 id="在nexus2配置升级选项"><a href="#在nexus2配置升级选项" class="headerlink" title="在nexus2配置升级选项"></a>在nexus2配置升级选项</h3><p>Click to expand Administration in the left-hand panel.  点击打开左侧的Administration菜单<br>Click the Capabilities menu item to open the respective screen. 点击Capabilities菜单<br>Click the New button to access the Create new capability modal. 点击new按钮新建功能模块<br>Select Upgrade: Agent as your capability Type. 新的功能模块的类型选择Upgrade: Agent，如下图：</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggxr5i0wxaj31o30u0128.jpg" alt="image-20200720212050412" style="zoom: 67%;max-width:60%" /><h3 id="在nexus3同步nexus2配置"><a href="#在nexus3同步nexus2配置" class="headerlink" title="在nexus3同步nexus2配置"></a>在nexus3同步nexus2配置</h3><h4 id="添加新功能-升级（update）"><a href="#添加新功能-升级（update）" class="headerlink" title="添加新功能 - 升级（update）"></a>添加新功能 - 升级（update）</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggxr8v5rpkj316q0u0guh.jpg" alt="image-20200720212414033" style="zoom:67%;max-width:60%" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggxr9s9nkaj31is0u0gtd.jpg" alt="image-20200720212508235" style="zoom:67%;max-width:60%" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggxrawfjzij31b60aedhf.jpg" alt="image-20200720212612038" style="zoom:67%;max-width:60%" /><h4 id="通过升级（update）同步数据"><a href="#通过升级（update）同步数据" class="headerlink" title="通过升级（update）同步数据"></a>通过升级（update）同步数据</h4><img src="http://wangzhangtao.com/img/body/png/41.nexus%E6%B5%8B%E8%AF%95/image-20200721133427509.png" alt="image-20200721133427509" style="zoom:67%;max-width:60%" /><h4 id="此向导将帮助您从Nexus-2升级"><a href="#此向导将帮助您从Nexus-2升级" class="headerlink" title="此向导将帮助您从Nexus 2升级"></a><strong>此向导将帮助您从Nexus 2升级</strong></h4><img src="http://wangzhangtao.com/img/body/png/41.nexus%E6%B5%8B%E8%AF%95/image-20200721141510549.png" alt="image-20200721141510549" style="zoom:67%;max-width:60%" /><h4 id="添加Nexus2的配置地址和token"><a href="#添加Nexus2的配置地址和token" class="headerlink" title="添加Nexus2的配置地址和token"></a>添加Nexus2的配置地址和token</h4><img src="http://wangzhangtao.com/img/body/png/41.nexus%E6%B5%8B%E8%AF%95/image-20200721133912098.png" alt="image-20200721133912098" style="zoom:67%;max-width:60%" /><h4 id="您希望同步什么信息"><a href="#您希望同步什么信息" class="headerlink" title="您希望同步什么信息"></a><strong>您希望同步什么信息</strong></h4><ul><li><p>存储库配置和内容</p></li><li><p>服务器配置信息</p></li></ul><img src="http://wangzhangtao.com/img/body/png/41.nexus%E6%B5%8B%E8%AF%95/image-20200721134020508.png" alt="image-20200721134020508" style="zoom:67%;max-width:60%" /><h4 id="仓库数据同步类型"><a href="#仓库数据同步类型" class="headerlink" title="仓库数据同步类型"></a><strong>仓库数据同步类型</strong></h4><p>模型有三个选择，Hard link (fastest)    Filesystem copy (slow)  Download (slowest)</p><p>这里由于我们是在同一台机器升级，所以我们选择Hard link (fastest)，这样比较快</p><img src="http://wangzhangtao.com/img/body/png/41.nexus%E6%B5%8B%E8%AF%95/image-20200721134202325.png" alt="image-20200721134202325" style="zoom:67%;max-width:60%" /><h4 id="选择同步的仓库"><a href="#选择同步的仓库" class="headerlink" title="选择同步的仓库"></a><strong>选择同步的仓库</strong></h4><img src="http://wangzhangtao.com/img/body/png/41.nexus%E6%B5%8B%E8%AF%95/image-20200721135514388.png" alt="image-20200721135514388" style="zoom:67%;max-width:60%" /><h4 id="查看将要更新的仓库信息"><a href="#查看将要更新的仓库信息" class="headerlink" title="查看将要更新的仓库信息"></a>查看将要更新的仓库信息</h4><img src="http://wangzhangtao.com/img/body/png/41.nexus%E6%B5%8B%E8%AF%95/image-20200721135631179.png" alt="image-20200721135631179" style="zoom:67%;max-width:60%" /><img src="http://wangzhangtao.com/img/body/png/41.nexus%E6%B5%8B%E8%AF%95/image-20200721135705166.png" alt="image-20200721135705166" style="zoom:90%;max-width:90%" /><h4 id="信息更新完毕，继续"><a href="#信息更新完毕，继续" class="headerlink" title="信息更新完毕，继续"></a>信息更新完毕，继续</h4><img src="http://wangzhangtao.com/img/body/png/41.nexus%E6%B5%8B%E8%AF%95/image-20200721135732636.png" alt="image-20200721135732636" style="zoom:67%;max-width:60%" /><h4 id="开通同步仓库"><a href="#开通同步仓库" class="headerlink" title="开通同步仓库"></a>开通同步仓库</h4><img src="http://wangzhangtao.com/img/body/41.nexus2%E5%8D%87%E7%BA%A7%E5%88%B0nexus3/image-20200722200855563.png" alt="image-20200722200855563" style="zoom:67%;max-width:60%" /><h4 id="等待完成以后-直接continue"><a href="#等待完成以后-直接continue" class="headerlink" title="等待完成以后,直接continue"></a>等待完成以后,直接continue</h4><img src="http://wangzhangtao.com/img/body/png/41.nexus%E6%B5%8B%E8%AF%95/image-20200721141104490.png" alt="image-20200721141104490" style="zoom:67%;max-width:70%" /><img src="http://wangzhangtao.com/img/body/png/41.nexus%E6%B5%8B%E8%AF%95/image-20200721141122719.png" alt="image-20200721141122719" style="zoom:90%;max-width:90%" /><p>Any future changes to repositories will not be synchronized. Proceed?  将来对存储库的任何更改都不会同步。继续吗？</p><p>意思是后续如果原始库有变动，nexus3新版将不会改变，选择<code>Yes</code>确认。</p><h4 id="点击Done-数据同步结束"><a href="#点击Done-数据同步结束" class="headerlink" title="点击Done,数据同步结束"></a>点击Done,数据同步结束</h4><img src="http://wangzhangtao.com/img/body/png/41.nexus%E6%B5%8B%E8%AF%95/image-20200721141223280.png" alt="image-20200721141223280" style="zoom:67%;max-width:70%" /><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><h3 id="nexus的路径升级以后有变化"><a href="#nexus的路径升级以后有变化" class="headerlink" title="nexus的路径升级以后有变化"></a>nexus的路径升级以后有变化</h3><p>http://*****:8081/nexus/content/groups/public/   以前的地址</p><p>http://*****:8082/repository/public/    新的地址</p><p>这样需要修改maven的 .m2/setting.xml文件, 在打包的时候才能找到正确的依赖包.</p><p>如果不这样处理, 我的方法是在仓库前端加一个nginx代理</p><p>vim nexos.conf</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">  listen 8081;</span><br><span class="line">  server_name ***\***;</span><br><span class="line">  location &#x2F; &#123;</span><br><span class="line">  rewrite ^&#x2F;nexus&#x2F;content&#x2F;groups&#x2F;(.*) http:&#x2F;&#x2F;******:8082&#x2F;repository&#x2F;$1 permanent;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">upstream nexus-server&#123;</span><br><span class="line">    server 127.0.0.1:8082;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name nexus.od.com;</span><br><span class="line">    client_max_body_size 200m;</span><br><span class="line"></span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        proxy_set_header Host $host;</span><br><span class="line">        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">        proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">        proxy_pass http:&#x2F;&#x2F;nexus-server;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location &#x2F;aaa &#123;</span><br><span class="line">        return 301 https:&#x2F;&#x2F;nexus.od.com;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location ~ &#x2F;.well-known &#123;</span><br><span class="line">        root &#x2F;tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen 443 ssl;</span><br><span class="line">    server_name nexus.od.com;</span><br><span class="line"></span><br><span class="line">    ssl                  on;</span><br><span class="line">    ssl_certificate cert&#x2F;od.pem;</span><br><span class="line">    ssl_certificate_key cert&#x2F;od.key;</span><br><span class="line">    ssl_session_timeout  5m;</span><br><span class="line">    ssl_protocols  TLSv1 TLSv1.1 TLSv1.2;</span><br><span class="line">    ssl_ciphers  HIGH:!RC4:!MD5:!aNULL:!eNULL:!NULL:!DH:!EDH:!EXP:+MEDIUM;</span><br><span class="line">    ssl_prefer_server_ciphers   on;</span><br><span class="line"></span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        proxy_set_header Host $host;</span><br><span class="line">        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">        proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">        proxy_set_header X-Forwarded-Proto &quot;https&quot;;</span><br><span class="line">        proxy_pass http:&#x2F;&#x2F;nexus-server;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Rewtite 匹配/nexus/  <em>跳转到 nexus真正地址/</em>  参数保持不变，这样就不需要其他人进行修改，保持原有url不变</p><h3 id="报错1：Unsupported-major-minor-version-52-0"><a href="#报错1：Unsupported-major-minor-version-52-0" class="headerlink" title="报错1：Unsupported major.minor version 52.0"></a>报错1：Unsupported major.minor version 52.0</h3><h4 id="错误展示"><a href="#错误展示" class="headerlink" title="错误展示"></a>错误展示</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nexus opt]# tailf nexus-2.14.18-01&#x2F;logs&#x2F;wrapper.log </span><br><span class="line">wrapper  | --&gt; Wrapper Started as Daemon</span><br><span class="line">wrapper  | Launching a JVM...</span><br><span class="line">wrapper  | JVM exited while loading the application.</span><br><span class="line">jvm 1    | Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: org&#x2F;sonatype&#x2F;nexus&#x2F;bootstrap&#x2F;jsw&#x2F;JswLauncher : Unsupported major.minor version 52.0</span><br><span class="line">jvm 1    |      at java.lang.ClassLoader.defineClass1(Native Method)</span><br><span class="line">jvm 1    |      at java.lang.ClassLoader.defineClass(ClassLoader.java:800)</span><br><span class="line">jvm 1    |      at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)</span><br></pre></td></tr></table></figure><h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><p>Unsupported major.minor version 52.0意思是java版本不对，应该是 java-8</p><p>请查看java版本号java -version</p><h3 id="nexus运行时报错：Connect-to-sonatype-download-global-ssl-fastly-net-443"><a href="#nexus运行时报错：Connect-to-sonatype-download-global-ssl-fastly-net-443" class="headerlink" title="nexus运行时报错：Connect to sonatype-download.global.ssl.fastly.net:443"></a>nexus运行时报错：Connect to sonatype-download.global.ssl.fastly.net:443</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2020-07-22 21:35:41,186+0800 WARN  [qtp413756060-447]  admin com.sonatype.nexus.plugins.outreach.internal.outreach.SonatypeOutreach - Could not download page bundle</span><br><span class="line">org.apache.http.conn.HttpHostConnectException: Connect to sonatype-download.global.ssl.fastly.net:443 [sonatype-download.global.ssl.fastly.net&#x2F;69.171.235.64] failed: Connection timed out (Connection timed out)</span><br><span class="line">        at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:159)</span><br><span class="line">        at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:373)</span><br><span class="line">        at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:381)</span><br><span class="line">        at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:237)</span><br><span class="line">        at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185)</span><br><span class="line">        at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89)</span><br><span class="line">        at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:111)</span><br></pre></td></tr></table></figure><h4 id="处理方法："><a href="#处理方法：" class="headerlink" title="处理方法："></a>处理方法：</h4><p>​    登录账号，打开【System】–》【Capabilities】，将【Outreach:Management】禁用即可。</p><p>​    Nexus 3 禁用 outreach 功能的方式请参考下面文档：  </p><p> <a href="https://support.sonatype.com/hc/en-us/articles/213464978-How-to-avoid-Could-not-download-page-bundle-messages" target="_blank" rel="noopener">https://support.sonatype.com/hc/en-us/articles/213464978-How-to-avoid-Could-not-download-page-bundle-messages</a> </p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh20p2qwpbj30yu0gpjsx.jpg" alt="image-20200722214124634" style="zoom:80%;max-width:60%" /><h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a><strong>参考文档</strong></h3><p><a href="https://blog.51cto.com/phospherus" target="_blank" rel="noopener">zmoke</a> 的文章 <a href="https://blog.51cto.com/phospherus/2124475" target="_blank" rel="noopener">nexus 2.X版本升级 3.X版本</a></p><p>官网 <a href="https://help.sonatype.com/repomanager3/upgrading?_ga=2.231278495.1805470020.1595228361-129423342.1591932545#Upgrading-Upgrading2.xto3.y" target="_blank" rel="noopener">Upgrading-Upgrading2.xto3.y</a></p><p>官网 <a href="https://help.sonatype.com/repomanager3/installation/upgrading-from-nexus-repository-manager-2/upgrade-compatibility---repository-manager-2-to-3" target="_blank" rel="noopener">upgrade-compatibility—repository-manager-2-to-3</a>  [官网为什么莫有更新到最新版本呢？忘了吗]</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggxgtlvtsjj31kh0u0dlm.jpg" alt="image-20200720152321741" style="zoom: 67%;max-width:50%" />]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> nexus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> maven </tag>
            
            <tag> nexus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用软件-部署activemq</title>
      <link href="/2020/07/16/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6-%E9%83%A8%E7%BD%B2activemq/"/>
      <url>/2020/07/16/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6-%E9%83%A8%E7%BD%B2activemq/</url>
      
        <content type="html"><![CDATA[<h2 id="docker-安装部署-activemq"><a href="#docker-安装部署-activemq" class="headerlink" title="docker 安装部署 activemq"></a>docker 安装部署 activemq</h2><h3 id="docker部署-activemq服务"><a href="#docker部署-activemq服务" class="headerlink" title="docker部署 activemq服务"></a>docker部署 activemq服务</h3><p>搜索 ActiveMQ 镜像</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker search activemq</span><br></pre></td></tr></table></figure><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggssu9fir3j30ya08a41t.jpg" alt="image-20200716143131997" style="zoom:67%;max-width: 85%" /><p>获取 ActiveMQ 镜像 <a href="https://hub.docker.com/r/webcenter/activemq/tags" target="_blank" rel="noopener">activemq hub.docker</a></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull webcenter&#x2F;activemq:5.14.3</span><br></pre></td></tr></table></figure><p>查看本地镜像</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test7 ~]# docker images | grep activemq</span><br><span class="line">webcenter&#x2F;activemq                                          5.14.3              ab2a33f6de2b        3 years ago         422MB</span><br></pre></td></tr></table></figure><p>docker 启动 ActiveMQ 命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker run -d --name activemq -p 61616:61616 -p 8161:8161 webcenter&#x2F;activemq</span><br></pre></td></tr></table></figure><ul><li><p>61616是 activemq 的容器使用端口（映射为61616）</p></li><li><p>8161是 web 页面管理端口（对外映射为8161)</p></li></ul><p>使用 docker ps 查看 ActiveMQ 已经运行了</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test7 ~]# docker ps | grep mq</span><br><span class="line">047281f996d6        webcenter&#x2F;activemq:5.14.3   &quot;&#x2F;app&#x2F;run.sh&quot;       28 hours ago        Up 27 hours         1883&#x2F;tcp, 5672&#x2F;tcp, 61613-61614&#x2F;tcp, 0.0.0.0:8162-&gt;8161&#x2F;tcp, 0.0.0.0:61617-&gt;61616&#x2F;tcp   activemq</span><br></pre></td></tr></table></figure><p>进入 ActiveMQ</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@test7 ~]# docker exec -it activemq &#x2F;bin&#x2F;bash</span><br><span class="line">root@047281f996d6:&#x2F;opt&#x2F;activemq#</span><br></pre></td></tr></table></figure><h3 id="修改默认密码"><a href="#修改默认密码" class="headerlink" title="修改默认密码"></a>修改默认密码</h3><p>如果不设置，默认的账号名和密码为</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@047281f996d6:&#x2F;opt&#x2F;activemq# grep -v &quot;^#\|^$&quot; conf&#x2F;jetty-realm.properties </span><br><span class="line"># username: password [,rolename ...]</span><br><span class="line">admin: admin, admin   #管理员权限</span><br><span class="line">user: user, user  #用户权限</span><br></pre></td></tr></table></figure><p>如果想要修改用户名和密码，修改这个文件就可以了</p><h4 id="设置开启认证-默认开启"><a href="#设置开启认证-默认开启" class="headerlink" title="设置开启认证(默认开启)"></a>设置开启认证(默认开启)</h4><p>修改文件 conf/jetty.xml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property name&#x3D;&quot;authenticate&quot; value&#x3D;&quot;true&quot; &#x2F;&gt;，true：需要认证； false：不需要认证。</span><br></pre></td></tr></table></figure><h2 id="k8s-安装部署单节点activemq"><a href="#k8s-安装部署单节点activemq" class="headerlink" title="k8s 安装部署单节点activemq"></a>k8s 安装部署单节点activemq</h2><h3 id="创建资源配置清单"><a href="#创建资源配置清单" class="headerlink" title="创建资源配置清单"></a>创建资源配置清单</h3><p><strong>activemq-config.yaml</strong> </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: activemq-config</span><br><span class="line">data:</span><br><span class="line">  jetty-realm.properties: |</span><br><span class="line">    admin: admin, admin   #管理员权限</span><br><span class="line">    user: user, user  #用户权限</span><br></pre></td></tr></table></figure><p><strong>deploy.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1  </span><br><span class="line">kind: Deployment  </span><br><span class="line">metadata:  </span><br><span class="line">  name: activemq  </span><br><span class="line">spec:  </span><br><span class="line">  replicas: 1  </span><br><span class="line">  selector:  </span><br><span class="line">    matchLabels:  </span><br><span class="line">      app: activemq  </span><br><span class="line">  template:  </span><br><span class="line">    metadata:  </span><br><span class="line">      labels:  </span><br><span class="line">        app: activemq  </span><br><span class="line">    spec:  </span><br><span class="line">      containers:  </span><br><span class="line">      - name: activemq</span><br><span class="line">        image: webcenter&#x2F;activemq:5.14.3</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        volumeMounts:  # 挂载磁盘</span><br><span class="line">        - name: activemq-data</span><br><span class="line">          mountPath: &#x2F;data</span><br><span class="line">        - name: activemq-config</span><br><span class="line">          mountPath: &#x2F;opt&#x2F;activemq&#x2F;conf&#x2F;jetty-realm.properties</span><br><span class="line">          subPath: jetty-realm.properties</span><br><span class="line">      volumes:</span><br><span class="line">      - name: activemq-data</span><br><span class="line">        nfs:</span><br><span class="line">          server: 192.168.70.10</span><br><span class="line">          path: &#x2F;data&#x2F;nfs&#x2F;stage&#x2F;activemq&#x2F;data  </span><br><span class="line">      - name: activemq-config</span><br><span class="line">        configMap:</span><br><span class="line">          name: activemq-config</span><br></pre></td></tr></table></figure><p>日志: /var/log/activemq</p><p><strong>svc.yaml</strong> </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1  </span><br><span class="line">kind: Service  </span><br><span class="line">metadata:  </span><br><span class="line">  name: activemq</span><br><span class="line">spec:  </span><br><span class="line">  selector:  </span><br><span class="line">    app: activemq  </span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:  </span><br><span class="line">  - name: activemq8161 </span><br><span class="line">    protocol: TCP</span><br><span class="line">    port: 8161</span><br><span class="line">    targetPort: 8161</span><br><span class="line">    nodePort: 30001</span><br><span class="line">  - name: activemq61616</span><br><span class="line">    protocol: TCP</span><br><span class="line">    port: 61616</span><br><span class="line">    targetPort: 61616</span><br><span class="line">    nodePort: 30002</span><br></pre></td></tr></table></figure><p><strong>ingress.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: activemq</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: activemq.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: activemq</span><br><span class="line">          servicePort: activemq8161</span><br></pre></td></tr></table></figure><h3 id="执行资源配置清单"><a href="#执行资源配置清单" class="headerlink" title="执行资源配置清单"></a>执行资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f activemq-config.yaml</span><br><span class="line">kubectl apply -f deploy.yaml</span><br><span class="line">kubectl apply -f svc.yaml</span><br><span class="line">kubectl apply -f ingress.yaml</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> mq队列 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> k8s </tag>
            
            <tag> Linux </tag>
            
            <tag> mq队列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>体验python魔力</title>
      <link href="/2020/07/15/%E4%BD%93%E9%AA%8Cpython%E9%AD%94%E5%8A%9B/"/>
      <url>/2020/07/15/%E4%BD%93%E9%AA%8Cpython%E9%AD%94%E5%8A%9B/</url>
      
        <content type="html"><![CDATA[<h2 id="已知两个1-30之间的数，甲知道两数之和，乙知道两数之积"><a href="#已知两个1-30之间的数，甲知道两数之和，乙知道两数之积" class="headerlink" title="已知两个1~30之间的数，甲知道两数之和，乙知道两数之积"></a>已知两个1~30之间的数，甲知道两数之和，乙知道两数之积</h2><h3 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h3><p>已知两个1~30之间的数，甲知道两数之和，乙知道两数之积。<br>甲问乙：“你知道是哪两个数吗？”乙说：“不知道”；<br>乙问甲：“你知道是哪两个数吗？”甲说：“也不知道”；<br>于是，乙说：“那我知道了”；<br>随后甲也说：“那我也知道了”；</p><p>这两个数是多少？</p><p>这是一个排列组合题，我使用的是Python来计算，</p><h3 id="解答："><a href="#解答：" class="headerlink" title="解答："></a>解答：</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># coding:utf8</span><br><span class="line"># 作者：王章涛</span><br><span class="line"># 时间：2018年3月31日</span><br><span class="line"></span><br><span class="line"># 得到1-30两个数和与积的字典及其可能的组合</span><br><span class="line">count_dict &#x3D; &#123;&#125;</span><br><span class="line">mult_dict &#x3D; &#123;&#125;</span><br><span class="line">for x in range(1,30):</span><br><span class="line">for y in range(x,30):</span><br><span class="line">count &#x3D; x + y</span><br><span class="line">mult &#x3D; x * y</span><br><span class="line">item &#x3D; (x,y)</span><br><span class="line">if count_dict.get(count):</span><br><span class="line">count_dict.get(count).append( item )</span><br><span class="line">else:</span><br><span class="line">count_dict[ count ] &#x3D; [ item ]</span><br><span class="line"></span><br><span class="line">if mult_dict.get( mult ):</span><br><span class="line">mult_dict.get( mult ).append( item )</span><br><span class="line">else:</span><br><span class="line">mult_dict[ mult ] &#x3D; [ item ]</span><br><span class="line">print &quot;和的字典集为count_dict: &quot;, count_dict</span><br><span class="line">print &quot;积的字典积为mult_dict: &quot;, mult_dict</span><br><span class="line"></span><br><span class="line">#甲问乙，乙回答说不知道，说明，通过乙得到的积可以推出两种或两种以上的组合</span><br><span class="line"># 乙得到的积德集合可能为</span><br><span class="line">mult_list &#x3D; [ mult for mult in mult_dict if mult_dict[mult].__len__() &gt; 1 ] # 通过积的字典得到的可能集合数大于1</span><br><span class="line">print &quot;乙可能得到得积mult_list：&quot;,mult_list</span><br><span class="line"></span><br><span class="line"># 乙问甲，甲回答说不知道，说明，甲通过和可以得到至少有两组组合的积在乙可能得到的集合中</span><br><span class="line">def count_mult( count_list ):</span><br><span class="line">    count_list_shai &#x3D; []</span><br><span class="line">    for count in count_list:</span><br><span class="line">        count_shai &#x3D; [ ( item1,item2 )  for item1,item2 in count_dict[ count ] if item1 * item2 in mult_list ]</span><br><span class="line">        if count_shai.__len__() &gt; 1:</span><br><span class="line">            count_list_shai.append( count )</span><br><span class="line">    return count_list_shai</span><br><span class="line">count_list &#x3D; count_mult( count_dict.keys() )</span><br><span class="line">print &quot;甲可能得到的和为count_list：&quot;, count_list</span><br><span class="line"></span><br><span class="line">#紧接着，乙说知道数组了，说明，通过乙的积只有一组集合推导出的和符合甲的要求，所以乙才可以猜出结果，所以</span><br><span class="line">def mult_count( mult_list ):</span><br><span class="line">    mult_list_shai &#x3D; []</span><br><span class="line">    for mult in mult_list:</span><br><span class="line">        mult_shai &#x3D; [ (item1, item2 ) for item1,item2 in mult_dict[ mult ] if item1 + item2 in count_list ]</span><br><span class="line">        if mult_shai.__len__() &#x3D;&#x3D; 1:</span><br><span class="line">            mult_list_shai.append( mult )</span><br><span class="line">    return mult_list_shai</span><br><span class="line">mult_result &#x3D; mult_count( mult_list )</span><br><span class="line">print &quot;积应该为mult_result:&quot;, mult_result</span><br><span class="line"></span><br><span class="line">#由积推导出可能的集合</span><br><span class="line">num_result &#x3D; [ ( item1 , item2 ) for mult in mult_result  for item1,item2 in mult_dict[ mult ] if item1 + item2 in count_list ]</span><br><span class="line">count_result &#x3D; [ item1 + item2 for item1 , item2 in num_result ]  # 由集合推导出可能的和</span><br><span class="line">print &quot;和应该为count:&quot;, count_result</span><br><span class="line">print &quot;两个数的数组集应该为&quot;, num_result</span><br><span class="line">#最后甲也说我也知道了，这个条件我不知道有没有用了</span><br></pre></td></tr></table></figure><h3 id="输出的结果就是"><a href="#输出的结果就是" class="headerlink" title="输出的结果就是"></a>输出的结果就是</h3><h4 id="和的字典集为count-dict"><a href="#和的字典集为count-dict" class="headerlink" title="和的字典集为count_dict:"></a>和的字典集为count_dict:</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;2: [(1, 1)], 3: [(1, 2)], 4: [(1, 3), (2, 2)], 5: [(1, 4), (2, 3)], 6: [(1, 5), (2, 4), (3, 3)], 7: [(1, 6), (2, 5), (3, 4)], 8: [(1, 7), (2, 6), (3, 5), (4, 4)], 9: [(1, 8), (2, 7), (3, 6), (4, 5)], 10: [(1, 9), (2, 8), (3, 7), (4, 6), (5, 5)], 11: [(1, 10), (2, 9), (3, 8), (4, 7), (5, 6)], 12: [(1, 11), (2, 10), (3, 9), (4, 8), (5, 7), (6, 6)], 13: [(1, 12), (2, 11), (3, 10), (4, 9), (5, 8), (6, 7)], 14: [(1, 13), (2, 12), (3, 11), (4, 10), (5, 9), (6, 8), (7, 7)], 15: [(1, 14), (2, 13), (3, 12), (4, 11), (5, 10), (6, 9), (7, 8)], 16: [(1, 15), (2, 14), (3, 13), (4, 12), (5, 11), (6, 10), (7, 9), (8, 8)], 17: [(1, 16), (2, 15), (3, 14), (4, 13), (5, 12), (6, 11), (7, 10), (8, 9)], 18: [(1, 17), (2, 16), (3, 15), (4, 14), (5, 13), (6, 12), (7, 11), (8, 10), (9, 9)], 19: [(1, 18), (2, 17), (3, 16), (4, 15), (5, 14), (6, 13), (7, 12), (8, 11), (9, 10)], 20: [(1, 19), (2, 18), (3, 17), (4, 16), (5, 15), (6, 14), (7, 13), (8, 12), (9, 11), (10, 10)], 21: [(1, 20), (2, 19), (3, 18), (4, 17), (5, 16), (6, 15), (7, 14), (8, 13), (9, 12), (10, 11)], 22: [(1, 21), (2, 20), (3, 19), (4, 18), (5, 17), (6, 16), (7, 15), (8, 14), (9, 13), (10, 12), (11, 11)], 23: [(1, 22), (2, 21), (3, 20), (4, 19), (5, 18), (6, 17), (7, 16), (8, 15), (9, 14), (10, 13), (11, 12)], 24: [(1, 23), (2, 22), (3, 21), (4, 20), (5, 19), (6, 18), (7, 17), (8, 16), (9, 15), (10, 14), (11, 13), (12, 12)], 25: [(1, 24), (2, 23), (3, 22), (4, 21), (5, 20), (6, 19), (7, 18), (8, 17), (9, 16), (10, 15), (11, 14), (12, 13)], 26: [(1, 25), (2, 24), (3, 23), (4, 22), (5, 21), (6, 20), (7, 19), (8, 18), (9, 17), (10, 16), (11, 15), (12, 14), (13, 13)], 27: [(1, 26), (2, 25), (3, 24), (4, 23), (5, 22), (6, 21), (7, 20), (8, 19), (9, 18), (10, 17), (11, 16), (12, 15), (13, 14)], 28: [(1, 27), (2, 26), (3, 25), (4, 24), (5, 23), (6, 22), (7, 21), (8, 20), (9, 19), (10, 18), (11, 17), (12, 16), (13, 15), (14, 14)], 29: [(1, 28), (2, 27), (3, 26), (4, 25), (5, 24), (6, 23), (7, 22), (8, 21), (9, 20), (10, 19), (11, 18), (12, 17), (13, 16), (14, 15)], 30: [(1, 29), (2, 28), (3, 27), (4, 26), (5, 25), (6, 24), (7, 23), (8, 22), (9, 21), (10, 20), (11, 19), (12, 18), (13, 17), (14, 16), (15, 15)], 31: [(2, 29), (3, 28), (4, 27), (5, 26), (6, 25), (7, 24), (8, 23), (9, 22), (10, 21), (11, 20), (12, 19), (13, 18), (14, 17), (15, 16)], 32: [(3, 29), (4, 28), (5, 27), (6, 26), (7, 25), (8, 24), (9, 23), (10, 22), (11, 21), (12, 20), (13, 19), (14, 18), (15, 17), (16, 16)], 33: [(4, 29), (5, 28), (6, 27), (7, 26), (8, 25), (9, 24), (10, 23), (11, 22), (12, 21), (13, 20), (14, 19), (15, 18), (16, 17)], 34: [(5, 29), (6, 28), (7, 27), (8, 26), (9, 25), (10, 24), (11, 23), (12, 22), (13, 21), (14, 20), (15, 19), (16, 18), (17, 17)], 35: [(6, 29), (7, 28), (8, 27), (9, 26), (10, 25), (11, 24), (12, 23), (13, 22), (14, 21), (15, 20), (16, 19), (17, 18)], 36: [(7, 29), (8, 28), (9, 27), (10, 26), (11, 25), (12, 24), (13, 23), (14, 22), (15, 21), (16, 20), (17, 19), (18, 18)], 37: [(8, 29), (9, 28), (10, 27), (11, 26), (12, 25), (13, 24), (14, 23), (15, 22), (16, 21), (17, 20), (18, 19)], 38: [(9, 29), (10, 28), (11, 27), (12, 26), (13, 25), (14, 24), (15, 23), (16, 22), (17, 21), (18, 20), (19, 19)], 39: [(10, 29), (11, 28), (12, 27), (13, 26), (14, 25), (15, 24), (16, 23), (17, 22), (18, 21), (19, 20)], 40: [(11, 29), (12, 28), (13, 27), (14, 26), (15, 25), (16, 24), (17, 23), (18, 22), (19, 21), (20, 20)], 41: [(12, 29), (13, 28), (14, 27), (15, 26), (16, 25), (17, 24), (18, 23), (19, 22), (20, 21)], 42: [(13, 29), (14, 28), (15, 27), (16, 26), (17, 25), (18, 24), (19, 23), (20, 22), (21, 21)], 43: [(14, 29), (15, 28), (16, 27), (17, 26), (18, 25), (19, 24), (20, 23), (21, 22)], 44: [(15, 29), (16, 28), (17, 27), (18, 26), (19, 25), (20, 24), (21, 23), (22, 22)], 45: [(16, 29), (17, 28), (18, 27), (19, 26), (20, 25), (21, 24), (22, 23)], 46: [(17, 29), (18, 28), (19, 27), (20, 26), (21, 25), (22, 24), (23, 23)], 47: [(18, 29), (19, 28), (20, 27), (21, 26), (22, 25), (23, 24)], 48: [(19, 29), (20, 28), (21, 27), (22, 26), (23, 25), (24, 24)], 49: [(20, 29), (21, 28), (22, 27), (23, 26), (24, 25)], 50: [(21, 29), (22, 28), (23, 27), (24, 26), (25, 25)], 51: [(22, 29), (23, 28), (24, 27), (25, 26)], 52: [(23, 29), (24, 28), (25, 27), (26, 26)], 53: [(24, 29), (25, 28), (26, 27)], 54: [(25, 29), (26, 28), (27, 27)], 55: [(26, 29), (27, 28)], 56: [(27, 29), (28, 28)], 57: [(28, 29)], 58: [(29, 29)]&#125;</span><br></pre></td></tr></table></figure><h4 id="积的字典积为mult-dict"><a href="#积的字典积为mult-dict" class="headerlink" title="积的字典积为mult_dict:"></a>积的字典积为mult_dict:</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;1: [(1, 1)], 2: [(1, 2)], 3: [(1, 3)], 4: [(1, 4), (2, 2)], 5: [(1, 5)], 6: [(1, 6), (2, 3)], 7: [(1, 7)], 8: [(1, 8), (2, 4)], 9: [(1, 9), (3, 3)], 10: [(1, 10), (2, 5)], 11: [(1, 11)], 12: [(1, 12), (2, 6), (3, 4)], 13: [(1, 13)], 14: [(1, 14), (2, 7)], 15: [(1, 15), (3, 5)], 16: [(1, 16), (2, 8), (4, 4)], 17: [(1, 17)], 18: [(1, 18), (2, 9), (3, 6)], 19: [(1, 19)], 20: [(1, 20), (2, 10), (4, 5)], 21: [(1, 21), (3, 7)], 22: [(1, 22), (2, 11)], 23: [(1, 23)], 24: [(1, 24), (2, 12), (3, 8), (4, 6)], 25: [(1, 25), (5, 5)], 26: [(1, 26), (2, 13)], 27: [(1, 27), (3, 9)], 28: [(1, 28), (2, 14), (4, 7)], 29: [(1, 29)], 30: [(2, 15), (3, 10), (5, 6)], 32: [(2, 16), (4, 8)], 33: [(3, 11)], 34: [(2, 17)], 35: [(5, 7)], 36: [(2, 18), (3, 12), (4, 9), (6, 6)], 38: [(2, 19)], 39: [(3, 13)], 40: [(2, 20), (4, 10), (5, 8)], 42: [(2, 21), (3, 14), (6, 7)], 44: [(2, 22), (4, 11)], 45: [(3, 15), (5, 9)], 46: [(2, 23)], 48: [(2, 24), (3, 16), (4, 12), (6, 8)], 49: [(7, 7)], 50: [(2, 25), (5, 10)], 51: [(3, 17)], 52: [(2, 26), (4, 13)], 54: [(2, 27), (3, 18), (6, 9)], 55: [(5, 11)], 56: [(2, 28), (4, 14), (7, 8)], 57: [(3, 19)], 58: [(2, 29)], 60: [(3, 20), (4, 15), (5, 12), (6, 10)], 522: [(18, 29)], 63: [(3, 21), (7, 9)], 64: [(4, 16), (8, 8)], 65: [(5, 13)], 66: [(3, 22), (6, 11)], 68: [(4, 17)], 69: [(3, 23)], 70: [(5, 14), (7, 10)], 72: [(3, 24), (4, 18), (6, 12), (8, 9)], 75: [(3, 25), (5, 15)], 76: [(4, 19)], 77: [(7, 11)], 78: [(3, 26), (6, 13)], 525: [(21, 25)], 80: [(4, 20), (5, 16), (8, 10)], 81: [(3, 27), (9, 9)], 594: [(22, 27)], 84: [(3, 28), (4, 21), (6, 14), (7, 12)], 85: [(5, 17)], 598: [(23, 26)], 87: [(3, 29)], 88: [(4, 22), (8, 11)], 90: [(5, 18), (6, 15), (9, 10)], 91: [(7, 13)], 92: [(4, 23)], 95: [(5, 19)], 96: [(4, 24), (6, 16), (8, 12)], 609: [(21, 29)], 98: [(7, 14)], 99: [(9, 11)], 100: [(4, 25), (5, 20), (10, 10)], 102: [(6, 17)], 529: [(23, 23)], 104: [(4, 26), (8, 13)], 105: [(5, 21), (7, 15)], 108: [(4, 27), (6, 18), (9, 12)], 621: [(23, 27)], 110: [(5, 22), (10, 11)], 112: [(4, 28), (7, 16), (8, 14)], 616: [(22, 28)], 114: [(6, 19)], 115: [(5, 23)], 116: [(4, 29)], 117: [(9, 13)], 119: [(7, 17)], 120: [(5, 24), (6, 20), (8, 15), (10, 12)], 121: [(11, 11)], 125: [(5, 25)], 126: [(6, 21), (7, 18), (9, 14)], 128: [(8, 16)], 130: [(5, 26), (10, 13)], 132: [(6, 22), (11, 12)], 133: [(7, 19)], 135: [(5, 27), (9, 15)], 136: [(8, 17)], 138: [(6, 23)], 140: [(5, 28), (7, 20), (10, 14)], 143: [(11, 13)], 144: [(6, 24), (8, 18), (9, 16), (12, 12)], 145: [(5, 29)], 147: [(7, 21)], 600: [(24, 25)], 150: [(6, 25), (10, 15)], 152: [(8, 19)], 153: [(9, 17)], 154: [(7, 22), (11, 14)], 667: [(23, 29)], 156: [(6, 26), (12, 13)], 160: [(8, 20), (10, 16)], 161: [(7, 23)], 162: [(6, 27), (9, 18)], 675: [(25, 27)], 676: [(26, 26)], 165: [(11, 15)], 625: [(25, 25)], 168: [(6, 28), (7, 24), (8, 21), (12, 14)], 169: [(13, 13)], 170: [(10, 17)], 171: [(9, 19)], 174: [(6, 29)], 175: [(7, 25)], 176: [(8, 22), (11, 16)], 180: [(9, 20), (10, 18), (12, 15)], 182: [(7, 26), (13, 14)], 184: [(8, 23)], 187: [(11, 17)], 700: [(25, 28)], 189: [(7, 27), (9, 21)], 190: [(10, 19)], 192: [(8, 24), (12, 16)], 195: [(13, 15)], 196: [(7, 28), (14, 14)], 198: [(9, 22), (11, 18)], 200: [(8, 25), (10, 20)], 203: [(7, 29)], 204: [(12, 17)], 546: [(21, 26)], 207: [(9, 23)], 208: [(8, 26), (13, 16)], 209: [(11, 19)], 210: [(10, 21), (14, 15)], 725: [(25, 29)], 540: [(20, 27)], 216: [(8, 27), (9, 24), (12, 18)], 729: [(27, 27)], 220: [(10, 22), (11, 20)], 221: [(13, 17)], 224: [(8, 28), (14, 16)], 225: [(9, 25), (15, 15)], 228: [(12, 19)], 550: [(22, 25)], 230: [(10, 23)], 231: [(11, 21)], 232: [(8, 29)], 234: [(9, 26), (13, 18)], 551: [(19, 29)], 238: [(14, 17)], 240: [(10, 24), (12, 20), (15, 16)], 552: [(23, 24)], 242: [(11, 22)], 243: [(9, 27)], 756: [(27, 28)], 638: [(22, 29)], 247: [(13, 19)], 250: [(10, 25)], 252: [(9, 28), (12, 21), (14, 18)], 253: [(11, 23)], 255: [(15, 17)], 256: [(16, 16)], 260: [(10, 26), (13, 20)], 261: [(9, 29)], 520: [(20, 26)], 264: [(11, 24), (12, 22)], 812: [(28, 29)], 266: [(14, 19)], 270: [(10, 27), (15, 18)], 783: [(27, 29)], 272: [(16, 17)], 273: [(13, 21)], 275: [(11, 25)], 276: [(12, 23)], 280: [(10, 28), (14, 20)], 644: [(23, 28)], 513: [(19, 27)], 285: [(15, 19)], 286: [(11, 26), (13, 22)], 572: [(22, 26)], 288: [(12, 24), (16, 18)], 289: [(17, 17)], 290: [(10, 29)], 294: [(14, 21)], 297: [(11, 27)], 299: [(13, 23)], 300: [(12, 25), (15, 20)], 304: [(16, 19)], 648: [(24, 27)], 306: [(17, 18)], 308: [(11, 28), (14, 22)], 312: [(12, 26), (13, 24)], 624: [(24, 26)], 315: [(15, 21)], 650: [(25, 26)], 319: [(11, 29)], 320: [(16, 20)], 322: [(14, 23)], 323: [(17, 19)], 324: [(12, 27), (18, 18)], 325: [(13, 25)], 841: [(29, 29)], 330: [(15, 22)], 567: [(21, 27)], 702: [(26, 27)], 336: [(12, 28), (14, 24), (16, 21)], 338: [(13, 26)], 340: [(17, 20)], 342: [(18, 19)], 345: [(15, 23)], 348: [(12, 29)], 350: [(14, 25)], 351: [(13, 27)], 352: [(16, 22)], 357: [(17, 21)], 360: [(15, 24), (18, 20)], 361: [(19, 19)], 364: [(13, 28), (14, 26)], 368: [(16, 23)], 374: [(17, 22)], 375: [(15, 25)], 377: [(13, 29)], 378: [(14, 27), (18, 21)], 575: [(23, 25)], 380: [(19, 20)], 384: [(16, 24)], 576: [(24, 24)], 390: [(15, 26)], 391: [(17, 23)], 392: [(14, 28)], 396: [(18, 22)], 399: [(19, 21)], 400: [(16, 25), (20, 20)], 728: [(26, 28)], 405: [(15, 27)], 406: [(14, 29)], 408: [(17, 24)], 580: [(20, 29)], 414: [(18, 23)], 416: [(16, 26)], 418: [(19, 22)], 420: [(15, 28), (20, 21)], 425: [(17, 25)], 696: [(24, 29)], 754: [(26, 29)], 432: [(16, 27), (18, 24)], 435: [(15, 29)], 437: [(19, 23)], 560: [(20, 28)], 440: [(20, 22)], 441: [(21, 21)], 442: [(17, 26)], 448: [(16, 28)], 672: [(24, 28)], 450: [(18, 25)], 456: [(19, 24)], 588: [(21, 28)], 459: [(17, 27)], 460: [(20, 23)], 462: [(21, 22)], 464: [(16, 29)], 468: [(18, 26)], 532: [(19, 28)], 475: [(19, 25)], 476: [(17, 28)], 480: [(20, 24)], 483: [(21, 23)], 484: [(22, 22)], 486: [(18, 27)], 493: [(17, 29)], 494: [(19, 26)], 500: [(20, 25)], 528: [(22, 24)], 504: [(18, 28), (21, 24)], 506: [(22, 23)], 784: [(28, 28)]&#125;</span><br></pre></td></tr></table></figure><h4 id="乙可能得到得积mult-list："><a href="#乙可能得到得积mult-list：" class="headerlink" title="乙可能得到得积mult_list："></a>乙可能得到得积mult_list：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[4, 6, 8, 9, 10, 12, 14, 15, 16, 18, 20, 21, 22, 24, 25, 26, 27, 28, 30, 32, 36, 40, 42, 44, 45, 48, 50, 52, 54, 56, 60, 63, 64, 66, 70, 72, 75, 78, 80, 81, 84, 88, 90, 96, 100, 104, 105, 108, 110, 112, 120, 126, 130, 132, 135, 140, 144, 150, 154, 156, 160, 162, 168, 176, 180, 182, 189, 192, 196, 198, 200, 208, 210, 216, 220, 224, 225, 234, 240, 252, 260, 264, 270, 280, 286, 288, 300, 308, 312, 324, 336, 360, 364, 378, 400, 420, 432, 504]</span><br></pre></td></tr></table></figure><h4 id="甲可能得到的和为count-list："><a href="#甲可能得到的和为count-list：" class="headerlink" title="甲可能得到的和为count_list："></a>甲可能得到的和为count_list：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43]</span><br></pre></td></tr></table></figure><h4 id="结果："><a href="#结果：" class="headerlink" title="结果："></a>结果：</h4><p>积应该为mult_result: [4, 432]<br>和应该为count: [5, 43]<br>两个数的数组集应该为 [(1, 4), (16, 27)]</p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 小游戏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>六、Alpine和apk的使用</title>
      <link href="/2020/07/14/%E5%85%AD%E3%80%81alpine%E5%92%8Capk%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
      <url>/2020/07/14/%E5%85%AD%E3%80%81alpine%E5%92%8Capk%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="Alpine使用"><a href="#Alpine使用" class="headerlink" title="Alpine使用"></a>Alpine使用</h2><h3 id="BusyBox简介"><a href="#BusyBox简介" class="headerlink" title="BusyBox简介"></a>BusyBox简介</h3><p>BusyBox很适合容器部署，恰巧因为它在设计时没有刻意考虑容器。BusyBox被其开发人员称为“嵌入式Linux的瑞士军刀”，它作为一个单一的小型可执行文件，包含一百多个最常用linux命令和工具的软件,他甚至还集成了一个http服务器和一个telnet服务器,而所有这一切功能却只有区区1M左右的大小。这也“迫使”它在容器技术出现之前，就可以开始采用类似容器的方法进行部署了。</p><p>BusyBox可以使用Linux或其他POSIX操作系统作为其基础进行部署，并将它们与许多常见的Linux实用程序捆绑在一起。如此一来，它成为了一个紧凑的单文件可执行文件，其中包含“完整”Linux发行版的许多功能——尽管这些完整版本中的不少其他功能选项，都以节省空间的名义，被从BusyBox中删除了。</p><h3 id="alpine简介"><a href="#alpine简介" class="headerlink" title="alpine简介"></a>alpine简介</h3><p><a href="https://link.jianshu.com?t=https%3A%2F%2Fwww.alpinelinux.org%2F" target="_blank" rel="noopener">Alpine Linux</a>是一个轻型Linux发行版，它不同于通常的Linux发行版，Alpine采用了musl libc 和 BusyBox以减少系统的体积和运行时的资源消耗。Alpine Linux提供了自己的包管理工具：apk，我们可以通过<a href="https://link.jianshu.com?t=https%3A%2F%2Fpkgs.alpinelinux.org%2Fpackages" target="_blank" rel="noopener">https://pkgs.alpinelinux.org/packages</a> 查询包信息。</p><p>BusyBox是因为是单一可执行文件而体积很小， 而Alpine Linux则是使用强化的内核，为其前身BusyBox的紧凑、简单的目标增加安全性。相较于BusyBox，Alpine Linux能让开发人员更容易添加功能。它的发行版基于BusyBox和musl库之上，因此在添加功能的方便性或结构紧凑度的这些维度上，Alpine Linux一枝独秀。</p><p>Docker官方也已开始推荐使用Alpine替代之前的Ubuntu来作为基础镜像，因为这样会带来多个好处，包括镜像下载速度加快，镜像安全性提高，占用更少的主机磁盘空间等。</p><p>Alpine Docker为了精简体积，默认是没有安装bash的。 需要的话可以查看官方文档：<a href="https://www.cyberciti.biz/faq/alpine-linux-install-bash-using-apk-command/" target="_blank" rel="noopener">https://www.cyberciti.biz/faq/alpine-linux-install-bash-using-apk-command/</a></p><h3 id="基础容器镜像"><a href="#基础容器镜像" class="headerlink" title="基础容器镜像"></a>基础容器镜像</h3><ul><li><p>java基础镜像 docker.io/jeanblanchard/alpine-glibc</p></li><li><p>nginx基础镜像 nginx:1.16.1-alpine</p></li></ul><h2 id="apk使用"><a href="#apk使用" class="headerlink" title="apk使用"></a>apk使用</h2><h3 id="设置国内镜像"><a href="#设置国内镜像" class="headerlink" title="设置国内镜像"></a>设置国内镜像</h3><p><strong>查看Alpine版本号</strong>,我这里的版本号为3.11</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~ # cat &#x2F;etc&#x2F;os-release </span><br><span class="line">NAME&#x3D;&quot;Alpine Linux&quot;</span><br><span class="line">ID&#x3D;alpine</span><br><span class="line">VERSION_ID&#x3D;3.11.2</span><br><span class="line">PRETTY_NAME&#x3D;&quot;Alpine Linux v3.11&quot;</span><br></pre></td></tr></table></figure><p><strong>修改repositories源</strong>（alpine系统专用源文件在/etc/apk/repositories）</p><p>三组数据源选其中一组（一对）即可，注意仓库版本号和Alpine版本号匹配</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vi &#x2F;etc&#x2F;apk&#x2F;repositories</span><br><span class="line"># alpine系统默认数据源，下载较慢</span><br><span class="line">http:&#x2F;&#x2F;dl-cdn.alpinelinux.org&#x2F;alpine&#x2F;v3.11&#x2F;main</span><br><span class="line">http:&#x2F;&#x2F;dl-cdn.alpinelinux.org&#x2F;alpine&#x2F;v3.11&#x2F;community</span><br><span class="line"># aliyun数据源（建议选这个）</span><br><span class="line">https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;alpine&#x2F;v3.11&#x2F;main&#x2F;</span><br><span class="line">https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;alpine&#x2F;v3.11&#x2F;community&#x2F;</span><br><span class="line"># ustc数据源</span><br><span class="line">https:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;alpine&#x2F;v3.11&#x2F;main&#x2F;</span><br><span class="line">https:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;alpine&#x2F;v3.11&#x2F;community&#x2F;</span><br></pre></td></tr></table></figure><p>更新最新本地镜像源</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~ # apk update</span><br><span class="line">fetch https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;alpine&#x2F;v3.11&#x2F;main&#x2F;x86_64&#x2F;APKINDEX.tar.gz</span><br><span class="line">fetch https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;alpine&#x2F;v3.11&#x2F;community&#x2F;x86_64&#x2F;APKINDEX.tar.gz</span><br><span class="line">v3.11.6-93-ga95c3541d2 [https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;alpine&#x2F;v3.11&#x2F;main&#x2F;]</span><br><span class="line">v3.11.6-94-g67535a078c [https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;alpine&#x2F;v3.11&#x2F;community&#x2F;]</span><br><span class="line">OK: 11288 distinct packages available</span><br></pre></td></tr></table></figure><h3 id="安装telnet"><a href="#安装telnet" class="headerlink" title="安装telnet"></a>安装telnet</h3><p>Alpine镜像中的telnet在3.7版本后被转移至busybox-extras包中，需要使用apk单独安装。所以通过apk search telnet查找包再安装的方式已经不能用了</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~ # apk add busybox-extras</span><br></pre></td></tr></table></figure><h3 id="alpine系统中添加字体"><a href="#alpine系统中添加字体" class="headerlink" title="alpine系统中添加字体"></a>alpine系统中添加字体</h3><p>拷贝字体文件放到文件夹/usr/share/fonts/下。字体文件请自行寻找</p><p>alpine系统中安装font-adobe-100dpi，如果不安装这个软件，字体不会生效</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装查看字体命令</span><br><span class="line">apk add font-adobe-100dpi</span><br><span class="line"></span><br><span class="line"># 刷新字体缓存</span><br><span class="line">fc-cache -fv</span><br><span class="line"></span><br><span class="line"># 查看字体</span><br><span class="line">fc-list | grep &#39;宋体&#39;</span><br></pre></td></tr></table></figure><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggqohmhnpcj30qz02x3z1.jpg" alt="image-20200714182954461" style="zoom:67%;max-width: 80%" /><h3 id="apk安装telnet报错"><a href="#apk安装telnet报错" class="headerlink" title="apk安装telnet报错"></a>apk安装telnet报错</h3><h4 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash-4.3# apk add telnet</span><br><span class="line">ERROR: unsatisfiable constraints:</span><br><span class="line">  telnet (missing):</span><br><span class="line">    required by: world[telnet]</span><br></pre></td></tr></table></figure><h4 id="安装telnet-1"><a href="#安装telnet-1" class="headerlink" title="安装telnet"></a>安装telnet</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~ # apk add busybox-extras</span><br></pre></td></tr></table></figure><h2 id="CentOS、Ubuntu、Debian三个linux比较"><a href="#CentOS、Ubuntu、Debian三个linux比较" class="headerlink" title="CentOS、Ubuntu、Debian三个linux比较"></a>CentOS、Ubuntu、Debian三个linux比较</h2><h3 id="CentOS"><a href="#CentOS" class="headerlink" title="CentOS"></a>CentOS</h3><p>你会发现非常多的商业公司部署在生产环境上的服务器都是使用的CentOS系统，CentOS是从RHEL源代码编译的社区重新发布版。CentOS简约，命令行下的人性化做得比较好，稳定，有着强大的英文文档与开发社区的支持。与Redhat有着相同的渊源。虽然不单独提供商业支持，但往往可以从Redhat中找到一丝线索。相对debian来说，CentOS略显体积大一点。是一个非常成熟的Linux发行版。</p><h3 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h3><p>Ubuntu近些年的粉丝越来越多，Ubuntu有着漂亮的用户界面，完善的包管理系统，强大的软件源支持，丰富的技术社区，Ubuntu还对大多数硬件有着良好的兼容性，包括最新的图形显卡等等。这一切让Ubuntu越来越向大众化方向发展。但别忘了：你所需要的只是一个简约、稳定、易用的服务器系统而已！</p><h3 id="Debian"><a href="#Debian" class="headerlink" title="Debian"></a>Debian</h3><p>一般来说Debian作为适合于服务器的操作系统，它比Ubuntu要稳定得多。可以说稳定得无与伦比了。debian整个系统，只要应用层面不出现逻辑缺陷，基本上固若金汤，是个常年不需要重启的系统（当然，这是夸张了点，但并没有夸大其稳定性）。debian整个系统基础核心非常小，不仅稳定，而且占用硬盘空间小，占用内存小。128M的VPS即可以流畅运行Debian，而CentOS则会略显吃力。但是由于Debian的发展路线，使它的帮助文档相对于CentOS略少，技术资料也少一些。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul><li><a href="https://blog.csdn.net/educast/article/details/38315433" target="_blank" rel="noopener">CentOS、Ubuntu、Debian三个linux比较异同</a></li></ul><h2 id="镜像是什么操作系统"><a href="#镜像是什么操作系统" class="headerlink" title="镜像是什么操作系统"></a>镜像是什么操作系统</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;bin# cat &#x2F;etc&#x2F;issue </span><br><span class="line">Debian GNU&#x2F;Linux 10 \n \l</span><br></pre></td></tr></table></figure><h2 id="Debian使用镜像源"><a href="#Debian使用镜像源" class="headerlink" title="Debian使用镜像源"></a>Debian使用镜像源</h2><p>编辑/etc/apt/sources.list文件, 在文件最前面添加以下条目(操作前请做好相应备份)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;apt&#x2F;sources.list &#x2F;etc&#x2F;apt&#x2F;sources.list.bak</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;apt&#x2F;sources.list</span><br><span class="line">deb https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian&#x2F; buster main contrib non-free</span><br><span class="line"># deb-src https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian&#x2F; buster main contrib non-free</span><br><span class="line">deb https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian&#x2F; buster-updates main contrib non-free</span><br><span class="line"># deb-src https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian&#x2F; buster-updates main contrib non-free</span><br><span class="line">deb https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian&#x2F; buster-backports main contrib non-free</span><br><span class="line"># deb-src https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian&#x2F; buster-backports main contrib non-free</span><br><span class="line">deb https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian-security buster&#x2F;updates main contrib non-free</span><br><span class="line"># deb-src https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;debian-security buster&#x2F;updates main contrib non-free</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">apt-get update</span><br><span class="line">apt-get -y install inetutils-ping telnet procps net-tools</span><br></pre></td></tr></table></figure><p>镜像列表: <a href="http://www.debian.org/mirror/list" target="_blank" rel="noopener">http://www.debian.org/mirror/list</a></p><p><strong>查看版本号</strong> cat /etc/os-release</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt search ps</span><br><span class="line">apt-get -y install procps [包含ps]</span><br><span class="line">apt-get -y install net-tools [包含netstat ifconfig]</span><br><span class="line">apt-get -y install inetutils-ping [ping]</span><br><span class="line">apt-get -y install telnet</span><br></pre></td></tr></table></figure><h2 id="Ubuntu-修改-apt-get-源为国内镜像源的方法"><a href="#Ubuntu-修改-apt-get-源为国内镜像源的方法" class="headerlink" title="Ubuntu 修改 apt-get 源为国内镜像源的方法"></a>Ubuntu 修改 apt-get 源为国内镜像源的方法</h2><p>编辑/etc/apt/sources.list文件, 在文件最前面添加以下条目(操作前请做好相应备份)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;apt&#x2F;sources.list &#x2F;etc&#x2F;apt&#x2F;sources.list.bak</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;apt&#x2F;sources.list</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-security main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-security main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-updates main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-updates main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-backports main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-backports main restricted universe multivers</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-proposed main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-proposed main restricted universe multiverse</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">apt-get update</span><br></pre></td></tr></table></figure><h3 id="参考文献-1"><a href="#参考文献-1" class="headerlink" title="参考文献"></a>参考文献</h3><ul><li><a href="https://www.cnblogs.com/wuzaipei/p/12891652.html" target="_blank" rel="noopener">Ubuntu 修改 apt-get 源为国内镜像源的方法</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> Alpine </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> Alpine </tag>
            
            <tag> apk </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>maven仓库</title>
      <link href="/2020/07/14/maven%E4%BB%93%E5%BA%93/"/>
      <url>/2020/07/14/maven%E4%BB%93%E5%BA%93/</url>
      
        <content type="html"><![CDATA[<h2 id="maven-release版本不更新原因"><a href="#maven-release版本不更新原因" class="headerlink" title="maven release版本不更新原因"></a>maven release版本不更新原因</h2><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>有个2.5.1版本的jar包在私服上已经存在，A机器修改代码，但是没有修改版本号，重新上传到私服后，B机器更新不下来，除非B机器删除本地maven仓库中的jar包才能更新下来。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!--B机器 pom.xml配置--&gt;</span><br><span class="line"> &lt;dependency&gt;</span><br><span class="line">     &lt;groupId&gt;com.github.snakerflow&lt;&#x2F;groupId&gt;</span><br><span class="line">     &lt;artifactId&gt;snaker-core&lt;&#x2F;artifactId&gt;</span><br><span class="line">     &lt;version&gt;2.5.1&lt;&#x2F;version&gt;</span><br><span class="line"> &lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="初次尝试"><a href="#初次尝试" class="headerlink" title="初次尝试"></a>初次尝试</h3><p>最初，以为是更新策略的问题，将release版本配置为总是更新，还是更新不下来。如下，</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">!--C:\Users\DELL\.m2\settings.xml--&gt;</span><br><span class="line"></span><br><span class="line">&lt;profile&gt;</span><br><span class="line">    &lt;id&gt;nexus&lt;&#x2F;id&gt;</span><br><span class="line">    &lt;repositories&gt;</span><br><span class="line">        &lt;repository&gt;</span><br><span class="line">            &lt;id&gt;central&lt;&#x2F;id&gt;</span><br><span class="line">            &lt;url&gt;http:&#x2F;&#x2F;central&lt;&#x2F;url&gt;</span><br><span class="line">            &lt;releases&gt;</span><br><span class="line">                &lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">                &lt;updatePolicy&gt;always&lt;&#x2F;updatePolicy&gt;</span><br><span class="line">            &lt;&#x2F;releases&gt;</span><br><span class="line">            &lt;snapshots&gt;</span><br><span class="line">                &lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">                &lt;updatePolicy&gt;always&lt;&#x2F;updatePolicy&gt;</span><br><span class="line">            &lt;&#x2F;snapshots&gt;</span><br><span class="line">        &lt;&#x2F;repository&gt;</span><br><span class="line">    &lt;&#x2F;repositories&gt;</span><br><span class="line">&lt;&#x2F;profile&gt;</span><br></pre></td></tr></table></figure><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>在《maven实战》P87一书中找到了更新原理。<br>如果是release版本，首先从本地查找，如果有，则使用本地，否则从远程服务器下载。 因为，本地已存在，所以不会更新。所以，只有删除本地，才能更新到最新。<br>为了更新不再删除，改了jar的版本号，将jar包的版本号改为2.5.2-SNAPSHOT，同时修改依赖的版本号，就能下载到最新版本了。</p><h3 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h3><h4 id="RELEASE"><a href="#RELEASE" class="headerlink" title="RELEASE"></a>RELEASE</h4><p>如果版本号配置为RELEASE ，那么用本地MAVEN_REPOSITORY/groupid/artifactid/maven-metadata.xml，与服务器的maven-metadata.xml合并，会使用最大的release版本版本号。</p><p>举例，这种情况就会使用最大release版本号2.5.2，而不是2.5.3-snapshot，因为2.5.3-snapshot是快照版本不是发布版本，应该尽量避免使用RELEASE，因为版本是不稳定的。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;com.github.snakerflow&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;snaker-core&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;RELEASE&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!--本地服务器的maven-metadata.xml--&gt;</span><br><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;metadata&gt;</span><br><span class="line">  &lt;groupId&gt;com.github.snakerflow&lt;&#x2F;groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;snaker-core&lt;&#x2F;artifactId&gt;</span><br><span class="line">  &lt;versioning&gt;</span><br><span class="line">    &lt;release&gt;2.5.1&lt;&#x2F;release&gt;</span><br><span class="line">    &lt;versions&gt;</span><br><span class="line">      &lt;version&gt;2.5.1&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;&#x2F;versions&gt;</span><br><span class="line">    &lt;lastUpdated&gt;20170908072352&lt;&#x2F;lastUpdated&gt;</span><br><span class="line">  &lt;&#x2F;versioning&gt;</span><br><span class="line">&lt;&#x2F;metadata&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!--远程服务器的maven-metadata.xml--&gt;</span><br><span class="line">&lt;metadata&gt;</span><br><span class="line">    &lt;groupId&gt;com.github.snakerflow&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;snaker-core&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;versioning&gt;</span><br><span class="line">        &lt;release&gt;2.5.1&lt;&#x2F;release&gt;</span><br><span class="line">        &lt;versions&gt;</span><br><span class="line">            &lt;version&gt;2.5.1&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;version&gt;2.5.2&lt;&#x2F;version&gt;</span><br><span class="line">            &lt;version&gt;2.5.3-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;versions&gt;</span><br><span class="line">    &lt;lastUpdated&gt;20170905010758&lt;&#x2F;lastUpdated&gt;</span><br><span class="line">    &lt;&#x2F;versioning&gt;</span><br><span class="line">&lt;&#x2F;metadata&gt;</span><br></pre></td></tr></table></figure><h4 id="SNAPSHOT"><a href="#SNAPSHOT" class="headerlink" title="SNAPSHOT"></a>SNAPSHOT</h4><p>snapshot版本与release，略微不同，它比较的是lastUpdated，哪个新就下载哪个，所以如果版本号是x.x.x-SNAPSHOT，肯定会更新下来。</p><h4 id="LATEST"><a href="#LATEST" class="headerlink" title="LATEST"></a>LATEST</h4><p>最新版本，则是发布版和快照中，最新的版本，所以，如果version配置为latest，则版本也不是稳定的。</p><hr><p>原文链接：wangjun5159 的 <a href="https://blog.csdn.net/wangjun5159/article/details/77854572" target="_blank" rel="noopener">maven release版本不更新原因分析</a></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> 常见问题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> maven </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二、k8s常用命令</title>
      <link href="/2020/07/14/%E4%BA%8C%E3%80%81k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2020/07/14/%E4%BA%8C%E3%80%81k8s%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="k8s常用命令"><a href="#k8s常用命令" class="headerlink" title="k8s常用命令"></a>k8s常用命令</h2><p>修改启动命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">command: [ &quot;&#x2F;bin&#x2F;bash&quot;, &quot;-ce&quot;, &quot;tail -f &#x2F;dev&#x2F;null&quot; ]</span><br></pre></td></tr></table></figure><h2 id="pod失败"><a href="#pod失败" class="headerlink" title="pod失败"></a>pod失败</h2><h3 id="pod显示状态为Evicted"><a href="#pod显示状态为Evicted" class="headerlink" title="pod显示状态为Evicted"></a>pod显示状态为Evicted</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nginx-ingress-zz9qs   0&#x2F;1     Evicted   0          22s    &lt;none&gt;           s1     &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>Evicted：即驱赶的意思，意思是当节点出现异常时，kubernetes将有相应的机制驱赶该节点上的Pod。<br> 多见于资源不足时导致的驱赶。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 知识点 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 知识点 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>部署harbor集群</title>
      <link href="/2020/07/14/%E9%83%A8%E7%BD%B2harbor%E9%9B%86%E7%BE%A4/"/>
      <url>/2020/07/14/%E9%83%A8%E7%BD%B2harbor%E9%9B%86%E7%BE%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="nginx配置"><a href="#nginx配置" class="headerlink" title="nginx配置"></a>nginx配置</h2><p>部署nginx,实现高可用访问</p><h3 id="配置文件harbor-conf"><a href="#配置文件harbor-conf" class="headerlink" title="配置文件harbor.conf"></a>配置文件harbor.conf</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">upstream harbor &#123;</span><br><span class="line">    ip_hash;</span><br><span class="line">    server 192.168.2.2:443;</span><br><span class="line">    server 192.168.2.3:443;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen      80;</span><br><span class="line">    server_name harbor.od.com;</span><br><span class="line">    return      308 https:&#x2F;&#x2F;$host$request_uri;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen  443 ssl;</span><br><span class="line">    server_name harbor.od.com;</span><br><span class="line">    access_log &#x2F;data&#x2F;logs&#x2F;nginx&#x2F;harbor.log;</span><br><span class="line">  </span><br><span class="line">    ssl_certificate od.pem;  # SSL 证书 </span><br><span class="line">    ssl_certificate_key od.key;  # SSL 私钥</span><br><span class="line">    client_max_body_size 0;</span><br><span class="line">    chunked_transfer_encoding on;</span><br><span class="line"></span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        proxy_pass https:&#x2F;&#x2F;harbor;</span><br><span class="line">        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">        proxy_set_header Host $host;</span><br><span class="line">        proxy_set_header X-Forwarded-Proto https;</span><br><span class="line">        proxy_redirect off;</span><br><span class="line">        proxy_ssl_verify off;</span><br><span class="line">        proxy_ssl_session_reuse on;</span><br><span class="line">        proxy_http_version 1.1;</span><br><span class="line">&#125;</span><br><span class="line">    location &#x2F;v2&#x2F; &#123;</span><br><span class="line">        proxy_pass https:&#x2F;&#x2F;harbor&#x2F;v2&#x2F;;</span><br><span class="line">        proxy_set_header Host $host;</span><br><span class="line">        proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">        proxy_set_header X-Forwarded-Proto $scheme;</span><br><span class="line">        proxy_ssl_verify off;</span><br><span class="line">        proxy_ssl_session_reuse on;</span><br><span class="line">        proxy_buffering off;</span><br><span class="line">        proxy_request_buffering off;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="nginx日志"><a href="#nginx日志" class="headerlink" title="nginx日志"></a>nginx日志</h3><p>docker下载代码时，nginx日志</p><p>docker pull harbor.od.com/public/rabbitmq:v3.8.6-management</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">39.106.100.203 - - [20&#x2F;Aug&#x2F;2020:17:37:37 +0800] &quot;GET &#x2F;v2&#x2F; HTTP&#x2F;1.0&quot; 401 87 &quot;-&quot; &quot;docker&#x2F;18.06.0-ce go&#x2F;go1.10.3 git-commit&#x2F;0ffa825 kernel&#x2F;3.10.0-1062.4.1.el7.x86_64 os&#x2F;linux arch&#x2F;amd64 UpstreamClient(Docker-Client&#x2F;18.06.0-ce \x5C(linux\x5C))&quot;</span><br><span class="line">39.106.100.203 - - [20&#x2F;Aug&#x2F;2020:17:37:37 +0800] &quot;GET &#x2F;service&#x2F;token?scope&#x3D;repository%3Apublic%2Frabbitmq%3Apull&amp;service&#x3D;harbor-registry HTTP&#x2F;1.1&quot; 200 967 &quot;-&quot; &quot;docker&#x2F;18.06.0-ce go&#x2F;go1.10.3 git-commit&#x2F;0ffa825 kernel&#x2F;3.10.0-1062.4.1.el7.x86_64 os&#x2F;linux arch&#x2F;amd64 UpstreamClient(Docker-Client&#x2F;18.06.0-ce \x5C(linux\x5C))&quot;</span><br><span class="line">39.106.100.203 - - [20&#x2F;Aug&#x2F;2020:17:37:37 +0800] &quot;GET &#x2F;v2&#x2F;public&#x2F;rabbitmq&#x2F;manifests&#x2F;v3.8.6-management HTTP&#x2F;1.0&quot; 200 2829 &quot;-&quot; &quot;docker&#x2F;18.06.0-ce go&#x2F;go1.10.3 git-commit&#x2F;0ffa825 kernel&#x2F;3.10.0-1062.4.1.el7.x86_64 os&#x2F;linux arch&#x2F;amd64 UpstreamClient(Docker-Client&#x2F;18.06.0-ce \x5C(linux\x5C))&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> harbor </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> harbor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>四、git使用</title>
      <link href="/2020/07/13/%E5%9B%9B%E3%80%81git%E4%BD%BF%E7%94%A8/"/>
      <url>/2020/07/13/%E5%9B%9B%E3%80%81git%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="git知识点"><a href="#git知识点" class="headerlink" title="git知识点"></a>git知识点</h2><h3 id="git添加文件忽略"><a href="#git添加文件忽略" class="headerlink" title="git添加文件忽略"></a>git添加文件忽略</h3><p>在文件.gitignore中写入文件名即可，支持正则匹配。比如hexo的忽略文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 hexo]# cat .gitignore </span><br><span class="line">.DS_Store</span><br><span class="line">Thumbs.db</span><br><span class="line">db.json</span><br><span class="line">*.log</span><br><span class="line">node_modules&#x2F;</span><br><span class="line">public&#x2F;</span><br><span class="line">.deploy*&#x2F;</span><br></pre></td></tr></table></figure><h2 id="git-diff获取差异文件名显示中文乱码"><a href="#git-diff获取差异文件名显示中文乱码" class="headerlink" title="git diff获取差异文件名显示中文乱码"></a>git diff获取差异文件名显示中文乱码</h2><blockquote><p>参考文章 <a href="https://blog.csdn.net/hanlizhong85/article/details/80642571" target="_blank" rel="noopener">git diff获取差异文件名显示中文乱码</a></p></blockquote><p>通过git diff命令对前后两次commit版本进行差异化的对比，中文名的文件显示为乱码。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 hexo]# git diff HEAD^ HEAD --name-only</span><br><span class="line">&quot;source&#x2F;_posts&#x2F;wangzt&#x2F;linux&#x2F;1.linux\351\232\217\347\254\224-1.md&quot;</span><br><span class="line">&quot;source&#x2F;_posts&#x2F;wangzt&#x2F;linux&#x2F;2.linux\351\232\217\347\254\224-2.md&quot;</span><br></pre></td></tr></table></figure><p>git diff接受的编码格式为utf-8，而文件名是以gb2312格式编写的，所以git diff显示了乱码。</p><p>解决办法有以下两种：</p><ol><li><strong>执行以下命令，修改core.quotepath配置：</strong></li></ol><p><code>git config --global core.quotepath false</code></p><p> core.quotepath设为false的话，就不会对编码大于0x80的字符进行quote。中文显示正常。</p><ol start="2"><li><strong>通过vim编辑器，将文件名另存为utf-8格式</strong></li></ol>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一、Typora更换主题详细步骤</title>
      <link href="/2020/07/13/%E4%B8%80%E3%80%81typora%E6%9B%B4%E6%8D%A2%E4%B8%BB%E9%A2%98%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4/"/>
      <url>/2020/07/13/%E4%B8%80%E3%80%81typora%E6%9B%B4%E6%8D%A2%E4%B8%BB%E9%A2%98%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="Typora优化"><a href="#Typora优化" class="headerlink" title="Typora优化"></a>Typora优化</h2><h3 id="如何让-Typora-插入图片居左对齐"><a href="#如何让-Typora-插入图片居左对齐" class="headerlink" title="如何让 Typora 插入图片居左对齐"></a>如何让 Typora 插入图片居左对齐</h3><p>使用 Typora 插入图片，图片一直是居中的，有时候插入小图片，居中感觉有点难看，通过HTML标签可以改变位置。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;img src &#x3D; &quot;.&#x2F;images&#x2F;python&#x2F;logo.png&quot; align&#x3D;&quot;center&quot;&gt;</span><br></pre></td></tr></table></figure><h3 id="Typora调整字体大小"><a href="#Typora调整字体大小" class="headerlink" title="Typora调整字体大小"></a>Typora调整字体大小</h3><p>偏好设置 -&gt; 外观 -&gt; 字体大小 -&gt; 自定义，就可以设置字体大小了。</p><img src="http://wangzhangtao.com/img/body/jike/image-20200925172243788.png" alt="image-20200925172243788" style="zoom:70%;max-width: 70%" /><h2 id="Typora更换主题"><a href="#Typora更换主题" class="headerlink" title="Typora更换主题"></a>Typora更换主题</h2><p>看到Typora原始的主题不太好看，于是想换一下主题。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggpbf359f3j30x20jy40a.jpg" alt="Typora原始例子" style="zoom:67%;max-width: 50%;" /><p>1、首先去Typora的官网，在首页的右上方看到主题选项点击。这里直接点击<a href="http://theme.typora.io/" target="_blank" rel="noopener">Typora主题</a>即可</p><p>2、跳转到这个页面，点击自己喜欢的主题（这里我选择主题是<a href="http://theme.typora.io/theme/softgreen/" target="_blank" rel="noopener">softgreen</a>）</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggpblhj8ywj31h60u0b29.jpg" alt="image-20200713141518400" style="zoom:80%;max-width:50%" /><p>3、进入到下载页面，点击右面的<a href="https://github.com/pomopopo/typora-theme-softgreen" target="_blank" rel="noopener">Download</a></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggpbq02k8vj31lm0u0n55.jpg" alt="softgreen-download" style="zoom:67%;max-width: 50%" /><p>4、进入主题的GitHub页面，点击箭头所示的链接，下载主题的安装包</p><p>直接下载包可能是500错误，我是下载的源码</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;pomopopo&#x2F;typora-theme-softgreen.git</span><br></pre></td></tr></table></figure><p>5、打开Typora –&gt; 偏好设置 –&gt; 外观 –&gt; 打开主题文件夹</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggpby9sdz9j30jg0d9myg.jpg" alt="image-20200713143029456" style="zoom:67%;max-width: 60%" /><p>6、然后将css文件放到主题 themes文件夹下</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggqh2ksaxqj30jc0aw75g.jpg" alt="image-20200714141308221" style="zoom:67%;max-width: 60%" /><p>7、重启之后，就可以看到自己下载的主题啦！</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggqh4s21tmj30v10ig0vg.jpg" alt="image-20200714141520963" style="zoom:67%;max-width: 60%" />]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
          <category> Typora </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 其他 </tag>
            
            <tag> Typora </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vCenterServer的部署和使用</title>
      <link href="/2020/07/09/vcenterserver%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BD%BF%E7%94%A8/"/>
      <url>/2020/07/09/vcenterserver%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="部署vCenter-Server-6"><a href="#部署vCenter-Server-6" class="headerlink" title="部署vCenter Server 6"></a>部署vCenter Server 6</h1><p>待添加</p><h1 id="vCenter-Server-6的使用"><a href="#vCenter-Server-6的使用" class="headerlink" title="vCenter Server 6的使用"></a>vCenter Server 6的使用</h1><h2 id="设置VMware虚拟机开机自启"><a href="#设置VMware虚拟机开机自启" class="headerlink" title="设置VMware虚拟机开机自启"></a>设置VMware虚拟机开机自启</h2><blockquote><p>原文地址 <a href="https://blog.csdn.net/hzfw2008/article/details/86599600" target="_blank" rel="noopener">ESXi设置虚拟机随宿主机自动启动（转）</a></p></blockquote><p>1、选择host主机——&gt;右侧选择“配置”页签——&gt;选择“虚拟机启动/关机”；</p><img src="http://wangzhangtao.com/img/body/2.vCenterServer%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BD%BF%E7%94%A8/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h6ZncyMDA4,size_16,color_FFFFFF,t_70.png" alt="img" style="zoom:67%;" /><p>2、点击右侧“属性”——&gt;勾选“允许虚拟机与系统一起启动和停止”；</p><img src="http://wangzhangtao.com/img/body/2.vCenterServer%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BD%BF%E7%94%A8/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h6ZncyMDA4,size_16,color_FFFFFF,t_70-20200907112252872.png" alt="img" style="zoom:67%;" /><p>3、选择一个虚拟机,然后“上移”到”自动启动”队列中；</p><img src="http://wangzhangtao.com/img/body/2.vCenterServer%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8C%E4%BD%BF%E7%94%A8/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h6ZncyMDA4,size_16,color_FFFFFF,t_70-20200907112417720.png" alt="img" style="zoom:67%;" /><p>4、选定的虚拟机状态已启用，其余虚拟机状态仍为禁用。可以从启动宿主机看看效果！注意：一定要在VM处于关闭状态下设置，否则是无效的！</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
          <category> 服务器 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 其他 </tag>
            
            <tag> 服务器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>三、linux问题收集</title>
      <link href="/2020/07/08/%E4%B8%89%E3%80%81linux%E9%97%AE%E9%A2%98%E6%94%B6%E9%9B%86/"/>
      <url>/2020/07/08/%E4%B8%89%E3%80%81linux%E9%97%AE%E9%A2%98%E6%94%B6%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h2 id="zookeeper启动失败"><a href="#zookeeper启动失败" class="headerlink" title="zookeeper启动失败"></a>zookeeper启动失败</h2><p>zookeeper启动报错 <code>The current epoch, 1d, is older than the last zxid, 30064771074</code></p><p>启动Zookeeper时遇到如下问题：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Reading snapshot &#x2F;usr&#x2F;local&#x2F;zk&#x2F;data&#x2F;version-2&#x2F;snapshot.800000003</span><br><span class="line">2019-12-09 23:16:41,955 [myid:1] - ERROR [main:QuorumPeer@453] - Unable to load database on disk</span><br><span class="line">java.io.IOException: The current epoch, 1d, is older than the last zxid, 30064771074</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:435)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:409)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:156)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:116)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:79)</span><br><span class="line">2019-12-09 23:16:41,966 [myid:1] - ERROR [main:QuorumPeerMain@94] - Unexpected exception, exiting abnormally</span><br><span class="line">java.lang.RuntimeException: Unable to run quorum server </span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:454)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:409)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:156)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:116)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:79)</span><br><span class="line">Caused by: java.io.IOException: The current epoch, 8, is older than the last zxid, 38654705667</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:435)</span><br><span class="line"></span><br><span class="line">... 4 more</span><br></pre></td></tr></table></figure><p>这里，意思是说当前的阶段比最新的阶段（zxid为30064771074）要老。</p><p><strong>换句话说，即<code>此节点上的Zookeeper</code>所处阶段与<code>当前ClouderaManager中Zookeeper</code>的阶段<code>不匹配</code>，导致无法启动此节点上面的<code>Zookeeper Quorum Server</code>。</strong></p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>既然版本不匹配，那么，我们选择删除的是问题节点上的版本数据，即删除/data/zookeeper/data/下的version-2文件夹下的所有东西：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo rm -r &#x2F;data&#x2F;zookeeper&#x2F;data&#x2F;version-2&#x2F;*</span><br></pre></td></tr></table></figure><h3 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h3><p>不要删除version-2文件夹，否则可能会报错no space left on device。<br>  如果误删，请参考<a href="https://blog.csdn.net/pre_tender/article/details/103431258" target="_blank" rel="noopener">ZooKeeper报错：Unable to access datadir, exiting abnormally</a><br>不要删除myid。<br>  如果误删，请先删除此Slave2的Zookeeper server，然后将/data/zookeeper/data/目录删除，再重新给Slave2添加Zookeeper Server</p><hr><p>原文链接：pre_tender博主的 <a href="https://blog.csdn.net/pre_tender/article/details/103431145" target="_blank" rel="noopener">Zookeeper启动报错：The current epoch…</a></p><h2 id="Jenkins任务结束时-自动关闭衍生的子进程"><a href="#Jenkins任务结束时-自动关闭衍生的子进程" class="headerlink" title="Jenkins任务结束时,自动关闭衍生的子进程"></a>Jenkins任务结束时,自动关闭衍生的子进程</h2><p>在jenkins执行任务时，发现任务结束,jenkins自动关闭衍生的子进程，需要设置变量  JENKINS_NODE_COOKIE=dontkillme</p><h2 id="下载centos镜像"><a href="#下载centos镜像" class="headerlink" title="下载centos镜像"></a>下载centos镜像</h2><p><a href="https://hub.docker.com/_/centos" target="_blank" rel="noopener">centos docker-hub</a> </p><p>选择自己的版本,我这里选择的是7.6.1810</p><p>docker pull centos:7.6.1810</p><p>docker run -it –rm –name centos7 –rm centos:7.6.1810 /bin/bash</p><h2 id="centos7安装中文宋体"><a href="#centos7安装中文宋体" class="headerlink" title="centos7安装中文宋体"></a>centos7安装中文宋体</h2><p><strong>安装字体插件</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y fontconfig mkfontscale</span><br></pre></td></tr></table></figure><p><strong>刷新字体</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fc-cache -fv</span><br></pre></td></tr></table></figure><p><strong>查看字体</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@iZ250234rf8Z TrueType]# fc-list |grep SimSun</span><br><span class="line">&#x2F;usr&#x2F;share&#x2F;fonts&#x2F;chinese&#x2F;TrueType&#x2F;simsun.ttc: SimSun,宋体:style&#x3D;Regular,常规</span><br><span class="line">&#x2F;usr&#x2F;share&#x2F;fonts&#x2F;chinese&#x2F;TrueType&#x2F;simsun.ttc: NSimSun,新宋体:style&#x3D;Regular,常规</span><br></pre></td></tr></table></figure><p>字体默认文件夹 /usr/share/fonts</p><h3 id="关闭SSH其他用户会话连接"><a href="#关闭SSH其他用户会话连接" class="headerlink" title="关闭SSH其他用户会话连接"></a>关闭SSH其他用户会话连接</h3><p>在一些生产平台往往看到一大堆的用户SSH连接到同一台服务器，或者连接后没有正常关闭进程还驻留在系统内。限制SSH连接数与手动断开空闲连接也有必要之举，这里写出手动剔出其他用户的过程。</p><h4 id="1-查看系统在线用户"><a href="#1-查看系统在线用户" class="headerlink" title="1. 查看系统在线用户"></a>1. 查看系统在线用户</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@apache ~]# w </span><br><span class="line">14:15:41 up 42 days, 56 min, 2 users, load average: 0.07, 0.02, 0.00 </span><br><span class="line">USER   TTY   FROM       LOGIN@  IDLE  JCPU  PCPU WHAT </span><br><span class="line">root   pts&#x2F;0  116.204.64.165  14:15  0.00s 0.06s 0.04s w </span><br><span class="line">root   pts&#x2F;1  116.204.64.165  14:15  2.00s 0.02s 0.02s –bash</span><br></pre></td></tr></table></figure><h4 id="2、查看当前自己占用终端"><a href="#2、查看当前自己占用终端" class="headerlink" title="2、查看当前自己占用终端"></a>2、查看当前自己占用终端</h4><p>查看自己占用终端，别把自己干掉了</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@apache ~]# who am i </span><br><span class="line">root   pts&#x2F;0    2013-01-16 14:15 (116.204.64.165)</span><br></pre></td></tr></table></figure><h4 id="3、通知该用户将要关闭他："><a href="#3、通知该用户将要关闭他：" class="headerlink" title="3、通知该用户将要关闭他："></a>3、通知该用户将要关闭他：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@apache ~]# echo &quot;I will close your connection&quot; &gt; &#x2F;dev&#x2F;pts&#x2F;2</span><br></pre></td></tr></table></figure><p>这样他的终端将显示该信息</p><p><img src="http://wangzhangtao.com/img/body/2.linux%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98-2/image-20200805110825280.png" alt="image-20200805110825280"></p><h4 id="4、用pkill-命令剔除对方"><a href="#4、用pkill-命令剔除对方" class="headerlink" title="4、用pkill 命令剔除对方"></a>4、用pkill 命令剔除对方</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@apache ~]# pkill -kill -t pts&#x2F;1</span><br></pre></td></tr></table></figure><h4 id="5、用w命令查看结果"><a href="#5、用w命令查看结果" class="headerlink" title="5、用w命令查看结果"></a>5、用w命令查看结果</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@apache ~]# w </span><br><span class="line">14:19:47 up 42 days, 1:00, 1 user, load average: 0.00, 0.00, 0.00 </span><br><span class="line">USER   TTY   FROM       LOGIN@  IDLE  JCPU  PCPU WHAT </span><br><span class="line">root   pts&#x2F;0  116.204.64.165  14:15  0.00s 0.03s 0.00s w</span><br></pre></td></tr></table></figure><h4 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h4><p>如果最后查看还是没有干掉，建议加上-9 强制杀死。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@apache ~]# pkill -9 -t pts&#x2F;1</span><br></pre></td></tr></table></figure><h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><ul><li><a href="https://www.cnblogs.com/my-jin/p/5484165.html" target="_blank" rel="noopener">关闭SSH其他用户会话连接</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>三、linux问题收集</title>
      <link href="/2020/07/08/%E4%B8%89%E3%80%81linux%E9%97%AE%E9%A2%98%E6%94%B6%E9%9B%86/"/>
      <url>/2020/07/08/%E4%B8%89%E3%80%81linux%E9%97%AE%E9%A2%98%E6%94%B6%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h2 id="zookeeper启动失败"><a href="#zookeeper启动失败" class="headerlink" title="zookeeper启动失败"></a>zookeeper启动失败</h2><p>zookeeper启动报错 <code>The current epoch, 1d, is older than the last zxid, 30064771074</code></p><p>启动Zookeeper时遇到如下问题：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Reading snapshot &#x2F;usr&#x2F;local&#x2F;zk&#x2F;data&#x2F;version-2&#x2F;snapshot.800000003</span><br><span class="line">2019-12-09 23:16:41,955 [myid:1] - ERROR [main:QuorumPeer@453] - Unable to load database on disk</span><br><span class="line">java.io.IOException: The current epoch, 1d, is older than the last zxid, 30064771074</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:435)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:409)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:156)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:116)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:79)</span><br><span class="line">2019-12-09 23:16:41,966 [myid:1] - ERROR [main:QuorumPeerMain@94] - Unexpected exception, exiting abnormally</span><br><span class="line">java.lang.RuntimeException: Unable to run quorum server </span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:454)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:409)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:156)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:116)</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:79)</span><br><span class="line">Caused by: java.io.IOException: The current epoch, 8, is older than the last zxid, 38654705667</span><br><span class="line">at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:435)</span><br><span class="line"></span><br><span class="line">... 4 more</span><br></pre></td></tr></table></figure><p>这里，意思是说当前的阶段比最新的阶段（zxid为30064771074）要老。</p><p><strong>换句话说，即<code>此节点上的Zookeeper</code>所处阶段与<code>当前ClouderaManager中Zookeeper</code>的阶段<code>不匹配</code>，导致无法启动此节点上面的<code>Zookeeper Quorum Server</code>。</strong></p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>既然版本不匹配，那么，我们选择删除的是问题节点上的版本数据，即删除/data/zookeeper/data/下的version-2文件夹下的所有东西：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo rm -r &#x2F;data&#x2F;zookeeper&#x2F;data&#x2F;version-2&#x2F;*</span><br></pre></td></tr></table></figure><h3 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h3><p>不要删除version-2文件夹，否则可能会报错no space left on device。<br>  如果误删，请参考<a href="https://blog.csdn.net/pre_tender/article/details/103431258" target="_blank" rel="noopener">ZooKeeper报错：Unable to access datadir, exiting abnormally</a><br>不要删除myid。<br>  如果误删，请先删除此Slave2的Zookeeper server，然后将/data/zookeeper/data/目录删除，再重新给Slave2添加Zookeeper Server</p><hr><p>原文链接：pre_tender博主的 <a href="https://blog.csdn.net/pre_tender/article/details/103431145" target="_blank" rel="noopener">Zookeeper启动报错：The current epoch…</a></p><h2 id="Jenkins任务结束时-自动关闭衍生的子进程"><a href="#Jenkins任务结束时-自动关闭衍生的子进程" class="headerlink" title="Jenkins任务结束时,自动关闭衍生的子进程"></a>Jenkins任务结束时,自动关闭衍生的子进程</h2><p>在jenkins执行任务时，发现任务结束,jenkins自动关闭衍生的子进程，需要设置变量  JENKINS_NODE_COOKIE=dontkillme</p><h2 id="下载centos镜像"><a href="#下载centos镜像" class="headerlink" title="下载centos镜像"></a>下载centos镜像</h2><p><a href="https://hub.docker.com/_/centos" target="_blank" rel="noopener">centos docker-hub</a> </p><p>选择自己的版本,我这里选择的是7.6.1810</p><p>docker pull centos:7.6.1810</p><p>docker run -it –rm –name centos7 –rm centos:7.6.1810 /bin/bash</p><h2 id="centos7安装中文宋体"><a href="#centos7安装中文宋体" class="headerlink" title="centos7安装中文宋体"></a>centos7安装中文宋体</h2><p><strong>安装字体插件</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y fontconfig mkfontscale</span><br></pre></td></tr></table></figure><p><strong>刷新字体</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">fc-cache -fv</span><br></pre></td></tr></table></figure><p><strong>查看字体</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@iZ250234rf8Z TrueType]# fc-list |grep SimSun</span><br><span class="line">&#x2F;usr&#x2F;share&#x2F;fonts&#x2F;chinese&#x2F;TrueType&#x2F;simsun.ttc: SimSun,宋体:style&#x3D;Regular,常规</span><br><span class="line">&#x2F;usr&#x2F;share&#x2F;fonts&#x2F;chinese&#x2F;TrueType&#x2F;simsun.ttc: NSimSun,新宋体:style&#x3D;Regular,常规</span><br></pre></td></tr></table></figure><p>字体默认文件夹 /usr/share/fonts</p><h3 id="关闭SSH其他用户会话连接"><a href="#关闭SSH其他用户会话连接" class="headerlink" title="关闭SSH其他用户会话连接"></a>关闭SSH其他用户会话连接</h3><p>在一些生产平台往往看到一大堆的用户SSH连接到同一台服务器，或者连接后没有正常关闭进程还驻留在系统内。限制SSH连接数与手动断开空闲连接也有必要之举，这里写出手动剔出其他用户的过程。</p><h4 id="1-查看系统在线用户"><a href="#1-查看系统在线用户" class="headerlink" title="1. 查看系统在线用户"></a>1. 查看系统在线用户</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@apache ~]# w </span><br><span class="line">14:15:41 up 42 days, 56 min, 2 users, load average: 0.07, 0.02, 0.00 </span><br><span class="line">USER   TTY   FROM       LOGIN@  IDLE  JCPU  PCPU WHAT </span><br><span class="line">root   pts&#x2F;0  116.204.64.165  14:15  0.00s 0.06s 0.04s w </span><br><span class="line">root   pts&#x2F;1  116.204.64.165  14:15  2.00s 0.02s 0.02s –bash</span><br></pre></td></tr></table></figure><h4 id="2、查看当前自己占用终端"><a href="#2、查看当前自己占用终端" class="headerlink" title="2、查看当前自己占用终端"></a>2、查看当前自己占用终端</h4><p>查看自己占用终端，别把自己干掉了</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@apache ~]# who am i </span><br><span class="line">root   pts&#x2F;0    2013-01-16 14:15 (116.204.64.165)</span><br></pre></td></tr></table></figure><h4 id="3、通知该用户将要关闭他："><a href="#3、通知该用户将要关闭他：" class="headerlink" title="3、通知该用户将要关闭他："></a>3、通知该用户将要关闭他：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@apache ~]# echo &quot;I will close your connection&quot; &gt; &#x2F;dev&#x2F;pts&#x2F;2</span><br></pre></td></tr></table></figure><p>这样他的终端将显示该信息</p><p><img src="http://wangzhangtao.com/img/body/2.linux%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98-2/image-20200805110825280.png" alt="image-20200805110825280"></p><h4 id="4、用pkill-命令剔除对方"><a href="#4、用pkill-命令剔除对方" class="headerlink" title="4、用pkill 命令剔除对方"></a>4、用pkill 命令剔除对方</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@apache ~]# pkill -kill -t pts&#x2F;1</span><br></pre></td></tr></table></figure><h4 id="5、用w命令查看结果"><a href="#5、用w命令查看结果" class="headerlink" title="5、用w命令查看结果"></a>5、用w命令查看结果</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@apache ~]# w </span><br><span class="line">14:19:47 up 42 days, 1:00, 1 user, load average: 0.00, 0.00, 0.00 </span><br><span class="line">USER   TTY   FROM       LOGIN@  IDLE  JCPU  PCPU WHAT </span><br><span class="line">root   pts&#x2F;0  116.204.64.165  14:15  0.00s 0.03s 0.00s w</span><br></pre></td></tr></table></figure><h4 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h4><p>如果最后查看还是没有干掉，建议加上-9 强制杀死。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@apache ~]# pkill -9 -t pts&#x2F;1</span><br></pre></td></tr></table></figure><h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><ul><li><a href="https://www.cnblogs.com/my-jin/p/5484165.html" target="_blank" rel="noopener">关闭SSH其他用户会话连接</a></li></ul><h3 id="nginx报错could-not-build-optimal-server-names-hash"><a href="#nginx报错could-not-build-optimal-server-names-hash" class="headerlink" title="nginx报错could not build optimal server_names_hash"></a>nginx报错could not build optimal server_names_hash</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@nginx-wiki vhosts]# nginx -s reload</span><br><span class="line">nginx: [warn] could not build optimal server_names_hash, you should increase either server_names_hash_max_size: 512 or server_names_hash_bucket_size: 64; ignoring server_names_hash_bucket_size</span><br></pre></td></tr></table></figure><p>解决方法是在 nginx 配置文件的 http 段中增加如下配置：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    ...</span><br><span class="line">    server_names_hash_max_size 512;</span><br><span class="line">    server_names_hash_bucket_size 128;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>一、生态-etcd数据库备份和恢复</title>
      <link href="/2020/07/01/%E4%B8%80%E3%80%81%E7%94%9F%E6%80%81-etcd%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E5%92%8C%E6%81%A2%E5%A4%8D/"/>
      <url>/2020/07/01/%E4%B8%80%E3%80%81%E7%94%9F%E6%80%81-etcd%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E5%92%8C%E6%81%A2%E5%A4%8D/</url>
      
        <content type="html"><![CDATA[<h2 id="对于etcd-api-v3数据备份与恢复方法"><a href="#对于etcd-api-v3数据备份与恢复方法" class="headerlink" title="对于etcd api v3数据备份与恢复方法"></a>对于etcd api v3数据备份与恢复方法</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export ETCDCTL_API&#x3D;3</span><br><span class="line">ETCDCTL_API&#x3D;3 etcdctl --endpoints localhost:2379 snapshot save snapshot.db （单机备份）</span><br><span class="line">etcdctl --cert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.crt --key&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;server.key --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;ca.crt snapshot save k8s-snapshot.db （kubeadm单机备份）</span><br><span class="line">etcdctl --endpoints&#x3D;&quot;https:&#x2F;&#x2F;192.168.70.21:2379,https:&#x2F;&#x2F;192.168.70.22:2379,https:&#x2F;&#x2F;192.168.70.23:2379&quot; --cert&#x3D;&#x2F;opt&#x2F;etcd&#x2F;certs&#x2F;etcd-peer.pem --key&#x3D;&#x2F;opt&#x2F;etcd&#x2F;certs&#x2F;etcd-peer-key.pem --cacert&#x3D;&#x2F;opt&#x2F;etcd&#x2F;certs&#x2F;ca.pem  snapshot save mysnapshot.db （集群备份）  </span><br><span class="line"></span><br><span class="line"> # etcdctl snapshot restore snapshot.db --name m3 --data-dir&#x3D;&#x2F;home&#x2F;etcd_data （还原）</span><br></pre></td></tr></table></figure><blockquote><p>恢复后的文件需要修改权限为 etcd:etcd<br> –name:重新指定一个数据目录，可以不指定，默认为 default.etcd<br> –data-dir：指定数据目录<br> 建议使用时不指定 name 但指定 data-dir，并将 data-dir 对应于 etcd 服务中配置的 data-dir</p></blockquote><h2 id="实践方法"><a href="#实践方法" class="headerlink" title="实践方法"></a>实践方法</h2><h3 id="单机备份"><a href="#单机备份" class="headerlink" title="单机备份"></a><strong>单机备份</strong></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# etcdctl --endpoints 127.0.0.1:2379 snapshot save snashot.db</span><br><span class="line">Snapshot saved at snashot.db</span><br><span class="line">[root@k8s-master1 ~]# ll</span><br><span class="line">-rw-r--r--   1 root root 3756064 Apr 18 10:38 snashot.db</span><br><span class="line">[root@k8s-master1 ~]#</span><br></pre></td></tr></table></figure><h3 id="集群备份"><a href="#集群备份" class="headerlink" title="集群备份"></a><strong>集群备份</strong></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# etcdctl --endpoints&#x3D;&quot;https:&#x2F;&#x2F;192.168.32.129:2379,https:&#x2F;&#x2F;192.168.32.130:2379,192.168.32.128:2379&quot; --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;cert&#x2F;ca.pem --key&#x3D;&#x2F;etc&#x2F;etcd&#x2F;cert&#x2F;etcd-key.pem --cert&#x3D;&#x2F;etc&#x2F;etcd&#x2F;cert&#x2F;etcd.pem  snapshot save snashot.db</span><br><span class="line">Snapshot saved at snashot.db</span><br><span class="line"></span><br><span class="line">[root@k8s-master1 ~]# ll</span><br><span class="line">-rw-r--r--   1 root root 3756064 Apr 18 10:53 snashot.db</span><br></pre></td></tr></table></figure><h3 id="数据恢复"><a href="#数据恢复" class="headerlink" title="数据恢复"></a><strong>数据恢复</strong></h3><p>做下面的操作,请慎重,有可能造成集群崩溃数据丢失.请在实验环境测试.</p><p>执行命令:systemctl stop etcd<br> 所有节点的etcd服务全部停止.</p><p>执行命令:rm -rf /var/lib/etcd/<br> 所有节点删除etcd的数据</p><h3 id="恢复v3的数据"><a href="#恢复v3的数据" class="headerlink" title="恢复v3的数据"></a><strong>恢复v3的数据</strong></h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]#  etcdctl --name&#x3D;k8s-master1 --endpoints&#x3D;&quot;https:&#x2F;&#x2F;192.168.32.128:2379&quot; --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;cert&#x2F;ca.pem --key&#x3D;&#x2F;etc&#x2F;etcd&#x2F;cert&#x2F;etcd-key.pem --cert&#x3D;&#x2F;etc&#x2F;etcd&#x2F;cert&#x2F;etcd.pem --initial-cluster-token&#x3D;etcd-cluster-1 --initial-advertise-peer-urls&#x3D;https:&#x2F;&#x2F;192.168.32.128:2380 --initial-cluster&#x3D;k8s-master1&#x3D;https:&#x2F;&#x2F;192.168.32.128:2380,k8s-master2&#x3D;https:&#x2F;&#x2F;192.168.32.129:2380,k8s-master3&#x3D;https:&#x2F;&#x2F;192.168.32.130:2380 --data-dir&#x3D;&#x2F;var&#x2F;lib&#x2F;etcd snapshot restore snashot1.db</span><br><span class="line">2019-04-18 13:43:42.570882 I | mvcc: restore compact to 148651</span><br><span class="line">2019-04-18 13:43:42.584194 I | etcdserver&#x2F;membership: added member 4c99f52323a3e391 [https:&#x2F;&#x2F;192.168.32.129:2380] to cluster 2a0978507970d828</span><br><span class="line">2019-04-18 13:43:42.584224 I | etcdserver&#x2F;membership: added member 5a74b01f28ece933 [https:&#x2F;&#x2F;192.168.32.128:2380] to cluster 2a0978507970d828</span><br><span class="line">2019-04-18 13:43:42.584234 I | etcdserver&#x2F;membership: added member b29b94ace458096d [https:&#x2F;&#x2F;192.168.32.130:2380] to cluster 2a0978507970d828</span><br><span class="line"></span><br><span class="line">[root@k8s-master2 ~]# etcdctl --name&#x3D;k8s-master2 --endpoints&#x3D;&quot;https:&#x2F;&#x2F;192.168.32.129:2379&quot; --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;cert&#x2F;ca.pem --key&#x3D;&#x2F;etc&#x2F;etcd&#x2F;cert&#x2F;etcd-key.pem --cert&#x3D;&#x2F;etc&#x2F;etcd&#x2F;cert&#x2F;etcd.pem --initial-cluster-token&#x3D;etcd-cluster-1 --initial-advertise-peer-urls&#x3D;https:&#x2F;&#x2F;192.168.32.129:2380 --initial-cluster&#x3D;k8s-master1&#x3D;https:&#x2F;&#x2F;192.168.32.128:2380,k8s-master2&#x3D;https:&#x2F;&#x2F;192.168.32.129:2380,k8s-master3&#x3D;https:&#x2F;&#x2F;192.168.32.130:2380 --data-dir&#x3D;&#x2F;var&#x2F;lib&#x2F;etcd snapshot restore snashot1.db</span><br><span class="line">2019-04-18 13:43:56.313096 I | mvcc: restore compact to 148651</span><br><span class="line">2019-04-18 13:43:56.324779 I | etcdserver&#x2F;membership: added member 4c99f52323a3e391 [https:&#x2F;&#x2F;192.168.32.129:2380] to cluster 2a0978507970d828</span><br><span class="line">2019-04-18 13:43:56.324806 I | etcdserver&#x2F;membership: added member 5a74b01f28ece933 [https:&#x2F;&#x2F;192.168.32.128:2380] to cluster 2a0978507970d828</span><br><span class="line">2019-04-18 13:43:56.324819 I | etcdserver&#x2F;membership: added member b29b94ace458096d [https:&#x2F;&#x2F;192.168.32.130:2380] to cluster 2a0978507970d828</span><br><span class="line"></span><br><span class="line">[root@k8s-master3 ~]# etcdctl --name&#x3D;k8s-master3 --endpoints&#x3D;&quot;https:&#x2F;&#x2F;192.168.32.130:2379&quot; --cacert&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;cert&#x2F;ca.pem --key&#x3D;&#x2F;etc&#x2F;etcd&#x2F;cert&#x2F;etcd-key.pem --cert&#x3D;&#x2F;etc&#x2F;etcd&#x2F;cert&#x2F;etcd.pem --initial-cluster-token&#x3D;etcd-cluster-1 --initial-advertise-peer-urls&#x3D;https:&#x2F;&#x2F;192.168.32.130:2380 --initial-cluster&#x3D;k8s-master1&#x3D;https:&#x2F;&#x2F;192.168.32.128:2380,k8s-master2&#x3D;https:&#x2F;&#x2F;192.168.32.129:2380,k8s-master3&#x3D;https:&#x2F;&#x2F;192.168.32.130:2380 --data-dir&#x3D;&#x2F;var&#x2F;lib&#x2F;etcd snapshot restore snashot1.db</span><br><span class="line">2019-04-18 13:44:10.643115 I | mvcc: restore compact to 148651</span><br><span class="line">2019-04-18 13:44:10.649920 I | etcdserver&#x2F;membership: added member 4c99f52323a3e391 [https:&#x2F;&#x2F;192.168.32.129:2380] to cluster 2a0978507970d828</span><br><span class="line">2019-04-18 13:44:10.649957 I | etcdserver&#x2F;membership: added member 5a74b01f28ece933 [https:&#x2F;&#x2F;192.168.32.128:2380] to cluster 2a0978507970d828</span><br><span class="line">2019-04-18 13:44:10.649973 I | etcdserver&#x2F;membership: added member b29b94ace458096d [https:&#x2F;&#x2F;192.168.32.130:2380] to cluster 2a0978507970d828</span><br></pre></td></tr></table></figure><p>服务起不来</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# tail -n 30 &#x2F;var&#x2F;log&#x2F;messages</span><br><span class="line">Apr 18 13:46:41 k8s-master1 systemd: Starting Etcd Server...</span><br><span class="line">Apr 18 13:46:41 k8s-master1 etcd: etcd Version: 3.3.7</span><br><span class="line">Apr 18 13:46:41 k8s-master1 etcd: Git SHA: 56536de55</span><br><span class="line">Apr 18 13:46:41 k8s-master1 etcd: Go Version: go1.9.6</span><br><span class="line">Apr 18 13:46:41 k8s-master1 etcd: Go OS&#x2F;Arch: linux&#x2F;amd64</span><br><span class="line">Apr 18 13:46:41 k8s-master1 etcd: setting maximum number of CPUs to 1, total number of available CPUs is 1</span><br><span class="line">Apr 18 13:46:41 k8s-master1 etcd: error listing data dir: &#x2F;var&#x2F;lib&#x2F;etcd</span><br><span class="line">Apr 18 13:46:41 k8s-master1 systemd: etcd.service: main process exited, code&#x3D;exited, status&#x3D;1&#x2F;FAILURE</span><br><span class="line">Apr 18 13:46:41 k8s-master1 systemd: Failed to start Etcd Server.</span><br><span class="line">Apr 18 13:46:41 k8s-master1 systemd: Unit etcd.service entered failed state.</span><br><span class="line">Apr 18 13:46:41 k8s-master1 systemd: etcd.service failed.</span><br><span class="line">Apr 18 13:46:41 k8s-master1 flanneld: timed out</span><br><span class="line">Apr 18 13:46:41 k8s-master1 flanneld: E0418 13:46:41.858283   63943 main.go:349] Couldn&#39;t fetch network config: client: etcd cluster is unavailable or misconfigured; error #0: EOF</span><br><span class="line">Apr 18 13:46:41 k8s-master1 flanneld: ; error #1: EOF</span><br><span class="line">Apr 18 13:46:41 k8s-master1 flanneld: ; error #2: EOF</span><br></pre></td></tr></table></figure><p>修改数据目录权限,默认是root:root<br> chown -R etcd:etcd /var/lib/etcd<br> 恢复正常.</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@k8s-master1 ~]# etcdctl member list</span><br><span class="line">4c99f52323a3e391, started, k8s-master2, https:&#x2F;&#x2F;192.168.32.129:2380, https:&#x2F;&#x2F;192.168.32.129:2379</span><br><span class="line">5a74b01f28ece933, started, k8s-master1, https:&#x2F;&#x2F;192.168.32.128:2380, https:&#x2F;&#x2F;192.168.32.128:2379</span><br><span class="line">b29b94ace458096d, started, k8s-master3, https:&#x2F;&#x2F;192.168.32.130:2380, https:&#x2F;&#x2F;192.168.32.130:2379</span><br></pre></td></tr></table></figure><p>可以看到v3的数据恢复成功.</p><h2 id="参考地址"><a href="#参考地址" class="headerlink" title="参考地址"></a>参考地址</h2><p><a href="https://www.cnblogs.com/xiaoyaojinzhazhadehangcheng/p/11605941.html" target="_blank" rel="noopener">https://www.cnblogs.com/xiaoyaojinzhazhadehangcheng/p/11605941.html</a></p><p><a href="https://blog.csdn.net/liukuan73/article/details/78986652" target="_blank" rel="noopener">https://blog.csdn.net/liukuan73/article/details/78986652</a></p><p><a href="https://blog.51cto.com/goome/2380854" target="_blank" rel="noopener">https://blog.51cto.com/goome/2380854</a></p><p><a href="https://yq.aliyun.com/articles/336781" target="_blank" rel="noopener">https://yq.aliyun.com/articles/336781</a></p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 知识点 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 知识点 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>about</title>
      <link href="/2020/07/01/about/"/>
      <url>/2020/07/01/about/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><p>邮箱：<a href="mailto:2358124488@qq.com">2358124488@qq.com</a><br>QQ: 2358124488</p><h2 id="求职意向"><a href="#求职意向" class="headerlink" title="求职意向"></a>求职意向</h2><p>求职岗位：Linux运维工程师和Kubernetes运维工程师     </p><p>工作地点：北京  在职</p><h2 id="专业技能"><a href="#专业技能" class="headerlink" title="专业技能"></a>专业技能</h2><ol><li><p>操作系统：精通Linux操作系统；</p></li><li><p>容器：熟练使用kubernetes部署系统，包括elkf、prometheus、zookeeper集群等各种依赖服务；熟练使用docker；</p></li><li><p>平台：熟练阿里云云平台的使用，比如ECS、OSS、SLB、访问控制等；</p></li><li><p>语言：熟练使用shell，Python进行编程，掌握grep、sed、awk等系统命令；</p></li><li><p>数据库：熟练使用Mysql、redis数据库；</p></li><li><p>网络安全：熟练使用sql审核平台、jumpserver、openvpn安装和使用；</p></li><li><p>批量管理：熟练ansible命令进行批量服务器的管理；</p></li><li><p>框架：熟练使用flask，django开源框架,并以框架为基础，做二次开发，方便运维工作；</p></li><li><p>集群：熟练Keepalived+lvs、LNMP集群，gitlab主从同步；</p></li><li><p>服务器搭建：熟练Linux服务器下的搭建(DNS、DHCP、FTP、PXE，nginx，jenkins等)；</p></li><li><p>监控：熟练使用Zabbix，prometheus监控软件使用；</p></li><li><p>硬件：熟练vmware esxi，vsphere-clients搭建和使用。</p></li></ol><h2 id="项目经验"><a href="#项目经验" class="headerlink" title="项目经验"></a>项目经验</h2><h3 id="项目：二进制kubernetes高可用集群搭建"><a href="#项目：二进制kubernetes高可用集群搭建" class="headerlink" title="项目：二进制kubernetes高可用集群搭建"></a>项目：二进制kubernetes高可用集群搭建</h3><h4 id="项目描述："><a href="#项目描述：" class="headerlink" title="项目描述："></a>项目描述：</h4><p> 我们和阿里合作，开发了一套家装项目，使用k8s做基础服务。k8s集群具有实现容器的自动化部署、自动扩缩容、维护等功能，解决了环境依赖问题。</p><h4 id="项目技术"><a href="#项目技术" class="headerlink" title="项目技术"></a>项目技术</h4><ol><li><p>docker的使用和dockerfile文件编写，harbor集群搭建；</p></li><li><p>二进制k8s高可用集群安装和使用；</p></li><li><p>log-polit+kafka+logstash+es+kibana日志收集，promethues+grafana+alertmanger监控，zookeeper集群等；</p></li><li><p>helm使用发布项目，项目jar包通过initContainers注入到基础镜像，不同项目的差异化数据存储在数据库中。</p></li></ol><h4 id="项目收获："><a href="#项目收获：" class="headerlink" title="项目收获："></a>项目收获：</h4><ol><li><p>对自动化发布和容器技术有了更深入的了解；</p></li><li><p>实现kubernetes集群的自动化部署、扩缩容、维护等功能。减少了环境依赖，增强了系统的稳定性；</p></li><li><p>新技术的学习方法，要追本溯源。</p></li></ol><h3 id="项目：服务从阿里云经典网络迁移到VPC私有网络"><a href="#项目：服务从阿里云经典网络迁移到VPC私有网络" class="headerlink" title="项目：服务从阿里云经典网络迁移到VPC私有网络"></a>项目：服务从阿里云经典网络迁移到VPC私有网络</h3><h4 id="项目描述：-1"><a href="#项目描述：-1" class="headerlink" title="项目描述："></a>项目描述：</h4><p>由于阿里云VPC网络安全、可控、易用、可扩展，根据阿里云推荐，我们要将阿里云全部ECS主机从经典网络迁移到VPC网络。(项目环境：阿里云全部ECS主机)</p><h4 id="项目技术："><a href="#项目技术：" class="headerlink" title="项目技术："></a>项目技术：</h4><ol><li><p>VPC网络架构设计和阿里云的使用；</p></li><li><p>rabbitmq集群，gitlab主从集群，redis主从集群，zookeeper高可用，glusterfs集群等；</p></li><li><p>基于dubbo调用的MVC服务架构。</p></li></ol><h4 id="项目收获：-1"><a href="#项目收获：-1" class="headerlink" title="项目收获："></a>项目收获：</h4><ol><li><p>通过对阿里云环境的重新构建，让我对阿里云的使用更加熟练；</p></li><li><p>通过对各个服务的的迁移，对各个服务器上的服务有了清楚地了解；</p></li><li><p>通过线下环境的迁移模拟，对各个软件有了更深的认识；</p></li><li><p>通过迁移计划的设计和总结，增加了我的规划和文档能力。</p></li></ol><h2 id="工作经历"><a href="#工作经历" class="headerlink" title="工作经历"></a>工作经历</h2><h3 id="工作单位：某保险互联网科技有限公司"><a href="#工作单位：某保险互联网科技有限公司" class="headerlink" title="工作单位：某保险互联网科技有限公司"></a>工作单位：某保险互联网科技有限公司</h3><h4 id="工作时间："><a href="#工作时间：" class="headerlink" title="工作时间："></a>工作时间：</h4><p>2015.9 - 2018.4</p><h4 id="担任职位："><a href="#担任职位：" class="headerlink" title="担任职位："></a>担任职位：</h4><p>运维工程师</p><h4 id="工作描述"><a href="#工作描述" class="headerlink" title="工作描述"></a>工作描述</h4><ol><li><p>配合测试和开发人员，解决生产中遇到的问题；</p></li><li><p>对官网架构进行优化，实现gitlab主从同步；</p></li><li><p>搭建Linux服务器和dns服务，为测试准备好服务器；</p></li><li><p>Postgresql、mysql等数据库权限的管理和业务的数据提取；</p></li><li><p>对openVPN私有通道的开发、管理和维护；</p></li><li><p>使用Python开发运维管理平台，减轻运维工作量，减少失误；</p></li></ol><h3 id="工作单位：某家装科技有限公司"><a href="#工作单位：某家装科技有限公司" class="headerlink" title="工作单位：某家装科技有限公司"></a>工作单位：某家装科技有限公司</h3><h4 id="工作时间：-1"><a href="#工作时间：-1" class="headerlink" title="工作时间："></a>工作时间：</h4><p>2018.6 - 至今</p><h4 id="担任职位：-1"><a href="#担任职位：-1" class="headerlink" title="担任职位："></a>担任职位：</h4><p>系统运维工程师</p><h4 id="工作描述："><a href="#工作描述：" class="headerlink" title="工作描述："></a>工作描述：</h4><ol><li><p>负责业务系统的优化及监控，提升业务连续性及可用性；</p></li><li><p>开发自动化运维平台、提高运维效率，规范操作流程、减少重复性工作；</p></li><li><p>负责相关系统环境搭建、数据备份、日志收集、系统监控、故障排查等；</p></li><li><p>负责系统故障的应急响应，快速定位，并结合系统运行状态进行优化。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> live </category>
          
          <category> 自我介绍 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生活 </tag>
            
            <tag> 自我介绍 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一、前端经验收集</title>
      <link href="/2020/07/01/%E4%B8%80%E3%80%81%E5%89%8D%E7%AB%AF%E7%BB%8F%E9%AA%8C%E6%94%B6%E9%9B%86/"/>
      <url>/2020/07/01/%E4%B8%80%E3%80%81%E5%89%8D%E7%AB%AF%E7%BB%8F%E9%AA%8C%E6%94%B6%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h2 id="前端经验收集"><a href="#前端经验收集" class="headerlink" title="前端经验收集"></a>前端经验收集</h2><h3 id="问题1-npm-run-build时报错：Failed-to-launch-chrome"><a href="#问题1-npm-run-build时报错：Failed-to-launch-chrome" class="headerlink" title="问题1: npm run build时报错：Failed to launch chrome"></a>问题1: npm run build时报错：Failed to launch chrome</h3><p>​    在mac上npm run build成功，但是在centos7上构建却失败，报错日志如下，为什么会这样？</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 wang-web]# npm run build</span><br><span class="line"></span><br><span class="line">&gt; wang@0.1.0 build &#x2F;data&#x2F;app&#x2F;wang-web</span><br><span class="line">&gt; icejs build</span><br><span class="line"></span><br><span class="line">ice.js 1.6.0</span><br><span class="line">                                         98% after emittingError: Failed to launch chrome!</span><br><span class="line">&#x2F;data&#x2F;app&#x2F;wang-web&#x2F;node_modules&#x2F;_puppeteer@1.20.0@puppeteer&#x2F;.local-chromium&#x2F;linux-686378&#x2F;chrome-linux&#x2F;chrome: error while loading shared libraries: libXss.so.1: cannot open shared object file: No such file or directory</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TROUBLESHOOTING: https:&#x2F;&#x2F;github.com&#x2F;GoogleChrome&#x2F;puppeteer&#x2F;blob&#x2F;master&#x2F;docs&#x2F;troubleshooting.md</span><br><span class="line"></span><br><span class="line">    at onClose (&#x2F;data&#x2F;app&#x2F;wang-web&#x2F;node_modules&#x2F;_puppeteer@1.20.0@puppeteer&#x2F;lib&#x2F;Launcher.js:348:14)</span><br><span class="line">    at Interface.&lt;anonymous&gt; (&#x2F;data&#x2F;app&#x2F;wang-web&#x2F;node_modules&#x2F;_puppeteer@1.20.0@puppeteer&#x2F;lib&#x2F;Launcher.js:337:50)</span><br><span class="line">    at Interface.emit (events.js:322:22)</span><br><span class="line">    at Interface.close (readline.js:409:8)</span><br><span class="line">    at Socket.onend (readline.js:187:10)</span><br><span class="line">    at Socket.emit (events.js:322:22)</span><br><span class="line">    at endReadableNT (_stream_readable.js:1187:12)</span><br><span class="line">    at processTicksAndRejections (internal&#x2F;process&#x2F;task_queues.js:84:21)</span><br><span class="line">[Prerenderer - PuppeteerRenderer] Unable to start Puppeteer</span><br><span class="line">(node:1688) UnhandledPromiseRejectionWarning: TypeError: Cannot read property &#39;close&#39; of null</span><br><span class="line">    at PuppeteerRenderer.destroy (&#x2F;data&#x2F;app&#x2F;wang-web&#x2F;node_modules&#x2F;_@prerenderer_renderer-puppeteer@0.2.0@@prerenderer&#x2F;renderer-puppeteer&#x2F;es6&#x2F;renderer.js:140:21)</span><br><span class="line">    at Prerenderer.destroy (&#x2F;data&#x2F;app&#x2F;wang-web&#x2F;node_modules&#x2F;_@prerenderer_prerenderer@0.7.2@@prerenderer&#x2F;prerenderer&#x2F;es6&#x2F;index.js:87:20)</span><br><span class="line">    at &#x2F;data&#x2F;app&#x2F;wang-web&#x2F;node_modules&#x2F;_prerender-spa-plugin@3.4.0@prerender-spa-plugin&#x2F;es6&#x2F;index.js:144:29</span><br><span class="line">    at processTicksAndRejections (internal&#x2F;process&#x2F;task_queues.js:97:5)</span><br><span class="line">(node:1688) UnhandledPromiseRejectionWarning: Unhandled promise rejection. This error originated either by throwing inside of an async function without a catch block, or by rejecting a promise which was not handled with .catch(). To terminate the node process on unhandled promise rejection, use the CLI flag &#96;--unhandled-rejections&#x3D;strict&#96; (see https:&#x2F;&#x2F;nodejs.org&#x2F;api&#x2F;cli.html#cli_unhandled_rejections_mode). (rejection id: 1)</span><br><span class="line">(node:1688) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.</span><br></pre></td></tr></table></figure><p><strong>解决方案：</strong></p><p>因为你缺少依赖包。</p><p>在centos上的问题，puppeteer含有两个包puppeteer VS  puppeteer-core,什么区别呢？简单来说就是puppeteer = api+chromeium（最新），puppeteer-core = only Api,官方是这么说的：</p><blockquote><p><code>puppeteer-core</code> 与 <code>puppeteer</code> 不同的地方：</p><ul><li><code>puppeteer-core</code> 在安装时不会自动下载 Chromium。</li><li><code>puppeteer-core</code>忽略所有的 <code>PUPPETEER_*</code> env 变量.</li></ul></blockquote><p>当你自己安装chromeium的时候，在启动（puppeteer.launch()）的时候需要指定chromeium的启动路径，当然自己安装存在更新的问题，可能由于更新不及时导致与puppeteer的api不匹配的情况，所以我们推荐直接安装puppeteer。然后呢，当你在centos上安装的时候毫无疑问的会报错。因为缺少chromeium启动的依赖，本人在centos6和7上都遇到了坑，下面是官方列举的centos上的所需依赖：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install pango.x86_64 libXcomposite.x86_64 libXcursor.x86_64 libXdamage.x86_64 libXext.x86_64 libXi.x86_64 libXtst.x86_64 cups-libs.x86_64 libXScrnSaver.x86_64 libXrandr.x86_64 GConf2.x86_64 alsa-lib.x86_64 atk.x86_64 gtk3.x86_64 ipa-gothic-fonts xorg-x11-fonts-100dpi xorg-x11-fonts-75dpi xorg-x11-utils xorg-x11-fonts-cyrillic xorg-x11-fonts-Type1 xorg-x11-fonts-misc</span><br></pre></td></tr></table></figure><p>当你启动chromeium时（推荐在服务器上测试的时候先进入到node_modules/puppeteer/.local-chromium/linux-496140/chrome-linux/chrome目录下执行./chrome进行测试，能成功运行代表通过puppeteer的api调用也能成功）你可能会遇到例如：</p><blockquote><p>error while loading shared libraries: libpangocairo-1.0.so.0: cannot open shared object file: No such file or directory 这样的错误，提示你缺少的是一个so文件，你可以在执行文件目录下执行这个命令进行查看缺少哪些依赖： ldd chrome | grep not 命令执行后会显示你当前缺少的依赖，当你不知道对应的.so在哪个包中的时候执行这个命令： yum provides | grep xx.so.0</p></blockquote><p>找到对应的包进行yum install 安装，注意32还是64位的，不要装错。    </p><p>如果你启动也有问题，可以看看参考文档，看看和你的报错一样吗。</p><h4 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h4><ul><li>一个懒癌程序员 <a href="https://zhangzippo.github.io/posts/2019/04/25/_25puppteererror.html" target="_blank" rel="noopener">puppeteer填坑指南</a></li></ul><h3 id="问题-gyp-ERR-stack-Error-EACCES-permission-denied-mkdir"><a href="#问题-gyp-ERR-stack-Error-EACCES-permission-denied-mkdir" class="headerlink" title="问题:  gyp ERR! stack Error: EACCES: permission denied, mkdir"></a>问题:  gyp ERR! stack Error: EACCES: permission denied, mkdir</h3><p>解决方案：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo npm i --unsafe-perm</span><br></pre></td></tr></table></figure><p><strong>原因：权限问题</strong></p><p>就是说 npm 出于安全考虑不支持以 root 用户运行，即使你用 root 用户身份运行了，npm 会自动转成一个叫 nobody  的用户来运行，而这个用户几乎没有任何权限。这样的话如果你脚本里有一些需要权限的操作，比如写文件（尤其是写  /root/.node-gyp），就会崩掉了。</p><p>为了避免这种情况，要么按照 npm 的规矩来，专门建一个用于运行 npm 的高权限用户；要么加 –unsafe-perm 参数，这样就不会切换到 nobody 上，运行时是哪个用户就是哪个用户，即是 root。</p><h4 id="参考文档-1"><a href="#参考文档-1" class="headerlink" title="参考文档"></a>参考文档</h4><ul><li><a href="https://blog.csdn.net/qq_31325079/article/details/102565223" target="_blank" rel="noopener">gyp ERR! stack Error: EACCES: permission denied, mkdir问题解决方案</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
          <category> 前端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 其他 </tag>
            
            <tag> 前端 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二、shell经验收集</title>
      <link href="/2020/06/30/%E4%BA%8C%E3%80%81shell%E7%BB%8F%E9%AA%8C%E6%94%B6%E9%9B%86/"/>
      <url>/2020/06/30/%E4%BA%8C%E3%80%81shell%E7%BB%8F%E9%AA%8C%E6%94%B6%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<p>查询IP地址归属地 <u><a href="https://www.ipip.net/ip.html" target="_blank" rel="noopener">https://www.ipip.net/ip.html</a></u></p><h2 id="镜像快照"><a href="#镜像快照" class="headerlink" title="镜像快照"></a>镜像快照</h2><h3 id="阿里云镜像快照"><a href="#阿里云镜像快照" class="headerlink" title="阿里云镜像快照"></a>阿里云镜像快照</h3><p><a href="https://developer.aliyun.com/mirror/" target="_blank" rel="noopener">https://developer.aliyun.com/mirror/</a></p><h3 id="清华大学镜像快照"><a href="#清华大学镜像快照" class="headerlink" title="清华大学镜像快照"></a>清华大学镜像快照</h3><p><a href="https://mirrors.tuna.tsinghua.edu.cn/help/AOSP/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/help/AOSP/</a></p><h3 id="mvn仓库及镜像"><a href="#mvn仓库及镜像" class="headerlink" title="mvn仓库及镜像"></a>mvn仓库及镜像</h3><p>代理仓库<code>maven-central</code>的默认源地址为中央仓库<code>https://repo1.maven.org/maven2</code></p><p>阿里的源地址：<code>http://maven.aliyun.com/nexus/content/groups/public</code></p><h2 id="Shell经验收集"><a href="#Shell经验收集" class="headerlink" title="Shell经验收集"></a>Shell经验收集</h2><h3 id="vimdiff-的颜色主题导致背景色看不清"><a href="#vimdiff-的颜色主题导致背景色看不清" class="headerlink" title="vimdiff 的颜色主题导致背景色看不清"></a>vimdiff 的颜色主题导致背景色看不清</h3><p>在 ~/.vimrc 中加入</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if &amp;diff</span><br><span class="line">    colorscheme evening</span><br><span class="line">endif</span><br></pre></td></tr></table></figure><h2 id="安装rar解压"><a href="#安装rar解压" class="headerlink" title="安装rar解压"></a>安装rar解压</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm -ivh http:&#x2F;&#x2F;mirrors.whsir.com&#x2F;centos&#x2F;whsir-release-centos.noarch.rpm</span><br><span class="line">yum install rar</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> shell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python经验收集</title>
      <link href="/2020/06/29/python%E7%BB%8F%E9%AA%8C%E6%94%B6%E9%9B%86/"/>
      <url>/2020/06/29/python%E7%BB%8F%E9%AA%8C%E6%94%B6%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h2 id="安装python2-7"><a href="#安装python2-7" class="headerlink" title="安装python2.7"></a>安装python2.7</h2><p>1.安装Development Tools</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum groupinstall -y &#39;development tools&#39;</span><br></pre></td></tr></table></figure><p>2安装SSL、bz2、zlib来为Python的安装做好准备工作</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y zlib-devel bzip2-devel openssl-devel xz-libs wget</span><br></pre></td></tr></table></figure><p>3.下载Python-2.7.12.tar.xz并解压</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;www.python.org&#x2F;ftp&#x2F;python&#x2F;2.7.12&#x2F;Python-2.7.12.tar.xz</span><br><span class="line">tar -xvf Python-2.7.12.tar.xz </span><br><span class="line">cd Python-2.7.12</span><br><span class="line">.&#x2F;configure --prefix&#x3D;&#x2F;usr&#x2F;local&#x2F;python2.7</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line">ln -s &#x2F;usr&#x2F;local&#x2F;python2.7&#x2F;bin&#x2F;python2.7 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;python2.7</span><br></pre></td></tr></table></figure><h2 id="安装pip"><a href="#安装pip" class="headerlink" title="安装pip"></a>安装pip</h2><p>1.安装setuptools（下载链接可从<a href="https://pypi.python.org/pypi/setuptools#code-of-conduct寻找）" target="_blank" rel="noopener">https://pypi.python.org/pypi/setuptools#code-of-conduct寻找）</a></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget --no-check-certificate https:&#x2F;&#x2F;pypi.python.org&#x2F;packages&#x2F;1f&#x2F;7a&#x2F;6b239a65d452b04ad8068193ae313b386e6fc745b92cd4584fccebecebf0&#x2F;setuptools-25.1.6.tar.gz</span><br><span class="line">tar -zxf setuptools-25.1.6.tar.gz</span><br><span class="line">cd setuptools-25.1.6</span><br><span class="line">python2.7 setup.py install</span><br></pre></td></tr></table></figure><p>2.下载pip和安装pip, <a href="https://pip.pypa.io/en/stable/installing/" target="_blank" rel="noopener">官网pip安装</a> </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl https:&#x2F;&#x2F;bootstrap.pypa.io&#x2F;get-pip.py -o get-pip.py</span><br><span class="line">python2.7 get-pip.py</span><br><span class="line">ln -s &#x2F;usr&#x2F;local&#x2F;python2.7&#x2F;bin&#x2F;pip &#x2F;usr&#x2F;local&#x2F;bin&#x2F;pip</span><br></pre></td></tr></table></figure><h2 id="Python经验收集"><a href="#Python经验收集" class="headerlink" title="Python经验收集"></a>Python经验收集</h2><h3 id="问题1-Python命令使用tab键自动补全"><a href="#问题1-Python命令使用tab键自动补全" class="headerlink" title="问题1:Python命令使用tab键自动补全"></a>问题1:Python命令使用tab键自动补全</h3><p>首先查看python路径</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import sys</span><br><span class="line">&gt;&gt;&gt; sys.path</span><br></pre></td></tr></table></figure><p>python路径里包含 /usr/local/lib/python2.7</p><p>创建tab.py文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;usr&#x2F;local&#x2F;lib&#x2F;python2.7&#x2F;tab.py</span><br><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;env python</span><br><span class="line">import readline,rlcompleter</span><br><span class="line">readline.parse_and_bind(&#39;tab: complete&#39;)</span><br></pre></td></tr></table></figure><p>命令行使用import导入模块，或者在/etc/profile设置文件所在的PATH全局变量，让系统自动加载。</p><p># 找一个readline.os文件，放到Python可识别路径中</p><p>修改根目录的.bashrc文件，添加如下一行</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export PYTHONSTARTUP&#x3D;&#x2F;usr&#x2F;local&#x2F;lib&#x2F;python2.7&#x2F;tab.py</span><br></pre></td></tr></table></figure><p>重新加载环境变量   </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#source .bashrc</span><br></pre></td></tr></table></figure><p>这样再进入python编辑器就可以自动补全</p><h3 id="问题2-python2-7命令行无法使用退格键、方向键"><a href="#问题2-python2-7命令行无法使用退格键、方向键" class="headerlink" title="问题2: python2.7命令行无法使用退格键、方向键"></a>问题2: python2.7命令行无法使用退格键、方向键</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install readline</span><br></pre></td></tr></table></figure><h3 id="问题3-MySQLdb包导入失败"><a href="#问题3-MySQLdb包导入失败" class="headerlink" title="问题3: MySQLdb包导入失败"></a>问题3: MySQLdb包导入失败</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import MySQLdb </span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">ImportError: No module named MySQLdb</span><br></pre></td></tr></table></figure><p>解决方法</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install MySQL-python</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>操作系统VMware-esxi安装</title>
      <link href="/2020/06/24/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9Fvmware-esxi%E5%AE%89%E8%A3%85/"/>
      <url>/2020/06/24/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9Fvmware-esxi%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h2 id="通过软碟通制作u盘启动"><a href="#通过软碟通制作u盘启动" class="headerlink" title="通过软碟通制作u盘启动"></a>通过软碟通制作u盘启动</h2><h3 id="Exsi安装顺序"><a href="#Exsi安装顺序" class="headerlink" title="Exsi安装顺序"></a>Exsi安装顺序</h3><p><strong>1. 通过软碟通制作u盘启动</strong></p><p><strong>2. 将系统磁盘制作raid卷</strong></p><p><strong>3. 使用u盘安装exsi系统</strong></p><p><strong>4. 服务器添加IP地址</strong></p><p><strong>5. 添加虚拟机</strong></p><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><p>0.1 软碟通安装包  </p><p>链接:<a href="https://pan.baidu.com/s/1OLoGr713pq7rVv_H7NR6ew" target="_blank" rel="noopener">https://pan.baidu.com/s/1OLoGr713pq7rVv_H7NR6ew</a>  密码:mk9i</p><p>0.2 vmware exsi安装包（6.5, 其他版本亦可）</p><p>链接:<a href="https://pan.baidu.com/s/1CbvhaWgfkH_XUWq_oNp-aw" target="_blank" rel="noopener">https://pan.baidu.com/s/1CbvhaWgfkH_XUWq_oNp-aw</a>  密码:5uli</p><h3 id="制作u盘启动"><a href="#制作u盘启动" class="headerlink" title="制作u盘启动"></a>制作u盘启动</h3><p>1.1 在电脑桌面双击<strong>UltralSO</strong>(软碟通)图标，如图所示</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3g569qfwj30bx0adtg1.jpg" alt="1.1" style="zoom: 50%;max-width: 33%;" /><p>1.2 这时，会弹出欢迎的对话框，点击<strong>继续试用</strong>就行，如图所示</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3g671kjwj309607j74e.jpg" alt="1.2" style="zoom:99%;max-width: 50%;" /><p>1.3 来到软碟通界面后，找到你磁盘中的.iso文件，然后，双击进行加载，如图所示</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3g63xwyfj30dc09i763.jpg" alt="1.3" style="zoom:67%;max-width: 50%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3g6u1pzuj316m0u0qcb.jpg" alt="1.3.2" style="zoom: 33%;max-width: 50%;" /><p>1.4 然后，点击菜单栏启动下列表中“写入磁盘映像”，如图所示</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3g72xsofj30la0fcafh.jpg" alt="1.4" style="zoom:50%;max-width: 50%;" /><p>1.5 这时，会弹出一个“写入磁盘映像”对话框，你需要插上U盘，如图所示</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3g78zkghj30uc0s241c.jpg" alt="1.5" style="zoom: 33%;max-width: 50%;" /><p>1.6 然后，你需要进行对U盘的格式化，点击“格式化”，如图所示</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3g7dp0luj30g40waq5y.jpg" alt="1.6" style="zoom:33%;max-width: 25%;" /><p>1.7 这时，会告诉你U盘中的数据会被全部清除，注意进行备份，然后点击确定，如图所示</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3g7hk6avj30u00v4jy1.jpg" alt="1.7" style="zoom: 33%;max-width: 50%;" /><p>1.8 这时，你就会看到提示你格式完毕，点击确定，如图所示</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3g7m9bylj30u00v2wk1.jpg" alt="1.8" style="zoom: 33%;max-width: 50%;" /><p>1.9 然后，我们开始制作启动盘了，点击写入，开始制作，如图所示</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3hl19rnyj30ui0s6zmu.jpg" alt="1.9" style="zoom:67%;max-width: 50%;" /><p>1.10 这时，你就会看到写入进度，等待写入完毕就是了，如图所示</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3g7s7wq6j30u20riacy.jpg" alt="1.10" style="zoom: 33%;max-width: 50%;" /><h2 id="将dell系统磁盘制作raid卷"><a href="#将dell系统磁盘制作raid卷" class="headerlink" title="将dell系统磁盘制作raid卷"></a>将dell系统磁盘制作raid卷</h2><p><a href="https://wenku.baidu.com/view/5dc515440408763231126edb6f1aff00bed570ad.html" target="_blank" rel="noopener">https://wenku.baidu.com/view/5dc515440408763231126edb6f1aff00bed570ad.html</a></p><p><a href="https://www.cnblogs.com/passzhang/articles/8672029.html" target="_blank" rel="noopener">https://www.cnblogs.com/passzhang/articles/8672029.html</a></p><p>如果是已经有raid分区，看需要是不是需要重做。</p><p>2.0.1 CTRL+R 进入raid配置界面</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gtdzexxj30c808t409.jpg" alt="2.0.1" style="zoom: 67%;max-width: 67%;" /><p>2.0.2 按F2进行操作。弹出选择框，删除之前配置的磁盘组</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gtgsbn5j30ci09edm7.jpg" alt="2.0.2" style="zoom: 67%;max-width: 67%;" /><p>2.1. 按照屏幕下方的虚拟磁盘管理器提示,在VD Mgmt菜单(可以通过CTRL+P/CTRL+N切换菜单），按F2展开虚拟磁盘创建菜单</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gtk4a1wj30z00qgncx.jpg" alt="2.1" style="zoom:33%;max-width: 50%;" /><p>2.2.在虚拟磁盘创建窗口，按回车键选择”Create New VD”创建新虚拟磁盘 </p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gtou77yj310e0pw4g3.jpg" alt="2.2" style="zoom:33%;max-width: 50%;" /><p>2.3、在RAID Level选项按回车，可以出现能够支持的RAID级别，RAID卡能够支持的级别有RAID0/1/5/10/50，根据具体配置的硬盘数量不同，这个位置可能出现的选项也会有所区别。</p><p>选择不同的级别，选项会有所差别。选择好需要配置的RAID级别（我们这里以RAID5为例），按回车确认。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gtqplsnj310c0p8b29.jpg" alt="2.3" style="zoom:33%;max-width: 50%;" /><p>2.4、确认RAID级别以后，按向下方向键，将光标移至Physical Disks列表中，上下移动至需要选择的硬盘位置，按空格键来选择（移除）列表中的硬盘，当选择的硬盘数量达到这个RAID级别所需的要求时，Basic Settings的VD Size中可以显示这个RAID的默认容量信息。有X标志为选中的硬盘。</p><p>选择完硬盘后按Tab键，可以将光标移至VD Size栏，VD Size可以手动设定大小，也就是说可以不用将所有的容量配置在一个虚拟磁盘中。如果这个虚拟磁盘没有使用我们所配置的RAID5阵列所有的容量，剩余的空间可以配置为另外的一个虚拟磁盘，但是配置下一个虚拟磁盘时必须返回VD Mgmt创建（可以参考第13步，会有详细说明）。VD Name根据需要设置，也可为空。</p><p>注：各RAID级别最少需要的硬盘数量，RAID0=1，RAID1=2，RAID5=3，RAID10=4，RAID50=6</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3guhdwlaj310q0p2e81.jpg" alt="2.4" style="zoom:33%;max-width: 50%;" /><p>2.5、修改高级设置，选择完VD Size后，可以按向下方向键，或者Tab键，将光标移至Advanced Settings处，按空格键开启（禁用）高级设置。如果开启后（红框处有X标志为开启），可以修改Stripe Element Size大小，以及阵列的Read Policy与Write Policy，Initialize处可以选择是否在阵列配置的同时进行初始化。</p><p>高级设置默认为关闭（不可修改），如果没有特殊要求，建议不要修改此处的设置。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3guqrgvlj310q0p4e81.jpg" alt="2.5" style="zoom:33%;max-width: 50%;" /><p>2.6、上述的配置确认完成后，按Tab键，将光标移至OK处，按回车，会出现如下的提示，如果是一个全新的阵列，建议进行初始化操作，如果配置阵列的目的是为了恢复之前的数据，则不要进行初始化。按回车确认即可继续。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3guzl2hpj310w0s04mw.jpg" alt="2.6" style="zoom:33%;max-width: 50%;" /><p>2.7、配置完成后，会返回至VD Mgmt主界面，将光标移至图中Virtual Disk 0处，按回车。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gv9y5xdj310w0gs7l3.jpg" alt="2.7" style="zoom: 50%;max-width: 50%;" /><p>2.8、可以看到刚才配置成功的虚拟磁盘信息，查看完成后按esc键可以返回主界面</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gvd995ij310o0p2b29.jpg" alt="2.8" style="zoom:50%;max-width: 50%;" /><p>2.9、在此界面，将光标移至图中Virtual Disk 0处，按F2键可以展开对此虚拟磁盘操作的菜单。</p><p>注：左边有+标志的，将光标移至此处，按向右方向键，可以展开子菜单，按向左方向键，可以关闭子菜单</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gvgcumoj310i0gmwut.jpg" alt="2.9" style="zoom:50%;max-width: 50%;" /><p>2.10、如下图红框所示，可以对刚才配置成功的虚拟磁盘（Virtual Disk 0）进行初始化（Initialization），一致性校验（Consistency Check），删除，查看属性等操作。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gvivqfpj310k0p0tzx.jpg" alt="2.10" style="zoom:50%;max-width: 50%;" /><p>2.11、如果我们要对此虚拟磁盘进行初始化，可以将光标移至Initialization处，回车后选择Start Init。此时会弹出提示窗口，初始化将会清除所有数据，如果确认要进行初始化操作，在OK处按回车即可继续。</p><p>注：初始化会清除硬盘、阵列中的所有信息，并且无法恢复</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gvlktvoj310w0rw7v4.jpg" alt="2.11" style="zoom:50%;max-width: 50%;" /><p>2.12、确认后可以看到初始化的进度，左边红框处为百分比表示，右边红框处表示目前所作的操作。等待初始化进行为100%，虚拟磁盘的配置完成。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gvohkcaj310o0rs1iu.jpg" alt="2.12" style="zoom:50%;max-width: 50%;" /><p>2.13、如果刚才配置虚拟磁盘的时候没有使用阵列的全部容量，剩余的容量可以在这里划分使用。将光标移至Space allocation处，按向右方向键展开此菜单</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gvqpovjj310c0hatnf.jpg" alt="2.13" style="zoom:50%;max-width: 50%;" /><p>2.14、将光标移至<em>Free Space</em>处，按F2键，至第15步，或者直接按回车至第16步</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gwg0yvvj310g0h8aq6.jpg" alt="2.14" style="zoom:67%;max-width: 50%;" /><p>2.15、在弹出的Add New VD处按回车键。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gvvwihnj310g0h8aq6.jpg" alt="2.15" style="zoom:67%;max-width: 50%;" /><p>2.16、再次进入配置虚拟磁盘的界面，此时左边红框处为刚才配置的虚拟磁盘已经选择的物理磁盘信息，右边红框处可以选择这次要划分的容量空间。同样，如果不全部划分，可以再次返回第13步，进行再一个虚拟磁盘的创建。</p><p>注：由于虚拟磁盘的建立是基于刚才所创建的阵列，所以RAID Level与刚才所创建的相同，无法更改。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gzeavfwj310g0h8dxn.jpg" alt="2.16" style="zoom:67%;max-width: 50%;" /><p>2.17、每一次创建，都会在Virtual Disks中添加新的虚拟磁盘。这些虚拟磁盘都是在同一个磁盘组（也就是我们刚才所配置的RAID5）上划分的。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3gw0n3iuj310e0h4duv.jpg" alt="2.17" style="zoom:67%;max-width: 50%;" /><h2 id="使用u盘启动并安装exsi系统"><a href="#使用u盘启动并安装exsi系统" class="headerlink" title="使用u盘启动并安装exsi系统"></a>使用u盘启动并安装exsi系统</h2><blockquote><p>注意1⚠️：如果磁盘没有系统，则默认会从u盘启动，如果有系统，则要改成u盘启动</p><p>注意2⚠️：先在VMware模拟一下安装步骤</p></blockquote><p>3.1 开始exsi安装</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3h37yoc0j30fe085dg2.jpg" alt="1" style="zoom:67%;max-width: 50%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3h38d6wsj30fe0bj74o.jpg" alt="2" style="zoom:67%;max-width: 50%;" /><p>3.2 进入VMare exsi配置页面</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3h39sd00j30fe0bngma.jpg" alt="3" style="zoom:67%;max-width: 50%;" /><p>3.3 接受协议</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3h3c64tej30fd0bijsw.jpg" alt="4" style="zoom:67%;max-width: 50%;" /><p>3.4 安装在本地</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3h3j8dklj30fe0bdaat.jpg" alt="5" style="zoom:67%;max-width: 50%;" /><p>3.5 键盘的输入方式：选择us default</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3h3ikqlwj30fe0bfq3j.jpg" alt="6" style="zoom:67%;max-width: 50%;" /><p>3.6 设置root密码，最小长度7位</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3h3kjt4jj30fd0b4mxk.jpg" alt="7" style="zoom:67%;max-width: 50%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3h3ndm0ij30fd0ba0t2.jpg" alt="8" style="zoom:67%;max-width: 50%;" /><p>3.7 开始安装exsi</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3h3pu5ccj30fe0bd74r.jpg" alt="9" style="zoom:67%;max-width: 50%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3h3s1ikzj30fe0b5dfz.jpg" alt="10" style="zoom:67%;max-width: 50%;" /><p>3.8 安装好以后，需要重启服务</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3h3u0ex0j30fe0b6aax.jpg" alt="11" style="zoom:67%;max-width: 50%;" /><p>3.9 安装完成。如果你出现这个界面，恭喜你已经安装成功。</p><p>如果服务器没有IP地址，下面需要配置IPv4地址，有的话可以通过浏览器直接访问。</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3h5ojt1xj30fe0bjt9c.jpg" alt="12" style="zoom:67%;max-width: 50%;" /><h2 id="配置服务器IP地址"><a href="#配置服务器IP地址" class="headerlink" title="配置服务器IP地址"></a>配置服务器IP地址</h2><p>4.1、选择F2配置服务器信息，输入root控制台密码</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3ha28cuyj30fe0bi0tc.jpg" alt="4.1" style="zoom:67%;max-width: 50%;" /><p>4.2、 再按F2进入到设置菜单 ，选择“Configure Management Network”并回车</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3ha42lusj30fe07ddgp.jpg" alt="4.2" style="zoom:67%;max-width: 50%;" /><p>4.3、 选择”IPv4 Configuration”</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3ha6igk8j31cy0e2gql.jpg" alt="4.3" style="zoom:67%;max-width: 50%;" /><p>4.4、先选择 上下光标到 Set static IPv4 addRess and network configration,按空格键选中</p><p>配置IPv4地址</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3halivghj30v80fadm7.jpg" alt="4.4" style="zoom:67%;max-width: 50%;" /><p>4.5、保存网络设置</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3hasz6x6j30s00bs0w4.jpg" alt="4.5" style="zoom:67%;max-width: 50%;" /><p>4.6、查看设置的IPv4地址</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3havq031j316y0dcn3f.jpg" alt="4.6" style="zoom:67%;max-width: 50%;" /><p>4.7、通过设置的IPv4地址在浏览器即可访问，也可以通过ssh访问</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3hayi75bj30fe0cmq3y.jpg" alt="4.7" style="zoom:67%;max-width: 50%;" /><p>4.8、启动SSH登陆</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3hb1e9ixj31vg0kkqb8.jpg" alt="4.8" style="zoom:67%;max-width: 50%;" /><p>4.9、ssh 登陆： ssh <a href="mailto:root@192.168.70.2">root@192.168.70.2</a></p><p>smbiosDump 查看系统资源</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3hb46wa5j31gc0mm48e.jpg" alt="4.9" style="zoom:67%;max-width: 50%;" /><h2 id="添加虚拟机"><a href="#添加虚拟机" class="headerlink" title="添加虚拟机"></a>添加虚拟机</h2><p>5.1 进入到exsi管理界面</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3hgcv5k9j30fe06jmyd.jpg" alt="5.1" style="zoom:67%;max-width: 50%;" /><p>5.2 选择虚拟机中添加虚拟机</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3hgengxfj30fe06et9e.jpg" alt="5.2" style="zoom:67%;max-width: 50%;" /><p>5.3 选择创建类型： 创建虚拟机</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3hggowpqj30fe09nmyg.jpg" alt="5.3" style="zoom:67%;max-width: 50%;" /><p>5.4 选择名称和虚拟机操作系统</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3hgjdrnqj30fe09pwfx.jpg" alt="5.4" style="zoom:67%;max-width: 50%;" /><p>5.5 选择存储</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3hgpqv1dj30fd09mdhm.jpg" alt="5.5" style="zoom:67%;max-width: 50%;" /><p>5.6 自定义系统资源，主要包括CPU，内存，硬盘，CD/DVD驱动器选择指定的ios文件</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3hgppc76j30fe09lgmr.jpg" alt="5.6" style="zoom:67%;max-width: 50%;" /><p>5.7 确认信息完成，启动时就开始安装</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3hgtoqzdj30fe09pwg6.jpg" alt="5.7" style="zoom:67%;max-width: 50%;" /><p>5.8 下图是我的某个示例图</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg3hy8medxj31si0u0dt5.jpg" alt="示例图" style="zoom:67%;max-width: 50%;" /><h3 id="通过VMware-ESXI6-0设置开机自启"><a href="#通过VMware-ESXI6-0设置开机自启" class="headerlink" title="通过VMware ESXI6.0设置开机自启"></a>通过VMware ESXI6.0设置开机自启</h3><img src="http://wangzhangtao.com/img/body/2.vCenterServer%25E7%259A%2584%25E9%2583%25A8%25E7%25BD%25B2%25E5%2592%258C%25E4%25BD%25BF%25E7%2594%25A8/image-20200907140938091.png" alt="image-20200907140938091" style="zoom:67%;max-width:70%" />]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
          <category> 服务器 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 其他 </tag>
            
            <tag> 服务器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用软件-ELKF+kafka集群</title>
      <link href="/2020/06/17/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6-elkf-kafka%E9%9B%86%E7%BE%A4/"/>
      <url>/2020/06/17/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6-elkf-kafka%E9%9B%86%E7%BE%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="ELKF介绍"><a href="#ELKF介绍" class="headerlink" title="ELKF介绍"></a>ELKF介绍</h2><h3 id="elk介绍"><a href="#elk介绍" class="headerlink" title="elk介绍"></a>elk介绍</h3><p>ELK是三个开源软件的缩写，分别表示：Elasticsearch , Logstash, Kibana ,  它们都是开源软件。新增了一个FileBeat，它是一个轻量级的日志收集处理工具(Agent)，Filebeat占用资源少，适合于在各个服务器上搜集日志后传输给Logstash，官方也推荐此工具。  Elasticsearch是个开源分布式搜索引擎，提供搜集、分析、存储数据三大功能。它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。 Logstash  主要是用来日志的搜集、分析、过滤日志的工具，支持大量的数据获取方式。一般工作方式为c/s架构，client端安装在需要收集日志的主机上，server端负责将收到的各节点日志进行过滤、修改等操作在一并发往elasticsearch上去。 Kibana 也是一个开源和免费的工具，Kibana可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web  界面，可以帮助汇总、分析和搜索重要数据日志。 </p><h3 id="kakfa介绍"><a href="#kakfa介绍" class="headerlink" title="kakfa介绍"></a>kakfa介绍</h3><p>kafka  是一个分布式的基于push-subscribe的消息系统，它具备快速、可扩展、可持久化的特点。它现在是Apache旗下的一个开源系统，作为Hadoop生态系统的一部分，被各种商业公司广泛应用。它的最大的特性就是可以实时的处理大量数据以满足各种需求场景：比如基于hadoop的批处理系统、低延迟的实时系统、storm/spark流式处理引擎。 特点：</p><ul><li><strong>高吞吐量、低延迟</strong>：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒</li><li><strong>可扩展性</strong>：kafka集群支持热扩展</li><li><strong>持久性、可靠性</strong>：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失</li><li><strong>容错性</strong>：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败）</li><li><strong>高并发</strong>：支持数千个客户端同时读写</li></ul><h3 id="zookeeper介绍"><a href="#zookeeper介绍" class="headerlink" title="zookeeper介绍"></a>zookeeper介绍</h3><p>ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，它包含一个简单的原语集，分布式应用程序可以基于它实现同步服务，配置维护和命名服务等。 集群角色：</p><ul><li>Leader服务器是整个zookeeper集群工作机制中的核心</li><li>Follower服务器是zookeeper集群状态的跟随者</li><li>Observer 服务器充当一个观察者的角色</li></ul><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><pre><code>系统版本 CentOS 7.6.1810</code></pre><p>​     jdk：8u202</p><h3 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h3><table><thead><tr><th>主机名</th><th>IP</th><th>备住</th><th>资源</th></tr></thead><tbody><tr><td>wang-13</td><td>192.168.70.13</td><td>zk+kafka+ES+logstash+kibana+nginx</td><td>2核4G</td></tr><tr><td>wang-14</td><td>192.168.70.13</td><td>zk+kafka+ES</td><td>2核4G</td></tr><tr><td>wang-15</td><td>192.168.70.13</td><td>zk+kafka+ES</td><td>2核4G</td></tr></tbody></table><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg12s777v2j31lm0gygny.jpg" alt="ELKF+kafka架构图"></p><h3 id="安装jdk（所有）"><a href="#安装jdk（所有）" class="headerlink" title="安装jdk（所有）"></a>安装jdk（所有）</h3><blockquote><p>官网地址 <a href="https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html" target="_blank" rel="noopener">JDK 8u202 and earlier</a>   <a href="https://download.oracle.com/otn/java/jdk/8u202-b08/1961070e4c9b4e26a04e7f5a083f551e/jdk-8u202-linux-x64.tar.gz" target="_blank" rel="noopener">jdk-8u202-linux</a>  :)  </p></blockquote><p>下载官网软件包到运维主机 /data/soft/centos7/jdk-8u202-linux-x64.tar.gz</p><p><strong>在运维主机创建jdk安装脚本</strong></p><p><code>vim /data/shell/install_java.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 准备文件夹</span><br><span class="line">mkdir &#x2F;opt&#x2F;src</span><br><span class="line">mkdir &#x2F;usr&#x2F;java</span><br><span class="line"></span><br><span class="line"># 拷贝安装包并解压</span><br><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;jdk-8u202-linux-x64.tar.gz -O &#x2F;opt&#x2F;src&#x2F;jdk-8u202-linux-x64.tar.gz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;jdk-8u202-linux-x64.tar.gz -C &#x2F;usr&#x2F;java&#x2F;</span><br><span class="line">ln -s &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_202 &#x2F;usr&#x2F;java&#x2F;jdk</span><br><span class="line"></span><br><span class="line"># 配置默认全局变量</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; &#x2F;etc&#x2F;profile</span><br><span class="line">#JAVA HOME</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk</span><br><span class="line">export PATH&#x3D;\$PATH:\$JAVA_HOME&#x2F;bin:\$JAVA_HOME&#x2F;sbin</span><br><span class="line">export CLASSPATH&#x3D;\$CLASSPATH:\$JAVA_HOME&#x2F;lib:\$JAVA_HOME&#x2F;lib&#x2F;tools.jar</span><br><span class="line">EOF</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line"></span><br><span class="line"># 检查java是否可用</span><br><span class="line">java -version</span><br></pre></td></tr></table></figure><p>执行远程脚本安装java</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_java.sh | sh</span><br></pre></td></tr></table></figure><h3 id="安装npm"><a href="#安装npm" class="headerlink" title="安装npm"></a>安装npm</h3><blockquote><p>官网下载地址 <a href="https://nodejs.org/dist/v12.16.2/node-v12.16.2-linux-x64.tar.xz" target="_blank" rel="noopener">node-v12.16.2</a></p><p>下载到运维主机 /data/soft/centos7/node-v12.16.2-linux-x64.tar.xz</p></blockquote><p><strong>在运维主机创建jdk安装脚本</strong></p><p><code>vim /data/shell/install_node.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;node-v12.16.2-linux-x64.tar.xz -O &#x2F;opt&#x2F;src&#x2F;node-v12.16.2-linux-x64.tar.xz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;node-v12.16.2-linux-x64.tar.xz -C &#x2F;usr&#x2F;</span><br><span class="line">ln -s &#x2F;usr&#x2F;node-v12.16.2-linux-x64 &#x2F;usr&#x2F;node</span><br><span class="line"></span><br><span class="line"># 配置全局变量</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; &#x2F;etc&#x2F;profile</span><br><span class="line">#NODE HOME</span><br><span class="line">export NODE_HOME&#x3D;&#x2F;usr&#x2F;node</span><br><span class="line">export PATH&#x3D;\$PATH:\$NODE_HOME&#x2F;bin</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 查看并验证node</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line">node -v</span><br><span class="line">npm -v</span><br><span class="line"></span><br><span class="line"># 安装淘宝镜像源</span><br><span class="line">npm install -g cnpm --registry&#x3D;https:&#x2F;&#x2F;registry.npm.taobao.org</span><br><span class="line">cnpm -v</span><br></pre></td></tr></table></figure><p>执行远程脚本安装npm</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_node.sh | sh</span><br></pre></td></tr></table></figure><h2 id="部署zookeeper集群"><a href="#部署zookeeper集群" class="headerlink" title="部署zookeeper集群"></a>部署zookeeper集群</h2><h3 id="下载zookeeper"><a href="#下载zookeeper" class="headerlink" title="下载zookeeper"></a>下载zookeeper</h3><blockquote><p>下载zookeeper: <a href="https://archive.apache.org/dist/zookeeper/" target="_blank" rel="noopener">下载地址</a>   <a href="https://archive.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz" target="_blank" rel="noopener">版本3.4.14</a></p></blockquote><p>下载官网软件包到运维主机 /data/soft/centos7/zookeeper-3.4.14.tar.gz</p><h3 id="解压安装包到服务器"><a href="#解压安装包到服务器" class="headerlink" title="解压安装包到服务器"></a>解压安装包到服务器</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;opt&#x2F;src&#x2F; </span><br><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;zookeeper-3.4.14.tar.gz </span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;zookeeper-3.4.14.tar.gz -C &#x2F;opt&#x2F;</span><br><span class="line">ln -s &#x2F;opt&#x2F;zookeeper-3.4.14  &#x2F;opt&#x2F;zookeeper</span><br><span class="line">mkdir -pv &#x2F;data&#x2F;zookeeper&#x2F;data &#x2F;data&#x2F;zookeeper&#x2F;logs</span><br></pre></td></tr></table></figure><h3 id="编辑配置文件"><a href="#编辑配置文件" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;zookeeper&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">tickTime&#x3D;2000</span><br><span class="line">initLimit&#x3D;10</span><br><span class="line">syncLimit&#x3D;5</span><br><span class="line">dataDir&#x3D;&#x2F;data&#x2F;zookeeper&#x2F;data</span><br><span class="line">dataLogDir&#x3D;&#x2F;data&#x2F;zookeeper&#x2F;logs</span><br><span class="line">clientPort&#x3D;2181</span><br><span class="line"></span><br><span class="line">server.1&#x3D;192.168.70.13:2888:3888</span><br><span class="line">server.2&#x3D;192.168.70.14:2888:3888</span><br><span class="line">server.3&#x3D;192.168.70.15:2888:3888</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>创建myid文件，里面是zookeeper标签,内容就是对应的你当前 ip 下的 server.x</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-13 ~]# echo &quot;1&quot; &gt; &#x2F;data&#x2F;zookeeper&#x2F;data&#x2F;myid</span><br></pre></td></tr></table></figure><h3 id="启动zookeeper"><a href="#启动zookeeper" class="headerlink" title="启动zookeeper"></a>启动zookeeper</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-13 ~]# &#x2F;opt&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;opt&#x2F;zookeeper&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br></pre></td></tr></table></figure><p>等zookeeper集群启动一半以上，再查看状态</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-14 src]# &#x2F;opt&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh status </span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;opt&#x2F;zookeeper&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Mode: leader</span><br></pre></td></tr></table></figure><h2 id="部署kafka集群"><a href="#部署kafka集群" class="headerlink" title="部署kafka集群"></a>部署kafka集群</h2><blockquote><p><a href="http://mirror.bit.edu.cn/apache/kafka/2.0.0/kafka_2.11-2.0.0.tgz" target="_blank" rel="noopener">kafka2.0.0北理工镜像</a>  <a href="https://archive.apache.org/dist/kafka/2.2.0/kafka_2.12-2.2.0.tgz" target="_blank" rel="noopener">官网kafka2.2.0</a></p><p>下载到运维主机 /data/soft/centos7/kafka_2.12-2.2.0.tgz</p></blockquote><p>解压Kafka到服务器</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;kafka_2.12-2.2.0.tgz -O &#x2F;opt&#x2F;src&#x2F;kafka_2.12-2.2.0.tgz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;kafka_2.12-2.2.0.tgz -C &#x2F;opt&#x2F;</span><br><span class="line">ln -s &#x2F;opt&#x2F;kafka_2.12-2.2.0 &#x2F;opt&#x2F;kafka</span><br><span class="line">mkdir -p &#x2F;data&#x2F;kafka&#x2F;logs</span><br></pre></td></tr></table></figure><p>修改配置文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;kafka&#x2F;config&#x2F;server.properties </span><br><span class="line">broker.id&#x3D;1</span><br><span class="line">listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;192.168.70.13:9092</span><br><span class="line">num.network.threads&#x3D;3</span><br><span class="line">num.io.threads&#x3D;8</span><br><span class="line">socket.send.buffer.bytes&#x3D;102400</span><br><span class="line">socket.receive.buffer.bytes&#x3D;102400</span><br><span class="line">socket.request.max.bytes&#x3D;104857600</span><br><span class="line">log.dirs&#x3D;&#x2F;data&#x2F;kafka&#x2F;logs</span><br><span class="line">num.partitions&#x3D;1</span><br><span class="line">num.recovery.threads.per.data.dir&#x3D;1</span><br><span class="line">offsets.topic.replication.factor&#x3D;1</span><br><span class="line">transaction.state.log.replication.factor&#x3D;1</span><br><span class="line">transaction.state.log.min.isr&#x3D;1</span><br><span class="line">log.retention.hours&#x3D;168</span><br><span class="line">log.segment.bytes&#x3D;1073741824</span><br><span class="line">log.retention.check.interval.ms&#x3D;300000</span><br><span class="line">zookeeper.connect&#x3D;192.168.70.13:2181,192.168.70.13:2181,192.168.70.13:2181</span><br><span class="line">zookeeper.connection.timeout.ms&#x3D;6000</span><br><span class="line">group.initial.rebalance.delay.ms&#x3D;0</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><blockquote><p>这两个参数需要修改broker.id zookeeper.connect， 添加 listeners  </p></blockquote><p>启动Kafka</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-13 ~]# &#x2F;opt&#x2F;kafka&#x2F;bin&#x2F;kafka-server-start.sh -daemon &#x2F;opt&#x2F;kafka&#x2F;config&#x2F;server.properties</span><br><span class="line">[root@wang-13 ~]# netstat -luntp|grep 9092   </span><br><span class="line">tcp6       0      0 192.168.70.13:9092      :::*                    LISTEN      55398&#x2F;java</span><br></pre></td></tr></table></figure><h3 id="测试消息生产和消费"><a href="#测试消息生产和消费" class="headerlink" title="测试消息生产和消费"></a>测试消息生产和消费</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-13 ~]# &#x2F;opt&#x2F;kafka&#x2F;bin&#x2F;kafka-topics.sh --create --zookeeper 192.168.70.13:2181 --replication-factor 1 --partitions 1 --topic test  # 创建topic  </span><br><span class="line">Created topic test.</span><br><span class="line">[root@wang-13 ~]# &#x2F;opt&#x2F;kafka&#x2F;bin&#x2F;kafka-topics.sh --list --zookeeper 192.168.70.13:2181  </span><br><span class="line">test                   # 查看创建的topic</span><br><span class="line">[root@wang-13 ~]# &#x2F;opt&#x2F;kafka&#x2F;bin&#x2F;kafka-console-producer.sh --broker-list 192.168.70.13:9092 --topic test </span><br><span class="line">&gt;Hello World!          # 模拟客户端发送消息(开2个终端效果会更好)</span><br><span class="line">&gt;exit</span><br><span class="line">&gt;</span><br><span class="line">[root@wang-13 ~]# &#x2F;opt&#x2F;kafka&#x2F;bin&#x2F;kafka-console-consumer.sh --bootstrap-server 192.168.70.13:9092 --topic test --from-beginning         </span><br><span class="line">&quot;Hello World!&quot;         # 模拟客户端接收信息 </span><br><span class="line">exit</span><br><span class="line">[root@wang-13 ~]# &#x2F;opt&#x2F;kafka&#x2F;bin&#x2F;kafka-topics.sh --delete --zookeeper 192.168.70.13:2181 --topic test             # 删除topic</span><br><span class="line">Topic test is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true</span><br></pre></td></tr></table></figure><h2 id="部署filebeat"><a href="#部署filebeat" class="headerlink" title="部署filebeat"></a>部署filebeat</h2><blockquote><p> 官网地址 <a href="https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.8.3-linux-x86_64.tar.gz" target="_blank" rel="noopener">filebeat-6.8.3-linux</a></p><p>下载到运维主机 /data/soft/centos7/filebeat-6.8.3-linux-x86_64.tar.gz</p></blockquote><p>解压安装包到服务器</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;filebeat-6.8.3-linux-x86_64.tar.gz -O &#x2F;opt&#x2F;src&#x2F;filebeat-6.8.3-linux-x86_64.tar.gz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;filebeat-6.8.3-linux-x86_64.tar.gz -C &#x2F;opt&#x2F;</span><br><span class="line">ln -s &#x2F;opt&#x2F;filebeat-6.8.3-linux-x86_64 &#x2F;opt&#x2F;filebeat</span><br></pre></td></tr></table></figure><p>修改配置文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;filebeat&#x2F;filebeat.yml</span><br><span class="line">filebeat.prospectors:</span><br><span class="line">- type: log</span><br><span class="line">  enabled: true</span><br><span class="line">  paths:</span><br><span class="line">    - &#x2F;var&#x2F;log&#x2F;messages</span><br><span class="line">  fields:</span><br><span class="line">    log_topics: syslog</span><br><span class="line">- type: log</span><br><span class="line">  enabed: true</span><br><span class="line">  paths:</span><br><span class="line">    - &#x2F;data&#x2F;logs&#x2F;catalina.out</span><br><span class="line">  multiline.pattern: &#39;[0-9]&#123;4&#125;-[0-9]&#123;2&#125;-[0-9]&#123;2&#125;&#39;</span><br><span class="line">  multiline.negate: true</span><br><span class="line">  multiline.match: after</span><br><span class="line">  fields:</span><br><span class="line">    log_topics: tomcat</span><br><span class="line">output.kafka:</span><br><span class="line">  enabled: true</span><br><span class="line">  hosts: [&quot;192.168.70.13:9092&quot;,&quot;192.168.70.14:9092&quot;,&quot;192.168.70.15:9092&quot;]</span><br><span class="line">  topic: &#39;%&#123;[fields.log_topics]&#125;&#39;</span><br><span class="line">  partition.hash:</span><br><span class="line">    reachable_only: false</span><br><span class="line">  compression: gzip</span><br><span class="line">  max_message_bytes: 1000000</span><br><span class="line">  required_acks: 1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>启动filebeat</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;opt&#x2F;filebeat&#x2F;filebeat -c &#x2F;opt&#x2F;filebeat&#x2F;filebeat.yml &amp;</span><br></pre></td></tr></table></figure><h3 id="测试filebeat"><a href="#测试filebeat" class="headerlink" title="测试filebeat"></a>测试filebeat</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-13 ~]# echo &#39;hello word!!!&#39; &gt;&gt; &#x2F;data&#x2F;logs&#x2F;catalina.out</span><br><span class="line"># 查看topic列表</span><br><span class="line">[root@wang-13 ~]# &#x2F;opt&#x2F;kafka&#x2F;bin&#x2F;kafka-topics.sh --list --zookeeper 192.168.70.13:2181  </span><br><span class="line">__consumer_offsets</span><br><span class="line">syslog</span><br><span class="line">test</span><br><span class="line">tomcat</span><br><span class="line"></span><br><span class="line"># 查看日志信息</span><br><span class="line">[root@wang-13 ~]# &#x2F;opt&#x2F;kafka&#x2F;bin&#x2F;kafka-console-consumer.sh --bootstrap-server 192.168.70.13:9092 --topic tomcat --from-beginning</span><br><span class="line">&#123;&quot;@timestamp&quot;:&quot;2020-06-18T09:31:08.887Z&quot;,&quot;@metadata&quot;:&#123;&quot;beat&quot;:&quot;filebeat&quot;,&quot;type&quot;:&quot;doc&quot;,&quot;version&quot;:&quot;6.8.3&quot;,&quot;topic&quot;:&quot;tomcat&quot;&#125;,&quot;input&quot;:&#123;&quot;type&quot;:&quot;log&quot;&#125;,&quot;beat&quot;:&#123;&quot;name&quot;:&quot;wang-13.host.com&quot;,&quot;hostname&quot;:&quot;wang-13.host.com&quot;,&quot;version&quot;:&quot;6.8.3&quot;&#125;,&quot;source&quot;:&quot;&#x2F;data&#x2F;logs&#x2F;catalina.out&quot;,&quot;log&quot;:&#123;&quot;file&quot;:&#123;&quot;path&quot;:&quot;&#x2F;data&#x2F;logs&#x2F;catalina.out&quot;&#125;&#125;,&quot;message&quot;:&quot;hello word!!!&quot;,&quot;prospector&quot;:&#123;&quot;type&quot;:&quot;log&quot;&#125;,&quot;fields&quot;:&#123;&quot;log_topics&quot;:&quot;tomcat&quot;&#125;,&quot;host&quot;:&#123;&quot;name&quot;:&quot;wang-13.host.com&quot;&#125;,&quot;offset&quot;:0&#125;</span><br></pre></td></tr></table></figure><h2 id="部署elasticsearch集群"><a href="#部署elasticsearch集群" class="headerlink" title="部署elasticsearch集群"></a>部署elasticsearch集群</h2><blockquote><p> <a href="https://www.elastic.co/" target="_blank" rel="noopener">官网</a>  <a href="https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.8.6.tar.gz" target="_blank" rel="noopener">下载地址</a>  </p><p> 安装包下载到运维主机 /data/soft/centos7/elasticsearch-6.8.3.tar.gz</p></blockquote><h3 id="解压安装包到主机"><a href="#解压安装包到主机" class="headerlink" title="解压安装包到主机"></a>解压安装包到主机</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;elasticsearch-6.8.3.tar.gz -O &#x2F;opt&#x2F;src&#x2F;elasticsearch-6.8.3.tar.gz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;elasticsearch-6.8.3.tar.gz -C &#x2F;opt&#x2F;</span><br><span class="line">ln -s &#x2F;opt&#x2F;elasticsearch-6.8.3 &#x2F;opt&#x2F;elasticsearch</span><br><span class="line">mkdir -p &#x2F;data&#x2F;elasticsearch&#x2F;&#123;data,logs&#125;</span><br><span class="line">cd &#x2F;opt&#x2F;elasticsearch</span><br></pre></td></tr></table></figure><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;elasticsearch&#x2F;config&#x2F;elasticsearch.yml </span><br><span class="line">cluster.name: ELK                                     #集群名称</span><br><span class="line">node.name: wang-13.host.com                           #节点名称</span><br><span class="line">path.data: &#x2F;data&#x2F;elasticsearch&#x2F;data                   #data存储路径</span><br><span class="line">path.logs: &#x2F;data&#x2F;elasticsearch&#x2F;logs                   #log存储路径</span><br><span class="line">network.host: 0.0.0.0                                 #监听地址</span><br><span class="line">http.port: 9200                                       #监听端口</span><br><span class="line">discovery.zen.ping.unicast.hosts: [&quot;wang-13.host.com&quot;, &quot;wang-14.host.com&quot;,&quot;wang-15.host.com&quot;]  #集群节点发现列表</span><br><span class="line">discovery.zen.minimum_master_nodes: 1                 #集群可做master的最小节点数</span><br><span class="line">node.master: true                                     #指定该节点是否有资格被选举成为node，默认是true，es是默认集群中的第一台机器为master，如果这台机挂了就会重新选举master</span><br><span class="line">node.data: true                                       # 指定该节点是否存储索引数据，默认为true</span><br><span class="line">transport.tcp.compress: true                          #设置是否压缩tcp传输时的数据，默认为false，不压缩</span><br><span class="line">http.cors.enabled: true                               #开启跨域访问支持，默认为false</span><br><span class="line">http.cors.allow-origin: &quot;*&quot;                           # 跨域访问允许的域名地址</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><blockquote><p>注意修改节点名称 node.name</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed -i &quot;s&amp;node.name: wang-13.host.com&amp;node.name: wang-$&#123;HOSTNUM&#125;.host.com&amp;&quot; &#x2F;opt&#x2F;elasticsearch&#x2F;config&#x2F;elasticsearch.yml </span><br></pre></td></tr></table></figure></blockquote><p>修改用户和文件描述符</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建普通用户</span><br><span class="line">useradd -s &#x2F;bin&#x2F;bash -M es</span><br><span class="line">chown -R es.es &#x2F;opt&#x2F;elasticsearch-6.8.3  &#x2F;data&#x2F;elasticsearch&#x2F;</span><br><span class="line"></span><br><span class="line"># 修改文件描述符</span><br><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;es.conf</span><br><span class="line">es hard nofile 65536</span><br><span class="line">es soft fsize unlimited</span><br><span class="line">es hard memlock unlimited</span><br><span class="line">es soft memlock unlimited</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 调整内核参数</span><br><span class="line">sysctl -w vm.max_map_count&#x3D;262144</span><br><span class="line">echo &quot;vm.max_map_count&#x3D;262144&quot; &gt; &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure><h3 id="启动elasticsearch"><a href="#启动elasticsearch" class="headerlink" title="启动elasticsearch"></a>启动elasticsearch</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-13 elasticsearch]# su es -c &quot;&#x2F;opt&#x2F;elasticsearch&#x2F;bin&#x2F;elasticsearch -d&quot;</span><br><span class="line">[root@wang-13 elasticsearch]# netstat -luntp|grep 9200   </span><br><span class="line">tcp6       0      0 :::9200                 :::*                    LISTEN      50579&#x2F;java</span><br></pre></td></tr></table></figure><h3 id="查看集群状态"><a href="#查看集群状态" class="headerlink" title="查看集群状态"></a>查看集群状态</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-13 ~]# curl -XGET &#39;http:&#x2F;&#x2F;localhost:9200&#x2F;_cluster&#x2F;health?pretty&#39;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;cluster_name&quot; : &quot;ELK&quot;,</span><br><span class="line">  &quot;status&quot; : &quot;green&quot;,</span><br><span class="line">  &quot;timed_out&quot; : false,</span><br><span class="line">  &quot;number_of_nodes&quot; : 3,</span><br><span class="line">  &quot;number_of_data_nodes&quot; : 3,</span><br><span class="line">  &quot;active_primary_shards&quot; : 0,</span><br><span class="line">  &quot;active_shards&quot; : 0,</span><br><span class="line">  &quot;relocating_shards&quot; : 0,</span><br><span class="line">  &quot;initializing_shards&quot; : 0,</span><br><span class="line">  &quot;unassigned_shards&quot; : 0,</span><br><span class="line">  &quot;delayed_unassigned_shards&quot; : 0,</span><br><span class="line">  &quot;number_of_pending_tasks&quot; : 0,</span><br><span class="line">  &quot;number_of_in_flight_fetch&quot; : 0,</span><br><span class="line">  &quot;task_max_waiting_in_queue_millis&quot; : 0,</span><br><span class="line">  &quot;active_shards_percent_as_number&quot; : 100.0</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="安装head插件"><a href="#安装head插件" class="headerlink" title="安装head插件"></a>安装head插件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y git</span><br><span class="line">cd &#x2F;opt&#x2F;src&#x2F;</span><br><span class="line">git clone git:&#x2F;&#x2F;github.com&#x2F;mobz&#x2F;elasticsearch-head.git</span><br><span class="line">cd elasticsearch-head&#x2F;</span><br><span class="line">cnpm install grunt -save</span><br><span class="line">cnpm install</span><br><span class="line">npm run start &amp;</span><br></pre></td></tr></table></figure><h2 id="部署logstash"><a href="#部署logstash" class="headerlink" title="部署logstash"></a>部署logstash</h2><blockquote><p>官网地址 <a href="https://artifacts.elastic.co/downloads/logstash/logstash-6.8.3.tar.gz" target="_blank" rel="noopener">logstash-6.8.3</a></p><p>下载到运维主机 /data/soft/centos7/logstash-6.8.3.tar.gz</p></blockquote><p>解压logstash安装包</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;logstash-6.8.3.tar.gz -O &#x2F;opt&#x2F;src&#x2F;logstash-6.8.3.tar.gz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;logstash-6.8.3.tar.gz -C &#x2F;opt&#x2F;</span><br><span class="line">ln -s &#x2F;opt&#x2F;logstash-6.8.3 &#x2F;opt&#x2F;logstash</span><br><span class="line">mkdir -pv &#x2F;data&#x2F;logstash&#x2F;data &#x2F;data&#x2F;logstash&#x2F;logs</span><br></pre></td></tr></table></figure><h3 id="修改logstash配置文件"><a href="#修改logstash配置文件" class="headerlink" title="修改logstash配置文件"></a>修改logstash配置文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;logstash&#x2F;config&#x2F;logstash.yml</span><br><span class="line">path.data: &#x2F;data&#x2F;logstash&#x2F;data</span><br><span class="line">http.host: &quot;192.168.70.13&quot;</span><br><span class="line">path.logs: &#x2F;data&#x2F;logstash&#x2F;logs</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>配置logstash从ZK将日志传入ES</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;logstash&#x2F;config&#x2F;logstash.conf</span><br><span class="line">input&#123;</span><br><span class="line">  kafka &#123;</span><br><span class="line">    type &#x3D;&gt; &quot;tomcat&quot;</span><br><span class="line">    bootstrap_servers &#x3D;&gt; &quot;192.168.70.13:9092,192.168.70.14:9092,192.168.70.15:9092&quot;</span><br><span class="line">    group_id &#x3D;&gt; &quot;ecs&quot;    #logstash 集群需相同</span><br><span class="line">    topics &#x3D;&gt; [&quot;tomcat&quot;]</span><br><span class="line">  &#125;</span><br><span class="line">  kafka &#123;</span><br><span class="line">    type &#x3D;&gt; &quot;syslog&quot;</span><br><span class="line">    bootstrap_servers &#x3D;&gt; &quot;192.168.70.13:9092,192.168.70.14:9092,192.168.70.15:9092&quot;</span><br><span class="line">    group_id &#x3D;&gt; &quot;ecs1&quot;</span><br><span class="line">    topics &#x3D;&gt; [&quot;syslog&quot;]</span><br><span class="line">    codec &#x3D;&gt; &quot;json&quot;  # json格式日志格式化</span><br><span class="line">  &#125;</span><br><span class="line">&#125;    </span><br><span class="line">filter &#123;</span><br><span class="line">  if [type] &#x3D;&#x3D; &quot;tomcat&quot; &#123;</span><br><span class="line">    grok &#123;</span><br><span class="line">      match &#x3D;&gt; &#123; &quot;message&quot; &#x3D;&gt; &quot;%&#123;COMBINEDAPACHELOG&#125;&quot;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line">  if [type] &#x3D;&#x3D; &quot;tomcat&quot; &#123;</span><br><span class="line">    elasticsearch &#123;</span><br><span class="line">      hosts &#x3D;&gt; [&quot;192.168.70.13:9200&quot;,&quot;192.168.70.14:9200&quot;,&quot;192.168.70.15:9200&quot;]</span><br><span class="line">      index &#x3D;&gt; &quot;tomcat-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  if [type] &#x3D;&#x3D; &quot;syslog&quot; &#123;</span><br><span class="line">    elasticsearch &#123;</span><br><span class="line">      hosts &#x3D;&gt; [&quot;192.168.70.13:9200&quot;,&quot;192.168.70.14:9200&quot;,&quot;192.168.70.15:9200&quot;]</span><br><span class="line">      index &#x3D;&gt; &quot;syslog-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    # 输出日志到本地文件</span><br><span class="line">    # file &#123;</span><br><span class="line">    #     path &#x3D;&gt; &quot;&#x2F;data&#x2F;elk&#x2F;loggz&#x2F;%&#123;type&#125;&#x2F;%&#123;hostip&#125;&#x2F;%&#123;[log][file][path]&#125;%&#123;+YYYY-MM&#125;&#x2F;%&#123;type&#125;_%&#123;hostip&#125;_%&#123;+YYYY-MM-dd&#125;.log.gz&quot;</span><br><span class="line">    #     codec &#x3D;&gt; line &#123; format &#x3D;&gt; &quot;%&#123;message&#125;&quot;&#125;</span><br><span class="line">    #     gzip &#x3D;&gt; true 是否对日志压缩，可不加</span><br><span class="line">    # &#125;</span><br><span class="line"></span><br><span class="line">    # stdout &#123; codec &#x3D;&gt; rubydebug &#125; 输出日志到前台，调试使用</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="启动logstash"><a href="#启动logstash" class="headerlink" title="启动logstash"></a>启动logstash</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;opt&#x2F;logstash&#x2F;bin&#x2F;logstash</span><br></pre></td></tr></table></figure><p>查看es索引，确认数据是否传输到es</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-13 ~]# curl http:&#x2F;&#x2F;192.168.70.13:9200&#x2F;_cat&#x2F;indices?v</span><br><span class="line">health status index                        uuid                   pri rep docs.count </span><br><span class="line">green  open   syslog-2020.06.18            exoYb4l2T2a2O4BarzzJRw   5   1          6            0    135.3kb         67.6kb</span><br></pre></td></tr></table></figure><h2 id="部署kibana"><a href="#部署kibana" class="headerlink" title="部署kibana"></a>部署kibana</h2><blockquote><p><a href="https://www.elastic.co/cn/" target="_blank" rel="noopener">中文官网</a>  <a href="https://artifacts.elastic.co/downloads/kibana/kibana-6.8.3-linux-x86_64.tar.gz" target="_blank" rel="noopener">kibana-6.8.3-linux</a></p><p>下载到运维主机 /data/soft/centos7/kibana-6.8.3-linux-x86_64.tar.gz </p></blockquote><h3 id="解压kibana安转包"><a href="#解压kibana安转包" class="headerlink" title="解压kibana安转包"></a>解压kibana安转包</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;kibana-6.8.3-linux-x86_64.tar.gz -O &#x2F;opt&#x2F;src&#x2F;kibana-6.8.3-linux-x86_64.tar.gz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;kibana-6.8.3-linux-x86_64.tar.gz -C &#x2F;opt&#x2F;</span><br><span class="line">ln -s &#x2F;opt&#x2F;kibana-6.8.3-linux-x86_64 &#x2F;opt&#x2F;kibana</span><br><span class="line">mkdir -p &#x2F;data&#x2F;kibana&#x2F;logs</span><br></pre></td></tr></table></figure><h3 id="编辑配置文件-1"><a href="#编辑配置文件-1" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;kibana&#x2F;config&#x2F;kibana.yml</span><br><span class="line">server.port: 5601</span><br><span class="line">server.host: &quot;192.168.70.13&quot; #内网地址</span><br><span class="line">elasticsearch.url: &quot;http:&#x2F;&#x2F;192.168.70.13:9200&quot;</span><br><span class="line">logging.dest: &#x2F;data&#x2F;kibana&#x2F;logs&#x2F;kibana.log</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="启动kibana"><a href="#启动kibana" class="headerlink" title="启动kibana"></a>启动kibana</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-13 ~]# &#x2F;opt&#x2F;kibana&#x2F;bin&#x2F;kibana &amp;</span><br><span class="line">[root@wang-13 ~]# netstat -anput | grep 5601   </span><br><span class="line">tcp        0      0 192.168.70.13:5601      0.0.0.0:*               LISTEN      67246&#x2F;node</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;opt&#x2F;src </span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;anbai-inc&#x2F;Kibana_Hanization.git</span><br><span class="line">cd Kibana_Hanization&#x2F;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-13 old]# python main.py &#x2F;usr&#x2F;share&#x2F;kibana&#x2F;</span><br><span class="line">恭喜，Kibana汉化完成！</span><br></pre></td></tr></table></figure><h3 id="nginx验证登录kibana"><a href="#nginx验证登录kibana" class="headerlink" title="nginx验证登录kibana"></a>nginx验证登录kibana</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install epel-release nginx httpd-tools</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;kibana.conf</span><br><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name kibana2.od.com;</span><br><span class="line">    auth_basic &quot;Restricted Access&quot;;</span><br><span class="line">    auth_basic_user_file &#x2F;etc&#x2F;nginx&#x2F;kibana-user;</span><br><span class="line">    access_log &#x2F;data&#x2F;logs&#x2F;nginx&#x2F;doc.log;</span><br><span class="line"></span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        proxy_pass http:&#x2F;&#x2F;192.168.70.13:5601;</span><br><span class="line">        proxy_http_version 1.1;</span><br><span class="line">        proxy_set_header Upgrade \$http_upgrade;</span><br><span class="line">        proxy_set_header Connection &#39;upgrade&#39;;</span><br><span class="line">        proxy_set_header Host \$host;</span><br><span class="line">        proxy_cache_bypass \$http_upgrade;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>创建登陆文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-13 ～]# htpasswd -cm &#x2F;etc&#x2F;nginx&#x2F;kibana-user kibana</span><br><span class="line">New password: </span><br><span class="line">Re-type new password: </span><br><span class="line">Adding password for user kibana</span><br></pre></td></tr></table></figure><h3 id="启动nginx"><a href="#启动nginx" class="headerlink" title="启动nginx"></a>启动nginx</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nginx -t</span><br><span class="line">systemctl start nginx</span><br><span class="line">systemctl enable nginx</span><br></pre></td></tr></table></figure><h3 id="访问kibana界面"><a href="#访问kibana界面" class="headerlink" title="访问kibana界面"></a>访问kibana界面</h3><p><a href="http://kibana2.od.com" target="_blank" rel="noopener">http://kibana2.od.com</a></p><img src="http://wangzhangtao.com/img/body/jike/image-20200618153834710.png" alt="kibana展示界面" style="zoom:67%;max-width: 70%" />]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> ELK </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> ELK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用软件-nexus私服</title>
      <link href="/2020/06/16/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6-nexus%E7%A7%81%E6%9C%8D/"/>
      <url>/2020/06/16/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6-nexus%E7%A7%81%E6%9C%8D/</url>
      
        <content type="html"><![CDATA[<h2 id="Nexus介绍"><a href="#Nexus介绍" class="headerlink" title="Nexus介绍"></a>Nexus介绍</h2><p>　　<a href="http://nexus.sonatype.org/" target="_blank" rel="noopener">Nexus</a> 是Maven仓库管理器，如果你使用Maven，你可以从<a href="http://repo1.maven.org/maven2/" target="_blank" rel="noopener">Maven中央仓库</a> 下载所需要的构件（artifact），但这通常不是一个好的做法，你应该在本地架设一个Maven仓库服务器，Nexus  还可以在代理远程仓库的同时维护本地仓库，以降低中央仓库的负荷，节省外网带宽和时间。</p><p>​        Nexus 是“开箱即用”的系统，不需要数据库，它使用文件系统加 Lucene 来组织数据，支持 WebDAV 与 LDAP  安全身份认证。Nexus 还提供了强大的仓库管理功能，构件搜索功能，它基于 REST，友好的 UI 是一个 extjs 的 REST  客户端，它占用较少的内存，基于简单文件系统而非数据库。Nexus 极大地简化了本地内部仓库的维护和外部仓库的访问。</p><h3 id="Nexus3-x-和-2-x-版本比较"><a href="#Nexus3-x-和-2-x-版本比较" class="headerlink" title="Nexus3.x 和 2.x 版本比较"></a>Nexus3.x 和 2.x 版本比较</h3><p>本篇 Nexus 选择 3.x 版本，Nexus3.x 相较 2.x 版本有很大的改变：</p><p>1）从底层重构，从而提高性能，增强扩展能力，并改善用户体验<br>2）升级界面，增加更多的浏览，搜索和管理功能<br>3）提供安装包，使部署更简单（安装完自动添加成服务，省去手动添加的麻烦）<br>4）增加 Docker，NuGet，npm，Bower的支持<br>5）提供新的管理接口，从而能自动管理任务。<br>注意：3.x版本只能运行在JVM8及以上</p><h3 id="本地构建nexus私服优势"><a href="#本地构建nexus私服优势" class="headerlink" title="本地构建nexus私服优势"></a>本地构建nexus私服优势</h3><p>本地内部仓库在本地构建nexus私服的好处有：</p><p>1）加速构建、稳定；<br>2）节省带宽、节省中央maven仓库的带宽；<br>3）控制和审计；<br>4）能够部署第三方构件；<br>5）可以建立本地内部仓库、可以建立公共仓库</p><h2 id="Nexus部署"><a href="#Nexus部署" class="headerlink" title="Nexus部署"></a>Nexus部署</h2><blockquote><p>nexus下载地址1: <a href="https://www.sonatype.com/download-oss-sonatype" target="_blank" rel="noopener">https://www.sonatype.com/download-oss-sonatype</a> (国内似乎打不开)</p><p>nexus下载地址2.<a href="https://help.sonatype.com/repomanager3/download/download-archives---repository-manager-3" target="_blank" rel="noopener">https://help.sonatype.com/repomanager3/download/download-archives---repository-manager-3</a></p><p>下载地址 <a href="http://download.sonatype.com/nexus/3/nexus-3.22.1-02-unix.tar.gz" target="_blank" rel="noopener">nexus-3.22.1-02-unix.tar.gz</a> 2020-04-16发布</p></blockquote><h3 id="解压nexus"><a href="#解压nexus" class="headerlink" title="解压nexus"></a>解压nexus</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp wang-200:&#x2F;data&#x2F;soft&#x2F;centos7&#x2F;nexus-3.22.1-02-unix.tar.gz &#x2F;opt&#x2F;src&#x2F; </span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;nexus-3.22.1-02-unix.tar.gz -C &#x2F;opt&#x2F;</span><br><span class="line">ln -s &#x2F;opt&#x2F;nexus-3.22.1-02 &#x2F;opt&#x2F;nexus</span><br><span class="line">ln -s &#x2F;opt&#x2F;nexus&#x2F;bin&#x2F;nexus &#x2F;etc&#x2F;init.d&#x2F;nexus</span><br><span class="line">cd &#x2F;opt&#x2F;nexus&#x2F;</span><br></pre></td></tr></table></figure><h3 id="启动nexus"><a href="#启动nexus" class="headerlink" title="启动nexus"></a>启动nexus</h3><h4 id="修改nexus启动内存大小"><a href="#修改nexus启动内存大小" class="headerlink" title="修改nexus启动内存大小"></a>修改nexus启动内存大小</h4><p>修改nexus启动参数 <code>bin/nexus.vmoptions</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-Xms2703m</span><br><span class="line">-Xmx2703m</span><br><span class="line">-XX:MaxDirectMemorySize&#x3D;2703m</span><br><span class="line">-XX:+UnlockDiagnosticVMOptions</span><br><span class="line">-XX:+LogVMOutput</span><br><span class="line">-XX:LogFile&#x3D;..&#x2F;sonatype-work&#x2F;nexus3&#x2F;log&#x2F;jvm.log</span><br><span class="line">-XX:-OmitStackTraceInFastThrow</span><br><span class="line">-Djava.net.preferIPv4Stack&#x3D;true</span><br><span class="line">-Dkaraf.home&#x3D;.</span><br><span class="line">-Dkaraf.base&#x3D;.</span><br><span class="line">-Dkaraf.etc&#x3D;etc&#x2F;karaf</span><br><span class="line">-Djava.util.logging.config.file&#x3D;etc&#x2F;karaf&#x2F;java.util.logging.properties</span><br><span class="line">-Dkaraf.data&#x3D;..&#x2F;sonatype-work&#x2F;nexus3</span><br><span class="line">-Dkaraf.log&#x3D;..&#x2F;sonatype-work&#x2F;nexus3&#x2F;log</span><br><span class="line">-Djava.io.tmpdir&#x3D;..&#x2F;sonatype-work&#x2F;nexus3&#x2F;tmp</span><br><span class="line">-Dkaraf.startLocalConsole&#x3D;false</span><br></pre></td></tr></table></figure><h4 id="用ROOT用户启动nexus"><a href="#用ROOT用户启动nexus" class="headerlink" title="用ROOT用户启动nexus"></a>用ROOT用户启动nexus</h4><p>可使用两个命令启动①./nexus start ②./nexus run(初次启动建议使用此命令,会显示启动日志)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-14 nexus]# vim bin&#x2F;nexus.rc </span><br><span class="line">run_as_user&#x3D;&quot;root&quot;</span><br><span class="line">[root@wang-14 nexus]# bin&#x2F;nexus run</span><br><span class="line">2020-06-12 14:01:49,100+0800 INFO  [jetty-main-1] *SYSTEM org.eclipse.jetty.server.Server - Started @143576ms</span><br><span class="line">2020-06-12 14:01:49,101+0800 INFO  [jetty-main-1] *SYSTEM org.sonatype.nexus.bootstrap.jetty.JettyServer - </span><br><span class="line">-------------------------------------------------</span><br><span class="line"></span><br><span class="line">Started Sonatype Nexus OSS 3.22.1-02</span><br><span class="line"></span><br><span class="line">-------------------------------------------------</span><br></pre></td></tr></table></figure><h3 id="登陆nexus"><a href="#登陆nexus" class="headerlink" title="登陆nexus"></a>登陆nexus</h3><p>访问：<a href="http://localhost:8081/" target="_blank" rel="noopener">http://localhost:8081/</a></p><p>使用Nexus 内置账户admin/admin123登陆：</p><p>点击右上角的Log in，输入账号和密码 登陆，如果登陆失败，注意提示密码在文件/opt/sonatype-work/nexus3/admin.password里</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfph49cxqnj31860degms.jpg" alt="image-20200612140858036" style="zoom:67%;max-width: 50%" /><p><strong>第一步：帮助向导</strong> This wizard will help you complete required setup tasks.</p><p><strong>第二步：修改密码</strong>（Please choose a password for the admin user）</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfphcs8pz2j31as0cejsi.jpg" alt="修改密码" style="zoom:67%;max-width: 50%" /><p><strong>第三部：配置免密访问</strong> Configure Anonymous Access</p><p>我设置的是不允许，因为有公司的代码，可以根据自己的需要</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfv4ogx6vtj31dy0d8gnl.jpg" alt="配置免密访问" style="zoom:67%;max-width: 50%;" /><p><strong>第四步：安装结束</strong> The setup tasks have been completed, enjoy using Nexus Repository Manager!</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfv4og0iv4j31o80kqjug.jpg" alt="image-20200612144114692" style="zoom:67%;max-width: 50%;" /><h2 id="添加用户"><a href="#添加用户" class="headerlink" title="添加用户"></a>添加用户</h2><p>创建本地用户</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfpk6t4pegj320r0u045y.jpg" alt="image-20200612155508580" style="zoom:67%;max-width: 60%;" /><p>填写用户信息</p><img src="/Users/wangzt/Library/Application%20Support/typora-user-images/image-20200612155733411.png" alt="image-20200612155733411" style="zoom:67%;max-width: 60%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggykm1js0ij317x0u07dc.jpg" alt="image-20200612155733411" style="zoom:67%;max-width: 60%;" /><p>Grant 可以选择用户权限：</p><ul><li>nx-admin – 管理员权限</li><li>nx-anonymous – 无差别用户权限，无法看到设置图标并且无法进入设置页面</li></ul><h2 id="Nexus仓库管理"><a href="#Nexus仓库管理" class="headerlink" title="Nexus仓库管理"></a>Nexus仓库管理</h2><h3 id="管理菜单"><a href="#管理菜单" class="headerlink" title="管理菜单"></a>管理菜单</h3><p>在管理菜单中主要是配置Blob Stores和Repositories，其中Blob Stores  是用来配置资源的保存位置的，可以将不同的远程资源保存到服务器的不同位置上(类似于Nexus 2的远程资源保存路径配置，在Nexus  3中进行了分组管理)，Repositories 则用来配置远程资源和本地资源，</p><h3 id="Nexus仓库类型介绍"><a href="#Nexus仓库类型介绍" class="headerlink" title="Nexus仓库类型介绍"></a>Nexus仓库类型介绍</h3><ul><li>hosted，本地仓库，通常我们会部署自己的构件到这一类型的仓库。比如公司的第二方库。</li></ul><ul><li>proxy，代理仓库，它们被用来代理远程的公共仓库，如maven中央仓库。</li></ul><ul><li>group，仓库组，用来合并多个hosted/proxy仓库，当你的项目希望在多个repository使用资源时就不需要多次引用了，只需要引用一个group即可。</li></ul><h3 id="管理本地仓库"><a href="#管理本地仓库" class="headerlink" title="管理本地仓库"></a>管理本地仓库</h3><p>我们前面讲到类型为hosted的为本地仓库，Nexus预定义了3个本地仓库，分别是Releases, Snapshots, 3rd Party. 分别讲一下这三个预置的仓库都是做什么用的:</p><p><strong>Releases:</strong> 这里存放我们自己项目中发布的构建, 通常是Release版本的, 比如我们自己做了一个FTP Server的项目, 生成的构件为ftpserver.war, 我们就可以把这个构建发布到Nexus的Releases本地仓库. 关于符合发布后面会有介绍.</p><p><strong>Snapshots:</strong>这个仓库非常的有用, 它的目的是让我们可以发布那些非release版本, 非稳定版本,  比如我们在trunk下开发一个项目,在正式release之前你可能需要临时发布一个版本给你的同伴使用, 因为你的同伴正在依赖你的模块开发,  那么这个时候我们就可以发布Snapshot版本到这个仓库, 你的同伴就可以通过简单的命令来获取和使用这个临时版本.</p><p><strong>3rd Party:</strong>顾名思义, 第三方库, 你可能会问不是有中央仓库来管理第三方库嘛,没错, 这里的是指可以让你添加自己的第三方库, 比如有些构件在中央仓库是不存在的. 比如你在中央仓库找不到Oracle 的JDBC驱动, 这个时候我们就需要自己添加到3rdparty仓库。 </p><h3 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h3><p>添加Repositories仓库</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfpk49nt3oj31ul0u0qcm.jpg" alt="image-20200612152955528" style="zoom:67%;max-width: 60%;" /><p>这里我们有三种 maven 仓库可选</p><img src="https://techlog.cn/article/list/images/c6700184e7c325ad24bb34260125342b.png?id=3378041&amp;v=1" alt="img" style="zoom:67%;max-width: 80%;" /><p>他们的区别是：</p><ol><li>proxy – 远程仓库的代理，当用户向这个仓库请求一个 artifact，他会先在本地查找，如果找不到的话，就会从远程仓库下载，然后返回给用户</li><li>hosted – 宿主仓库，用户可以 deploy 到 hosted 中，也可以手工上传构件到 hosted 里，在 central repository 是获取不到的，就需要手工上传到hosted里</li><li>group – 仓库组，在 maven 里没有这个概念，是 nexus 特有的。目的是将上述多个仓库聚合，对用户暴露统一的地址</li></ol><h4 id="创建-hosted-仓库"><a href="#创建-hosted-仓库" class="headerlink" title="创建 hosted 仓库"></a>创建 hosted 仓库</h4><p>这里我们创建两个 hosted 仓库</p><img src="https://techlog.cn/article/list/images/de32d62ee9699b8bc2966779534493c6.png?id=3378042&amp;v=1" alt="img" style="zoom:67%;max-width: 60%;" /><img src="https://techlog.cn/article/list/images/166263a194b8082adcccd0095f72bc83.png?id=3378043&amp;v=2" alt="img" style="zoom:67%;max-width: 60%;" /><p>他们分别用来管理正式版 jar 包和 SNAPSHOT 包，区别在于是否允许重复上传</p><h4 id="上传到nexus私服"><a href="#上传到nexus私服" class="headerlink" title="上传到nexus私服"></a>上传到nexus私服</h4><ol><li>修改Maven的settings.xml文件，加入认证机制</li></ol><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;settings xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;SETTINGS&#x2F;1.0.0&quot;</span><br><span class="line">xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;SETTINGS&#x2F;1.0.0</span><br><span class="line">https:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;settings-1.0.0.xsd&quot;&gt;</span><br><span class="line">&lt;servers&gt;</span><br><span class="line">&lt;server&gt;</span><br><span class="line">&lt;id&gt;releases&lt;&#x2F;id&gt;</span><br><span class="line">&lt;username&gt;admin&lt;&#x2F;username&gt;</span><br><span class="line">&lt;password&gt;admin123&lt;&#x2F;password&gt;</span><br><span class="line">&lt;&#x2F;server&gt;</span><br><span class="line">&lt;server&gt;</span><br><span class="line">&lt;id&gt;snapshots&lt;&#x2F;id&gt;</span><br><span class="line">&lt;username&gt;admin&lt;&#x2F;username&gt;</span><br><span class="line">&lt;password&gt;admin123&lt;&#x2F;password&gt;</span><br><span class="line">&lt;&#x2F;server&gt;</span><br><span class="line">&lt;&#x2F;servers&gt;</span><br><span class="line">&lt;&#x2F;settings&gt;</span><br></pre></td></tr></table></figure><p><strong>配置 pom.xml</strong></p><p>我们创建一个项目，然后配置 pom.xml</p><p>最重要的是 distributionManagement 节点的配置，引用我们在 settings.xml 中配置的 nexus 私服 id</p><p>配置中的 url 通过下图按钮处点击获取即可</p><img src="https://techlog.cn/article/list/images/5be309fcea685ded58269e1e43450dc3.png?id=3378045&amp;v=1" alt="img" style="zoom:67%;max-width: 80%;" /><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;distributionManagement&gt;</span><br><span class="line">&lt;repository&gt;</span><br><span class="line">&lt;id&gt;releases&lt;&#x2F;id&gt;</span><br><span class="line">&lt;name&gt;Release Repository&lt;&#x2F;name&gt;</span><br><span class="line">&lt;url&gt;http:&#x2F;&#x2F;localhost:8081&#x2F;repository&#x2F;java&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">&lt;&#x2F;repository&gt;</span><br><span class="line">&lt;snapshotRepository&gt;</span><br><span class="line">&lt;id&gt;snapshots&lt;&#x2F;id&gt;</span><br><span class="line">&lt;url&gt;http:&#x2F;&#x2F;localhost:8081&#x2F;repository&#x2F;java&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">&lt;&#x2F;snapshotRepository&gt;</span><br><span class="line">&lt;&#x2F;distributionManagement&gt;</span><br></pre></td></tr></table></figure><p><strong>打包上传</strong></p><p>接下来我们执行 mvn deploy -e 就可以实现打包上传了</p><p>通过页面，我们可以看到已经上传成功</p><p><img src="https://techlog.cn/article/list/images/604cbeffc7beebc5fefdaed563585d39.png?id=3378044&v=1" alt="img"></p><h4 id="创建repository仓库"><a href="#创建repository仓库" class="headerlink" title="创建repository仓库"></a>创建repository仓库</h4><blockquote><p>代理仓库<code>maven-central</code>的默认源地址为中央仓库<code>https://repo1.maven.org/maven2</code></p><p>阿里的源地址：<code>http://maven.aliyun.com/nexus/content/groups/public</code></p><p>阿里云mvn镜像指导 <code>https://maven.aliyun.com/mvn/guide</code></p></blockquote><p>添加repository仓库ali, 路径为 <a href="https://maven.aliyun.com/repository/public" target="_blank" rel="noopener">https://maven.aliyun.com/repository/public</a></p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfv4vbit3mj31fg0u0k1p.jpg" alt="添加repository仓库ali" style="zoom:67%;max-width: 60%;" /><p>如果需要认证，我们还要配置http认证</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfv4vgxotxj317807iq37.jpg" alt="配置http认证" style="zoom:67%;max-width: 60%;" /><h3 id="创建仓库组group"><a href="#创建仓库组group" class="headerlink" title="创建仓库组group"></a>创建仓库组group</h3><p>将仓库放入仓库组中，我们就不需要修改配置了，且可以集中管理</p><p>创建三个仓库组group：public; releases; snapshots</p><ul><li><a href="http://localhost:8081/repository/public/" target="_blank" rel="noopener">http://localhost:8081/repository/public/</a></li></ul><ul><li><a href="http://localhost:8081/repository/releases/" target="_blank" rel="noopener">http://localhost:8081/repository/releases/</a></li></ul><ul><li><a href="http://localhost:8081/repository/snapshots/" target="_blank" rel="noopener">http://localhost:8081/repository/snapshots/</a></li></ul><h4 id="添加组public-另外两个类似"><a href="#添加组public-另外两个类似" class="headerlink" title="添加组public,另外两个类似"></a>添加组public,另外两个类似</h4><p>public仓库组一般包含：releases,snapshots,thirdparty, central 四个项目</p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfv4zfto4fj319y0u0wnd.jpg" alt="添加组public" style="zoom:67%;max-width: 60%;" /><h2 id="Maven-配置Nexus-镜像"><a href="#Maven-配置Nexus-镜像" class="headerlink" title="Maven 配置Nexus 镜像"></a>Maven 配置Nexus 镜像</h2><p>　　　Nexus 镜像搭建起来后，那么如何在Maven中使用该镜像资源呢？  主要是通过调整maven配置来识别该镜像资源，从而将maven资源请求转发到该镜像资源库上。通常的做法为在当前用户的目录下.m2文件夹中创建settings.xml文件，其中指定mirror,如下所示：</p><pre><code>&lt;!-- 使用Mirror配置节可以强制所有包请求都会被转向内网Nexus服务器的地址 --&gt;  &lt;mirrors&gt;      &lt;mirror&gt;          &lt;id&gt;mirror&lt;/id&gt;          &lt;mirrorOf&gt;!snapshots,!releases&lt;/mirrorOf&gt;          &lt;name&gt;mirror&lt;/name&gt;          &lt;url&gt;http://localhost:8081/repository/public/&lt;/url&gt;      &lt;/mirror&gt;  &lt;/mirrors&gt;</code></pre><p>上述配置信息是所有的远程资源镜像的访问都会通过该镜像来访问，这也是大部分公司采用的方案，因为这样能够保证公司对远程资源的访问次数是有限的，公司内部的开发人员访问的都是公司内部的资源。开发人员不需要链接外网去下载远程镜像服务器上的资源，只需要该公司内部镜像去访问一次即可。当然了，mirrorOf可以指定为代理部分远程资源，详情见官方说明：<a href="http://maven.apache.org/guides/mini/guide-mirror-settings.html" target="_blank" rel="noopener">http://maven.apache.org/guides/mini/guide-mirror-settings.html</a></p><h3 id="Nexus-启用上传功能"><a href="#Nexus-启用上传功能" class="headerlink" title="Nexus 启用上传功能"></a>Nexus 启用上传功能</h3><p>　　团队有了私有的镜像服务器以后，团队内部代码的依赖也就不再需要源码依赖了，大家可以通过发布不同版本的jar到nexus镜像上来供调用者直接通过Maven下载使用，这样不同研发人员直接的依赖也就没有那么强了，大家可以基于已经发布的版本进行各自的开发。</p><p>　　那么如何发布个人的jar资源到团队内部镜像上呢？ </p><p>　　1. 在Nexus  中创建一个developer的角色，拥有的权利为【nx-repository-view-maven2-<em>-edit】和【nx-repository-view-maven2-</em>-add】权利，如果该角色将来可能还有nuget,npm相关上传权利，则将其权利改为【nx-repository-view-<em>-</em>-edit】和【nx-repository-view-<em>-</em>-add】权利。</p><p>　　2. 创建用户，用户拥有的角色为【nx-anonymous】和刚创建的【developer】角色。其中nx-anonymous角色是nexus默认自带的角色。</p><p>　　3. 在.m2文件夹下的settings.xml配置文件中增加<servers>的配置。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!-- 配置Maven服务器的账号信息，自动化部署的时候需要用到 --&gt;</span><br><span class="line">  &lt;servers&gt;</span><br><span class="line">    &lt;server&gt;</span><br><span class="line">      &lt;id&gt;server_id&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;!-- 使用公共的developer&#x2F;password账号进行日常的发布管理 --&gt;</span><br><span class="line">      &lt;username&gt;developer&lt;&#x2F;username&gt;</span><br><span class="line">      &lt;password&gt;password&lt;&#x2F;password&gt;</span><br><span class="line">    &lt;&#x2F;server&gt;</span><br><span class="line">  &lt;&#x2F;servers&gt;</span><br></pre></td></tr></table></figure><p>在需要上传jar资源的项目的pom.xml中增加<distributionManagement>配置，其中<strong>设置需要跟.m2文件夹下的settings.xml中下的id相同。需要指定nexus中配置的hosts Repository资源的地址。</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;distributionManagement&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;server_id&lt;&#x2F;id&gt;</span><br><span class="line">        &lt;name&gt;Nexus Release Repository&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;url&gt;http:&#x2F;&#x2F;nexus_ip:8081&#x2F;repository&#x2F;releases&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;repository&gt;</span><br><span class="line">&lt;&#x2F;distributionManagement&gt;</span><br></pre></td></tr></table></figure><p>通过maven 编译项目，并通过mvn deploy 来发布jar资源到团队内部的镜像服务器即可。</p><h3 id="Nexus-启用SNAPSHOTS"><a href="#Nexus-启用SNAPSHOTS" class="headerlink" title="Nexus 启用SNAPSHOTS"></a>Nexus 启用SNAPSHOTS</h3><p>　　团队内部在开发过程中为了相互可以互不影响的开发，需要时常将未稳定版的jar发布出来供团队其他人员调用，这时候建议使用SNAPSHOT版本，那么SNAPSHOT版本怎么发布到Nexus上呢。如果以当前的配置，发布SNAPSHOT过程会失败，因为Nexus默认是不启用SNAPSHOT的。那么怎么启用SNAPSHOT及如何上传SNAPSHOT版本资源呢？</p><p>　　启用SNAPSHOT的方式为在.m2文件夹下的settings.xml中增加<profile>设置</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!-- 这个默认配置决定了我们的Maven服务器开启snapshot配置，否则不能下载SNAPSHOTS的相关资源 --&gt; </span><br><span class="line">&lt;profiles&gt;</span><br><span class="line">   &lt;profile&gt;</span><br><span class="line">     &lt;activation&gt;</span><br><span class="line">       &lt;activeByDefault&gt;true&lt;&#x2F;activeByDefault&gt;</span><br><span class="line">     &lt;&#x2F;activation&gt;</span><br><span class="line">     &lt;repositories&gt;</span><br><span class="line">       &lt;repository&gt;</span><br><span class="line">         &lt;id&gt;repository_name&lt;&#x2F;id&gt;</span><br><span class="line">         &lt;name&gt;Nexus Public Repository&lt;&#x2F;name&gt;</span><br><span class="line">         &lt;url&gt;http:&#x2F;&#x2F;nexus_ip:8081&#x2F;repository&#x2F;maven-public&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">         &lt;releases&gt;</span><br><span class="line">           &lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">         &lt;&#x2F;releases&gt;</span><br><span class="line">         &lt;snapshots&gt;</span><br><span class="line">           &lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">           &lt;updatePolicy&gt;always&lt;&#x2F;updatePolicy&gt;</span><br><span class="line">         &lt;&#x2F;snapshots&gt;</span><br><span class="line">       &lt;&#x2F;repository&gt;</span><br><span class="line">     &lt;&#x2F;repositories&gt;</span><br><span class="line">   &lt;&#x2F;profile&gt;</span><br><span class="line"> &lt;&#x2F;profiles&gt;</span><br></pre></td></tr></table></figure><p>需要发布jar资源到团队内部镜像服务器的项目的pom.xml配置<distributionManagement>增加snapshot的支持，最终的pom.xml增加的<distributionManagement>如下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;distributionManagement&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;nexus-releases&lt;&#x2F;id&gt;</span><br><span class="line">        &lt;name&gt;Nexus Release Repository&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;url&gt;http:&#x2F;&#x2F;nexus_ip:8081&#x2F;repository&#x2F;yoyi-releases&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;repository&gt;</span><br><span class="line">    &lt;snapshotRepository&gt;</span><br><span class="line">        &lt;id&gt;nexus-snapshots&lt;&#x2F;id&gt;</span><br><span class="line">        &lt;name&gt;Nexus Snapshot Repository&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;url&gt;http:&#x2F;&#x2F;nexus_ip:8081&#x2F;repository&#x2F;yoyi-snapshots&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;snapshotRepository&gt;</span><br><span class="line">&lt;&#x2F;distributionManagement&gt;</span><br></pre></td></tr></table></figure><h3 id="settings-xml-文件"><a href="#settings-xml-文件" class="headerlink" title="settings.xml 文件"></a>settings.xml 文件</h3><p>配置.m2/settings.xml 就可以使用mvn了</p><p>这里是一个完整的例子，注意自己修改地址</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;settings xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;SETTINGS&#x2F;1.0.0&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">          xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;SETTINGS&#x2F;1.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;settings-1.0.0.xsd&quot;&gt;</span><br><span class="line">    &lt;mirrors&gt;</span><br><span class="line">        &lt;mirror&gt;</span><br><span class="line">            &lt;id&gt;mirror&lt;&#x2F;id&gt;</span><br><span class="line">            &lt;mirrorOf&gt;!snapshots,!releases&lt;&#x2F;mirrorOf&gt;</span><br><span class="line">            &lt;name&gt;mirror&lt;&#x2F;name&gt;</span><br><span class="line">            &lt;url&gt;http:&#x2F;&#x2F;localhost:8081&#x2F;repository&#x2F;public&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">        &lt;&#x2F;mirror&gt;</span><br><span class="line">    &lt;&#x2F;mirrors&gt;</span><br><span class="line">    &lt;servers&gt;</span><br><span class="line">        &lt;server&gt;</span><br><span class="line">            &lt;id&gt;central&lt;&#x2F;id&gt;</span><br><span class="line">            &lt;username&gt;user&lt;&#x2F;username&gt;</span><br><span class="line">            &lt;password&gt;user123&lt;&#x2F;password&gt;</span><br><span class="line">        &lt;&#x2F;server&gt;</span><br><span class="line">        &lt;server&gt;</span><br><span class="line">            &lt;id&gt;releases&lt;&#x2F;id&gt;</span><br><span class="line">            &lt;username&gt;user&lt;&#x2F;username&gt;</span><br><span class="line">            &lt;password&gt;user123&lt;&#x2F;password&gt;</span><br><span class="line">        &lt;&#x2F;server&gt;</span><br><span class="line">&lt;server&gt;</span><br><span class="line">            &lt;id&gt;snapshots&lt;&#x2F;id&gt;</span><br><span class="line">            &lt;username&gt;user&lt;&#x2F;username&gt;</span><br><span class="line">            &lt;password&gt;user123&lt;&#x2F;password&gt;</span><br><span class="line">        &lt;&#x2F;server&gt;</span><br><span class="line">    &lt;&#x2F;servers&gt;</span><br><span class="line">    &lt;profiles&gt;</span><br><span class="line">        &lt;profile&gt;</span><br><span class="line">            &lt;id&gt;nexus&lt;&#x2F;id&gt;</span><br><span class="line">            &lt;repositories&gt;</span><br><span class="line">                &lt;repository&gt;</span><br><span class="line">                    &lt;id&gt;central&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;url&gt;http:&#x2F;&#x2F;localhost:8081&#x2F;repository&#x2F;public&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">                    &lt;releases&gt;</span><br><span class="line">                        &lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">                    &lt;&#x2F;releases&gt;</span><br><span class="line">                    &lt;snapshots&gt;</span><br><span class="line">                        &lt;enabled&gt;false&lt;&#x2F;enabled&gt;</span><br><span class="line">                    &lt;&#x2F;snapshots&gt;</span><br><span class="line">                &lt;&#x2F;repository&gt;</span><br><span class="line">                &lt;repository&gt;</span><br><span class="line">                    &lt;id&gt;releases&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;url&gt;http:&#x2F;&#x2F;localhost:8081&#x2F;repository&#x2F;releases&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">                    &lt;releases&gt;</span><br><span class="line">                        &lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">                    &lt;&#x2F;releases&gt;</span><br><span class="line">                    &lt;snapshots&gt;</span><br><span class="line">                        &lt;enabled&gt;false&lt;&#x2F;enabled&gt;</span><br><span class="line">                    &lt;&#x2F;snapshots&gt;</span><br><span class="line">                &lt;&#x2F;repository&gt;</span><br><span class="line">                &lt;repository&gt;</span><br><span class="line">                    &lt;id&gt;snapshots&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;url&gt;http:&#x2F;&#x2F;localhost:8081&#x2F;repository&#x2F;snapshots&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">                    &lt;releases&gt;</span><br><span class="line">                        &lt;enabled&gt;false&lt;&#x2F;enabled&gt;</span><br><span class="line">                    &lt;&#x2F;releases&gt;</span><br><span class="line">                    &lt;snapshots&gt;</span><br><span class="line">                        &lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">                        &lt;updatePolicy&gt;always&lt;&#x2F;updatePolicy&gt;</span><br><span class="line">                    &lt;&#x2F;snapshots&gt;</span><br><span class="line">                &lt;&#x2F;repository&gt;</span><br><span class="line">            &lt;&#x2F;repositories&gt;</span><br><span class="line">            &lt;pluginRepositories&gt;</span><br><span class="line">                &lt;pluginRepository&gt;</span><br><span class="line">                    &lt;id&gt;central&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;url&gt;http:&#x2F;&#x2F;localhost:8081&#x2F;repository&#x2F;public&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">                    &lt;releases&gt;</span><br><span class="line">                        &lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">                    &lt;&#x2F;releases&gt;</span><br><span class="line">                    &lt;snapshots&gt;</span><br><span class="line">                        &lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">                    &lt;&#x2F;snapshots&gt;</span><br><span class="line">                &lt;&#x2F;pluginRepository&gt;</span><br><span class="line">                &lt;pluginRepository&gt;</span><br><span class="line">                    &lt;id&gt;releases&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;url&gt;http:&#x2F;&#x2F;localhost:8081&#x2F;repository&#x2F;releases&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">                    &lt;releases&gt;</span><br><span class="line">                        &lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">                    &lt;&#x2F;releases&gt;</span><br><span class="line">                    &lt;snapshots&gt;</span><br><span class="line">                        &lt;enabled&gt;false&lt;&#x2F;enabled&gt;</span><br><span class="line">                    &lt;&#x2F;snapshots&gt;</span><br><span class="line">                &lt;&#x2F;pluginRepository&gt;</span><br><span class="line">                &lt;pluginRepository&gt;</span><br><span class="line">                    &lt;id&gt;snapshots&lt;&#x2F;id&gt;</span><br><span class="line">                    &lt;url&gt;http:&#x2F;&#x2F;localhost:8081&#x2F;repository&#x2F;snapshots&#x2F;&lt;&#x2F;url&gt;</span><br><span class="line">                    &lt;releases&gt;</span><br><span class="line">                        &lt;enabled&gt;false&lt;&#x2F;enabled&gt;</span><br><span class="line">                    &lt;&#x2F;releases&gt;</span><br><span class="line">                    &lt;snapshots&gt;</span><br><span class="line">                        &lt;enabled&gt;true&lt;&#x2F;enabled&gt;</span><br><span class="line">                        &lt;updatePolicy&gt;always&lt;&#x2F;updatePolicy&gt;</span><br><span class="line">                    &lt;&#x2F;snapshots&gt;</span><br><span class="line">                &lt;&#x2F;pluginRepository&gt;</span><br><span class="line">            &lt;&#x2F;pluginRepositories&gt;</span><br><span class="line">        &lt;&#x2F;profile&gt;</span><br><span class="line">    &lt;&#x2F;profiles&gt;</span><br><span class="line">    &lt;activeProfiles&gt;</span><br><span class="line">        &lt;activeProfile&gt;nexus&lt;&#x2F;activeProfile&gt;</span><br><span class="line">    &lt;&#x2F;activeProfiles&gt;</span><br><span class="line">&lt;&#x2F;settings&gt;</span><br></pre></td></tr></table></figure><p>这样，你就可以通过 mvn clean install 命令引用第三方和公司本地jar包了</p><h3 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h3><p>​        说实话，nexus部署和使用挺简单的，但是因为以前对其操作少，都是拿来就用，所以理解不深。通过此次部署，对其jar包的引用顺序更深刻了。</p><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>​        感谢 龙潭斋 <a href="https://techlog.cn/article/list/10183220" target="_blank" rel="noopener">详解通过 Nexus3.x 搭建 Maven 私服</a></p><p>​        感谢 nexus官网 <a href="https://help.sonatype.com/repomanager3" target="_blank" rel="noopener">repomanager3使用文档</a></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
          <category> nexus </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> nexus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>五、生态-elk收集系统日志</title>
      <link href="/2020/06/10/%E4%BA%94%E3%80%81%E7%94%9F%E6%80%81-elk%E6%94%B6%E9%9B%86%E7%B3%BB%E7%BB%9F%E6%97%A5%E5%BF%97/"/>
      <url>/2020/06/10/%E4%BA%94%E3%80%81%E7%94%9F%E6%80%81-elk%E6%94%B6%E9%9B%86%E7%B3%BB%E7%BB%9F%E6%97%A5%E5%BF%97/</url>
      
        <content type="html"><![CDATA[<p><strong>特别鸣谢</strong> </p><p>​        filebeat 梦轻尘 <a href="https://www.cnblogs.com/uglyliu/p/12382214.html" target="_blank" rel="noopener">利用 Log-Pilot + Kafka+Elasticsearch + Kibana 搭建 kubernetes日志解决方案</a></p><p>​        国内镜像源 天凉好个秋  <a href="https://www.jianshu.com/p/9e291ca4d16d" target="_blank" rel="noopener">CentOS yum、docker、alpine apk、maven 国内镜像源</a></p><h2 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfn8lzlosbj310h0hxq5r.jpg" alt="elk架构图"></p><h2 id="部署elasticsearch"><a href="#部署elasticsearch" class="headerlink" title="部署elasticsearch"></a>部署elasticsearch</h2><blockquote><p> <a href="https://www.elastic.co/" target="_blank" rel="noopener">官网</a>  <a href="https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.8.6.tar.gz" target="_blank" rel="noopener">下载地址</a></p></blockquote><h3 id="下载安装包"><a href="#下载安装包" class="headerlink" title="下载安装包"></a>下载安装包</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget -o &#x2F;data&#x2F;soft&#x2F;centos7&#x2F;elasticsearch-6.8.3.tar.gz https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;downloads&#x2F;elasticsearch&#x2F;elasticsearch-6.8.3.tar.gz</span><br></pre></td></tr></table></figure><p>在wang-12上解压elasticsearch</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp wang-200:&#x2F;data&#x2F;soft&#x2F;centos7&#x2F;elasticsearch-6.8.3.tar.gz &#x2F;opt&#x2F;src&#x2F;elasticsearch-6.8.3.tar.gz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;elasticsearch-6.8.3.tar.gz -C &#x2F;opt&#x2F;</span><br><span class="line">ln -s &#x2F;opt&#x2F;elasticsearch-6.8.3 &#x2F;opt&#x2F;elasticsearch</span><br><span class="line">mkdir -p &#x2F;data&#x2F;elasticsearch&#x2F;&#123;data,logs&#125;</span><br><span class="line">cd &#x2F;opt&#x2F;elasticsearch</span><br></pre></td></tr></table></figure><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; config&#x2F;elasticsearch.yml</span><br><span class="line">cluster.name: es.od.com</span><br><span class="line">node.name: wang-12.host.com</span><br><span class="line">path.data: &#x2F;data&#x2F;elasticsearch&#x2F;data</span><br><span class="line">path.logs: &#x2F;data&#x2F;elasticsearch&#x2F;logs</span><br><span class="line">bootstrap.memory_lock: true</span><br><span class="line">network.host: 192.168.70.12</span><br><span class="line">http.port: 9200</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>修改jvm</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 elasticsearch]# vim config&#x2F;jvm.options </span><br><span class="line">-Xms512m  # 根据环境设置，-Xms和-Xmx设置为相同的值，推荐设置为机器内存的一半左右 </span><br><span class="line">-Xmx512m</span><br></pre></td></tr></table></figure><h3 id="创建普通用户"><a href="#创建普通用户" class="headerlink" title="创建普通用户"></a>创建普通用户</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">useradd -s &#x2F;bin&#x2F;bash -M es</span><br><span class="line">chown -R es.es &#x2F;opt&#x2F;elasticsearch-6.8.3  &#x2F;data&#x2F;elasticsearch&#x2F;</span><br></pre></td></tr></table></figure><p>文件描述符</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;es.conf</span><br><span class="line">es hard nofile 65536</span><br><span class="line">es soft fsize unlimited</span><br><span class="line">es hard memlock unlimited</span><br><span class="line">es soft memlock unlimited</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>调整内核参数</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sysctl -w vm.max_map_count&#x3D;262144</span><br><span class="line">echo &quot;vm.max_map_count&#x3D;262144&quot; &gt; &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure><h3 id="启动es"><a href="#启动es" class="headerlink" title="启动es"></a>启动es</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 elasticsearch]# su es -c &quot;&#x2F;opt&#x2F;elasticsearch&#x2F;bin&#x2F;elasticsearch -d&quot; </span><br><span class="line">[root@wang-12 elasticsearch]# netstat -luntp|grep 9200</span><br><span class="line">tcp6       0      0 192.168.70.12:9200      :::*                    LISTEN      124790&#x2F;java</span><br></pre></td></tr></table></figure><h3 id="调整ES日志模板"><a href="#调整ES日志模板" class="headerlink" title="调整ES日志模板"></a>调整ES日志模板</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -H &quot;Content-Type:application&#x2F;json&quot; -XPUT http:&#x2F;&#x2F;192.168.70.12:9200&#x2F;_template&#x2F;k8s -d &#39;&#123;</span><br><span class="line"> &quot;template&quot; : &quot;k8s*&quot;,</span><br><span class="line"> &quot;index_patterns&quot;: [&quot;k8s*&quot;], </span><br><span class="line"> &quot;settings&quot;: &#123;</span><br><span class="line">  &quot;number_of_shards&quot;: 5,</span><br><span class="line">  &quot;number_of_replicas&quot;: 0    </span><br><span class="line"> &#125;</span><br><span class="line">&#125;&#39;</span><br><span class="line"></span><br><span class="line"># 返回值 &#123;&quot;acknowledged&quot;:true&#125;</span><br></pre></td></tr></table></figure><blockquote><p>“number_of_replicas”: 0 生产为3份副本集，本es为单节点，不能配置副本集</p></blockquote><h2 id="部署Kafka"><a href="#部署Kafka" class="headerlink" title="部署Kafka"></a>部署Kafka</h2><p>在运维主机下载kafaka</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;archive.apache.org&#x2F;dist&#x2F;kafka&#x2F;2.2.0&#x2F;kafka_2.12-2.2.0.tgz -O &#x2F;data&#x2F;soft&#x2F;centos7&#x2F;kafka_2.12-2.2.0.tgz</span><br></pre></td></tr></table></figure><p>在wang-11解压kafaka</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp wang-200:&#x2F;data&#x2F;soft&#x2F;centos7&#x2F;kafka_2.12-2.2.0.tgz &#x2F;opt&#x2F;src&#x2F;</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;kafka_2.12-2.2.0.tgz -C &#x2F;opt&#x2F;</span><br><span class="line">ln -s &#x2F;opt&#x2F;kafka_2.12-2.2.0&#x2F; &#x2F;opt&#x2F;kafka</span><br><span class="line">mkdir &#x2F;data&#x2F;kafka&#x2F;logs -p</span><br><span class="line">cd &#x2F;opt&#x2F;kafka</span><br></pre></td></tr></table></figure><h3 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-11 kafka]# vim config&#x2F;server.properties</span><br><span class="line">log.dirs&#x3D;&#x2F;data&#x2F;kafka&#x2F;logs</span><br><span class="line">zookeeper.connect&#x3D;localhost:2181    # zk消息队列地址 </span><br><span class="line">log.flush.interval.messages&#x3D;10000</span><br><span class="line">log.flush.interval.ms&#x3D;1000</span><br><span class="line"># 最后两行追加</span><br><span class="line">delete.topic.enable&#x3D;true</span><br><span class="line">host.name&#x3D;wang-11.host.com</span><br></pre></td></tr></table></figure><h3 id="启动Kafka"><a href="#启动Kafka" class="headerlink" title="启动Kafka"></a>启动Kafka</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-11 kafka]# bin&#x2F;kafka-server-start.sh -daemon config&#x2F;server.properties</span><br><span class="line">[root@wang-11 kafka]# netstat -luntp|grep 9092</span><br><span class="line">tcp6       0      0 192.168.70.11:9092      :::*                    LISTEN      110908&#x2F;java</span><br></pre></td></tr></table></figure><h2 id="部署kafka-manager"><a href="#部署kafka-manager" class="headerlink" title="部署kafka-manager"></a>部署kafka-manager</h2><blockquote><p><a href="https://github.com/yahoo/kafka-manager" target="_blank" rel="noopener">官方github地址</a></p></blockquote><h3 id="准备镜像"><a href="#准备镜像" class="headerlink" title="准备镜像"></a>准备镜像</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull sheepkiller&#x2F;kafka-manager:latest</span><br><span class="line">docker tag 4e4a8c5dabab harbor.od.com&#x2F;infra&#x2F;kafka-manager:latest</span><br><span class="line">docker push harbor.od.com&#x2F;infra&#x2F;kafka-manager:latest</span><br></pre></td></tr></table></figure><h3 id="准备资源配置清单"><a href="#准备资源配置清单" class="headerlink" title="准备资源配置清单"></a>准备资源配置清单</h3><p>创建资源配置清单文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;kafka-manager</span><br><span class="line">cd &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;kafka-manager</span><br></pre></td></tr></table></figure><p><strong>deploy.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; deploy.yaml</span><br><span class="line">kind: Deployment</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: kafka-manager</span><br><span class="line">  namespace: infra</span><br><span class="line">  labels: </span><br><span class="line">    name: kafka-manager</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels: </span><br><span class="line">      name: kafka-manager</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels: </span><br><span class="line">        app: kafka-manager</span><br><span class="line">        name: kafka-manager</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: kafka-manager</span><br><span class="line">        image: harbor.od.com&#x2F;infra&#x2F;kafka-manager:latest</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 9000</span><br><span class="line">          protocol: TCP</span><br><span class="line">        env:</span><br><span class="line">        - name: ZK_HOSTS</span><br><span class="line">          value: zk1.od.com:2181</span><br><span class="line">        - name: APPLICATION_SECRET</span><br><span class="line">          value: letmein</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor</span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      securityContext: </span><br><span class="line">        runAsUser: 0</span><br><span class="line">      schedulerName: default-scheduler</span><br><span class="line">  strategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate: </span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">      maxSurge: 1</span><br><span class="line">  revisionHistoryLimit: 7</span><br><span class="line">  progressDeadlineSeconds: 600</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>svc.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; svc.yaml</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata: </span><br><span class="line">  name: kafka-manager</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 9000</span><br><span class="line">    targetPort: 9000</span><br><span class="line">  selector: </span><br><span class="line">    app: kafka-manager</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>ingress.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; ingress.yaml</span><br><span class="line">kind: Ingress</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata: </span><br><span class="line">  name: kafka-manager</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: km.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend: </span><br><span class="line">          serviceName: kafka-manager</span><br><span class="line">          servicePort: 9000</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单"><a href="#应用资源配置清单" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;kafka-manager&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;kafka-manager&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;kafka-manager&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><h3 id="web界面管理"><a href="#web界面管理" class="headerlink" title="web界面管理"></a>web界面管理</h3><p><a href="http://km.od.com/" target="_blank" rel="noopener">http://km.od.com/</a></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfmyhj8ku6j31cg0h6q4m.jpg" alt="kafka-manager展示界面"></p><p>添加集群</p><p>名称：kafka-od,     地址：zk1.od.com:2181</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfmyobsqnzj31ey0lgtag.jpg" alt="添加集群"></p><p>查看集群kafka-od信息</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfmys345e1j31he0kmwgc.jpg" alt="集群kafka-od信息"></p><h2 id="部署filebeat"><a href="#部署filebeat" class="headerlink" title="部署filebeat"></a>部署filebeat</h2><h3 id="下载镜像-二选一"><a href="#下载镜像-二选一" class="headerlink" title="下载镜像(二选一)"></a>下载镜像(二选一)</h3><blockquote><p>github地址：<a href="https://github.com/AliyunContainerService/log-pilot" target="_blank" rel="noopener">https://github.com/AliyunContainerService/log-pilot</a> </p><p>log-pilot官方介绍：<a href="https://yq.aliyun.com/articles/674327" target="_blank" rel="noopener">https://yq.aliyun.com/articles/674327</a> </p><p>log-pilot官方搭建：<a href="https://yq.aliyun.com/articles/674361?spm=a2c4e.11153940.0.0.21ae21c3mTKwWS" target="_blank" rel="noopener">https://yq.aliyun.com/articles/674361?spm=a2c4e.11153940.0.0.21ae21c3mTKwWS</a> </p></blockquote><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull registry.cn-hangzhou.aliyuncs.com&#x2F;acs&#x2F;log-pilot:0.9.7-filebeat</span><br><span class="line">docker tag 10a688e1229a harbor.od.com&#x2F;public&#x2F;log-pilot:0.9.7-filebeat</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;log-pilot:0.9.7-filebeat</span><br></pre></td></tr></table></figure><h3 id="自定义镜像-推荐，二选一"><a href="#自定义镜像-推荐，二选一" class="headerlink" title="自定义镜像(推荐，二选一)"></a>自定义镜像(推荐，二选一)</h3><p>修改log-pilot源码使其可以收集多行日志(以日期开头，刑如2020-02-29)</p><h4 id="拉取master的代码"><a href="#拉取master的代码" class="headerlink" title="拉取master的代码"></a>拉取master的代码</h4><p>一开始用的分支v0.9.5，但是报错了，没出结果</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;data&#x2F;soft&#x2F;docker</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;AliyunContainerService&#x2F;log-pilot.git</span><br><span class="line">cd log-pilot</span><br><span class="line">git tag</span><br><span class="line"># git checkout v0.9.5  #指定v0.9.5这个版本</span><br></pre></td></tr></table></figure><h4 id="修改filebeat模板"><a href="#修改filebeat模板" class="headerlink" title="修改filebeat模板"></a>修改filebeat模板</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 log-pilot]# vim assets&#x2F;filebeat&#x2F;filebeat.tpl</span><br><span class="line"></span><br><span class="line">&#123;&#123;range .configList&#125;&#125;</span><br><span class="line">- type: log</span><br><span class="line">  enabled: true</span><br><span class="line">  paths:</span><br><span class="line">      - &#123;&#123; .HostDir &#125;&#125;&#x2F;&#123;&#123; .File &#125;&#125;</span><br><span class="line">  multiline.pattern: &#39;^[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]&#39; #新增正则条件，以日期开头</span><br><span class="line">  multiline.negate: true                                           #新增</span><br><span class="line">  multiline.match: after                                           #新增</span><br><span class="line">  multiline.max_lines: 10000                                       #新增</span><br><span class="line">  scan_frequency: 10s</span><br><span class="line">  fields_under_root: true</span><br><span class="line">  &#123;&#123;if .Stdout&#125;&#125;</span><br><span class="line">  docker-json: true</span><br><span class="line">  &#123;&#123;end&#125;&#125;</span><br><span class="line">  &#123;&#123;if eq .Format &quot;json&quot;&#125;&#125;</span><br><span class="line">  json.keys_under_root: true</span><br><span class="line">  &#123;&#123;end&#125;&#125;</span><br><span class="line">  fields:</span><br><span class="line">      &#123;&#123;range $key, $value :&#x3D; .Tags&#125;&#125;</span><br><span class="line">      &#123;&#123; $key &#125;&#125;: &#123;&#123; $value &#125;&#125;</span><br><span class="line">      &#123;&#123;end&#125;&#125;</span><br><span class="line">      &#123;&#123;range $key, $value :&#x3D; $.container&#125;&#125;</span><br><span class="line">      &#123;&#123; $key &#125;&#125;: &#123;&#123; $value &#125;&#125;</span><br><span class="line">      &#123;&#123;end&#125;&#125;</span><br><span class="line">  tail_files: false</span><br><span class="line">  close_inactive: 2h</span><br><span class="line">  close_eof: false</span><br><span class="line">  close_removed: true</span><br><span class="line">  clean_removed: true</span><br><span class="line">  close_renamed: false</span><br><span class="line"></span><br><span class="line">&#123;&#123;end&#125;&#125;</span><br></pre></td></tr></table></figure><h4 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed -i &quot;s&amp;log-pilot:latest&amp;harbor.od.com&#x2F;infra&#x2F;log-pilot:v1.0.1-filebeat&amp;&quot; build-image.sh</span><br><span class="line"># sed -i &quot;s&amp;apk update&amp;sed -i &#39;s&#x2F;dl-cdn.alpinelinux.org&#x2F;mirrors.aliyun.com&#x2F;g&#39; &#x2F;etc&#x2F;apk&#x2F;repositories \&amp;\&amp; apk update&amp;&quot; Dockerfile.filebeat </span><br><span class="line">.&#x2F;build-image.sh</span><br><span class="line">docker push harbor.od.com&#x2F;infra&#x2F;log-pilot:v1.0.1-filebeat</span><br><span class="line"></span><br><span class="line">kubectl create secret docker-registry harbor --docker-server&#x3D;harbor.od.com --docker-username&#x3D;harbor --docker-password&#x3D;Harbo12345 -n kube-system</span><br></pre></td></tr></table></figure><h3 id="配置资源清单"><a href="#配置资源清单" class="headerlink" title="配置资源清单"></a>配置资源清单</h3><p>创建文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;log-pilot</span><br></pre></td></tr></table></figure><p><strong>ds.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;log-pilot&#x2F;ds.yaml</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: log-pilot</span><br><span class="line">  labels:</span><br><span class="line">    app: log-pilot</span><br><span class="line">  # 设置期望部署的namespace</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  updateStrategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: log-pilot</span><br><span class="line">      annotations:</span><br><span class="line">        scheduler.alpha.kubernetes.io&#x2F;critical-pod: &#39;&#39;</span><br><span class="line">    spec:</span><br><span class="line">      # 是否允许部署到Master节点上</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: node-role.kubernetes.io&#x2F;master</span><br><span class="line">        effect: NoSchedule</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor</span><br><span class="line">      containers:</span><br><span class="line">      - name: log-pilot</span><br><span class="line">        # 版本请参考https:&#x2F;&#x2F;github.com&#x2F;AliyunContainerService&#x2F;log-pilot&#x2F;releases</span><br><span class="line">        image: harbor.od.com&#x2F;infra&#x2F;log-pilot:v1.0.1-filebeat</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            memory: 500Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 200m</span><br><span class="line">            memory: 200Mi</span><br><span class="line">        env:</span><br><span class="line">          - name: &quot;NODE_NAME&quot;</span><br><span class="line">            valueFrom:</span><br><span class="line">              fieldRef:</span><br><span class="line">                fieldPath: spec.nodeName</span><br><span class="line">          - name: &quot;LOGGING_OUTPUT&quot;</span><br><span class="line">            value: &quot;kafka&quot;</span><br><span class="line">          - name: &quot;KAFKA_BROKERS&quot;</span><br><span class="line">            value: &quot;192.168.70.11:9092&quot;</span><br><span class="line">          - name: &quot;NODE_NAME&quot;</span><br><span class="line">            valueFrom:</span><br><span class="line">              fieldRef:</span><br><span class="line">                fieldPath: spec.nodeName</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: sock</span><br><span class="line">          mountPath: &#x2F;var&#x2F;run&#x2F;docker.sock</span><br><span class="line">        - name: root</span><br><span class="line">          mountPath: &#x2F;host</span><br><span class="line">          readOnly: true</span><br><span class="line">        - name: varlib</span><br><span class="line">          mountPath: &#x2F;var&#x2F;lib&#x2F;filebeat</span><br><span class="line">        - name: varlog</span><br><span class="line">          mountPath: &#x2F;var&#x2F;log&#x2F;filebeat</span><br><span class="line">        - name: localtime</span><br><span class="line">          mountPath: &#x2F;etc&#x2F;localtime</span><br><span class="line">          readOnly: true</span><br><span class="line">        livenessProbe:</span><br><span class="line">          failureThreshold: 3</span><br><span class="line">          exec:</span><br><span class="line">            command:</span><br><span class="line">            - &#x2F;pilot&#x2F;healthz</span><br><span class="line">          initialDelaySeconds: 10</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">          successThreshold: 1</span><br><span class="line">          timeoutSeconds: 2</span><br><span class="line">        securityContext:</span><br><span class="line">          capabilities:</span><br><span class="line">            add:</span><br><span class="line">            - SYS_ADMIN</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      volumes:</span><br><span class="line">      - name: sock</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &#x2F;var&#x2F;run&#x2F;docker.sock</span><br><span class="line">      - name: root</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &#x2F;</span><br><span class="line">      - name: varlib</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &#x2F;var&#x2F;lib&#x2F;filebeat</span><br><span class="line">          type: DirectoryOrCreate</span><br><span class="line">      - name: varlog</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &#x2F;var&#x2F;log&#x2F;filebeat</span><br><span class="line">          type: DirectoryOrCreate</span><br><span class="line">      - name: localtime</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &#x2F;etc&#x2F;localtime</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="执行资源配置清单"><a href="#执行资源配置清单" class="headerlink" title="执行资源配置清单"></a>执行资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;log-pilot&#x2F;ds.yaml</span><br></pre></td></tr></table></figure><h3 id="收集服务日志"><a href="#收集服务日志" class="headerlink" title="收集服务日志"></a>收集服务日志</h3><p>在dubbo-demo-consumer的deploy.yaml 添加环境变量</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 yaml]# vim test&#x2F;dubbo-demo-consumer&#x2F;deploy.yaml </span><br><span class="line">        env:</span><br><span class="line">        - name: aliyun_logs_k8s-out  #当然如果你不想使用aliyun这个关键字，Log-Pilot 也提供了环境变量 PILOT_LOG_PREFIX可以指定自己的声明式日志配置前缀，比如 PILOT_LOG_PREFIX: &quot;aliyun,custom&quot;，最好是和官方一致，省去多余的配置</span><br><span class="line">          value: &quot;stdout&quot;                 #需要收集的日志路径</span><br><span class="line">        - name: aliyun_logs_k8s-out_tags  #定义一个tag</span><br><span class="line">          value: &quot;topic&#x3D;k8s-fb-test-out&quot;  #kafka topic的名字，这个定义是关键，不定义这个，日志是无法输出到kafka内的         </span><br><span class="line"></span><br><span class="line">[root@wang-200 yaml]# kubectl apply -f test&#x2F;dubbo-demo-consumer&#x2F;deploy.yaml</span><br></pre></td></tr></table></figure><p><strong>使用Podpreset收集日志(有兴趣自己研究)</strong></p><h3 id="验证Kafka"><a href="#验证Kafka" class="headerlink" title="验证Kafka"></a>验证Kafka</h3><p>通过kafka-manager查看索引  <a href="http://km.od.com/clusters/kafka-od/topics" target="_blank" rel="noopener">http://km.od.com/clusters/kafka-od/topics</a></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfnghhmjcwj31q60hq40z.jpg" alt="索引k8s-fb-tset-out注册成功"></p><h2 id="部署logstash"><a href="#部署logstash" class="headerlink" title="部署logstash"></a>部署logstash</h2><h3 id="准备镜像-1"><a href="#准备镜像-1" class="headerlink" title="准备镜像"></a>准备镜像</h3><p>在运维主机拉取镜像</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull logstash:6.8.3</span><br><span class="line">docker tag 972bf55ad27b harbor.od.com&#x2F;infra&#x2F;logstash:v6.8.3</span><br><span class="line">docker push harbor.od.com&#x2F;infra&#x2F;logstash:v6.8.3</span><br></pre></td></tr></table></figure><h3 id="准备配置文件"><a href="#准备配置文件" class="headerlink" title="准备配置文件"></a>准备配置文件</h3><p>在运维主机创建文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir &#x2F;etc&#x2F;logstash&#x2F;</span><br></pre></td></tr></table></figure><p>准备测试环境配置文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;logstash&#x2F;logstash-test.conf</span><br><span class="line">input &#123;</span><br><span class="line">  kafka &#123;</span><br><span class="line">    bootstrap_servers &#x3D;&gt; &quot;192.168.70.11:9092&quot;</span><br><span class="line">    client_id &#x3D;&gt; &quot;192.168.70.200&quot;</span><br><span class="line">    consumer_threads &#x3D;&gt; 4</span><br><span class="line">    group_id &#x3D;&gt; &quot;k8s_test&quot;               # 为test组</span><br><span class="line">    topics_pattern &#x3D;&gt; &quot;k8s-fb-test-.*&quot;   # 只收集k8s-fb-test开头的topics</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">filter &#123;</span><br><span class="line">  json &#123;</span><br><span class="line">    source &#x3D;&gt; &quot;message&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    hosts &#x3D;&gt; [&quot;192.168.70.12:9200&quot;]</span><br><span class="line">    index &#x3D;&gt; &quot;k8s-test-%&#123;+YYYY.MM.DD&#125;&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>准备生产环境配置文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;logstash&#x2F;logstash-prod.conf</span><br><span class="line">input &#123;</span><br><span class="line">  kafka &#123;</span><br><span class="line">    bootstrap_servers &#x3D;&gt; &quot;192.168.70.11:9092&quot;</span><br><span class="line">    client_id &#x3D;&gt; &quot;192.168.70.200&quot;</span><br><span class="line">    consumer_threads &#x3D;&gt; 4</span><br><span class="line">    group_id &#x3D;&gt; &quot;k8s_prod&quot;                   </span><br><span class="line">    topics_pattern &#x3D;&gt; &quot;k8s-fb-prod-.*&quot; </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">filter &#123;</span><br><span class="line">  json &#123;</span><br><span class="line">    source &#x3D;&gt; &quot;message&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    hosts &#x3D;&gt; [&quot;192.168.70.12:9200&quot;]</span><br><span class="line">    index &#x3D;&gt; “k8s-prod-%&#123;+YYYY.MM.DD&#125;&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="测试logstash启动"><a href="#测试logstash启动" class="headerlink" title="测试logstash启动"></a>测试logstash启动</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# docker run -d --restart&#x3D;always --name logstash-test -v &#x2F;etc&#x2F;logstash:&#x2F;etc&#x2F;logstash  harbor.od.com&#x2F;infra&#x2F;logstash:v6.8.3 -f &#x2F;etc&#x2F;logstash&#x2F;logstash-test.conf</span><br><span class="line">[root@wang-200 ~]# docker ps -a|grep logstash</span><br><span class="line">53a32e867f2e        harbor.od.com&#x2F;infra&#x2F;logstash:v6.8.3                 &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;dock…&quot;   10 seconds ago      Up 9 seconds           5044&#x2F;tcp, 9600&#x2F;tcp          logstash-test</span><br></pre></td></tr></table></figure><h3 id="查看es是否接收数据"><a href="#查看es是否接收数据" class="headerlink" title="查看es是否接收数据"></a>查看es是否接收数据</h3><p>要等一会儿，不要着急，等它多产生一点数据。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;192.168.70.12:9200&#x2F;_cat&#x2F;indices?v</span><br></pre></td></tr></table></figure><h3 id="生产logstash启动"><a href="#生产logstash启动" class="headerlink" title="生产logstash启动"></a>生产logstash启动</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# docker run -d --restart&#x3D;always --name logstash-prod -v &#x2F;etc&#x2F;logstash:&#x2F;etc&#x2F;logstash  harbor.od.com&#x2F;infra&#x2F;logstash:v6.8.3 -f &#x2F;etc&#x2F;logstash&#x2F;logstash-prod.conf</span><br></pre></td></tr></table></figure><h2 id="部署kibana"><a href="#部署kibana" class="headerlink" title="部署kibana"></a>部署kibana</h2><h3 id="准备镜像-2"><a href="#准备镜像-2" class="headerlink" title="准备镜像"></a>准备镜像</h3><p>运维主机wang-200.host.com上</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull kibana:6.8.6</span><br><span class="line">docker tag adfab5632ef4 harbor.od.com&#x2F;infra&#x2F;kibana:v6.8.6</span><br><span class="line">docker push harbor.od.com&#x2F;infra&#x2F;kibana:v6.8.6</span><br></pre></td></tr></table></figure><h3 id="准备资源配置清单-1"><a href="#准备资源配置清单-1" class="headerlink" title="准备资源配置清单"></a>准备资源配置清单</h3><p>创建资源配置清单文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;kibana</span><br><span class="line">cd &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;kibana</span><br></pre></td></tr></table></figure><p>deploy.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;kibana&#x2F;deploy.yaml</span><br><span class="line">kind: Deployment</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: kibana</span><br><span class="line">  namespace: infra</span><br><span class="line">  labels: </span><br><span class="line">    name: kibana</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels: </span><br><span class="line">      name: kibana</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels: </span><br><span class="line">        app: kibana</span><br><span class="line">        name: kibana</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: kibana</span><br><span class="line">        image: harbor.od.com&#x2F;infra&#x2F;kibana:v6.8.6</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 5601</span><br><span class="line">          protocol: TCP</span><br><span class="line">        env:</span><br><span class="line">        - name: ELASTICSEARCH_URL</span><br><span class="line">          value: http:&#x2F;&#x2F;192.168.70.12:9200</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor</span><br><span class="line">      securityContext: </span><br><span class="line">        runAsUser: 0</span><br><span class="line">  strategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate: </span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">      maxSurge: 1</span><br><span class="line">  revisionHistoryLimit: 7</span><br><span class="line">  progressDeadlineSeconds: 600</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>svc.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;kibana&#x2F;svc.yaml</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata: </span><br><span class="line">  name: kibana</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 5601</span><br><span class="line">    targetPort: 5601</span><br><span class="line">  selector: </span><br><span class="line">    app: kibana</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>ingress.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;kibana&#x2F;ingress.yaml</span><br><span class="line">kind: Ingress</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata: </span><br><span class="line">  name: kibana</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: kibana.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend: </span><br><span class="line">          serviceName: kibana</span><br><span class="line">          servicePort: 5601</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="执行资源配置清单-1"><a href="#执行资源配置清单-1" class="headerlink" title="执行资源配置清单"></a>执行资源配置清单</h3><p>执行命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;kibana&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;kibana&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;kibana&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><h3 id="web展示"><a href="#web展示" class="headerlink" title="web展示"></a>web展示</h3><p><a href="http://kibana.od.com/" target="_blank" rel="noopener">http://kibana.od.com/</a></p><p><strong>添加索引k8s-test-*</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfngjil5q0j32180qa7c9.jpg" alt="添加索引k8s-test-*"></p><p><strong>elasticsearch收集的数据</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfngkq5zgsj321o0twaka.jpg" alt="elasticsearch收集的数据"></p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 老男孩 </category>
          
          <category> 实战交付 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 老男孩 </tag>
            
            <tag> 实战交付 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>四、生态-prometheus系统监控</title>
      <link href="/2020/06/09/%E5%9B%9B%E3%80%81%E7%94%9F%E6%80%81-prometheus%E7%B3%BB%E7%BB%9F%E7%9B%91%E6%8E%A7/"/>
      <url>/2020/06/09/%E5%9B%9B%E3%80%81%E7%94%9F%E6%80%81-prometheus%E7%B3%BB%E7%BB%9F%E7%9B%91%E6%8E%A7/</url>
      
        <content type="html"><![CDATA[<h2 id="Prometheus介绍"><a href="#Prometheus介绍" class="headerlink" title="Prometheus介绍"></a>Prometheus介绍</h2><p>​        由于docker容器的特殊性，传统的zabbix无法对k8s集群内的docker状态进行监控，所以需要使用prometheus来进行监控：</p><h3 id="什么是Prometheus"><a href="#什么是Prometheus" class="headerlink" title="什么是Prometheus?"></a>什么是Prometheus?</h3><p>Prometheus是由SoundCloud开发的开源监控报警系统和时序列数据库(TSDB)。Prometheus使用Go语言开发，是Google BorgMon监控系统的开源版本。<br> 2016年由Google发起Linux基金会旗下的原生云基金会(Cloud Native Computing Foundation), 将Prometheus纳入其下第二大开源项目。<br> Prometheus目前在开源社区相当活跃。<br> Prometheus和Heapster(Heapster是K8S的一个子项目，用于获取集群的性能数据。)相比功能更完善、更全面。Prometheus性能也足够支撑上万台规模的集群。</p><h3 id="Prometheus的特点"><a href="#Prometheus的特点" class="headerlink" title="Prometheus的特点"></a>Prometheus的特点</h3><ul><li><ul><li>多维度数据模型。</li><li>灵活的查询语言。</li><li>不依赖分布式存储，单个服务器节点是自主的。</li><li>通过基于HTTP的pull方式采集时序数据。</li><li>可以通过中间网关进行时序列数据推送。</li><li>通过服务发现或者静态配置来发现目标服务对象。</li><li>支持多种多样的图表和界面展示，比如Grafana等。</li></ul></li></ul><h3 id="prometheus架构图"><a href="#prometheus架构图" class="headerlink" title="prometheus架构图"></a>prometheus架构图</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfon4fvspsj311j0mjdjb.jpg" alt="prometheus架构图"></p><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>Prometheus的基本原理是通过HTTP协议周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口就可以接入监控。不需要任何SDK或者其他的集成过程。这样做非常适合做虚拟化环境监控系统，比如VM、Docker、Kubernetes等。输出被监控组件信息的HTTP接口被叫做exporter  。目前互联网公司常用的组件大部分都有exporter可以直接使用，比如Varnish、Haproxy、Nginx、MySQL、Linux系统信息(包括磁盘、内存、CPU、网络等等)。</p><h3 id="服务过程"><a href="#服务过程" class="headerlink" title="服务过程"></a>服务过程</h3><ul><li>Prometheus  Daemon负责定时去目标上抓取metrics(指标)数据，每个抓取目标需要暴露一个http服务的接口给它定时抓取。Prometheus支持通过配置文件、文本文件、Zookeeper、Consul、DNS SRV  Lookup等方式指定抓取目标。Prometheus采用PULL的方式进行监控，即服务器可以直接通过目标PULL数据或者间接地通过中间网关来Push数据。</li><li>Prometheus在本地存储抓取的所有数据，并通过一定规则进行清理和整理数据，并把得到的结果存储到新的时间序列中。</li><li>Prometheus通过PromQL和其他API可视化地展示收集的数据。Prometheus支持很多方式的图表可视化，例如Grafana、自带的Promdash以及自身提供的模版引擎等等。Prometheus还提供HTTP API的查询方式，自定义所需要的输出。</li><li>PushGateway支持Client主动推送metrics到PushGateway，而Prometheus只是定时去Gateway上抓取数据。</li><li>Alertmanager是独立于Prometheus的一个组件，可以支持Prometheus的查询语句，提供十分灵活的报警方式。</li></ul><h3 id="三大套件"><a href="#三大套件" class="headerlink" title="三大套件"></a>三大套件</h3><ul><li>Server 主要负责数据采集和存储，提供PromQL查询语言的支持。</li><li>Alertmanager 警告管理器，用来进行报警。</li><li>Push Gateway 支持临时性Job主动推送指标的中间网关。</li></ul><p>prometheus不同于zabbix，没有agent，使用的是针对不同服务的exporter：</p><p>prometheus官网：<a href="https://prometheus.io/" target="_blank" rel="noopener">官网地址</a></p><p>正常情况下，监控k8s集群及node，pod，常用的exporter有四个：</p><ul><li><strong>kube-state-metrics – 收集k8s集群master&amp;etcd等基本状态信息</strong></li><li><strong>node-exporter      – 收集k8s集群node信息</strong></li><li><strong>cadvisor           – 收集k8s集群docker容器内部使用资源信息</strong></li><li><strong>blackbox-exporte   – 收集k8s集群docker容器服务是否存活</strong></li></ul><p>接下来逐一创建以上exporter：</p><p>下载docker镜像，准备资源配置清单，应用资源配置清单：</p><h2 id="kube-state-metrics"><a href="#kube-state-metrics" class="headerlink" title="kube-state-metrics"></a>kube-state-metrics</h2><h3 id="下载docker镜像"><a href="#下载docker镜像" class="headerlink" title="下载docker镜像"></a>下载docker镜像</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull quay.io&#x2F;coreos&#x2F;kube-state-metrics:v1.5.0</span><br><span class="line">docker tag 91599517197a harbor.od.com&#x2F;public&#x2F;kube-state-metrics:v1.5.0</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;kube-state-metrics:v1.5.0</span><br></pre></td></tr></table></figure><h3 id="准备资源配置清单"><a href="#准备资源配置清单" class="headerlink" title="准备资源配置清单"></a>准备资源配置清单</h3><p>创建资源配置清单文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;kube-state-metrics</span><br></pre></td></tr></table></figure><p>rbac.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;kube-state-metrics&#x2F;rbac.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">    kubernetes.io&#x2F;cluster-service: &quot;true&quot;</span><br><span class="line">  name: kube-state-metrics</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">    kubernetes.io&#x2F;cluster-service: &quot;true&quot;</span><br><span class="line">  name: kube-state-metrics</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - configmaps</span><br><span class="line">  - secrets</span><br><span class="line">  - nodes</span><br><span class="line">  - pods</span><br><span class="line">  - services</span><br><span class="line">  - resourcequotas</span><br><span class="line">  - replicationcontrollers</span><br><span class="line">  - limitranges</span><br><span class="line">  - persistentvolumeclaims</span><br><span class="line">  - persistentvolumes</span><br><span class="line">  - namespaces</span><br><span class="line">  - endpoints</span><br><span class="line">  verbs:</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - policy</span><br><span class="line">  resources:</span><br><span class="line">  - poddisruptionbudgets</span><br><span class="line">  verbs:</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - extensions</span><br><span class="line">  resources:</span><br><span class="line">  - daemonsets</span><br><span class="line">  - deployments</span><br><span class="line">  - replicasets</span><br><span class="line">  verbs:</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - apps</span><br><span class="line">  resources:</span><br><span class="line">  - statefulsets</span><br><span class="line">  verbs:</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - batch</span><br><span class="line">  resources:</span><br><span class="line">  - cronjobs</span><br><span class="line">  - jobs</span><br><span class="line">  verbs:</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - autoscaling</span><br><span class="line">  resources:</span><br><span class="line">  - horizontalpodautoscalers</span><br><span class="line">  verbs:</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">    kubernetes.io&#x2F;cluster-service: &quot;true&quot;</span><br><span class="line">  name: kube-state-metrics</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: kube-state-metrics</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: kube-state-metrics</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>deploy.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;kube-state-metrics&#x2F;deploy.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    deployment.kubernetes.io&#x2F;revision: &quot;2&quot;</span><br><span class="line">  labels:</span><br><span class="line">    grafanak8sapp: &quot;true&quot;</span><br><span class="line">    app: kube-state-metrics</span><br><span class="line">  name: kube-state-metrics</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      grafanak8sapp: &quot;true&quot;</span><br><span class="line">      app: kube-state-metrics</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 25%</span><br><span class="line">      maxUnavailable: 25%</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        grafanak8sapp: &quot;true&quot;</span><br><span class="line">        app: kube-state-metrics</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: kube-state-metrics</span><br><span class="line">        image: harbor.od.com&#x2F;public&#x2F;kube-state-metrics:v1.5.0</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          name: http-metrics</span><br><span class="line">          protocol: TCP</span><br><span class="line">        readinessProbe:</span><br><span class="line">          failureThreshold: 3</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;healthz</span><br><span class="line">            port: 8080</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">          successThreshold: 1</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">      serviceAccountName: kube-state-metrics</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单"><a href="#应用资源配置清单" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><p>执行命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;kube-state-metrics&#x2F;rbac.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;kube-state-metrics&#x2F;deploy.yaml</span><br></pre></td></tr></table></figure><p>查看服务状态</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pod -n kube-system -o wide | grep kube-state-metrics</span><br><span class="line">kube-state-metrics-8669f776c6-cd9mq     1&#x2F;1     Running   0          71s   172.16.23.12   wang-23.host.com   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">[root@wang-200 ~]# curl http:&#x2F;&#x2F;172.16.23.12:8080&#x2F;healthz</span><br><span class="line">ok</span><br></pre></td></tr></table></figure><p>显示ok证明已经成功运行</p><h2 id="node-exporter"><a href="#node-exporter" class="headerlink" title="node-exporter"></a>node-exporter</h2><h3 id="下载docker镜像-1"><a href="#下载docker镜像-1" class="headerlink" title="下载docker镜像"></a>下载docker镜像</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull prom&#x2F;node-exporter:v0.15.0</span><br><span class="line">docker tag 12d51ffa2b22 harbor.od.com&#x2F;public&#x2F;node-exporter:v0.15.0</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;node-exporter:v0.15.0</span><br></pre></td></tr></table></figure><h3 id="准备资源配置清单-1"><a href="#准备资源配置清单-1" class="headerlink" title="准备资源配置清单"></a>准备资源配置清单</h3><p><strong>由于node-exporter是监控node的，所有需要每个节点启动一个，所以使用ds控制器</strong></p><p>准备资源配置清单文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;node-exporter</span><br></pre></td></tr></table></figure><p>ds.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;node-exporter&#x2F;ds.yaml</span><br><span class="line">kind: DaemonSet</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: node-exporter</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    daemon: &quot;node-exporter&quot;</span><br><span class="line">    grafanak8sapp: &quot;true&quot;</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      daemon: &quot;node-exporter&quot;</span><br><span class="line">      grafanak8sapp: &quot;true&quot;</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: node-exporter</span><br><span class="line">      labels:</span><br><span class="line">        daemon: &quot;node-exporter&quot;</span><br><span class="line">        grafanak8sapp: &quot;true&quot;</span><br><span class="line">    spec:</span><br><span class="line">      volumes:</span><br><span class="line">      - name: proc</span><br><span class="line">        hostPath: </span><br><span class="line">          path: &#x2F;proc</span><br><span class="line">          type: &quot;&quot;</span><br><span class="line">      - name: sys</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &#x2F;sys</span><br><span class="line">          type: &quot;&quot;</span><br><span class="line">      containers:</span><br><span class="line">      - name: node-exporter</span><br><span class="line">        image: harbor.od.com&#x2F;public&#x2F;node-exporter:v0.15.0</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        args:</span><br><span class="line">        - --path.procfs&#x3D;&#x2F;host_proc</span><br><span class="line">        - --path.sysfs&#x3D;&#x2F;host_sys</span><br><span class="line">        ports:</span><br><span class="line">        - name: node-exporter</span><br><span class="line">          hostPort: 9100</span><br><span class="line">          containerPort: 9100</span><br><span class="line">          protocol: TCP</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: sys</span><br><span class="line">          readOnly: true</span><br><span class="line">          mountPath: &#x2F;host_sys</span><br><span class="line">        - name: proc</span><br><span class="line">          readOnly: true</span><br><span class="line">          mountPath: &#x2F;host_proc</span><br><span class="line">      hostNetwork: true</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单-1"><a href="#应用资源配置清单-1" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;node-exporter&#x2F;ds.yaml</span><br></pre></td></tr></table></figure><p>验证pod状态</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pod -n kube-system -o wide | grep node-exporter</span><br><span class="line">node-exporter-967mm                     1&#x2F;1     Running   0          26s   192.168.70.24   wang-24.host.com   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">node-exporter-b9sn8                     1&#x2F;1     Running   0          26s   192.168.70.23   wang-23.host.com   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h2 id="cadvisor"><a href="#cadvisor" class="headerlink" title="cadvisor"></a>cadvisor</h2><h3 id="下载docker镜像-2"><a href="#下载docker镜像-2" class="headerlink" title="下载docker镜像"></a>下载docker镜像</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull google&#x2F;cadvisor:v0.28.3</span><br><span class="line">docker tag 75f88e3ec333 harbor.od.com&#x2F;public&#x2F;cadvisor:v0.28.3</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;cadvisor:v0.28.3</span><br></pre></td></tr></table></figure><h3 id="准备资源配置清单-2"><a href="#准备资源配置清单-2" class="headerlink" title="准备资源配置清单"></a>准备资源配置清单</h3><p>创建资源配置清单文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;cadvisor</span><br></pre></td></tr></table></figure><p>ds.yaml </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;cadvisor&#x2F;ds.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: cadvisor</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    app: cadvisor</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      name: cadvisor</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: cadvisor</span><br><span class="line">    spec:</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: node-role.kubernetes.io&#x2F;master</span><br><span class="line">        effect: NoSchedule</span><br><span class="line">      containers:</span><br><span class="line">      - name: cadvisor</span><br><span class="line">        image: harbor.od.com&#x2F;public&#x2F;cadvisor:v0.28.3</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: rootfs</span><br><span class="line">          mountPath: &#x2F;rootfs</span><br><span class="line">          readOnly: true</span><br><span class="line">        - name: var-run</span><br><span class="line">          mountPath: &#x2F;var&#x2F;run</span><br><span class="line">        - name: sys</span><br><span class="line">          mountPath: &#x2F;sys</span><br><span class="line">          readOnly: true</span><br><span class="line">        - name: docker</span><br><span class="line">          mountPath: &#x2F;var&#x2F;lib&#x2F;docker</span><br><span class="line">          readOnly: true</span><br><span class="line">        ports:</span><br><span class="line">          - name: http</span><br><span class="line">            containerPort: 4194</span><br><span class="line">            protocol: TCP</span><br><span class="line">        readinessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: 4194</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">        args:</span><br><span class="line">          - --housekeeping_interval&#x3D;10s</span><br><span class="line">          - --port&#x3D;4194</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      volumes:</span><br><span class="line">      - name: rootfs</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &#x2F;</span><br><span class="line">      - name: var-run</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &#x2F;var&#x2F;run</span><br><span class="line">      - name: sys</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &#x2F;sys</span><br><span class="line">      - name: docker</span><br><span class="line">        hostPath:</span><br><span class="line">          path: &#x2F;data&#x2F;docker</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>针对挂载资源，做一些调整(所有监控的机器)：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mount -o remount,rw &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;</span><br><span class="line">ln -s &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu,cpuacct &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpuacct,cpu</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单-2"><a href="#应用资源配置清单-2" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;cadvisor&#x2F;ds.yaml</span><br></pre></td></tr></table></figure><p>验证pod状态</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pod -n kube-system -o wide | grep cadvisor</span><br><span class="line">cadvisor-62t7c                          1&#x2F;1     Running   0          82s   192.168.70.24   wang-24.host.com   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">cadvisor-szlpj                          1&#x2F;1     Running   0          90s   192.168.70.23   wang-23.host.com   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h2 id="blackbox-exporter"><a href="#blackbox-exporter" class="headerlink" title="blackbox-exporter"></a>blackbox-exporter</h2><h3 id="下载docker镜像-3"><a href="#下载docker镜像-3" class="headerlink" title="下载docker镜像"></a>下载docker镜像</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull prom&#x2F;blackbox-exporter:v0.15.1</span><br><span class="line">docker tag 81b70b6158be  harbor.od.com&#x2F;public&#x2F;blackbox-exporter:v0.15.1</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;blackbox-exporter:v0.15.1</span><br></pre></td></tr></table></figure><h3 id="创建资源配置清单"><a href="#创建资源配置清单" class="headerlink" title="创建资源配置清单"></a>创建资源配置清单</h3><p>创建资源配置清单文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;blackbox-exporter</span><br></pre></td></tr></table></figure><p>cm.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;blackbox-exporter&#x2F;cm.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: blackbox-exporter</span><br><span class="line">  name: blackbox-exporter</span><br><span class="line">  namespace: kube-system</span><br><span class="line">data:</span><br><span class="line">  blackbox.yml: |-</span><br><span class="line">    modules:</span><br><span class="line">      http_2xx:</span><br><span class="line">        prober: http</span><br><span class="line">        timeout: 2s</span><br><span class="line">        http:</span><br><span class="line">          valid_http_versions: [&quot;HTTP&#x2F;1.1&quot;, &quot;HTTP&#x2F;2&quot;]</span><br><span class="line">          valid_status_codes: [200,301,302]</span><br><span class="line">          method: GET</span><br><span class="line">          preferred_ip_protocol: &quot;ip4&quot;</span><br><span class="line">      tcp_connect:</span><br><span class="line">        prober: tcp</span><br><span class="line">        timeout: 2s</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>deploy.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;blackbox-exporter&#x2F;deploy.yaml</span><br><span class="line">kind: Deployment</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: blackbox-exporter</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    app: blackbox-exporter</span><br><span class="line">  annotations:</span><br><span class="line">    deployment.kubernetes.io&#x2F;revision: 1</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: blackbox-exporter</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: blackbox-exporter</span><br><span class="line">    spec:</span><br><span class="line">      volumes:</span><br><span class="line">      - name: config</span><br><span class="line">        configMap:</span><br><span class="line">          name: blackbox-exporter</span><br><span class="line">          defaultMode: 420</span><br><span class="line">      containers:</span><br><span class="line">      - name: blackbox-exporter</span><br><span class="line">        image: harbor.od.com&#x2F;public&#x2F;blackbox-exporter:v0.15.1</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        args:</span><br><span class="line">        - --config.file&#x3D;&#x2F;etc&#x2F;blackbox_exporter&#x2F;blackbox.yml</span><br><span class="line">        - --log.level&#x3D;info</span><br><span class="line">        - --web.listen-address&#x3D;:9115</span><br><span class="line">        ports:</span><br><span class="line">        - name: blackbox-port</span><br><span class="line">          containerPort: 9115</span><br><span class="line">          protocol: TCP</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 200m</span><br><span class="line">            memory: 256Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 50Mi</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: config</span><br><span class="line">          mountPath: &#x2F;etc&#x2F;blackbox_exporter</span><br><span class="line">        readinessProbe:</span><br><span class="line">          tcpSocket:</span><br><span class="line">            port: 9115</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">          successThreshold: 1</span><br><span class="line">          failureThreshold: 3</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>svc.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;blackbox-exporter&#x2F;svc.yaml</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: blackbox-exporter</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: blackbox-exporter</span><br><span class="line">  ports:</span><br><span class="line">    - name: blackbox-port</span><br><span class="line">      protocol: TCP</span><br><span class="line">      port: 9115</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>ingress.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;blackbox-exporter&#x2F;ingress.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: blackbox-exporter</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: blackbox.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: blackbox-exporter</span><br><span class="line">          servicePort: blackbox-port</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单-3"><a href="#应用资源配置清单-3" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><p>执行命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;blackbox-exporter&#x2F;cm.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;blackbox-exporter&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;blackbox-exporter&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;blackbox-exporter&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p>查看pod运行状态</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pod -n kube-system | grep blackbox-exporter</span><br><span class="line">blackbox-exporter-659fc46b55-jr8hg      1&#x2F;1     Running   0          24s</span><br></pre></td></tr></table></figure><h3 id="访问域名测试："><a href="#访问域名测试：" class="headerlink" title="访问域名测试："></a>访问域名测试：</h3><p>访问blackbox.od.com，表示blackbox已经运行成功</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqor0nve4j31a20l4di6.jpg" alt="blackbox.od.com展示界面"></p><h2 id="部署prometheus-server"><a href="#部署prometheus-server" class="headerlink" title="部署prometheus server"></a>部署prometheus server</h2><p>下载docker镜像</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull prom&#x2F;prometheus:v2.14.0</span><br><span class="line">docker tag 7317640d555e harbor.od.com&#x2F;infra&#x2F;prometheus:v2.14.0</span><br><span class="line">docker push harbor.od.com&#x2F;infra&#x2F;prometheus:v2.14.0</span><br></pre></td></tr></table></figure><h3 id="准备资源配置清单-3"><a href="#准备资源配置清单-3" class="headerlink" title="准备资源配置清单"></a>准备资源配置清单</h3><p>创建资源配置清单文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;prometheus</span><br></pre></td></tr></table></figure><p>创建需要的目录：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir -p &#x2F;data&#x2F;nfs&#x2F;v1&#x2F;prometheus&#x2F;&#123;etc,prom-db&#125;</span><br></pre></td></tr></table></figure><p>拷贝配置文件中用到的证书：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;data&#x2F;nfs&#x2F;v1&#x2F;prometheus&#x2F;etc&#x2F;</span><br><span class="line">cp &#x2F;opt&#x2F;certs&#x2F;ca.pem .&#x2F;</span><br><span class="line">cp &#x2F;opt&#x2F;certs&#x2F;client.pem .&#x2F;</span><br><span class="line">cp &#x2F;opt&#x2F;certs&#x2F;client-key.pem .&#x2F;</span><br></pre></td></tr></table></figure><p>rbac.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;prometheus&#x2F;rbac.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">    kubernetes.io&#x2F;cluster-service: &quot;true&quot;</span><br><span class="line">  name: prometheus</span><br><span class="line">  namespace: infra</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">    kubernetes.io&#x2F;cluster-service: &quot;true&quot;</span><br><span class="line">  name: prometheus</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - nodes</span><br><span class="line">  - nodes&#x2F;metrics</span><br><span class="line">  - services</span><br><span class="line">  - endpoints</span><br><span class="line">  - pods</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - configmaps</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">- nonResourceURLs:</span><br><span class="line">  - &#x2F;metrics</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">    kubernetes.io&#x2F;cluster-service: &quot;true&quot;</span><br><span class="line">  name: prometheus</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: prometheus</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: prometheus</span><br><span class="line">  namespace: infra</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>deploy.yaml</p><blockquote><p>加上–web.enable-lifecycle启用远程热加载配置文件<br>调用指令是curl -X POST <a href="http://localhost:9090/-/reload" target="_blank" rel="noopener">http://localhost:9090/-/reload</a></p><p>storage.tsdb.min-block-duration=10m  #只加载10分钟数据到内</p><p>storage.tsdb.retention=72h   #保留72小时数据</p></blockquote><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;prometheus&#x2F;deploy.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    deployment.kubernetes.io&#x2F;revision: &quot;5&quot;</span><br><span class="line">  labels:</span><br><span class="line">    name: prometheus</span><br><span class="line">  name: prometheus</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  progressDeadlineSeconds: 600</span><br><span class="line">  replicas: 1</span><br><span class="line">  revisionHistoryLimit: 7</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: prometheus</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 1</span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: prometheus</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: prometheus</span><br><span class="line">        image: harbor.od.com&#x2F;infra&#x2F;prometheus:v2.14.0</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        command:</span><br><span class="line">        - &#x2F;bin&#x2F;prometheus</span><br><span class="line">        args:</span><br><span class="line">        - --config.file&#x3D;&#x2F;data&#x2F;etc&#x2F;prometheus.yml</span><br><span class="line">        - --storage.tsdb.path&#x3D;&#x2F;data&#x2F;prom-db</span><br><span class="line">        - --storage.tsdb.min-block-duration&#x3D;10m</span><br><span class="line">        - --storage.tsdb.retention&#x3D;72h</span><br><span class="line">        - --web.enable-lifecycle</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 9090</span><br><span class="line">          protocol: TCP</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: &#x2F;data</span><br><span class="line">          name: data</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: &quot;1000m&quot;</span><br><span class="line">            memory: &quot;1.5Gi&quot;</span><br><span class="line">          limits:</span><br><span class="line">            cpu: &quot;2000m&quot;</span><br><span class="line">            memory: &quot;3Gi&quot;</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor</span><br><span class="line">      securityContext:</span><br><span class="line">        runAsUser: 0</span><br><span class="line">      serviceAccountName: prometheus</span><br><span class="line">      volumes:</span><br><span class="line">      - name: data</span><br><span class="line">        nfs:</span><br><span class="line">          server: wang-200</span><br><span class="line">          path: &#x2F;data&#x2F;nfs&#x2F;v1&#x2F;prometheus</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>svc.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;prometheus&#x2F;svc.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: prometheus</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 9090</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 9090</span><br><span class="line">  selector:</span><br><span class="line">    app: prometheus</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>ingress.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;prometheus&#x2F;ingress.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io&#x2F;ingress.class: traefik</span><br><span class="line">  name: prometheus</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: prometheus.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: prometheus</span><br><span class="line">          servicePort: 9090</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>修改prometheus配置文件：别问为啥这么写，问就是不懂~</p><p>vim /data/nfs/v1/prometheus/etc/prometheus.yml </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">global:</span><br><span class="line">  scrape_interval:     15s</span><br><span class="line">  evaluation_interval: 15s</span><br><span class="line">scrape_configs:</span><br><span class="line">- job_name: &#39;etcd&#39;</span><br><span class="line">  tls_config:</span><br><span class="line">    ca_file: &#x2F;data&#x2F;etc&#x2F;ca.pem</span><br><span class="line">    cert_file: &#x2F;data&#x2F;etc&#x2F;client.pem</span><br><span class="line">    key_file: &#x2F;data&#x2F;etc&#x2F;client-key.pem</span><br><span class="line">  scheme: https</span><br><span class="line">  static_configs:</span><br><span class="line">  - targets:</span><br><span class="line">    - &#39;192.168.70.21:2379&#39;</span><br><span class="line">    - &#39;192.168.70.22:2379&#39;</span><br><span class="line">    - &#39;192.168.70.23:2379&#39;</span><br><span class="line">- job_name: &#39;kubernetes-apiservers&#39;</span><br><span class="line">  kubernetes_sd_configs:</span><br><span class="line">  - role: endpoints</span><br><span class="line">  scheme: https</span><br><span class="line">  tls_config:</span><br><span class="line">    ca_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;ca.crt</span><br><span class="line">  bearer_token_file: &#x2F;var&#x2F;run&#x2F;secrets&#x2F;kubernetes.io&#x2F;serviceaccount&#x2F;token</span><br><span class="line">  relabel_configs:</span><br><span class="line">  - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]</span><br><span class="line">    action: keep</span><br><span class="line">    regex: default;kubernetes;https</span><br><span class="line">- job_name: &#39;kubernetes-pods&#39;</span><br><span class="line">  kubernetes_sd_configs:</span><br><span class="line">  - role: pod</span><br><span class="line">  relabel_configs:</span><br><span class="line">  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]</span><br><span class="line">    action: keep</span><br><span class="line">    regex: true</span><br><span class="line">  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]</span><br><span class="line">    action: replace</span><br><span class="line">    target_label: __metrics_path__</span><br><span class="line">    regex: (.+)</span><br><span class="line">  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]</span><br><span class="line">    action: replace</span><br><span class="line">    regex: ([^:]+)(?::\d+)?;(\d+)</span><br><span class="line">    replacement: $1:$2</span><br><span class="line">    target_label: __address__</span><br><span class="line">  - action: labelmap</span><br><span class="line">    regex: __meta_kubernetes_pod_label_(.+)</span><br><span class="line">  - source_labels: [__meta_kubernetes_namespace]</span><br><span class="line">    action: replace</span><br><span class="line">    target_label: kubernetes_namespace</span><br><span class="line">  - source_labels: [__meta_kubernetes_pod_name]</span><br><span class="line">    action: replace</span><br><span class="line">    target_label: kubernetes_pod_name</span><br><span class="line">- job_name: &#39;kubernetes-kubelet&#39;</span><br><span class="line">  kubernetes_sd_configs:</span><br><span class="line">  - role: node</span><br><span class="line">  relabel_configs:</span><br><span class="line">  - action: labelmap</span><br><span class="line">    regex: __meta_kubernetes_node_label_(.+)</span><br><span class="line">  - source_labels: [__meta_kubernetes_node_name]</span><br><span class="line">    regex: (.+)</span><br><span class="line">    target_label: __address__</span><br><span class="line">    replacement: $&#123;1&#125;:10255</span><br><span class="line">- job_name: &#39;kubernetes-cadvisor&#39;</span><br><span class="line">  kubernetes_sd_configs:</span><br><span class="line">  - role: node</span><br><span class="line">  relabel_configs:</span><br><span class="line">  - action: labelmap</span><br><span class="line">    regex: __meta_kubernetes_node_label_(.+)</span><br><span class="line">  - source_labels: [__meta_kubernetes_node_name]</span><br><span class="line">    regex: (.+)</span><br><span class="line">    target_label: __address__</span><br><span class="line">    replacement: $&#123;1&#125;:4194</span><br><span class="line">- job_name: &#39;kubernetes-kube-state&#39;</span><br><span class="line">  kubernetes_sd_configs:</span><br><span class="line">  - role: pod</span><br><span class="line">  relabel_configs:</span><br><span class="line">  - action: labelmap</span><br><span class="line">    regex: __meta_kubernetes_pod_label_(.+)</span><br><span class="line">  - source_labels: [__meta_kubernetes_namespace]</span><br><span class="line">    action: replace</span><br><span class="line">    target_label: kubernetes_namespace</span><br><span class="line">  - source_labels: [__meta_kubernetes_pod_name]</span><br><span class="line">    action: replace</span><br><span class="line">    target_label: kubernetes_pod_name</span><br><span class="line">  - source_labels: [__meta_kubernetes_pod_label_grafanak8sapp]</span><br><span class="line">    regex: .*true.*</span><br><span class="line">    action: keep</span><br><span class="line">  - source_labels: [&#39;__meta_kubernetes_pod_label_daemon&#39;, &#39;__meta_kubernetes_pod_node_name&#39;]</span><br><span class="line">    regex: &#39;node-exporter;(.*)&#39;</span><br><span class="line">    action: replace</span><br><span class="line">    target_label: nodename</span><br><span class="line">- job_name: &#39;blackbox_http_pod_probe&#39;</span><br><span class="line">  metrics_path: &#x2F;probe</span><br><span class="line">  kubernetes_sd_configs:</span><br><span class="line">  - role: pod</span><br><span class="line">  params:</span><br><span class="line">    module: [http_2xx]</span><br><span class="line">  relabel_configs:</span><br><span class="line">  - source_labels: [__meta_kubernetes_pod_annotation_blackbox_scheme]</span><br><span class="line">    action: keep</span><br><span class="line">    regex: http</span><br><span class="line">  - source_labels: [__address__, __meta_kubernetes_pod_annotation_blackbox_port,  __meta_kubernetes_pod_annotation_blackbox_path]</span><br><span class="line">    action: replace</span><br><span class="line">    regex: ([^:]+)(?::\d+)?;(\d+);(.+)</span><br><span class="line">    replacement: $1:$2$3</span><br><span class="line">    target_label: __param_target</span><br><span class="line">  - action: replace</span><br><span class="line">    target_label: __address__</span><br><span class="line">    replacement: blackbox-exporter.kube-system:9115</span><br><span class="line">  - source_labels: [__param_target]</span><br><span class="line">    target_label: instance</span><br><span class="line">  - action: labelmap</span><br><span class="line">    regex: __meta_kubernetes_pod_label_(.+)</span><br><span class="line">  - source_labels: [__meta_kubernetes_namespace]</span><br><span class="line">    action: replace</span><br><span class="line">    target_label: kubernetes_namespace</span><br><span class="line">  - source_labels: [__meta_kubernetes_pod_name]</span><br><span class="line">    action: replace</span><br><span class="line">    target_label: kubernetes_pod_name</span><br><span class="line">- job_name: &#39;blackbox_tcp_pod_probe&#39;</span><br><span class="line">  metrics_path: &#x2F;probe</span><br><span class="line">  kubernetes_sd_configs:</span><br><span class="line">  - role: pod</span><br><span class="line">  params:</span><br><span class="line">    module: [tcp_connect]</span><br><span class="line">  relabel_configs:</span><br><span class="line">  - source_labels: [__meta_kubernetes_pod_annotation_blackbox_scheme]</span><br><span class="line">    action: keep</span><br><span class="line">    regex: tcp</span><br><span class="line">  - source_labels: [__address__, __meta_kubernetes_pod_annotation_blackbox_port]</span><br><span class="line">    action: replace</span><br><span class="line">    regex: ([^:]+)(?::\d+)?;(\d+)</span><br><span class="line">    replacement: $1:$2</span><br><span class="line">    target_label: __param_target</span><br><span class="line">  - action: replace</span><br><span class="line">    target_label: __address__</span><br><span class="line">    replacement: blackbox-exporter.kube-system:9115</span><br><span class="line">  - source_labels: [__param_target]</span><br><span class="line">    target_label: instance</span><br><span class="line">  - action: labelmap</span><br><span class="line">    regex: __meta_kubernetes_pod_label_(.+)</span><br><span class="line">  - source_labels: [__meta_kubernetes_namespace]</span><br><span class="line">    action: replace</span><br><span class="line">    target_label: kubernetes_namespace</span><br><span class="line">  - source_labels: [__meta_kubernetes_pod_name]</span><br><span class="line">    action: replace</span><br><span class="line">    target_label: kubernetes_pod_name</span><br><span class="line">- job_name: &#39;traefik&#39;</span><br><span class="line">  kubernetes_sd_configs:</span><br><span class="line">  - role: pod</span><br><span class="line">  relabel_configs:</span><br><span class="line">  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]</span><br><span class="line">    action: keep</span><br><span class="line">    regex: traefik</span><br><span class="line">  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]</span><br><span class="line">    action: replace</span><br><span class="line">    target_label: __metrics_path__</span><br><span class="line">    regex: (.+)</span><br><span class="line">  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]</span><br><span class="line">    action: replace</span><br><span class="line">    regex: ([^:]+)(?::\d+)?;(\d+)</span><br><span class="line">    replacement: $1:$2</span><br><span class="line">    target_label: __address__</span><br><span class="line">  - action: labelmap</span><br><span class="line">    regex: __meta_kubernetes_pod_label_(.+)</span><br><span class="line">  - source_labels: [__meta_kubernetes_namespace]</span><br><span class="line">    action: replace</span><br><span class="line">    target_label: kubernetes_namespace</span><br><span class="line">  - source_labels: [__meta_kubernetes_pod_name]</span><br><span class="line">    action: replace</span><br><span class="line">    target_label: kubernetes_pod_name</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单-4"><a href="#应用资源配置清单-4" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><p>执行命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;prometheus&#x2F;rbac.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;prometheus&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;prometheus&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;prometheus&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p>查看pod运行状态</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pod -n infra | grep prometheus</span><br><span class="line">prometheus-6847bb9f95-jntt6            1&#x2F;1     Running   0          39s</span><br></pre></td></tr></table></figure><p>浏览器验证：</p><p><a href="http://prometheus.od.com/targets" target="_blank" rel="noopener">http://prometheus.od.com/targets</a></p><p>这里点击status-targets，这里展示的就是我们在prometheus.yml中配置的job-name，这些targets基本可以满足我们收集数据的需求。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqqef86q7j30eq0eedh5.jpg" alt="prometheus监控项"></p><p>点击status-configuration就是我们的配置文件</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqqg87jkxj30ds099js1.jpg" alt="prometheu配置文件"></p><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><p>我们在配置文件中，除了etcd使用的静态配置以外，其他job都是使用的自动发现。</p><h4 id="静态配置"><a href="#静态配置" class="headerlink" title="静态配置"></a>静态配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">global:</span><br><span class="line">  scrape_interval:     15s</span><br><span class="line">  evaluation_interval: 15s</span><br><span class="line">scrape_configs:</span><br><span class="line">- job_name: &#39;etcd&#39;</span><br><span class="line">  tls_config:</span><br><span class="line">    ca_file: &#x2F;data&#x2F;etc&#x2F;ca.pem</span><br><span class="line">    cert_file: &#x2F;data&#x2F;etc&#x2F;client.pem</span><br><span class="line">    key_file: &#x2F;data&#x2F;etc&#x2F;client-key.pem</span><br><span class="line">  scheme: https</span><br><span class="line">  static_configs:</span><br><span class="line">  - targets:</span><br><span class="line">    - &#39;192.168.70.21:2379&#39;</span><br><span class="line">    - &#39;192.168.70.22:2379&#39;</span><br><span class="line">    - &#39;192.168.70.23:2379&#39;</span><br></pre></td></tr></table></figure><h4 id="自动发现"><a href="#自动发现" class="headerlink" title="自动发现"></a>自动发现</h4><p>自动发现资源是pod</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- job_name: &#39;blackbox_http_pod_probe&#39;</span><br><span class="line">  metrics_path: &#x2F;probe</span><br><span class="line">  kubernetes_sd_configs:</span><br><span class="line">  - role: pod</span><br><span class="line">  params:</span><br><span class="line">    module: [http_2xx]</span><br><span class="line">  relabel_configs:</span><br></pre></td></tr></table></figure><p>这里还有很多数据没有收集到，是因为我们在启动服务的时候，没有添加annotations，下面给需要收集数据的服务添加annotations</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqqilcdifj30bv0cu75b.jpg" alt="img"></p><h4 id="traefik"><a href="#traefik" class="headerlink" title="traefik"></a>traefik</h4><p>修改traefik的yaml：</p><p>从dashboard里找到traefik的yaml，跟labels同级添加annotations</p><p>查看prometheus的traffik自动发现规则，你会发现需要有这三个标签</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;annotations&quot;: &#123;</span><br><span class="line">  &quot;prometheus_io_scheme&quot;: &quot;traefik&quot;,</span><br><span class="line">  &quot;prometheus_io_path&quot;: &quot;&#x2F;metrics&quot;,</span><br><span class="line">  &quot;prometheus_io_port&quot;: &quot;8080&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqqlcy8p0j30bp0an0t6.jpg" alt="img"></p><p><strong>等待pod重启以后</strong>(⚠️)，再去prometheus上去看</p><p>通过这里查看pod是否有符合你要求的。 <a href="http://prometheus.od.com/service-discovery" target="_blank" rel="noopener">http://prometheus.od.com/service-discovery</a></p><p>我添加了标签没有自动更新，手动帮他更新(删除pod) </p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqrya030ij31vc0pa7a8.jpg" alt="image-20200613170856736"></p><h4 id="blackbox"><a href="#blackbox" class="headerlink" title="blackbox"></a>blackbox</h4><p>这个是检测容器内服务存活性的，也就是端口健康状态检查，分为tcp和http</p><p>首先准备两个服务，将dubbo-demo-service和dubbo-demo-consumer都调整为使用master镜像，不依赖apollo的（节省资源）</p><p>等两个服务起来以后，首先在dubbo-demo-service资源中添加一个TCP的annotation：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;annotations&quot;: &#123;</span><br><span class="line">  &quot;blackbox_port&quot;: &quot;20880&quot;,</span><br><span class="line">  &quot;blackbox_scheme&quot;: &quot;tcp&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqrzpk430j30ap06xt8x.jpg" alt="img"></p><p>等待pod更新（重启）后</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gft0dtadc1j31wq0dgado.jpg" alt="image-20200613171206561"></p><p>这里会自动发现我们服务中，运行tcp port端口为20880的服务，并监控其状态</p><p>添加dubbo-demo-consumer服务</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;annotations&quot;: &#123;</span><br><span class="line">  &quot;blackbox_path&quot;: &quot;&#x2F;hello?name&#x3D;health&quot;,</span><br><span class="line">  &quot;blackbox_port&quot;: &quot;8080&quot;,</span><br><span class="line">  &quot;blackbox_scheme&quot;: &quot;http&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqs2264msj30cs06odg4.jpg" alt="img"></p><p>等pod重启后查看监控项</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqs41f3ejj31xy0e0786.jpg" alt="image-20200613171447140"></p><p>去检查blackbox.od.com</p><p><a href="http://blackbox.od.com/" target="_blank" rel="noopener">http://blackbox.od.com/</a></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqs4zq8yij31hj0u0tfu.jpg" alt="image-20200613171547435"></p><h4 id="添加监控jvm信息"><a href="#添加监控jvm信息" class="headerlink" title="添加监控jvm信息"></a>添加监控jvm信息</h4><p>添加监控jvm信息的annotation</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&quot;annotations&quot;: &#123;</span><br><span class="line">  &quot;prometheus_io_scrape&quot;: &quot;true&quot;,</span><br><span class="line">  &quot;prometheus_io_port&quot;: &quot;12346&quot;,</span><br><span class="line">  &quot;prometheus_io_path&quot;: &quot;&#x2F;&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>dubbo-demo-service和dubbo-demo-consumer都添加</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqvk69dslj31ya0j079n.jpg" alt="image-20200613191358238"></p><p> 匹配规则，要去prometheus.yml中去看。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqs7vf64ej30fm06xq3m.jpg" alt="img"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqs80nb91j309f03e0ss.jpg" alt="img"></p><h2 id="grafana监控展示"><a href="#grafana监控展示" class="headerlink" title="grafana监控展示"></a>grafana监控展示</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull grafana&#x2F;grafana:5.4.2</span><br><span class="line">docker tag 6f18ddf9e552 harbor.od.com&#x2F;infra&#x2F;grafana:v5.4.2</span><br><span class="line">docker push harbor.od.com&#x2F;infra&#x2F;grafana:v5.4.2</span><br></pre></td></tr></table></figure><h3 id="准备资源配置清单-4"><a href="#准备资源配置清单-4" class="headerlink" title="准备资源配置清单"></a>准备资源配置清单</h3><p>创建资源配置清单文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;grafana</span><br><span class="line">[root@wang-200 ~]# mkdir &#x2F;data&#x2F;nfs&#x2F;v1&#x2F;grafana</span><br></pre></td></tr></table></figure><p>rbac.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;grafana&#x2F;rbac.yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">    kubernetes.io&#x2F;cluster-service: &quot;true&quot;</span><br><span class="line">  name: grafana</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;*&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - namespaces</span><br><span class="line">  - deployments</span><br><span class="line">  - pods</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">    kubernetes.io&#x2F;cluster-service: &quot;true&quot;</span><br><span class="line">  name: grafana</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: grafana</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: k8s-node</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>deploy.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;grafana&#x2F;deploy.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: grafana</span><br><span class="line">    name: grafana</span><br><span class="line">  name: grafana</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  progressDeadlineSeconds: 600</span><br><span class="line">  replicas: 1</span><br><span class="line">  revisionHistoryLimit: 7</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      name: grafana</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 1</span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: grafana</span><br><span class="line">        name: grafana</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: grafana</span><br><span class="line">        image: harbor.od.com&#x2F;infra&#x2F;grafana:v5.4.2</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 3000</span><br><span class="line">          protocol: TCP</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: &#x2F;var&#x2F;lib&#x2F;grafana</span><br><span class="line">          name: data</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor</span><br><span class="line">      securityContext:</span><br><span class="line">        runAsUser: 0</span><br><span class="line">      volumes:</span><br><span class="line">      - nfs:</span><br><span class="line">          server: wang-200</span><br><span class="line">          path: &#x2F;data&#x2F;nfs&#x2F;v1&#x2F;grafana</span><br><span class="line">        name: data</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>svc.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;grafana&#x2F;svc.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: grafana</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 3000</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 3000</span><br><span class="line">  selector:</span><br><span class="line">    app: grafana</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>ingress.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;grafana&#x2F;ingress.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: grafana</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: grafana.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: grafana</span><br><span class="line">          servicePort: 3000</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单-5"><a href="#应用资源配置清单-5" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><p>执行命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;grafana&#x2F;rbac.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;grafana&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;grafana&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;grafana&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p>查看pod启动状态</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pod -n infra | grep grafana               </span><br><span class="line">grafana-856f9db8c5-v874r               1&#x2F;1     Running   0          44s</span><br></pre></td></tr></table></figure><h3 id="通过页面配置grafana"><a href="#通过页面配置grafana" class="headerlink" title="通过页面配置grafana"></a>通过页面配置grafana</h3><p>浏览器访问grafana.od.com</p><p>默认用户名密码admin:admin</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqsuiu6o0j31ng0kugnt.jpg" alt="image-20200613174017876"></p><p>进入容器安装插件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl exec -it grafana-856f9db8c5-v874r -n infra &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grafana-cli plugins install grafana-kubernetes-app</span><br><span class="line">grafana-cli plugins install grafana-clock-panel</span><br><span class="line">grafana-cli plugins install grafana-piechart-panel</span><br><span class="line">grafana-cli plugins install briangann-gauge-panel</span><br><span class="line">grafana-cli plugins install natel-discrete-panel</span><br></pre></td></tr></table></figure><h3 id="配置数据源"><a href="#配置数据源" class="headerlink" title="配置数据源"></a>配置数据源</h3><p>选择prometheus，把三个证书添加进来</p><p>URL: <a href="http://prometheus.od.com" target="_blank" rel="noopener">http://prometheus.od.com</a></p><p>CA Cert: /opt/certs/ca.pem      Client Cert: /opt/certs/client.pem        Client Key:/opt/certs/client-key.pem</p><p><img src="https://img2018.cnblogs.com/blog/1034759/201912/1034759-20191218150843677-574385113.png" alt="img"></p><h3 id="使用插件kubernetes"><a href="#使用插件kubernetes" class="headerlink" title="使用插件kubernetes"></a>使用插件kubernetes</h3><p>重启grafana</p><p>找到我们刚才安装的插件里面的kubernetes,启用，</p><p><a href="http://grafana.od.com/plugins/grafana-kubernetes-app/edit" target="_blank" rel="noopener">http://grafana.od.com/plugins/grafana-kubernetes-app/edit</a></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqtnpuxlij31rc0lc4d6.jpg" alt="image-20200613180820337"></p><p>然后新建cluster, </p><p>URL: <a href="https://kubernetes.default，" target="_blank" rel="noopener">https://kubernetes.default，</a> 数据源为 prometheus</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gft1ezwx1cj30sp0h8wh1.jpg" alt="image-20200615160757653"></p><p> 添加完需要稍等几分钟，在没有取到数据之前，会报http forbidden，没关系，等一会就好。大概2-5分钟。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gft1e8teo7j30yi0ey0x2.jpg" alt="image-20200615160714630"></p><h2 id="配置alertmanager告警"><a href="#配置alertmanager告警" class="headerlink" title="配置alertmanager告警"></a>配置alertmanager告警</h2><p>下载docker镜像</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull docker.io&#x2F;prom&#x2F;alertmanager:v0.14.0</span><br><span class="line">docker tag 23744b2d645c harbor.od.com&#x2F;infra&#x2F;alertmanager:v0.14.0</span><br><span class="line">docker push harbor.od.com&#x2F;infra&#x2F;alertmanager:v0.14.0</span><br></pre></td></tr></table></figure><h3 id="配置资源配置清单"><a href="#配置资源配置清单" class="headerlink" title="配置资源配置清单"></a>配置资源配置清单</h3><p>创建资源配置清单文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;alertmanager</span><br></pre></td></tr></table></figure><p>cm.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;alertmanager&#x2F;cm.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: alertmanager-config</span><br><span class="line">  namespace: infra</span><br><span class="line">data:</span><br><span class="line">  config.yml: |-</span><br><span class="line">    global:</span><br><span class="line">      # 在没有报警的情况下声明为已解决的时间</span><br><span class="line">      resolve_timeout: 5m</span><br><span class="line">      # 配置邮件发送信息</span><br><span class="line">      smtp_smarthost: &#39;smtp.163.com:25&#39;</span><br><span class="line">      smtp_from: &#39;xxx@163.com&#39;</span><br><span class="line">      smtp_auth_username: &#39;xxx@163.com&#39;</span><br><span class="line">      smtp_auth_password: &#39;xxxxxx&#39;</span><br><span class="line">      smtp_require_tls: false</span><br><span class="line">    # 所有报警信息进入后的根路由，用来设置报警的分发策略</span><br><span class="line">    route:</span><br><span class="line">      # 这里的标签列表是接收到报警信息后的重新分组标签，例如，接收到的报警信息里面有许多具有 cluster&#x3D;A 和 alertname&#x3D;LatncyHigh 这样的标签的报警信息将会批量被聚合到一个分组里面</span><br><span class="line">      group_by: [&#39;alertname&#39;, &#39;cluster&#39;]</span><br><span class="line">      # 当一个新的报警分组被创建后，需要等待至少group_wait时间来初始化通知，这种方式可以确保您能有足够的时间为同一分组来获取多个警报，然后一起触发这个报警信息。</span><br><span class="line">      group_wait: 30s</span><br><span class="line"></span><br><span class="line">      # 当第一个报警发送后，等待&#39;group_interval&#39;时间来发送新的一组报警信息。</span><br><span class="line">      group_interval: 5m</span><br><span class="line"></span><br><span class="line">      # 如果一个报警信息已经发送成功了，等待&#39;repeat_interval&#39;时间来重新发送他们</span><br><span class="line">      repeat_interval: 5m</span><br><span class="line"></span><br><span class="line">      # 默认的receiver：如果一个报警没有被一个route匹配，则发送给默认的接收器</span><br><span class="line">      receiver: default</span><br><span class="line"></span><br><span class="line">    receivers:</span><br><span class="line">    - name: &#39;default&#39;</span><br><span class="line">      email_configs:</span><br><span class="line">      - to: &#39;xxxx@qq.com&#39;</span><br><span class="line">        send_resolved: true</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>deploy.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;alertmanager&#x2F;deploy.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: alertmanager</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: alertmanager</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: alertmanager</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: alertmanager</span><br><span class="line">        image: harbor.od.com&#x2F;infra&#x2F;alertmanager:v0.14.0</span><br><span class="line">        args:</span><br><span class="line">          - &quot;--config.file&#x3D;&#x2F;etc&#x2F;alertmanager&#x2F;config.yml&quot;</span><br><span class="line">          - &quot;--storage.path&#x3D;&#x2F;alertmanager&quot;</span><br><span class="line">        ports:</span><br><span class="line">        - name: alertmanager</span><br><span class="line">          containerPort: 9093</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: alertmanager-cm</span><br><span class="line">          mountPath: &#x2F;etc&#x2F;alertmanager</span><br><span class="line">      volumes:</span><br><span class="line">      - name: alertmanager-cm</span><br><span class="line">        configMap:</span><br><span class="line">          name: alertmanager-config</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>svc.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;alertmanager&#x2F;svc.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: alertmanager</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  selector: </span><br><span class="line">    app: alertmanager</span><br><span class="line">  ports:</span><br><span class="line">    - port: 80</span><br><span class="line">      targetPort: 9093</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>ingress</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;alertmanager&#x2F;ingress.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: alertmanager</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: alertmanager.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: alertmanager</span><br><span class="line">          servicePort: 80</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="执行资源配置清单"><a href="#执行资源配置清单" class="headerlink" title="执行资源配置清单"></a>执行资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;alertmanager&#x2F;cm.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;alertmanager&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;alertmanager&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;alertmanager&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p>查看alertmanager pod运行状态</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pod -n infra | grep alertmanager</span><br><span class="line">alertmanager-5d46bdc7b4-7jcmg          1&#x2F;1     Running   0          40s</span><br></pre></td></tr></table></figure><p>访问web页面</p><p><a href="http://alertmanager.od.com/#/alerts" target="_blank" rel="noopener">http://alertmanager.od.com/#/alerts</a></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gft0ew5yrkj31vw0ncacs.jpg" alt="image-20200615152624577"></p><h3 id="基础报警规则"><a href="#基础报警规则" class="headerlink" title="基础报警规则"></a>基础报警规则</h3><p><code>vim /data/nfs/v1/prometheus/etc/rules.yml</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">groups:</span><br><span class="line">- name: hostStatsAlert</span><br><span class="line">  rules:</span><br><span class="line">  - alert: hostCpuUsageAlert</span><br><span class="line">    expr: sum(avg without (cpu)(irate(node_cpu&#123;mode!&#x3D;&#39;idle&#39;&#125;[5m]))) by (instance) &gt; 0.85</span><br><span class="line">    for: 5m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; CPU usage above 85% (current value: &#123;&#123; $value &#125;&#125;%)&quot;</span><br><span class="line">  - alert: hostMemUsageAlert</span><br><span class="line">    expr: (node_memory_MemTotal - node_memory_MemAvailable)&#x2F;node_memory_MemTotal &gt; 0.85</span><br><span class="line">    for: 5m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;&#123;&#123; $labels.instance &#125;&#125; MEM usage above 85% (current value: &#123;&#123; $value &#125;&#125;%)&quot;</span><br><span class="line">  - alert: OutOfInodes</span><br><span class="line">    expr: node_filesystem_free&#123;fstype&#x3D;&quot;overlay&quot;,mountpoint &#x3D;&quot;&#x2F;&quot;&#125; &#x2F; node_filesystem_size&#123;fstype&#x3D;&quot;overlay&quot;,mountpoint &#x3D;&quot;&#x2F;&quot;&#125; * 100 &lt; 10</span><br><span class="line">    for: 5m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Out of inodes (instance &#123;&#123; $labels.instance &#125;&#125;)&quot;</span><br><span class="line">      description: &quot;Disk is almost running out of available inodes (&lt; 10% left) (current value: &#123;&#123; $value &#125;&#125;)&quot;</span><br><span class="line">  - alert: OutOfDiskSpace</span><br><span class="line">    expr: node_filesystem_free&#123;fstype&#x3D;&quot;overlay&quot;,mountpoint &#x3D;&quot;&#x2F;rootfs&quot;&#125; &#x2F; node_filesystem_size&#123;fstype&#x3D;&quot;overlay&quot;,mountpoint &#x3D;&quot;&#x2F;rootfs&quot;&#125; * 100 &lt; 10</span><br><span class="line">    for: 5m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Out of disk space (instance &#123;&#123; $labels.instance &#125;&#125;)&quot;</span><br><span class="line">      description: &quot;Disk is almost full (&lt; 10% left) (current value: &#123;&#123; $value &#125;&#125;)&quot;</span><br><span class="line">  - alert: UnusualNetworkThroughputIn</span><br><span class="line">    expr: sum by (instance) (irate(node_network_receive_bytes[2m])) &#x2F; 1024 &#x2F; 1024 &gt; 100</span><br><span class="line">    for: 5m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Unusual network throughput in (instance &#123;&#123; $labels.instance &#125;&#125;)&quot;</span><br><span class="line">      description: &quot;Host network interfaces are probably receiving too much data (&gt; 100 MB&#x2F;s) (current value: &#123;&#123; $value &#125;&#125;)&quot;</span><br><span class="line">  - alert: UnusualNetworkThroughputOut</span><br><span class="line">    expr: sum by (instance) (irate(node_network_transmit_bytes[2m])) &#x2F; 1024 &#x2F; 1024 &gt; 100</span><br><span class="line">    for: 5m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Unusual network throughput out (instance &#123;&#123; $labels.instance &#125;&#125;)&quot;</span><br><span class="line">      description: &quot;Host network interfaces are probably sending too much data (&gt; 100 MB&#x2F;s) (current value: &#123;&#123; $value &#125;&#125;)&quot;</span><br><span class="line">  - alert: UnusualDiskReadRate</span><br><span class="line">    expr: sum by (instance) (irate(node_disk_bytes_read[2m])) &#x2F; 1024 &#x2F; 1024 &gt; 50</span><br><span class="line">    for: 5m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Unusual disk read rate (instance &#123;&#123; $labels.instance &#125;&#125;)&quot;</span><br><span class="line">      description: &quot;Disk is probably reading too much data (&gt; 50 MB&#x2F;s) (current value: &#123;&#123; $value &#125;&#125;)&quot;</span><br><span class="line">  - alert: UnusualDiskWriteRate</span><br><span class="line">    expr: sum by (instance) (irate(node_disk_bytes_written[2m])) &#x2F; 1024 &#x2F; 1024 &gt; 50</span><br><span class="line">    for: 5m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Unusual disk write rate (instance &#123;&#123; $labels.instance &#125;&#125;)&quot;</span><br><span class="line">      description: &quot;Disk is probably writing too much data (&gt; 50 MB&#x2F;s) (current value: &#123;&#123; $value &#125;&#125;)&quot;</span><br><span class="line">  - alert: UnusualDiskReadLatency</span><br><span class="line">    expr: rate(node_disk_read_time_ms[1m]) &#x2F; rate(node_disk_reads_completed[1m]) &gt; 100</span><br><span class="line">    for: 5m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Unusual disk read latency (instance &#123;&#123; $labels.instance &#125;&#125;)&quot;</span><br><span class="line">      description: &quot;Disk latency is growing (read operations &gt; 100ms) (current value: &#123;&#123; $value &#125;&#125;)&quot;</span><br><span class="line">  - alert: UnusualDiskWriteLatency</span><br><span class="line">    expr: rate(node_disk_write_time_ms[1m]) &#x2F; rate(node_disk_writes_completedl[1m]) &gt; 100</span><br><span class="line">    for: 5m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Unusual disk write latency (instance &#123;&#123; $labels.instance &#125;&#125;)&quot;</span><br><span class="line">      description: &quot;Disk latency is growing (write operations &gt; 100ms) (current value: &#123;&#123; $value &#125;&#125;)&quot;</span><br><span class="line">- name: http_status</span><br><span class="line">  rules:</span><br><span class="line">  - alert: ProbeFailed</span><br><span class="line">    expr: probe_success &#x3D;&#x3D; 0</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: error</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Probe failed (instance &#123;&#123; $labels.instance &#125;&#125;)&quot;</span><br><span class="line">      description: &quot;Probe failed (current value: &#123;&#123; $value &#125;&#125;)&quot;</span><br><span class="line">  - alert: StatusCode</span><br><span class="line">    expr: probe_http_status_code &lt;&#x3D; 199 OR probe_http_status_code &gt;&#x3D; 400</span><br><span class="line">    for: 1m</span><br><span class="line">    labels:</span><br><span class="line">      severity: error</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Status Code (instance &#123;&#123; $labels.instance &#125;&#125;)&quot;</span><br><span class="line">      description: &quot;HTTP status code is not 200-399 (current value: &#123;&#123; $value &#125;&#125;)&quot;</span><br><span class="line">  - alert: SslCertificateWillExpireSoon</span><br><span class="line">    expr: probe_ssl_earliest_cert_expiry - time() &lt; 86400 * 30</span><br><span class="line">    for: 5m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;SSL certificate will expire soon (instance &#123;&#123; $labels.instance &#125;&#125;)&quot;</span><br><span class="line">      description: &quot;SSL certificate expires in 30 days (current value: &#123;&#123; $value &#125;&#125;)&quot;</span><br><span class="line">  - alert: SslCertificateHasExpired</span><br><span class="line">    expr: probe_ssl_earliest_cert_expiry - time()  &lt;&#x3D; 0</span><br><span class="line">    for: 5m</span><br><span class="line">    labels:</span><br><span class="line">      severity: error</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;SSL certificate has expired (instance &#123;&#123; $labels.instance &#125;&#125;)&quot;</span><br><span class="line">      description: &quot;SSL certificate has expired already (current value: &#123;&#123; $value &#125;&#125;)&quot;</span><br><span class="line">  - alert: BlackboxSlowPing</span><br><span class="line">    expr: probe_icmp_duration_seconds &gt; 2</span><br><span class="line">    for: 5m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Blackbox slow ping (instance &#123;&#123; $labels.instance &#125;&#125;)&quot;</span><br><span class="line">      description: &quot;Blackbox ping took more than 2s (current value: &#123;&#123; $value &#125;&#125;)&quot;</span><br><span class="line">  - alert: BlackboxSlowRequests</span><br><span class="line">    expr: probe_http_duration_seconds &gt; 2 </span><br><span class="line">    for: 5m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Blackbox slow requests (instance &#123;&#123; $labels.instance &#125;&#125;)&quot;</span><br><span class="line">      description: &quot;Blackbox request took more than 2s (current value: &#123;&#123; $value &#125;&#125;)&quot;</span><br><span class="line">  - alert: PodCpuUsagePercent</span><br><span class="line">    expr: sum(sum(label_replace(irate(container_cpu_usage_seconds_total[1m]),&quot;pod&quot;,&quot;$1&quot;,&quot;container_label_io_kubernetes_pod_name&quot;, &quot;(.*)&quot;))by(pod) &#x2F; on(pod) group_right kube_pod_container_resource_limits_cpu_cores *100 )by(container,namespace,node,pod,severity) &gt; 80</span><br><span class="line">    for: 5m</span><br><span class="line">    labels:</span><br><span class="line">      severity: warning</span><br><span class="line">    annotations:</span><br><span class="line">      summary: &quot;Pod cpu usage percent has exceeded 80% (current value: &#123;&#123; $value &#125;&#125;%)&quot;</span><br></pre></td></tr></table></figure><p>在/data/nfs/v1/prometheus/etc/prometheus.yml中添加配置：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">alerting:</span><br><span class="line">  alertmanagers:</span><br><span class="line">    - static_configs:</span><br><span class="line">        - targets: [&quot;alertmanager&quot;]</span><br><span class="line">rule_files:</span><br><span class="line"> - &quot;&#x2F;data&#x2F;etc&#x2F;rules.yml&quot;</span><br></pre></td></tr></table></figure><p>重载配置</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -X POST http:&#x2F;&#x2F;prometheus.od.com&#x2F;-&#x2F;reload</span><br></pre></td></tr></table></figure><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfqv8547e9j30d20ezaas.jpg" alt="img"></p><p> 以上这些就是我们的告警规则</p><p>测试告警：</p><p>把app命名空间里的dubbo-demo-service给停掉：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gft0f88hxqj30sh02bq2z.jpg" alt="img"></p><p> 看下blackbox里的信息： <a href="http://blackbox.od.com/" target="_blank" rel="noopener">http://blackbox.od.com/</a></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gft0ijndtej31dc0o80wo.jpg" alt="image-20200615153640360"></p><p>看下alert告警：</p><p><img src="https://img2018.cnblogs.com/blog/1034759/201912/1034759-20191218174052248-183200263.png" alt="img"></p><p> 红色的时候就开会发邮件告警：</p><p><img src="https://img2018.cnblogs.com/blog/1034759/201912/1034759-20191218174118507-915764907.png" alt="img"></p><p><strong>邮件告警示例图</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gft0kcu1enj30sf0h6q56.jpg" alt="邮件告警示例图"></p><p>后续上生产，还会更新如何添加微信、钉钉、短信告警</p><p> 如果需要自己定制告警规则和告警内容，需要研究一下promql，自己修改配置文件。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 老男孩 </category>
          
          <category> 实战交付 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 老男孩 </tag>
            
            <tag> 实战交付 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>三、交付-使用apollo配置中心</title>
      <link href="/2020/06/08/%E4%B8%89%E3%80%81%E4%BA%A4%E4%BB%98-%E4%BD%BF%E7%94%A8apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/"/>
      <url>/2020/06/08/%E4%B8%89%E3%80%81%E4%BA%A4%E4%BB%98-%E4%BD%BF%E7%94%A8apollo%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/</url>
      
        <content type="html"><![CDATA[<h2 id="configmap资源"><a href="#configmap资源" class="headerlink" title="configmap资源"></a>configmap资源</h2><p>在我们的环境中测试使用configmap资源，需要先对我们的环境进行一些准备</p><p> 新建一个zk环境(见以前部署zookeeper)，模拟测试环境跟生产环境：</p><h3 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h3><table><thead><tr><th>主机名</th><th>IP地址</th><th>角色</th></tr></thead><tbody><tr><td>wang-11.host.com</td><td>192.168.70.11</td><td>zk1.od.com(Test环境)</td></tr><tr><td>wang-12.host.com</td><td>192.168.70.12</td><td>zk2.od.com(Prod环境)</td></tr></tbody></table><h3 id="创建资源配置清单"><a href="#创建资源配置清单" class="headerlink" title="创建资源配置清单"></a>创建资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# cd &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-monitor&#x2F;</span><br></pre></td></tr></table></figure><h4 id="cm-yaml"><a href="#cm-yaml" class="headerlink" title="cm.yaml"></a>cm.yaml</h4><p>红色部分是配置文件的name，下面的是内容。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-monitor&#x2F;cm.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: dubbo-monitor-cm</span><br><span class="line">  namespace: infra</span><br><span class="line">data:</span><br><span class="line">  dubbo.properties: |</span><br><span class="line">    dubbo.container&#x3D;log4j,spring,registry,jetty</span><br><span class="line">    dubbo.application.name&#x3D;simple-monitor</span><br><span class="line">    dubbo.application.owner&#x3D;OldboyEdu</span><br><span class="line">    dubbo.registry.address&#x3D;zookeeper:&#x2F;&#x2F;zk1.od.com:2181</span><br><span class="line">    dubbo.protocol.port&#x3D;20880</span><br><span class="line">    dubbo.jetty.port&#x3D;8080</span><br><span class="line">    dubbo.jetty.directory&#x3D;&#x2F;dubbo-monitor-simple&#x2F;monitor</span><br><span class="line">    dubbo.charts.directory&#x3D;&#x2F;dubbo-monitor-simple&#x2F;charts</span><br><span class="line">    dubbo.statistics.directory&#x3D;&#x2F;dubbo-monitor-simple&#x2F;statistics</span><br><span class="line">    dubbo.log4j.file&#x3D;&#x2F;dubbo-monitor-simple&#x2F;logs&#x2F;dubbo-monitor.log</span><br><span class="line">    dubbo.log4j.level&#x3D;WARN</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="deploy-cm-yaml"><a href="#deploy-cm-yaml" class="headerlink" title="deploy-cm.yaml"></a>deploy-cm.yaml</h4><p>在dp里面如何使用configmap资源：</p><p>首先声明一个卷，卷的名字叫configmap-volume,然后指定这个卷使用的configmap</p><p>然后定义这个卷的挂载，挂载到哪里。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-monitor&#x2F;deploy-cm.yaml</span><br><span class="line">kind: Deployment</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: dubbo-monitor</span><br><span class="line">  namespace: infra</span><br><span class="line">  labels: </span><br><span class="line">    name: dubbo-monitor</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels: </span><br><span class="line">      name: dubbo-monitor</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels: </span><br><span class="line">        app: dubbo-monitor</span><br><span class="line">        name: dubbo-monitor</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: dubbo-monitor</span><br><span class="line">        image: harbor.od.com&#x2F;infra&#x2F;dubbo-monitor:latest</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          protocol: TCP</span><br><span class="line">        - containerPort: 20880</span><br><span class="line">          protocol: TCP</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        volumeMounts:</span><br><span class="line">          - name: configmap-volume</span><br><span class="line">            mountPath: &#x2F;dubbo-monitor-simple&#x2F;conf</span><br><span class="line">      volumes:</span><br><span class="line">        - name: configmap-volume</span><br><span class="line">          configMap:</span><br><span class="line">            name: dubbo-monitor-cm</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor</span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      securityContext: </span><br><span class="line">        runAsUser: 0</span><br><span class="line">      schedulerName: default-scheduler</span><br><span class="line">  strategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate: </span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">      maxSurge: 1</span><br><span class="line">  revisionHistoryLimit: 7</span><br><span class="line">  progressDeadlineSeconds: 600</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单"><a href="#应用资源配置清单" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-monitor&#x2F;cm.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-monitor&#x2F;deploy-cm.yaml</span><br></pre></td></tr></table></figure><p>去dashboard查看configmap资源：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfl5hx19pjj31bl0u0jwt.jpg" alt="configmap资源"></p><p>我们可以创建多个configmap资源，然后在dp中去挂载应用这些configmap资源，达到修改配置的功能。 </p><p>我们检查一下我们的容器：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pod -n infra</span><br><span class="line">NAME                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">dubbo-monitor-6676dd74cc-drlgz   1&#x2F;1     Running   0          3m4s</span><br></pre></td></tr></table></figure><p>已经起来了</p><p> 我们检查一下我们挂载的配置是不是我们定义的configmap资源中的配置：</p><p>我们把配置文件挂载到了<strong>/dubbo-monitor-simple/**</strong>conf** 这里，我们去看一下。（上面的dp-cm.yaml中声明的）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl exec -it dubbo-monitor-6676dd74cc-drlgz -n infra &#x2F;bin&#x2F;bash</span><br><span class="line">bash-4.3# cat &#x2F;dubbo-monitor-simple&#x2F;conf&#x2F;dubbo.properties</span><br><span class="line">dubbo.container&#x3D;log4j,spring,registry,jetty</span><br><span class="line">dubbo.application.name&#x3D;simple-monitor</span><br><span class="line">dubbo.application.owner&#x3D;OldboyEdu</span><br><span class="line">dubbo.registry.address&#x3D;zookeeper:&#x2F;&#x2F;zk1.od.com:2181</span><br><span class="line">dubbo.protocol.port&#x3D;20880</span><br><span class="line">dubbo.jetty.port&#x3D;8080</span><br><span class="line">dubbo.jetty.directory&#x3D;&#x2F;dubbo-monitor-simple&#x2F;monitor</span><br><span class="line">dubbo.charts.directory&#x3D;&#x2F;dubbo-monitor-simple&#x2F;charts</span><br><span class="line">dubbo.statistics.directory&#x3D;&#x2F;dubbo-monitor-simple&#x2F;statistics</span><br><span class="line">dubbo.log4j.file&#x3D;&#x2F;dubbo-monitor-simple&#x2F;logs&#x2F;dubbo-monitor.log</span><br><span class="line">dubbo.log4j.level&#x3D;WARN</span><br></pre></td></tr></table></figure><p>跟我们定义的一模一样。</p><h3 id="通过configmap更换配置"><a href="#通过configmap更换配置" class="headerlink" title="通过configmap更换配置"></a>通过configmap更换配置</h3><p>这里如果想有两种方法：</p><p>　　一、修改configmap 资源，然后apply一下更新资源，然后重启挂载这个configmap资源的deploy。</p><p>　　二、准备多个configmap资源，然后在deploy中更改挂载的configmap,apply以后，dp自动重启。</p><p>检查dubbo-monitor页面的注册信息：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfl5ppzu9zj322i0aaacs.jpg" alt="dubbo-monitor页面的注册信息"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfl5obgm1dj31z20eijut.jpg" alt="dubbo-monitor页面的数据源"></p><p>连接的zk1.od.com，下面我们模拟更换configmap资源，来切换环境：</p><p> 这里使用第二种方法，准备多个configmap，我们在准备一个configmap,就叫cm-pro.yaml:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-monitor&#x2F;cm.yaml &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-monitor&#x2F;cm-pro.yaml</span><br><span class="line">sed -i &quot;s&amp;dubbo-monitor-cm&amp;dubbo-monitor-cm-pro&amp;&quot; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-monitor&#x2F;cm-pro.yaml</span><br><span class="line">sed -i &quot;s&amp;zk1.od.com&amp;zk2.od.com&amp;&quot; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-monitor&#x2F;cm-pro.yaml</span><br></pre></td></tr></table></figure><p> 应用cm-pro.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-monitor&#x2F;cm-pro.yaml</span><br></pre></td></tr></table></figure><p>通过UI检查执行结果</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfl5uujaxbj315u0ckmxu.jpg" alt="配置字典cm"></p><p>dubbo-monitor-cm-pro的数据</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfl5we7g6zj317q0h6n0m.jpg" alt="dubbo-monitor-cm-pro的数据"></p><p> 然后我们修改dp-cm.yaml并应用</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed -i &quot;s&amp;dubbo-monitor-cm&amp;dubbo-monitor-cm-pro&amp;&quot; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-monitor&#x2F;deploy-cm.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-monitor&#x2F;deploy-cm.yaml</span><br></pre></td></tr></table></figure><p>新的dubbo-monitor pod起来后，我们进去看看是不是应用的新的configmap配置：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl exec -it dubbo-monitor-5cb756cc6c-xgqfg -n infra &#x2F;bin&#x2F;bash</span><br><span class="line">bash-4.3# cat &#x2F;dubbo-monitor-simple&#x2F;conf&#x2F;dubbo.properties </span><br><span class="line">dubbo.container&#x3D;log4j,spring,registry,jetty</span><br><span class="line">dubbo.application.name&#x3D;simple-monitor</span><br><span class="line">dubbo.application.owner&#x3D;OldboyEdu</span><br><span class="line">dubbo.registry.address&#x3D;zookeeper:&#x2F;&#x2F;zk2.od.com:2181</span><br><span class="line">dubbo.protocol.port&#x3D;20880</span><br><span class="line">dubbo.jetty.port&#x3D;8080</span><br><span class="line">dubbo.jetty.directory&#x3D;&#x2F;dubbo-monitor-simple&#x2F;monitor</span><br><span class="line">dubbo.charts.directory&#x3D;&#x2F;dubbo-monitor-simple&#x2F;charts</span><br><span class="line">dubbo.statistics.directory&#x3D;&#x2F;dubbo-monitor-simple&#x2F;statistics</span><br><span class="line">dubbo.log4j.file&#x3D;&#x2F;dubbo-monitor-simple&#x2F;logs&#x2F;dubbo-monitor.log</span><br><span class="line">dubbo.log4j.level&#x3D;WARN</span><br></pre></td></tr></table></figure><p>查看dubbo-monitor界面，已经是zk2.od.com了。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfl60lhcalj31ky06ugmk.jpg" alt="dubbo源换成了zk2.od.com"></p><p><strong>更新configmap资源来更改配置需要更新(删除/apply/update)pod，否则无效。</strong></p><blockquote><p>注意，我们这里使用的是mountPath，这个是挂载整个目录，会使容器内的被挂载目录中原有的文件不可见，我们原来脚本中的命令已经无法对挂载的目录操作了。</p></blockquote><h3 id="使用subPath挂载指定的文件"><a href="#使用subPath挂载指定的文件" class="headerlink" title="使用subPath挂载指定的文件"></a>使用subPath挂载指定的文件</h3><p>查看我们pod容器启动的命令可以看见：如果想单独挂载一个配置配件，而不是整个目录，如何操作：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfl64yqzvgj31ju05uwfl.jpg" alt="目录只读"></p><p>这里我使用之前的nginx:curl来做如何挂载单个的文件：</p><p>查看资源key的使用方法：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl explain pod.spec.containers.volumeMounts</span><br></pre></td></tr></table></figure><p>这里有个挂载方法是：subPath,使用这个方法，可以挂载指定的文件，要结合mountPath来使用：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">subPath      &lt;string&gt;</span><br><span class="line">  Path within the volume from which the container&#39;s volume should be mounted.</span><br><span class="line">  Defaults to &quot;&quot; (volume&#39;s root).</span><br></pre></td></tr></table></figure><p>查看我们原来实验做的nginx:curl这个容器：在default命名空间里。</p><p>我们实验的需求，把<strong>dubbo.properties</strong>这个配置文件挂载到/usr/lib/目录下，并且保证原来容器内/usr/lib/目录下的文件都还在：</p><p> 进入容器查看容器内/usr/lib/下有哪些文件：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl exec -it nginx-ds-92dn4 &#x2F;bin&#x2F;bash</span><br><span class="line">root@nginx-ds-92dn4:&#x2F;# ls &#x2F;usr&#x2F;lib&#x2F;</span><br><span class="line">apt  coreutils  dpkg  gcc  gnupg  libperl.so.5.14  libperl.so.5.14.2  locale  mime  perl  perl5  pt_chown  python2.6  python2.7  python3  tc  x86_64-linux-gnu</span><br></pre></td></tr></table></figure><p>在default命名空间下创建configmap资源：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-monitor&#x2F;cm.yaml &#x2F;root&#x2F;default&#x2F;cm-test.yaml</span><br><span class="line">sed -i &quot;s&amp;namespace: infra&amp;namespace: default&amp;&quot; &#x2F;root&#x2F;default&#x2F;cm-test.yaml</span><br><span class="line">kubectl apply -f  &#x2F;root&#x2F;default&#x2F;cm-test.yaml</span><br></pre></td></tr></table></figure><p>然后修改这个容器的资源配置清单,挂载configmap资源：一定要注意格式跟缩进<del>~</del></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# cp default&#x2F;nginx-ds.yml default&#x2F;nginx-ds-cm.yml</span><br><span class="line">[root@wang-200 ~]# vim default&#x2F;nginx-ds-cm.yml (添加部分)</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: configmap-volume</span><br><span class="line">          mountPath: &#x2F;usr&#x2F;lib&#x2F;dubbo-properties</span><br><span class="line">          subPath: dubbo-properties</span><br><span class="line">      volumes:</span><br><span class="line">      - name: configmap-volume</span><br><span class="line">        configMap:</span><br><span class="line">          name: dubbo-monitor-cm</span><br><span class="line">          defaultMode: 420</span><br><span class="line">[root@wang-200 ~]# kubectl apply -f &#x2F;root&#x2F;default&#x2F;nginx-ds-cm.yml</span><br></pre></td></tr></table></figure><p> 然后重启pod</p><p>登录进容器中，查看：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfl7blo5ahj30xf01sdg0.jpg" alt="新增配置文件dubbo-properties">     </p><p> 经过对比，我们原来/usr/lib/下的文件还在，并且新增了一个配置文件dubbo-properties这个配置文件。</p><h2 id="交付apollo配置中心到k8s"><a href="#交付apollo配置中心到k8s" class="headerlink" title="交付apollo配置中心到k8s"></a>交付apollo配置中心到k8s</h2><p>apollo官网：<a href="https://github.com/ctripcorp/apollo" target="_blank" rel="noopener">官方地址</a></p><h3 id="apollo架构图："><a href="#apollo架构图：" class="headerlink" title="apollo架构图："></a>apollo架构图：</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfl7lktr2qj30mw0fxk12.jpg" alt="apollo架构图"></p><h3 id="安装mysql数据库"><a href="#安装mysql数据库" class="headerlink" title="安装mysql数据库"></a>安装mysql数据库</h3><p>apollo需要使用数据库，这里使用mysql，注意版本需要在5.6以上：</p><p>本次环境mysql部署在192.168.70.12上，使用mariadb：10.1以上版本</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;MariaDB.repo</span><br><span class="line">[mariadb]</span><br><span class="line">name &#x3D; MariaDB</span><br><span class="line">baseurl &#x3D; https:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;mariadb&#x2F;yum&#x2F;10.1&#x2F;centos7-amd64&#x2F;</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;mariadb&#x2F;yum&#x2F;RPM-GPG-KEY-MariaDB</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>导入key并安装</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rpm --import https:&#x2F;&#x2F;mirrors.ustc.edu.cn&#x2F;mariadb&#x2F;yum&#x2F;RPM-GPG-KEY-MariaDB</span><br><span class="line">yum install MariaDB-server -y</span><br></pre></td></tr></table></figure><p>简单配置mysql：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 ~]# vi &#x2F;etc&#x2F;my.cnf.d&#x2F;server.cnf</span><br><span class="line">[mysqld]</span><br><span class="line">character_set_server &#x3D; utf8mb4</span><br><span class="line">collation_server &#x3D; utf8mb4_general_ci</span><br><span class="line">init_connect &#x3D; &quot;SET NAMES &#39;utf8mb4&#39;&quot;</span><br><span class="line"></span><br><span class="line">[root@wang-12 ~]# vi &#x2F;etc&#x2F;my.cnf.d&#x2F;mysql-clients.cnf</span><br><span class="line">[mysql]</span><br><span class="line">default-character-set &#x3D; utf8mb4</span><br></pre></td></tr></table></figure><p>启动数据库并设置开机自启</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start mariadb</span><br><span class="line">systemctl enable mariadb</span><br></pre></td></tr></table></figure><p>设置开启密码</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysqladmin -u root password</span><br></pre></td></tr></table></figure><p>登录检查字符集：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 ~]# mysql -uroot -p</span><br><span class="line">MariaDB [(none)]&gt; \s</span><br><span class="line">--------------</span><br><span class="line">mysql  Ver 15.1 Distrib 10.1.45-MariaDB, for Linux (x86_64) using readline 5.1</span><br><span class="line"></span><br><span class="line">Connection id:          6</span><br><span class="line">Current database:</span><br><span class="line">Current user:           root@localhost</span><br><span class="line">SSL:                    Not in use</span><br><span class="line">Current pager:          stdout</span><br><span class="line">Using outfile:          &#39;&#39;</span><br><span class="line">Using delimiter:        ;</span><br><span class="line">Server:                 MariaDB</span><br><span class="line">Server version:         10.1.45-MariaDB MariaDB Server</span><br><span class="line">Protocol version:       10</span><br><span class="line">Connection:             Localhost via UNIX socket</span><br><span class="line">Server characterset:    utf8mb4      # 字符集</span><br><span class="line">Db     characterset:    utf8mb4      # 字符集</span><br><span class="line">Client characterset:    utf8mb4      # 字符集</span><br><span class="line">Conn.  characterset:    utf8mb4      # 字符集</span><br><span class="line">UNIX socket:            &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql.sock</span><br><span class="line">Uptime:                 3 min 59 sec</span><br></pre></td></tr></table></figure><p>授权用户运维主机登陆</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; grant all on *.* to &quot;root&quot;@&quot;192.168.70.200&quot;  identified by &#39;admin&#39; WITH GRANT OPTION;          </span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure><h4 id="添加mysql-od-com域名解析："><a href="#添加mysql-od-com域名解析：" class="headerlink" title="添加mysql.od.com域名解析："></a>添加mysql.od.com域名解析：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 ~]# vim &#x2F;var&#x2F;named&#x2F;od.com.zone </span><br><span class="line">mysql           A       192.168.70.12</span><br><span class="line"></span><br><span class="line">[root@wang-12 ~]# systemctl restart named</span><br><span class="line">[root@wang-12 ~]# dig -t A mysql.od.com +short</span><br><span class="line">192.168.70.12</span><br></pre></td></tr></table></figure><h3 id="初始化脚本"><a href="#初始化脚本" class="headerlink" title="初始化脚本"></a>初始化脚本</h3><p>执行数据库初始化脚本：<a href="https://github.com/ctripcorp/apollo/blob/1.5.1/scripts/db/migration/configdb/V1.0.0__initialization.sql" target="_blank" rel="noopener">configdb初始化脚本</a></p><p>下载脚本：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget -O apolloconfig.sql  https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;ctripcorp&#x2F;apollo&#x2F;1.5.1&#x2F;scripts&#x2F;db&#x2F;migration&#x2F;configdb&#x2F;V1.0.0__initialization.sql</span><br></pre></td></tr></table></figure><p>执行初始化脚本：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -h wang-12 -p &lt; apolloconfig.sql</span><br></pre></td></tr></table></figure><p>检查数据库：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gflsc3w05mj30al05mjrt.jpg" alt="ApolloConfigDB数据库"></p><p> 给数据库授权：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grant INSERT,DELETE,UPDATE,SELECT on ApolloConfigDB.* to &#39;apolloconfig&#39;@&#39;192.168.70.%&#39;  identified by &quot;123456&quot;;</span><br></pre></td></tr></table></figure><p> 修改初始化数据：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; update ApolloConfigDB.ServerConfig set ServerConfig.Value&#x3D;&quot;http:&#x2F;&#x2F;config.od.com&#x2F;eureka&quot; where ServerConfig.Key&#x3D;&quot;eureka.service.url&quot;;</span><br></pre></td></tr></table></figure><p>交付顺序：</p><p>　　1、apolloconfigservice</p><p>　　2、adminservice</p><p>　　3、portal</p><p>下载apolloconfigservice的包：放到200上</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;github.com&#x2F;ctripcorp&#x2F;apollo&#x2F;releases&#x2F;download&#x2F;v1.5.1&#x2F;apollo-configservice-1.5.1-github.zip -O &#x2F;data&#x2F;soft&#x2F;centos7&#x2F;apollo-configservice-1.5.1-github.zip</span><br><span class="line">wget https:&#x2F;&#x2F;github.com&#x2F;ctripcorp&#x2F;apollo&#x2F;releases&#x2F;download&#x2F;v1.5.1&#x2F;apollo-adminservice-1.5.1-github.zip -O &#x2F;data&#x2F;soft&#x2F;centos7&#x2F;apollo-adminservice-1.5.1-github.zip</span><br><span class="line">wget https:&#x2F;&#x2F;github.com&#x2F;ctripcorp&#x2F;apollo&#x2F;releases&#x2F;download&#x2F;v1.5.1&#x2F;apollo-portal-1.5.1-github.zip -O &#x2F;data&#x2F;soft&#x2F;centos7&#x2F;apollo-portal-1.5.1-github.zip</span><br></pre></td></tr></table></figure><h3 id="交付apolloconfigservice"><a href="#交付apolloconfigservice" class="headerlink" title="交付apolloconfigservice"></a>交付apolloconfigservice</h3><p>解压安装包，制作镜像</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;dockerfile&#x2F;apollo-configservice</span><br><span class="line">unzip -o &#x2F;data&#x2F;soft&#x2F;centos7&#x2F;apollo-configservice-1.5.1-github.zip -d &#x2F;data&#x2F;dockerfile&#x2F;apollo-configservice&#x2F;</span><br><span class="line">cd &#x2F;data&#x2F;dockerfile&#x2F;apollo-configservice&#x2F;</span><br></pre></td></tr></table></figure><p>修改连接数据库配置：</p><p><code>vim config/application-github.properties</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spring.datasource.url &#x3D; jdbc:mysql:&#x2F;&#x2F;mysql.od.com:3306&#x2F;ApolloConfigDB?characterEncoding&#x3D;utf8</span><br><span class="line">spring.datasource.username &#x3D; apolloconfig</span><br><span class="line">spring.datasource.password &#x3D; 123456</span><br></pre></td></tr></table></figure><p>修改启动脚本：</p><p>将官网上的startup.sh内容替换进来 <a href="https://github.com/ctripcorp/apollo/blob/1.5.1/scripts/apollo-on-kubernetes/apollo-config-server/scripts/startup-kubernetes.sh" target="_blank" rel="noopener">脚本地址</a></p><p> 添加一行：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">APOLLO_CONFIG_SERVICE_NAME&#x3D;$(hostname -i)</span><br></pre></td></tr></table></figure><p>自行优化JVM</p><p>添加执行权限</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod u+x scripts&#x2F;startup.sh</span><br></pre></td></tr></table></figure><h4 id="通过dockerfile构建镜像"><a href="#通过dockerfile构建镜像" class="headerlink" title="通过dockerfile构建镜像"></a>通过dockerfile构建镜像</h4><p>编写dockerfile：<a href="https://github.com/ctripcorp/apollo/blob/1.5.1/scripts/apollo-on-kubernetes/apollo-config-server/Dockerfile" target="_blank" rel="noopener"> 官方地址</a></p><p>Dockerfile</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM harbor.od.com&#x2F;base&#x2F;jre8:8u112</span><br><span class="line"></span><br><span class="line">ENV VERSION 1.5.1</span><br><span class="line"></span><br><span class="line">RUN ln -sf &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai &#x2F;etc&#x2F;localtime &amp;&amp;\</span><br><span class="line">    echo &quot;Asia&#x2F;Shanghai&quot; &gt; &#x2F;etc&#x2F;timezone</span><br><span class="line"></span><br><span class="line">ADD apollo-configservice-$&#123;VERSION&#125;.jar &#x2F;apollo-configservice&#x2F;apollo-configservice.jar</span><br><span class="line">ADD config&#x2F; &#x2F;apollo-configservice&#x2F;config</span><br><span class="line">ADD scripts&#x2F; &#x2F;apollo-configservice&#x2F;scripts</span><br><span class="line"></span><br><span class="line">CMD [&quot;&#x2F;apollo-configservice&#x2F;scripts&#x2F;startup.sh&quot;]</span><br></pre></td></tr></table></figure><p>通过dockerfile构建镜像并推送到镜像仓库</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker build . -t harbor.od.com&#x2F;infra&#x2F;apollo-configservice:v1.5.1</span><br><span class="line">docker push harbor.od.com&#x2F;infra&#x2F;apollo-configservice:v1.5.1</span><br></pre></td></tr></table></figure><h3 id="编写资源配置清单"><a href="#编写资源配置清单" class="headerlink" title="编写资源配置清单"></a>编写资源配置清单</h3><p>创建资源配置清单文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;apollo-configservice</span><br></pre></td></tr></table></figure><p>cm.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;apollo-configservice&#x2F;cm.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: apollo-configservice-cm</span><br><span class="line">  namespace: infra</span><br><span class="line">data:</span><br><span class="line">  application-github.properties: |</span><br><span class="line">    # DataSource</span><br><span class="line">    spring.datasource.url &#x3D; jdbc:mysql:&#x2F;&#x2F;mysql.od.com:3306&#x2F;ApolloConfigDB?characterEncoding&#x3D;utf8</span><br><span class="line">    spring.datasource.username &#x3D; apolloconfig</span><br><span class="line">    spring.datasource.password &#x3D; 123456</span><br><span class="line">    eureka.service.url &#x3D; http:&#x2F;&#x2F;config.od.com&#x2F;eureka</span><br><span class="line">  app.properties: |</span><br><span class="line">    appId&#x3D;100003171</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>deploy.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;apollo-configservice&#x2F;deploy.yaml</span><br><span class="line">kind: Deployment</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: apollo-configservice</span><br><span class="line">  namespace: infra</span><br><span class="line">  labels: </span><br><span class="line">    name: apollo-configservice</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels: </span><br><span class="line">      name: apollo-configservice</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels: </span><br><span class="line">        app: apollo-configservice </span><br><span class="line">        name: apollo-configservice</span><br><span class="line">    spec:</span><br><span class="line">      volumes:</span><br><span class="line">      - name: configmap-volume</span><br><span class="line">        configMap:</span><br><span class="line">          name: apollo-configservice-cm</span><br><span class="line">      containers:</span><br><span class="line">      - name: apollo-configservice</span><br><span class="line">        image: harbor.od.com&#x2F;infra&#x2F;apollo-configservice:v1.5.1</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          protocol: TCP</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: configmap-volume</span><br><span class="line">          mountPath: &#x2F;apollo-configservice&#x2F;config</span><br><span class="line">        terminationMessagePath: &#x2F;dev&#x2F;termination-log</span><br><span class="line">        terminationMessagePolicy: File</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor</span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      securityContext: </span><br><span class="line">        runAsUser: 0</span><br><span class="line">      schedulerName: default-scheduler</span><br><span class="line">  strategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate: </span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">      maxSurge: 1</span><br><span class="line">  revisionHistoryLimit: 7</span><br><span class="line">  progressDeadlineSeconds: 600</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>svc.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;apollo-configservice&#x2F;svc.yaml</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata: </span><br><span class="line">  name: apollo-configservice</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector: </span><br><span class="line">    app: apollo-configservice</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>ingress.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;apollo-configservice&#x2F;ingress.yaml</span><br><span class="line">kind: Ingress</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata: </span><br><span class="line">  name: apollo-configservice</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: config.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend: </span><br><span class="line">          serviceName: apollo-configservice</span><br><span class="line">          servicePort: 8080</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单："><a href="#应用资源配置清单：" class="headerlink" title="应用资源配置清单："></a>应用资源配置清单：</h3><p>执行命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;apollo-configservice&#x2F;cm.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;apollo-configservice&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;apollo-configservice&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;apollo-configservice&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p>检查启动情况</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 apollo-configservice]# kubectl get pod -n infra</span><br><span class="line">NAME                                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">apollo-configservice-5f6555448-slw6h   1&#x2F;1     Running   0          24s</span><br></pre></td></tr></table></figure><p>查看pod启动日志</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> 2020-06-09 13:21:20.370  INFO 40 --- [ost-startStop-1] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...</span><br><span class="line">2020-06-09 13:21:20.799  INFO 40 --- [ost-startStop-1] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.</span><br></pre></td></tr></table></figure><p> 需要等到eureka启动以后才可以，接下来使用浏览器访问config.od.com:</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gflyzk21pxj31hm0u0wne.jpg" alt="config.od.com访问界面"></p><h2 id="交付adminservice"><a href="#交付adminservice" class="headerlink" title="交付adminservice"></a>交付adminservice</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;dockerfile&#x2F;apollo-adminservice</span><br><span class="line">unzip -o &#x2F;data&#x2F;soft&#x2F;centos7&#x2F;apollo-adminservice-1.5.1-github.zip -d &#x2F;data&#x2F;dockerfile&#x2F;apollo-adminservice&#x2F;</span><br><span class="line">cd &#x2F;data&#x2F;dockerfile&#x2F;apollo-adminservice&#x2F;</span><br></pre></td></tr></table></figure><p>由于使用了configmap资源将配置文件挂载出来了，所以不在修改配置文件，如需修改配置文件，请参考部署apollo-configservice时候的修改方法：</p><p>修改startup.sh: <a href="https://github.com/ctripcorp/apollo/blob/master/scripts/apollo-on-kubernetes/apollo-admin-server/scripts/startup-kubernetes.sh" target="_blank" rel="noopener">官方地址</a></p><p>将端口修改为：8080</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 apollo-adminservice]# vim scripts&#x2F;startup.sh</span><br><span class="line">SERVER_PORT&#x3D;8080</span><br><span class="line">APOLLO_ADMIN_SERVICE_NAME&#x3D;$(hostname -i)</span><br><span class="line">[root@wang-200 apollo-adminservice]# chmod u+x scripts&#x2F;startup.sh</span><br></pre></td></tr></table></figure><p>制作dockerfile：</p><p>dockerfile</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM stanleyws&#x2F;jre8:8u112</span><br><span class="line"></span><br><span class="line">ENV VERSION 1.5.1</span><br><span class="line"></span><br><span class="line">RUN ln -sf &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai &#x2F;etc&#x2F;localtime &amp;&amp;\</span><br><span class="line">    echo &quot;Asia&#x2F;Shanghai&quot; &gt; &#x2F;etc&#x2F;timezone</span><br><span class="line"></span><br><span class="line">ADD apollo-adminservice-$&#123;VERSION&#125;.jar &#x2F;apollo-adminservice&#x2F;apollo-adminservice.jar</span><br><span class="line">ADD config&#x2F; &#x2F;apollo-adminservice&#x2F;config</span><br><span class="line">ADD scripts&#x2F; &#x2F;apollo-adminservice&#x2F;scripts</span><br><span class="line"></span><br><span class="line">CMD [&quot;&#x2F;apollo-adminservice&#x2F;scripts&#x2F;startup.sh&quot;]</span><br></pre></td></tr></table></figure><p>通过dockerfile制作镜像并推送到镜像仓库</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker build -t harbor.od.com&#x2F;infra&#x2F;apollo-adminservice:v1.5.1 .</span><br><span class="line">docker push harbor.od.com&#x2F;infra&#x2F;apollo-adminservice:v1.5.1</span><br></pre></td></tr></table></figure><h3 id="制作资源配置清单"><a href="#制作资源配置清单" class="headerlink" title="制作资源配置清单"></a>制作资源配置清单</h3><p>创建文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;apollo-adminservice</span><br></pre></td></tr></table></figure><p>cm.yaml </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;apollo-adminservice&#x2F;cm.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: apollo-adminservice-cm</span><br><span class="line">  namespace: infra</span><br><span class="line">data:</span><br><span class="line">  application-github.properties: |</span><br><span class="line">    # DataSource</span><br><span class="line">    spring.datasource.url &#x3D; jdbc:mysql:&#x2F;&#x2F;mysql.od.com:3306&#x2F;ApolloConfigDB?characterEncoding&#x3D;utf8</span><br><span class="line">    spring.datasource.username &#x3D; apolloconfig</span><br><span class="line">    spring.datasource.password &#x3D; 123456</span><br><span class="line">    eureka.service.url &#x3D; http:&#x2F;&#x2F;config.od.com&#x2F;eureka</span><br><span class="line">  app.properties: |</span><br><span class="line">    appId&#x3D;100003172</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>deploy.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;apollo-adminservice&#x2F;deploy.yaml</span><br><span class="line">kind: Deployment</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: apollo-adminservice</span><br><span class="line">  namespace: infra</span><br><span class="line">  labels: </span><br><span class="line">    name: apollo-adminservice</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels: </span><br><span class="line">      name: apollo-adminservice</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels: </span><br><span class="line">        app: apollo-adminservice </span><br><span class="line">        name: apollo-adminservice</span><br><span class="line">    spec:</span><br><span class="line">      volumes:</span><br><span class="line">      - name: configmap-volume</span><br><span class="line">        configMap:</span><br><span class="line">          name: apollo-adminservice-cm</span><br><span class="line">      containers:</span><br><span class="line">      - name: apollo-adminservice</span><br><span class="line">        image: harbor.od.com&#x2F;infra&#x2F;apollo-adminservice:v1.5.1</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          protocol: TCP</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: configmap-volume</span><br><span class="line">          mountPath: &#x2F;apollo-adminservice&#x2F;config</span><br><span class="line">        terminationMessagePath: &#x2F;dev&#x2F;termination-log</span><br><span class="line">        terminationMessagePolicy: File</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor</span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      securityContext: </span><br><span class="line">        runAsUser: 0</span><br><span class="line">      schedulerName: default-scheduler</span><br><span class="line">  strategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate: </span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">      maxSurge: 1</span><br><span class="line">  revisionHistoryLimit: 7</span><br><span class="line">  progressDeadlineSeconds: 600</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单-1"><a href="#应用资源配置清单-1" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><p>执行命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;apollo-adminservice&#x2F;cm.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;apollo-adminservice&#x2F;deploy.yaml</span><br></pre></td></tr></table></figure><p>查看pod启动情况</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 apollo-adminservice]# kubectl get pod -n infra</span><br><span class="line">NAME                                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">apollo-adminservice-5cccf97c64-g2pl6   1&#x2F;1     Running   0          74s</span><br></pre></td></tr></table></figure><p>查看程序启动日志</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> 2020-06-09 13:39:38.269  INFO 40 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)</span><br><span class="line">2020-06-09 13:39:38.664  INFO 40 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]</span><br></pre></td></tr></table></figure><p> 通过config.od.com检查是否注册到了eureka：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gflzgb20blj31u00asmzz.jpg" alt="config注册到eureka"></p><h2 id="交付portal"><a href="#交付portal" class="headerlink" title="交付portal"></a>交付portal</h2><p>解压portal压缩包</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;dockerfile&#x2F;apollo-portal</span><br><span class="line">unzip -o &#x2F;data&#x2F;soft&#x2F;centos7&#x2F;apollo-portal-1.5.1-github.zip -d &#x2F;data&#x2F;dockerfile&#x2F;apollo-portal&#x2F;</span><br><span class="line">cd &#x2F;data&#x2F;dockerfile&#x2F;apollo-portal&#x2F;</span><br></pre></td></tr></table></figure><p>由于portal使用的是另一个portaldb，我们需要在数据库中新建portdb，并初始化：<a href="https://github.com/ctripcorp/apollo/blob/master/scripts/db/migration/portaldb/V1.0.0__initialization.sql" target="_blank" rel="noopener">初始化脚本</a></p><p>下载下来脚本</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget -O apollo-portal.sql https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;ctripcorp&#x2F;apollo&#x2F;1.5.1&#x2F;scripts&#x2F;db&#x2F;migration&#x2F;portaldb&#x2F;V1.0.0__initialization.sql</span><br></pre></td></tr></table></figure><p>初始化apollo-portal数据库</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -h wang-12 -p &lt; apollo-portal.sql</span><br></pre></td></tr></table></figure><p>创建用户并授权：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 apollo-portal]# mysql -h wang-12 -p</span><br><span class="line">MariaDB [(none)]&gt; grant INSERT,DELETE,UPDATE,SELECT on ApolloPortalDB.* to &quot;apolloportal&quot;@&quot;192.168.70.%&quot; identified by &quot;123456&quot;;</span><br></pre></td></tr></table></figure><p>查看数据库中的部门</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MariaDB [ApolloPortalDB]&gt; select * from ServerConfig\G ;</span><br><span class="line">*************************** 2. row ***************************</span><br><span class="line">                       Id: 2</span><br><span class="line">                      Key: organizations</span><br><span class="line">                    Value: [&#123;&quot;orgId&quot;:&quot;TEST1&quot;,&quot;orgName&quot;:&quot;样例部门1&quot;&#125;,&#123;&quot;orgId&quot;:&quot;TEST2&quot;,&quot;orgName&quot;:&quot;样例部门2&quot;&#125;]</span><br><span class="line">                  Comment: 部门列表</span><br><span class="line">                IsDeleted:  </span><br><span class="line">     DataChange_CreatedBy: default</span><br></pre></td></tr></table></figure><p>修改数据库,创建部门</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">update ServerConfig set Value&#x3D;&#39;[&#123;&quot;orgId&quot;:&quot;od01&quot;,&quot;orgName&quot;:&quot;Linux学院&quot;&#125;,&#123;&quot;orgId&quot;:&quot;od02&quot;,&quot;orgName&quot;:&quot;云计算学院&quot;&#125;,&#123;&quot;orgId&quot;:&quot;od03&quot;,&quot;orgName&quot;:&quot;Python学院&quot;&#125;]&#39; where Id&#x3D;2;</span><br></pre></td></tr></table></figure><p> 由于使用concigmap资源，故之做介绍，不在这里修改：</p><p>配置portal meta serice：</p><p>这里列出的是支持的环境列表配置：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 apollo-portal]# vim &#x2F;data&#x2F;dockerfile&#x2F;apollo-portal&#x2F;config&#x2F;apollo-env.properties</span><br></pre></td></tr></table></figure><p>修改startup.sh <a href="https://raw.githubusercontent.com/ctripcorp/apollo/1.5.1/scripts/apollo-on-kubernetes/apollo-portal-server/scripts/startup-kubernetes.sh" target="_blank" rel="noopener">官方地址raw</a>  <a href="https://github.com/ctripcorp/apollo/blob/master/scripts/apollo-on-kubernetes/apollo-portal-server/scripts/startup-kubernetes.sh" target="_blank" rel="noopener">官方地址html</a></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 apollo-portal]# vim scripts&#x2F;startup.sh</span><br><span class="line">SERVER_PORT&#x3D;8080</span><br><span class="line">APOLLO_PORTAL_SERVICE_NAME&#x3D;$(hostname -i)</span><br><span class="line"></span><br><span class="line">[root@wang-200 apollo-portal]# chmod u+x scripts&#x2F;startup.sh</span><br></pre></td></tr></table></figure><h3 id="制作镜像"><a href="#制作镜像" class="headerlink" title="制作镜像"></a>制作镜像</h3><p>Dockerfile文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM stanleyws&#x2F;jre8:8u112</span><br><span class="line"></span><br><span class="line">ENV VERSION 1.5.1</span><br><span class="line"></span><br><span class="line">RUN ln -sf &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai &#x2F;etc&#x2F;localtime &amp;&amp;\</span><br><span class="line">    echo &quot;Asia&#x2F;Shanghai&quot; &gt; &#x2F;etc&#x2F;timezone</span><br><span class="line"></span><br><span class="line">ADD apollo-portal-$&#123;VERSION&#125;.jar &#x2F;apollo-portal&#x2F;apollo-portal.jar</span><br><span class="line">ADD config&#x2F; &#x2F;apollo-portal&#x2F;config</span><br><span class="line">ADD scripts&#x2F; &#x2F;apollo-portal&#x2F;scripts</span><br><span class="line"></span><br><span class="line">CMD [&quot;&#x2F;apollo-portal&#x2F;scripts&#x2F;startup.sh&quot;]</span><br></pre></td></tr></table></figure><p>通过dockerfile制作镜像</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker build . -t harbor.od.com&#x2F;infra&#x2F;apollo-portal:v1.5.1</span><br><span class="line">docker push harbor.od.com&#x2F;infra&#x2F;apollo-portal:v1.5.1</span><br></pre></td></tr></table></figure><h3 id="编写资源配置清单-1"><a href="#编写资源配置清单-1" class="headerlink" title="编写资源配置清单"></a>编写资源配置清单</h3><p>创建资源配置清单文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;apollo-portal</span><br></pre></td></tr></table></figure><p>cm.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;apollo-portal&#x2F;cm.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: apollo-portal-cm</span><br><span class="line">  namespace: infra</span><br><span class="line">data:</span><br><span class="line">  application-github.properties: |</span><br><span class="line">    # DataSource</span><br><span class="line">    spring.datasource.url &#x3D; jdbc:mysql:&#x2F;&#x2F;mysql.od.com:3306&#x2F;ApolloPortalDB?characterEncoding&#x3D;utf8</span><br><span class="line">    spring.datasource.username &#x3D; apolloportal</span><br><span class="line">    spring.datasource.password &#x3D; 123456</span><br><span class="line">  app.properties: |</span><br><span class="line">    appId&#x3D;100003173</span><br><span class="line">  apollo-env.properties: |</span><br><span class="line">    dev.meta&#x3D;http:&#x2F;&#x2F;config.od.com</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>deploy.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;apollo-portal&#x2F;deploy.yaml</span><br><span class="line">kind: Deployment</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: apollo-portal</span><br><span class="line">  namespace: infra</span><br><span class="line">  labels: </span><br><span class="line">    name: apollo-portal</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels: </span><br><span class="line">      name: apollo-portal</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels: </span><br><span class="line">        app: apollo-portal </span><br><span class="line">        name: apollo-portal</span><br><span class="line">    spec:</span><br><span class="line">      volumes:</span><br><span class="line">      - name: configmap-volume</span><br><span class="line">        configMap:</span><br><span class="line">          name: apollo-portal-cm</span><br><span class="line">      containers:</span><br><span class="line">      - name: apollo-portal</span><br><span class="line">        image: harbor.od.com&#x2F;infra&#x2F;apollo-portal:v1.5.1</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          protocol: TCP</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: configmap-volume</span><br><span class="line">          mountPath: &#x2F;apollo-portal&#x2F;config</span><br><span class="line">        terminationMessagePath: &#x2F;dev&#x2F;termination-log</span><br><span class="line">        terminationMessagePolicy: File</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor</span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      securityContext: </span><br><span class="line">        runAsUser: 0</span><br><span class="line">      schedulerName: default-scheduler</span><br><span class="line">  strategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate: </span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">      maxSurge: 1</span><br><span class="line">  revisionHistoryLimit: 7</span><br><span class="line">  progressDeadlineSeconds: 600</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>svc.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;apollo-portal&#x2F;svc.yaml</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata: </span><br><span class="line">  name: apollo-portal</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector: </span><br><span class="line">    app: apollo-portal</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>ingress.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;apollo-portal&#x2F;ingress.yaml</span><br><span class="line">kind: Ingress</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata: </span><br><span class="line">  name: apollo-portal</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: portal.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend: </span><br><span class="line">          serviceName: apollo-portal</span><br><span class="line">          servicePort: 8080</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单-2"><a href="#应用资源配置清单-2" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><p>执行命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;apollo-portal&#x2F;cm.yaml</span><br><span class="line">kubectl create -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;apollo-portal&#x2F;deploy.yaml</span><br><span class="line">kubectl create -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;apollo-portal&#x2F;svc.yaml</span><br><span class="line">kubectl create -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;apollo-portal&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p>查看pod运行状态</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 apollo-portal]# kubectl get pod -n infra | grep portal</span><br><span class="line">apollo-portal-57bc86966d-vzc46         1&#x2F;1     Running   0          24s</span><br></pre></td></tr></table></figure><p>网页访问 portal.od.com</p><p>默认用户名：apollo</p><p>默认密码：  admin</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfm4nstac8j31j40mqtip.jpg" alt="apollo登陆界面"></p><p>登陆成功以后，先改密码，好习惯 :)</p><p>到此，apollo的三个组件都已经交付到k8s里了。</p><h2 id="配置服务使用apollo配置中心"><a href="#配置服务使用apollo配置中心" class="headerlink" title="配置服务使用apollo配置中心"></a>配置服务使用apollo配置中心</h2><p>使用配置中心，需要开发对代码进行调整，将一些配置，通过变量的形式配置到apollo中，服务通过配置中心来获取具体的配置</p><p>在配置中心修改新增如下配置：</p><p>dubbo-demo-service  dubbo服务提供者</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfm4ut85dij31jq0muq6i.jpg" alt="增加配置"></p><p>新增配置，并且发布</p><p>配置信息1: dubbo.registry  zookeeper://zk1.od.com:2181  zookeeper源地址</p><p>配置信息2: dubbo.port     20880    dubbo服务监听端口</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfm58kil3dj31yk0rm7a8.jpg" alt="apollo添加参数"></p><p> 重新打包镜像，使用apollo版本的代码：</p><p> 修改deploy.yaml，将镜像使用我们刚刚打包的这个：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# vim &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-server&#x2F;deploy.yml </span><br><span class="line">      - name: dubbo-demo-service</span><br><span class="line">        image: harbor.od.com&#x2F;app&#x2F;dubbo-demo-service:apollo_20200609_1800</span><br><span class="line">        env:</span><br><span class="line">        - name: C_OPTS</span><br><span class="line">          value: -Denv&#x3D;dev -Dapollo.meta&#x3D;http:&#x2F;&#x2F;config.od.com</span><br></pre></td></tr></table></figure><p> 应用资源配置清单：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-server&#x2F;deploy.yml</span><br></pre></td></tr></table></figure><p>等服务更新后，我们看到dubbo-demo-service已经注册进来了</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfm5usrrmhj31nu0gkq6p.jpg" alt="dubbo-demo-service注册成功"></p><h3 id="创建dubbo服务消费者"><a href="#创建dubbo服务消费者" class="headerlink" title="创建dubbo服务消费者"></a>创建dubbo服务消费者</h3><p>apollo中新建一个项目：dubbo-demo-web,新建配置dubbo.registry,值为zookeeper地址</p><p>项目信息：dubbo-demo-web  dubbo服务消费者</p><p>配置信息1:  dubbo.registry    zookeeper://zk1.od.com:2181  zookeeper源地址</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfm5nc3xc0j31yo0ksafg.jpg" alt="项目和配置"></p><p> 重新打包dubbo-demo-consumer镜像，使用apollo版本的代码：</p><p> 修改deploy.yaml，将镜像使用我们刚刚打包的这个：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# vim &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-consumer&#x2F;deploy.yml </span><br><span class="line">      - name: dubbo-demo-consumer</span><br><span class="line">        image: harbor.od.com&#x2F;app&#x2F;dubbo-demo-consumer:apollo_20200609_1800</span><br><span class="line">        env:</span><br><span class="line">        - name: C_OPTS</span><br><span class="line">          value: -Denv&#x3D;dev -Dapollo.meta&#x3D;http:&#x2F;&#x2F;config.od.com</span><br></pre></td></tr></table></figure><p> 应用资源配置清单：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-consumer&#x2F;deploy.yaml</span><br></pre></td></tr></table></figure><p>重启后验证注册成功。</p><h3 id="通过dubbo-monitor查看"><a href="#通过dubbo-monitor查看" class="headerlink" title="通过dubbo-monitor查看"></a>通过dubbo-monitor查看</h3><p>注意zookeeper源地址</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfm7d8sihlj31sw09adie.jpg" alt="consumer和service注册到zookeeper"></p><h2 id="分环境使用apollo配置中心"><a href="#分环境使用apollo配置中心" class="headerlink" title="分环境使用apollo配置中心"></a>分环境使用apollo配置中心</h2><p>要进行分环境，需要将现有实验环境进行拆分</p><p><strong>portal服务，可以各个环境共用，但是apollo-adminservice和apollo-configservice必须要分开。</strong></p><h3 id="准备zookeeper"><a href="#准备zookeeper" class="headerlink" title="准备zookeeper"></a>准备zookeeper</h3><p>zk环境拆分为test和prod环境</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 ~]# vi &#x2F;var&#x2F;named&#x2F;od.com.zone</span><br><span class="line">zk-test         A       192.168.70.11</span><br><span class="line">zk-prod         A       192.168.70.12</span><br></pre></td></tr></table></figure><p>重启named并测试</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 ~]# systemctl restart named       </span><br><span class="line">[root@wang-12 ~]# dig -t A zk-test.od.com +short    </span><br><span class="line">192.168.70.11</span><br></pre></td></tr></table></figure><p>namespace 分环境，创建test 和prod</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create ns test</span><br><span class="line">kubectl create ns prod</span><br></pre></td></tr></table></figure><p>创建secret，确保harbor的账号“harbor”存在</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create secret docker-registry harbor --docker-server&#x3D;harbor.od.com --docker-username&#x3D;harbor --docker-password&#x3D;Harbor12345 -n test</span><br><span class="line">kubectl create secret docker-registry harbor --docker-server&#x3D;harbor.od.com --docker-username&#x3D;harbor --docker-password&#x3D;Harbor12345 -n prod</span><br></pre></td></tr></table></figure><h3 id="准备数据库"><a href="#准备数据库" class="headerlink" title="准备数据库"></a>准备数据库</h3><p>数据库进行拆分，因实验资源有限，故使用分库的形式模拟分环境</p><p>修改数据库初始化脚本，分别创建ApolloConfigTestDB和ApolloConfigProdDB</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp apolloconfig.sql apolloconfig-test.sql</span><br><span class="line">sed -i &quot;s&amp;ApolloConfigDB&amp;ApolloConfigTestDB&amp;&quot; apolloconfig-test.sql </span><br><span class="line">mysql -h wang-12 -p &lt; apolloconfig-test.sql </span><br><span class="line"></span><br><span class="line">cp apolloconfig.sql apolloconfig-prod.sql</span><br><span class="line">sed -i &quot;s&amp;ApolloConfigDB&amp;ApolloConfigProdDB&amp;&quot; apolloconfig-prod.sql </span><br><span class="line">mysql -h wang-12 -p &lt; apolloconfig-prod.sql</span><br></pre></td></tr></table></figure><p>示例图：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfm7zc48fjj30lk02d3yr.jpg" alt="示例图"></p><p> 修改数据库中eureka的地址，这里用到了两个新的域名，自行在bind9中添加解析(我用的默认解析)</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; update ApolloConfigTestDB.ServerConfig set ServerConfig.Value&#x3D;&quot;http:&#x2F;&#x2F;config-test.od.com&#x2F;eureka&quot; where ServerConfig.Key&#x3D;&quot;eureka.service.url&quot;;</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; grant INSERT,DELETE,UPDATE,SELECT on ApolloConfigTestDB.* to &quot;apolloconfig&quot;@&quot;192.168.70.%&quot; identified by &quot;123456&quot;;</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; update ApolloConfigProdDB.ServerConfig set ServerConfig.Value&#x3D;&quot;http:&#x2F;&#x2F;config-prod.od.com&#x2F;eureka&quot; where ServerConfig.Key&#x3D;&quot;eureka.service.url&quot;;</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; grant INSERT,DELETE,UPDATE,SELECT on ApolloConfigProdDB.* to &quot;apolloconfig&quot;@&quot;192.168.70.%&quot; identified by &quot;123456&quot;;</span><br></pre></td></tr></table></figure><p>修改portal数据，支持fat和pro环境：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">update ApolloPortalDB.ServerConfig set Value&#x3D;&#39;fat,pro&#39; where Id&#x3D;1;</span><br></pre></td></tr></table></figure><p>修改portal的cm资源配置清单：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# vim &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;apollo-portal&#x2F;cm.yaml</span><br><span class="line">  apollo-env.properties: |</span><br><span class="line">    fat.meta&#x3D;http:&#x2F;&#x2F;config-test.od.com</span><br><span class="line">    pro.meta&#x3D;http:&#x2F;&#x2F;config-prod.od.com</span><br><span class="line"></span><br><span class="line">[root@wang-200 ~]# kubectl apply -f &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;apollo-portal&#x2F;cm.yaml</span><br></pre></td></tr></table></figure><p>通过浏览器，我们确认数据已经改变</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfm87rqx22j31po0nedjw.jpg" alt="image-20200609184417637"></p><p>分别创建修改两个环境的资源配置文件：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;test&#x2F;&#123;apollo-adminservice,apollo-configservice,dubbo-demo-server,dubbo-demo-consumer&#125;</span><br><span class="line">mkdir -p &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;prod&#x2F;&#123;apollo-adminservice,apollo-configservice,dubbo-demo-server,dubbo-demo-consumer&#125;</span><br></pre></td></tr></table></figure><h4 id="部署test环境的apollo-configservice"><a href="#部署test环境的apollo-configservice" class="headerlink" title="部署test环境的apollo-configservice"></a>部署test环境的apollo-configservice</h4><p>将之前的资源配置清单cp到对应环境的目录中，进行修改：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp apollo-configservice&#x2F;* test&#x2F;apollo-configservice&#x2F;</span><br><span class="line">sed -i &quot;s&amp;namespace: infra&amp;namespace: test&amp;&quot; test&#x2F;apollo-configservice&#x2F;*.yaml</span><br><span class="line">sed -i &quot;s&amp;ApolloConfigDB&amp;ApolloConfigTestDB&amp;&quot; test&#x2F;apollo-configservice&#x2F;cm.yaml </span><br><span class="line">sed -i &quot;s&amp;http:&#x2F;&#x2F;config.od.com&#x2F;eureka&amp;http:&#x2F;&#x2F;config-test.od.com&#x2F;eureka&amp;&quot; test&#x2F;apollo-configservice&#x2F;cm.yaml</span><br><span class="line">sed -i &quot;s&amp;config.od.com&amp;config-test.od.com&amp;&quot; test&#x2F;apollo-configservice&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p>执行资源配置清单</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f test&#x2F;apollo-configservice&#x2F;cm.yaml</span><br><span class="line">kubectl apply -f test&#x2F;apollo-configservice&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f test&#x2F;apollo-configservice&#x2F;svc.yaml </span><br><span class="line">kubectl apply -f test&#x2F;apollo-configservice&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p> 服务已经注册进来了</p><h4 id="部署test环境的apollo-adminservice"><a href="#部署test环境的apollo-adminservice" class="headerlink" title="部署test环境的apollo-adminservice"></a>部署test环境的apollo-adminservice</h4><p>修改apollo-adminservice的资源配置清单：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp apollo-adminservice&#x2F;* test&#x2F;apollo-adminservice&#x2F;</span><br><span class="line">sed -i &quot;s&amp;namespace: infra&amp;namespace: test&amp;&quot; test&#x2F;apollo-adminservice&#x2F;*.yaml</span><br><span class="line">sed -i &quot;s&amp;ApolloConfigDB&amp;ApolloConfigTestDB&amp;&quot; test&#x2F;apollo-adminservice&#x2F;cm.yaml </span><br><span class="line">sed -i &quot;s&amp;http:&#x2F;&#x2F;config.od.com&#x2F;eureka&amp;http:&#x2F;&#x2F;config-test.od.com&#x2F;eureka&amp;&quot; test&#x2F;apollo-adminservice&#x2F;cm.yaml</span><br></pre></td></tr></table></figure><p>应用资源配置清单：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f test&#x2F;apollo-configservice&#x2F;cm.yaml</span><br><span class="line">kubectl apply -f test&#x2F;apollo-configservice&#x2F;deploy.yaml</span><br></pre></td></tr></table></figure><h4 id="web端验证"><a href="#web端验证" class="headerlink" title="web端验证"></a>web端验证</h4><p><a href="http://config-test.od.com/" target="_blank" rel="noopener">http://config-test.od.com/</a></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfn0ptbb6uj31s60fitc0.jpg" alt="image-20200610111024199"></p><h4 id="部署prod环境的apollo-configservice"><a href="#部署prod环境的apollo-configservice" class="headerlink" title="部署prod环境的apollo-configservice"></a>部署prod环境的apollo-configservice</h4><p>将之前的资源配置清单cp到对应环境的目录中，进行修改：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp apollo-configservice&#x2F;* prod&#x2F;apollo-configservice&#x2F;</span><br><span class="line">sed -i &quot;s&amp;namespace: infra&amp;namespace: prod&amp;&quot; prod&#x2F;apollo-configservice&#x2F;*.yaml</span><br><span class="line">sed -i &quot;s&amp;ApolloConfigDB&amp;ApolloConfigProdDB&amp;&quot; prod&#x2F;apollo-configservice&#x2F;cm.yaml </span><br><span class="line">sed -i &quot;s&amp;http:&#x2F;&#x2F;config.od.com&#x2F;eureka&amp;http:&#x2F;&#x2F;config-prod.od.com&#x2F;eureka&amp;&quot; prod&#x2F;apollo-configservice&#x2F;cm.yaml</span><br><span class="line">sed -i &quot;s&amp;config.od.com&amp;config-prod.od.com&amp;&quot; prod&#x2F;apollo-configservice&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p>执行资源配置清单</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f prod&#x2F;apollo-configservice&#x2F;cm.yaml</span><br><span class="line">kubectl apply -f prod&#x2F;apollo-configservice&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f prod&#x2F;apollo-configservice&#x2F;svc.yaml </span><br><span class="line">kubectl apply -f prod&#x2F;apollo-configservice&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p> 服务已经注册进来了</p><h4 id="部署prod环境的apollo-adminservice"><a href="#部署prod环境的apollo-adminservice" class="headerlink" title="部署prod环境的apollo-adminservice"></a>部署prod环境的apollo-adminservice</h4><p>修改apollo-adminservice的资源配置清单：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp apollo-adminservice&#x2F;* prod&#x2F;apollo-adminservice&#x2F;</span><br><span class="line">sed -i &quot;s&amp;namespace: infra&amp;namespace: prod&amp;&quot; prod&#x2F;apollo-adminservice&#x2F;*.yaml</span><br><span class="line">sed -i &quot;s&amp;ApolloConfigDB&amp;ApolloConfigProdDB&amp;&quot; prod&#x2F;apollo-adminservice&#x2F;cm.yaml </span><br><span class="line">sed -i &quot;s&amp;http:&#x2F;&#x2F;config.od.com&#x2F;eureka&amp;http:&#x2F;&#x2F;config-prod.od.com&#x2F;eureka&amp;&quot; prod&#x2F;apollo-adminservice&#x2F;cm.yaml</span><br></pre></td></tr></table></figure><p>应用资源配置清单：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f prod&#x2F;apollo-configservice&#x2F;cm.yaml</span><br><span class="line">kubectl apply -f prod&#x2F;apollo-configservice&#x2F;deploy.yaml</span><br></pre></td></tr></table></figure><h4 id="web端验证-1"><a href="#web端验证-1" class="headerlink" title="web端验证"></a>web端验证</h4><p><a href="http://config-prod.od.com/" target="_blank" rel="noopener">http://config-prod.od.com/</a></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfn286y2wfj31sc0fewhw.jpg" alt="config-prod示例图"></p><p>两个服务都已经注册进来了，删除portal数据库中存储的关于之前项目的配置，接下来启动portal项目：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mysql -h mysql.od.com -p</span><br><span class="line">MariaDB [(none)]&gt; use ApolloPortalDB ;</span><br><span class="line">MariaDB [ApolloPortalDB]&gt; truncate table App;</span><br><span class="line">MariaDB [ApolloPortalDB]&gt; truncate table AppNamespace;</span><br></pre></td></tr></table></figure><p>查看pod是否已经起来了</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pod -n infra | grep apollo-portal</span><br><span class="line">apollo-portal-57bc86966d-s7qcn         1&#x2F;1     Running   0          66s</span><br></pre></td></tr></table></figure><h3 id="添加系统参数"><a href="#添加系统参数" class="headerlink" title="添加系统参数"></a>添加系统参数</h3><p>管理员工具 - 系统参数  <a href="http://portal.od.com/server_config.html" target="_blank" rel="noopener">http://portal.od.com/server_config.html</a></p><p>key : apollo.portal.envs ;    value : fat, pro ;    comment : 可支持的环境列表</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfn5y9dttkj31di0moq5y.jpg" alt="添加系统参数"></p><p>打开portal.od.com验证，并且创建两个项目：</p><p>首先创建dubbo-demo-service</p><p>项目信息：</p><p>Appid: dubbo-demo-service  应用名称：dubbo服务提供者</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfn62maid2j31o00mgjvy.jpg" alt="image-20200610141542406"></p><p> 添加配置：两个环境都添加上：注意连接地址一个是zk-test.od.com,一个是zk-prod.od.com</p><p>服务1:  key: dubbo.registry        value: zookeeper://zk-test.od.com:2181     command: dubbo数据源</p><p>服务2:  key: dubbo.port        value: 20880     command:  dubbo端口号</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfn689il9rj31s20d8tch.jpg" alt="image-20200610142108166"></p><p> 接下来创建dubbo-demo-web项目：同样是两个环境都发布，注意一个是zk-test.od.com,一个是zk-prod.od.com</p><p>项目信息： Appid: dubbo-demo-web             应用名称: dubbo服务消费者    部门：云计算学院(od02)</p><p>服务1:  key: dubbo.registry           value: zookeeper://zk-test.od.com:2181        command: dubbo数据源</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfn6bmc1ncj31rq0ag77c.jpg" alt="image-20200610142421764"></p><h3 id="交付dubbo服务分环境交付"><a href="#交付dubbo服务分环境交付" class="headerlink" title="交付dubbo服务分环境交付"></a>交付dubbo服务分环境交付</h3><h4 id="交付test环境的dubbo-demo-server"><a href="#交付test环境的dubbo-demo-server" class="headerlink" title="交付test环境的dubbo-demo-server"></a>交付test环境的dubbo-demo-server</h4><p>同样操作，修改之前项目的资源配置清单：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp dubbo-server&#x2F;* test&#x2F;dubbo-demo-server&#x2F;</span><br><span class="line">sed -i &quot;s&amp;namespace: app&amp;namespace: test&amp;&quot; test&#x2F;dubbo-demo-server&#x2F;deploy.yaml</span><br><span class="line">sed -i &quot;s&amp;config.od.com&amp;config-test.od.com&amp;&quot; test&#x2F;dubbo-demo-server&#x2F;deploy.yaml</span><br></pre></td></tr></table></figure><p>应用资源配置清单</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f test&#x2F;dubbo-demo-server&#x2F;deploy.yaml</span><br></pre></td></tr></table></figure><h4 id="交付test环境的dubbo-demo-consumer"><a href="#交付test环境的dubbo-demo-consumer" class="headerlink" title="交付test环境的dubbo-demo-consumer"></a>交付test环境的dubbo-demo-consumer</h4><p>修改之前项目的资源配置清单</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp dubbo-consumer&#x2F;* test&#x2F;dubbo-demo-consumer&#x2F;</span><br><span class="line">sed -i &quot;s&amp;namespace: app&amp;namespace: test&amp;&quot; test&#x2F;dubbo-demo-consumer&#x2F;*.yaml</span><br><span class="line">sed -i &quot;s&amp;config.od.com&amp;config-test.od.com&amp;&quot; test&#x2F;dubbo-demo-consumer&#x2F;deploy.yaml</span><br><span class="line">sed -i &quot;s&amp;demo.od.com&amp;demo-test.od.com&amp;&quot; test&#x2F;dubbo-demo-consumer&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p> 应用test环境的dubbo-consumer资源配置清单：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f test&#x2F;dubbo-demo-consumer&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f test&#x2F;dubbo-demo-consumer&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f test&#x2F;dubbo-demo-consumer&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><h4 id="交付prod环境的dubbo-demo-server服务"><a href="#交付prod环境的dubbo-demo-server服务" class="headerlink" title="交付prod环境的dubbo-demo-server服务"></a>交付prod环境的dubbo-demo-server服务</h4><h4 id="交付test环境的dubbo-demo-server-1"><a href="#交付test环境的dubbo-demo-server-1" class="headerlink" title="交付test环境的dubbo-demo-server"></a>交付test环境的dubbo-demo-server</h4><p>同样操作，修改之前项目的资源配置清单：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp dubbo-server&#x2F;* prod&#x2F;dubbo-demo-server&#x2F;</span><br><span class="line">sed -i &quot;s&amp;namespace: app&amp;namespace: prod&amp;&quot; prod&#x2F;dubbo-demo-server&#x2F;deploy.yaml</span><br><span class="line">sed -i &quot;s&amp;config.od.com&amp;config-prod.od.com&amp;&quot; prod&#x2F;dubbo-demo-server&#x2F;deploy.yaml</span><br></pre></td></tr></table></figure><p>应用资源配置清单</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f prod&#x2F;dubbo-demo-server&#x2F;deploy.yaml</span><br></pre></td></tr></table></figure><h4 id="交付prod环境的dubbo-demo-consumer服务"><a href="#交付prod环境的dubbo-demo-consumer服务" class="headerlink" title="交付prod环境的dubbo-demo-consumer服务"></a>交付prod环境的dubbo-demo-consumer服务</h4><p>修改之前项目的资源配置清单</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp dubbo-consumer&#x2F;* prod&#x2F;dubbo-demo-consumer&#x2F;</span><br><span class="line">sed -i &quot;s&amp;namespace: app&amp;namespace: prod&amp;&quot; prod&#x2F;dubbo-demo-consumer&#x2F;*.yaml</span><br><span class="line">sed -i &quot;s&amp;config.od.com&amp;config-prod.od.com&amp;&quot; prod&#x2F;dubbo-demo-consumer&#x2F;deploy.yaml</span><br><span class="line">sed -i &quot;s&amp;demo.od.com&amp;demo-prod.od.com&amp;&quot; prod&#x2F;dubbo-demo-consumer&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p> 应用test环境的dubbo-consumer资源配置清单：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f prod&#x2F;dubbo-demo-consumer&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f prod&#x2F;dubbo-demo-consumer&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f prod&#x2F;dubbo-demo-consumer&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p>访问web界面就有返回值了</p><p><a href="http://demo-test.od.com/hello" target="_blank" rel="noopener">http://demo-test.od.com/hello</a></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfn6rz9tv5j318s0eujto.jpg" alt="image-20200610144004965"></p><p><a href="http://demo-prod.od.com/hello?name=prod" target="_blank" rel="noopener">http://demo-prod.od.com/hello?name=prod</a></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfn705vne8j311i0e476h.jpg" alt="image-20200610144756768"></p><p>这是如果我们在测试环境发版成功并测试没有问题，就可以将生产也改为相同的版本就可以了。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 老男孩 </category>
          
          <category> 实战交付 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 老男孩 </tag>
            
            <tag> 实战交付 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二、交付-使用blue ocean流水线构建</title>
      <link href="/2020/06/05/%E4%BA%8C%E3%80%81%E4%BA%A4%E4%BB%98-%E4%BD%BF%E7%94%A8blue-ocean%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BA/"/>
      <url>/2020/06/05/%E4%BA%8C%E3%80%81%E4%BA%A4%E4%BB%98-%E4%BD%BF%E7%94%A8blue-ocean%E6%B5%81%E6%B0%B4%E7%BA%BF%E6%9E%84%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="交付dubbo-demo-service到k8s"><a href="#交付dubbo-demo-service到k8s" class="headerlink" title="交付dubbo-demo-service到k8s"></a>交付dubbo-demo-service到k8s</h2><h3 id="使用jenkins创建一个新的项目"><a href="#使用jenkins创建一个新的项目" class="headerlink" title="使用jenkins创建一个新的项目"></a>使用jenkins创建一个新的项目</h3><p>dubbo-demo,选择流水线构建</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfhfh5cyt9j30pf0afta7.jpg" alt="创建新的项目"></p><p> 勾选保存构建历史和指定项目为参数化构建项目：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfhfifql1nj30ky09rdgg.jpg" alt="保存构建历史和参数化构建"></p><p>添加构建参数：以下配置项，是王导根据多年生产经验总结出来的<strong>甩锅大法</strong>：</p><p>Jenkins流水线配置的十个参数</p><ul><li>app_name -&gt; 项目名称–例：dubbo-demo-service</li><li>image_name -&gt; docker镜像名称–例：app/dubbo-demo-service</li><li>git_repo -&gt; 项目的git地址–例:<a href="https://gitee.com/wangzhangtao/dubbo-demo-service.git" target="_blank" rel="noopener">https://gitee.com/wangzhangtao/dubbo-demo-service.git</a></li><li>git_ver -&gt; 项目的git版本号cid（或分支）–例：master(尽量使用commit id)</li><li>add_tag -&gt; 镜像标签，日期时间戳–例：$git_ver_$add_tag=master_191124_1400</li><li>mvn_dir -&gt; 编译项目的目录–例：./</li><li>target_dir -&gt; 编译后jar包存放的地址–例：./dubbo-demo-service/target</li><li>mvn_cmd -&gt; 执行编译所用的命令，可选参数-e -q,忽略标准输出，只输出错误输出， mvn clean package -Dmaven.test.skip=true</li><li>base_image -&gt; 项目的docker底包, –例：base/jre8:8u112</li><li>maven -&gt; maven软件的版本；–例：3.6.1</li></ul><p>除了base_image和maven是choice parameter，其他都是string parameter</p><p>添加完成后，效果如图：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfhgay3l0oj30m70j0tan.jpg" alt="image-20200605153619266"></p><p> 编写pipeline:仔细查看这个pipeline，里面都是我们上面编写的参数。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pipeline &#123;</span><br><span class="line">  agent any </span><br><span class="line">    stages &#123;</span><br><span class="line">      stage(&#39;pull&#39;) &#123; &#x2F;&#x2F;get project code from repo </span><br><span class="line">        steps &#123;</span><br><span class="line">          sh &quot;git clone $&#123;params.git_repo&#125; $&#123;params.app_name&#125;&#x2F;$&#123;env.BUILD_NUMBER&#125; &amp;&amp; cd $&#123;params.app_name&#125;&#x2F;$&#123;env.BUILD_NUMBER&#125; &amp;&amp; git checkout $&#123;params.git_ver&#125;&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      stage(&#39;build&#39;) &#123; &#x2F;&#x2F;exec mvn cmd</span><br><span class="line">        steps &#123;</span><br><span class="line">          sh &quot;cd $&#123;params.app_name&#125;&#x2F;$&#123;env.BUILD_NUMBER&#125;  &amp;&amp; &#x2F;var&#x2F;jenkins_home&#x2F;maven-$&#123;params.maven&#125;&#x2F;bin&#x2F;$&#123;params.mvn_cmd&#125;&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      stage(&#39;package&#39;) &#123; &#x2F;&#x2F;move jar file into project_dir</span><br><span class="line">        steps &#123;</span><br><span class="line">          sh &quot;cd $&#123;params.app_name&#125;&#x2F;$&#123;env.BUILD_NUMBER&#125; &amp;&amp; cd $&#123;params.target_dir&#125; &amp;&amp; mkdir project_dir &amp;&amp; mv *.jar .&#x2F;project_dir&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      stage(&#39;image&#39;) &#123; &#x2F;&#x2F;build image and push to registry</span><br><span class="line">        steps &#123;</span><br><span class="line">          writeFile file: &quot;$&#123;params.app_name&#125;&#x2F;$&#123;env.BUILD_NUMBER&#125;&#x2F;Dockerfile&quot;, text: &quot;&quot;&quot;FROM harbor.od.com&#x2F;$&#123;params.base_image&#125;</span><br><span class="line">ADD $&#123;params.target_dir&#125;&#x2F;project_dir &#x2F;opt&#x2F;project_dir&quot;&quot;&quot;</span><br><span class="line">          sh &quot;cd  $&#123;params.app_name&#125;&#x2F;$&#123;env.BUILD_NUMBER&#125; &amp;&amp; docker build -t harbor.od.com&#x2F;$&#123;params.image_name&#125;:$&#123;params.git_ver&#125;_$&#123;params.add_tag&#125; . &amp;&amp; docker push harbor.od.com&#x2F;$&#123;params.image_name&#125;:$&#123;params.git_ver&#125;_$&#123;params.add_tag&#125;&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="第一次构建服务"><a href="#第一次构建服务" class="headerlink" title="第一次构建服务"></a>第一次构建服务</h3><p><strong>填写完以后执行bulid：第一次构建需要下载很多依赖包，时间很长，看别人抽根烟，喝杯茶~</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfhik1hqidj30pc0jqace.jpg" alt="image-20200605165414887"></p><p> 经过漫长的等待后，已经构建完成了，可以点击open blue ocean 查看构建历史及过程：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfhjlcbxtvj30v80dfdgu.jpg" alt="部署过程"></p><p> 检查harbor是否已经有这版镜像：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfhjme9p1uj30p009kdgx.jpg" alt="刚刚推送的镜像"></p><h3 id="交付dubbo-demo-service服务到k8s"><a href="#交付dubbo-demo-service服务到k8s" class="headerlink" title="交付dubbo-demo-service服务到k8s"></a>交付dubbo-demo-service服务到k8s</h3><p>准备yml文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-server</span><br><span class="line">cd &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-server</span><br></pre></td></tr></table></figure><p>deploy.yml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-server&#x2F;deploy.yml</span><br><span class="line">kind: Deployment</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: dubbo-demo-service</span><br><span class="line">  namespace: app</span><br><span class="line">  labels: </span><br><span class="line">    name: dubbo-demo-service</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels: </span><br><span class="line">      name: dubbo-demo-service</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels: </span><br><span class="line">        app: dubbo-demo-service</span><br><span class="line">        name: dubbo-demo-service</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: dubbo-demo-service</span><br><span class="line">        image: harbor.od.com&#x2F;app&#x2F;dubbo-demo-service:master_20200605_1500</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 20880</span><br><span class="line">          protocol: TCP</span><br><span class="line">        env:</span><br><span class="line">        - name: JAR_BALL</span><br><span class="line">          value: dubbo-server.jar</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor</span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      securityContext: </span><br><span class="line">        runAsUser: 0</span><br><span class="line">      schedulerName: default-scheduler</span><br><span class="line">  strategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate: </span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">      maxSurge: 1</span><br><span class="line">  revisionHistoryLimit: 7</span><br><span class="line">  progressDeadlineSeconds: 600</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>由于我们使用的harbor私有镜像的项目是app，是个私有项目，所以需要创建secret资源：</p><p>创建 app命名空间：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 dubbo-server]# kubectl create ns app</span><br><span class="line">namespace&#x2F;app created</span><br></pre></td></tr></table></figure><p>创建secret资源：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create secret docker-registry harbor --docker-server&#x3D;harbor.od.com --docker-username&#x3D;admin --docker-password&#x3D;Harbor12345 -n app</span><br></pre></td></tr></table></figure><p>应用资源配置文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 dubbo-server]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-server&#x2F;deploy.yml</span><br><span class="line">deployment.extensions&#x2F;dubbo-demo-service created</span><br></pre></td></tr></table></figure><h3 id="检查构建结果"><a href="#检查构建结果" class="headerlink" title="检查构建结果"></a>检查构建结果</h3><p>检查pod是否创建：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 dubbo-server]# kubectl get pod -n app </span><br><span class="line">NAME                                  READY   STATUS    RESTARTS   AGE</span><br><span class="line">dubbo-demo-service-859f546558-l9mvr   1&#x2F;1     Running   0          2m51s</span><br></pre></td></tr></table></figure><p> 检查是否启动成功：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 dubbo-server]# kubectl logs dubbo-demo-service-859f546558-l9mvr -n app</span><br><span class="line">2020-06-05 17:53:54.796  INFO 1 --- [           main] com.od.dubbotest.Application             : Started Application in 4.73 seconds (JVM running for 6.757)</span><br><span class="line">Dubbo server started</span><br><span class="line">Dubbo 服务端已经启动</span><br></pre></td></tr></table></figure><p>检查dubbo-server服务是否已经注册到了zookeeper：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-11 ~]# &#x2F;opt&#x2F;zookeeper&#x2F;bin&#x2F;zkCli.sh</span><br><span class="line">[zk: localhost:2181(CONNECTED) 0] ls &#x2F;</span><br><span class="line">[dubbo, zookeeper]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls &#x2F;dubbo</span><br><span class="line">[com.od.dubbotest.api.HelloService]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2] ls &#x2F;dubbo&#x2F;com.od.dubbotest.api.HelloService</span><br><span class="line">[configurators, providers]</span><br><span class="line"></span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] ls &#x2F;dubbo&#x2F;com.od.dubbotest.api.HelloService&#x2F;providers</span><br><span class="line">[dubbo%3A%2F%2F172.16.24.7%3A20880%2Fcom.od.dubbotest.api.HelloService%3Fanyhost%3Dtrue%26application%3Ddubbo-demo-service%26dubbo%3D2.5.3%26interface%3Dcom.od.dubbotest.api.HelloService%26methods%3Dhello%26pid%3D1%26side%3Dprovider%26timestamp%3D1591350834758, dubbo%3A%2F%2F172.16.24.7%3A20880%2Fcom.od.dubbotest.api.HelloService%3Fanyhost%3Dtrue%26application%3Ddubbo-demo-service%26dubbo%3D2.5.3%26interface%3Dcom.od.dubbotest.api.HelloService%26methods%3Dhello%26pid%3D1%26side%3Dprovider%26timestamp%3D1591350833638]</span><br></pre></td></tr></table></figure><h2 id="交付dubbo-monitor监控服务到k8s"><a href="#交付dubbo-monitor监控服务到k8s" class="headerlink" title="交付dubbo-monitor监控服务到k8s"></a>交付dubbo-monitor监控服务到k8s</h2><h3 id="制作dubbo-monitor镜像"><a href="#制作dubbo-monitor镜像" class="headerlink" title="制作dubbo-monitor镜像"></a>制作dubbo-monitor镜像</h3><h4 id="在运维主机下载安装包"><a href="#在运维主机下载安装包" class="headerlink" title="在运维主机下载安装包"></a>在运维主机下载安装包</h4><blockquote><p>官网 <a href="https://github.com/Jeromefromcn/dubbo-monitor.git" target="_blank" rel="noopener">dubbo-monitor源码包 </a></p></blockquote><p><code>[root@wang-200 dubbo-monitor]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget -O &#x2F;data&#x2F;soft&#x2F;centos7&#x2F;dubbo-monitor-master.zip https:&#x2F;&#x2F;github.com&#x2F;Jeromefromcn&#x2F;dubbo-monitor&#x2F;archive&#x2F;master.zip</span><br><span class="line">scp wang-200:&#x2F;data&#x2F;soft&#x2F;centos7&#x2F;dubbo-monitor-master.zip &#x2F;opt&#x2F;src&#x2F;</span><br><span class="line">unzip &#x2F;opt&#x2F;src&#x2F;dubbo-monitor-master.zip -d &#x2F;opt&#x2F;src&#x2F;</span><br><span class="line">mv &#x2F;opt&#x2F;src&#x2F;dubbo-monitor-master &#x2F;opt&#x2F;src&#x2F;dubbo-monitor</span><br><span class="line">cd &#x2F;opt&#x2F;src&#x2F;dubbo-monitor</span><br></pre></td></tr></table></figure><h4 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h4><p>对应修改，不要全部删除内容。</p><p>vim /opt/src/dubbo-monitor/dubbo-monitor-simple/conf/dubbo_origin.properties</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dubbo.registry.address&#x3D;zookeeper:&#x2F;&#x2F;zk1.od.com:2181</span><br><span class="line">dubbo.protocol.port&#x3D;20880</span><br><span class="line">dubbo.jetty.port&#x3D;8080</span><br><span class="line">dubbo.jetty.directory&#x3D;&#x2F;dubbo-monitor-simple&#x2F;monitor</span><br><span class="line">dubbo.statistics.directory&#x3D;&#x2F;dubbo-monitor-simple&#x2F;statistics</span><br><span class="line">dubbo.charts.directory&#x3D;&#x2F;dubbo-monitor-simple&#x2F;charts</span><br><span class="line">dubbo.log4j.file&#x3D;logs&#x2F;dubbo-monitor.log</span><br></pre></td></tr></table></figure><p>优化修改Dockerfile并限制jvm资源，将最后的exec命令的后台&amp;符号删除，并且将exec命令下面的都干掉：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed -r -i -e &#39;&#x2F;^nohup&#x2F;&#123;p;:a;N;$!ba;d&#125;&#39;  .&#x2F;dubbo-monitor-simple&#x2F;bin&#x2F;start.sh &amp;&amp; sed  -r -i -e &quot;s%^nohup(.*)%exec \1%&quot;  .&#x2F;dubbo-monitor-simple&#x2F;bin&#x2F;start.sh</span><br></pre></td></tr></table></figure><p>执行docker build并上传镜像到我们的私有仓库：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp -a dubbo-monitor&#x2F; &#x2F;data&#x2F;dockerfile&#x2F;</span><br><span class="line">cd &#x2F;data&#x2F;dockerfile&#x2F;dubbo-monitor</span><br><span class="line">docker build . -t harbor.od.com&#x2F;infra&#x2F;dubbo-monitor:latest</span><br><span class="line">docker push harbor.od.com&#x2F;infra&#x2F;dubbo-monitor:latest</span><br></pre></td></tr></table></figure><h3 id="制作资源配置清单"><a href="#制作资源配置清单" class="headerlink" title="制作资源配置清单"></a>制作资源配置清单</h3><p>创建文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-monitor</span><br><span class="line">cd &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-monitor</span><br></pre></td></tr></table></figure><p><strong>deploy.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-monitor&#x2F;deploy.yaml</span><br><span class="line">kind: Deployment</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: dubbo-monitor</span><br><span class="line">  namespace: infra</span><br><span class="line">  labels: </span><br><span class="line">    name: dubbo-monitor</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels: </span><br><span class="line">      name: dubbo-monitor</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels: </span><br><span class="line">        app: dubbo-monitor</span><br><span class="line">        name: dubbo-monitor</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: dubbo-monitor</span><br><span class="line">        image: harbor.od.com&#x2F;infra&#x2F;dubbo-monitor:latest</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          protocol: TCP</span><br><span class="line">        - containerPort: 20880</span><br><span class="line">          protocol: TCP</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor</span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      securityContext: </span><br><span class="line">        runAsUser: 0</span><br><span class="line">      schedulerName: default-scheduler</span><br><span class="line">  strategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate: </span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">      maxSurge: 1</span><br><span class="line">  revisionHistoryLimit: 7</span><br><span class="line">  progressDeadlineSeconds: 600</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>svc.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-monitor&#x2F;svc.yaml</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata: </span><br><span class="line">  name: dubbo-monitor</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector: </span><br><span class="line">    app: dubbo-monitor</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>ingress.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-monitor&#x2F;ingress.yaml</span><br><span class="line">kind: Ingress</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata: </span><br><span class="line">  name: dubbo-monitor</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: dubbo-monitor.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend: </span><br><span class="line">          serviceName: dubbo-monitor</span><br><span class="line">          servicePort: 8080</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单"><a href="#应用资源配置清单" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><p>执行命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-monitor&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-monitor&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-monitor&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p>执行过程</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 dubbo-monitor]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-monitor&#x2F;deploy.yaml</span><br><span class="line">deployment.extensions&#x2F;dubbo-monitor created</span><br><span class="line">[root@wang-200 dubbo-monitor]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-monitor&#x2F;svc.yaml</span><br><span class="line">service&#x2F;dubbo-monitor created</span><br><span class="line">[root@wang-200 dubbo-monitor]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-monitor&#x2F;ingress.yaml</span><br><span class="line">ingress.extensions&#x2F;dubbo-monitor created</span><br></pre></td></tr></table></figure><h4 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h4><p><a href="http://dubbo-monitor.od.com/applications.html" target="_blank" rel="noopener">http://dubbo-monitor.od.com/applications.html</a></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfky1dmz17j31le0h0q87.jpg" alt="dubbo-monitor访问界面"></p><h3 id="交付构建dubbo-consumer服务"><a href="#交付构建dubbo-consumer服务" class="headerlink" title="交付构建dubbo-consumer服务"></a>交付构建dubbo-consumer服务</h3><p>我们这里的dubbo-consumer是dubbo-demo-service的消费者：</p><p>我们之前已经在jenkins配置好了流水线，只需要填写参数就行了。</p><p>由于dubbo-consumer用的gitee的私有仓库，需要添加公钥，这里大家可以自己找个client服务来做实验。</p><p>dubbo-demo-consumer构建参数</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfky1d38bpj31140u0wht.jpg" alt="dubbo-demo-consumer构建图"></p><p>dubbo-demo-consumer构建过程</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfky1f18m4j31p90u077y.jpg" alt="dubbo-demo-consumer构建过程"></p><p>下面是我们通过jenkins构建的镜像，已经上传到我们的harbor私有仓库当中了：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfky1bnclxj31gm0k2mzm.jpg" alt="harbor仓库中的consumer镜像"></p><p> 这里我们使用master_20200605_1600这个用来做模拟生产发版更新实验。</p><h3 id="准备资源配置清单"><a href="#准备资源配置清单" class="headerlink" title="准备资源配置清单"></a>准备资源配置清单</h3><p>创建需要的文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-consumer</span><br><span class="line">cd &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-consumer</span><br></pre></td></tr></table></figure><p>deploy.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-consumer&#x2F;deploy.yaml</span><br><span class="line">kind: Deployment</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: dubbo-demo-consumer</span><br><span class="line">  namespace: app</span><br><span class="line">  labels: </span><br><span class="line">    name: dubbo-demo-consumer</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels: </span><br><span class="line">      name: dubbo-demo-consumer</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels: </span><br><span class="line">        app: dubbo-demo-consumer</span><br><span class="line">        name: dubbo-demo-consumer</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: dubbo-demo-consumer</span><br><span class="line">        image: harbor.od.com&#x2F;app&#x2F;dubbo-demo-consumer:master_20200605_1600</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          protocol: TCP</span><br><span class="line">        - containerPort: 20880</span><br><span class="line">          protocol: TCP</span><br><span class="line">        env:</span><br><span class="line">        - name: JAR_BALL</span><br><span class="line">          value: dubbo-client.jar</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor</span><br><span class="line">      restartPolicy: Always</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      securityContext: </span><br><span class="line">        runAsUser: 0</span><br><span class="line">      schedulerName: default-scheduler</span><br><span class="line">  strategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate: </span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">      maxSurge: 1</span><br><span class="line">  revisionHistoryLimit: 7</span><br><span class="line">  progressDeadlineSeconds: 600</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>svc.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-consumer&#x2F;svc.yaml</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata: </span><br><span class="line">  name: dubbo-demo-consumer</span><br><span class="line">  namespace: app</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector: </span><br><span class="line">    app: dubbo-demo-consumer</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>ingress.yaml</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-consumer&#x2F;ingress.yaml</span><br><span class="line">kind: Ingress</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata: </span><br><span class="line">  name: dubbo-demo-consumer</span><br><span class="line">  namespace: app</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: demo.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend: </span><br><span class="line">          serviceName: dubbo-demo-consumer</span><br><span class="line">          servicePort: 8080</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单-1"><a href="#应用资源配置清单-1" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><p>执行命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-consumer&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-consumer&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-consumer&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p>执行过程</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 dubbo-consumer]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-consumer&#x2F;deploy.yaml</span><br><span class="line">deployment.extensions&#x2F;dubbo-demo-consumer created</span><br><span class="line">[root@wang-200 dubbo-consumer]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-consumer&#x2F;svc.yaml</span><br><span class="line">service&#x2F;dubbo-demo-consumer created</span><br><span class="line">[root@wang-200 dubbo-consumer]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dubbo-consumer&#x2F;ingress.yaml</span><br><span class="line">ingress.extensions&#x2F;dubbo-demo-consumer created</span><br></pre></td></tr></table></figure><p>查看pod是否运行成功</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 dubbo-consumer]# kubectl get pod -n app</span><br><span class="line">NAME                                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">dubbo-demo-consumer-5f69cf757c-cqglt   1&#x2F;1     Running   0          48s</span><br></pre></td></tr></table></figure><p> 查看log，是否启动成功：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 dubbo-consumer]# kubectl logs -f dubbo-demo-consumer-5f69cf757c-cqglt -n app</span><br><span class="line">Dubbo client started</span><br><span class="line">Dubbo 消费者端启动</span><br></pre></td></tr></table></figure><p>检查dubbo-monitor是否已经注册成功：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfky1e3x3jj31oc096whk.jpg" alt="dubbo-monitor注册图"></p><p> 浏览器访问<a href="http://demo.od.com/hello?name=slim" target="_blank" rel="noopener">http://demo.od.com/hello?name=slim</a></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfky1c3av7j30yq0bewg1.jpg" alt="demo页面访问展示"></p><h3 id="第一次系统升级演示"><a href="#第一次系统升级演示" class="headerlink" title="第一次系统升级演示"></a>第一次系统升级演示</h3><p>​    接下来我们模拟升级发版，我们提前修改了代码，并提交到了git仓库，发版的前提是使用jenkins提前构建了镜像并且上传到了我们的私有harbor仓库中，具体的构建流程不在赘述，只需要将远程git仓库的版本修改后构建就行了。</p><p>修改源码<a href="https://gitee.com/wangzhangtao/dubbo-demo-web/blob/master/dubbo-client/src/main/java/com/od/dubbotest/action/HelloAction.java" target="_blank" rel="noopener">dubbo-client/src/main/java/com/od/dubbotest/action/HelloAction.java</a></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">String str&#x3D;&quot;&lt;h1&gt;这是Dubbo 消费者端(springboot）第一次升级&lt;&#x2F;h1&gt;&quot;;</span><br></pre></td></tr></table></figure><p>修改dp.yaml资源配置清单，修改harbor镜像仓库中对应的tag版本：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfky1ckmm2j31fq0imtbi.jpg" alt="harbor镜像仓库中consumer新版本"></p><p>vim /data/k8s/yaml/dubbo-consumer/deploy.yaml </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: dubbo-demo-consumer</span><br><span class="line">    image: harbor.od.com&#x2F;app&#x2F;dubbo-demo-consumer:master_20200605_1700</span><br></pre></td></tr></table></figure><p> 应用修改后的资源配置清单，当然也可以在dashboard中进行在线修改：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl apply -f &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dubbo-consumer&#x2F;deploy.yaml</span><br><span class="line">deployment.extensions&#x2F;dubbo-demo-consumer configured</span><br></pre></td></tr></table></figure><p> 已经启动起来了，使用浏览器验证：<a href="http://demo.od.com/hello?name=slim" target="_blank" rel="noopener">http://demo.od.com/hello?name=slim</a></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfky1ek84vj315g0a6wgf.jpg" alt="consumer第一次升级">至此，我们一套完成的dubbo服务就已经交付到k8s集群当中了，并且也演示了如何发版。</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 老男孩 </category>
          
          <category> 实战交付 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 老男孩 </tag>
            
            <tag> 实战交付 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一、交付-基础环境准备</title>
      <link href="/2020/06/05/%E4%B8%80%E3%80%81%E4%BA%A4%E4%BB%98-%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"/>
      <url>/2020/06/05/%E4%B8%80%E3%80%81%E4%BA%A4%E4%BB%98-%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/</url>
      
        <content type="html"><![CDATA[<h2 id="实验架构图"><a href="#实验架构图" class="headerlink" title="实验架构图"></a>实验架构图</h2><p><img src="http://wangzhangtao.com/img/kubernetes/install/2020033123005972.png" alt="实验架构图"></p><h2 id="实验主机说明"><a href="#实验主机说明" class="headerlink" title="实验主机说明"></a>实验主机说明</h2><p>操作系统：7.6.1810； 内核：3.10.0</p><table><thead><tr><th>主机名</th><th>IP地址</th><th>角色</th><th>硬件配置</th></tr></thead><tbody><tr><td>wang-200.host.com (zzgw7-200)</td><td>192.168.70.200</td><td>运维节点</td><td>4核8G-100G</td></tr><tr><td>wang-11.host.com (zzgw7-11)</td><td>192.168.70.11</td><td>k8s代理节点</td><td>2核4G-100G</td></tr><tr><td>wang-12.host.com (zzgw7-12)</td><td>192.168.70.12</td><td>k8s代理节点</td><td>2核4G-100G</td></tr><tr><td>wang-21.host.com (zzgw7-21)</td><td>192.168.70.21</td><td>k8s控制节点</td><td>2核4G-100G</td></tr><tr><td>wang-22.host.com (zzgw7-22)</td><td>192.168.70.22</td><td>k8s控制节点</td><td>2核4G-100G</td></tr><tr><td>wang-23.host.com (zzgw7-21)</td><td>192.168.70.23</td><td>k8s控制节点, 工作节点</td><td>2核4G-100G</td></tr><tr><td>wang-24.host.com (zzgw7-22)</td><td>192.168.70.24</td><td>k8s工作节点</td><td>4核8G-100G</td></tr></tbody></table><h2 id="交付架构图"><a href="#交付架构图" class="headerlink" title="交付架构图"></a>交付架构图</h2><p>本次交付的服务架构图：因为zookeeper属于有状态服务，不建议将有状态服务，交付到k8s，如mysql，zk等。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfgcyc2yfdj319g0kqate.jpg" alt="服务架构图"></p><h2 id="部署zookeeper-单节点"><a href="#部署zookeeper-单节点" class="headerlink" title="部署zookeeper(单节点)"></a>部署zookeeper(单节点)</h2><h3 id="安装java"><a href="#安装java" class="headerlink" title="安装java"></a>安装java</h3><blockquote><p>官网地址 <a href="https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html" target="_blank" rel="noopener">JDK 8u202 and earlier</a>   <a href="https://download.oracle.com/otn/java/jdk/8u202-b08/1961070e4c9b4e26a04e7f5a083f551e/jdk-8u202-linux-x64.tar.gz" target="_blank" rel="noopener">jdk-8u202-linux</a>  :)  </p></blockquote><p>准备文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;opt&#x2F;src</span><br><span class="line">mkdir &#x2F;usr&#x2F;java</span><br></pre></td></tr></table></figure><p>拷贝安装包并解压</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp wang-200:&#x2F;data&#x2F;soft&#x2F;centos7&#x2F;jdk-8u221-linux-x64.tar.gz &#x2F;opt&#x2F;src&#x2F;</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;jdk-8u221-linux-x64.tar.gz -C &#x2F;usr&#x2F;java&#x2F;</span><br><span class="line">ln -s &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_221 &#x2F;usr&#x2F;java&#x2F;jdk</span><br></pre></td></tr></table></figure><p><strong>配置默认全局变量</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;&gt; &#x2F;etc&#x2F;profile</span><br><span class="line">#JAVA HOME</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk</span><br><span class="line">export PATH&#x3D;\$PATH:\$JAVA_HOME&#x2F;bin:\$JAVA_HOME&#x2F;sbin</span><br><span class="line">export CLASSPATH&#x3D;\$CLASSPATH:\$JAVA_HOME&#x2F;lib:\$JAVA_HOME&#x2F;lib&#x2F;tools.jar</span><br><span class="line">EOF</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure><p><strong>检查java是否可用</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-11 ~]# java version</span><br><span class="line">错误: 找不到或无法加载主类 version</span><br><span class="line">[root@wang-11 ~]# java -version</span><br><span class="line">java version &quot;1.8.0_221&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_221-b11)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.221-b11, mixed mode)</span><br></pre></td></tr></table></figure><h3 id="下载zookeeper"><a href="#下载zookeeper" class="headerlink" title="下载zookeeper"></a>下载zookeeper</h3><p>下载zookeeper: <a href="https://archive.apache.org/dist/zookeeper/" target="_blank" rel="noopener">下载地址</a>   <a href="https://archive.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz" target="_blank" rel="noopener">版本3.4.14</a></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp wang-200:&#x2F;data&#x2F;soft&#x2F;centos7&#x2F;zookeeper-3.4.14.tar.gz &#x2F;opt&#x2F;src&#x2F;zookeeper-3.4.14.tar.gz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;zookeeper-3.4.14.tar.gz -C &#x2F;opt&#x2F;</span><br><span class="line">ln -s &#x2F;opt&#x2F;zookeeper-3.4.14  &#x2F;opt&#x2F;zookeeper</span><br><span class="line">mkdir -pv &#x2F;data&#x2F;zookeeper&#x2F;data &#x2F;data&#x2F;zookeeper&#x2F;logs</span><br></pre></td></tr></table></figure><h4 id="编辑配置文件"><a href="#编辑配置文件" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h4><p><code>vi /opt/zookeeper/conf/zoo.cfg</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tickTime&#x3D;2000</span><br><span class="line">initLimit&#x3D;10</span><br><span class="line">syncLimit&#x3D;5</span><br><span class="line">dataDir&#x3D;&#x2F;data&#x2F;zookeeper&#x2F;data</span><br><span class="line">dataLogDir&#x3D;&#x2F;data&#x2F;zookeeper&#x2F;logs</span><br><span class="line">clientPort&#x3D;2181</span><br></pre></td></tr></table></figure><h4 id="配置DNS"><a href="#配置DNS" class="headerlink" title="配置DNS"></a>配置DNS</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim &#x2F;var&#x2F;named&#x2F;od.com.zone</span><br><span class="line">zk1             A       192.168.70.11</span><br><span class="line">systemctl reload named</span><br></pre></td></tr></table></figure><h4 id="重启named并检测"><a href="#重启named并检测" class="headerlink" title="重启named并检测"></a>重启named并检测</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 ~]# systemctl restart named   </span><br><span class="line">[root@wang-12 ~]# dig -t A zk1.od.com +short</span><br><span class="line">192.168.70.11</span><br></pre></td></tr></table></figure><h3 id="启动zookeeper"><a href="#启动zookeeper" class="headerlink" title="启动zookeeper"></a>启动zookeeper</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;opt&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh start</span><br></pre></td></tr></table></figure><h3 id="查看zookeeper情况"><a href="#查看zookeeper情况" class="headerlink" title="查看zookeeper情况"></a>查看zookeeper情况</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-11 ~]# &#x2F;opt&#x2F;zookeeper&#x2F;bin&#x2F;zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: &#x2F;opt&#x2F;zookeeper&#x2F;bin&#x2F;..&#x2F;conf&#x2F;zoo.cfg</span><br><span class="line">Mode: standalone</span><br></pre></td></tr></table></figure><p> 到此，zookeeper就搭建好了。</p><h2 id="交付jenkins到k8s集群"><a href="#交付jenkins到k8s集群" class="headerlink" title="交付jenkins到k8s集群"></a>交付jenkins到k8s集群</h2><h3 id="准备jenkins基础镜像"><a href="#准备jenkins基础镜像" class="headerlink" title="准备jenkins基础镜像"></a>准备jenkins基础镜像</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull jenkins&#x2F;jenkins:2.190.3</span><br><span class="line">docker tag jenkins&#x2F;jenkins:2.190.3 harbor.od.com&#x2F;public&#x2F;jenkins:v2.190.3</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;jenkins:v2.190.3</span><br></pre></td></tr></table></figure><h3 id="优化Jenkins镜像"><a href="#优化Jenkins镜像" class="headerlink" title="优化Jenkins镜像"></a>优化Jenkins镜像</h3><p>为了适应我们的环境，我们的jenkins不能直接使用，需要进行配置：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;data&#x2F;dockerfile&#x2F;jenkins&#x2F;</span><br><span class="line">cd &#x2F;data&#x2F;dockerfile&#x2F;jenkins</span><br></pre></td></tr></table></figure><p><code>vi Dockerfile</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM harbor.od.com&#x2F;public&#x2F;jenkins:v2.190.3</span><br><span class="line">USER root  </span><br><span class="line">RUN &#x2F;bin&#x2F;cp &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai &#x2F;etc&#x2F;localtime &amp;&amp;\ </span><br><span class="line">    echo &#39;Asia&#x2F;Shanghai&#39; &gt;&#x2F;etc&#x2F;timezone  #修改时区 改成东八区</span><br><span class="line"></span><br><span class="line">#加载用户密钥，dubbo服务拉取代码使用的ssh</span><br><span class="line">ADD id_rsa &#x2F;root&#x2F;.ssh&#x2F;id_rsa  </span><br><span class="line">#加载宿主机的docker配置文件，登录远程仓库的认证信息加载到容器里面。</span><br><span class="line">ADD config.json &#x2F;root&#x2F;.docker&#x2F;config.json  </span><br><span class="line"># 在jenkins容器内安装docker 客户端，jenkins要执行docker build，docker引擎用的是宿主机的docker引擎</span><br><span class="line">ADD get-docker.sh &#x2F;get-docker.sh </span><br><span class="line"></span><br><span class="line">RUN echo &quot;    StrictHostKeyChecking no&quot; &gt;&gt; &#x2F;etc&#x2F;ssh&#x2F;ssh_config &amp;&amp;\</span><br><span class="line">    &#x2F;get-docker.sh  # 跳过 ssh时候输入 yes 步骤，并执行安装docker</span><br></pre></td></tr></table></figure><p>将私钥加载到jenkins，将公钥配置到git仓库中，否则不能拉取代码：</p><p> 接下来创建Dockerfile中需要的文件：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -fsSL get.docker.com -o get-docker.sh</span><br><span class="line">chmod u+x get-docker.sh</span><br></pre></td></tr></table></figure><p>拷贝私钥,dubbo服务拉去代码使用ssh</p><p>拷贝docker认证配置文件,<strong>加载宿主机docker配置文件，登陆远程仓库的认证信息到容器里</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp &#x2F;root&#x2F;.ssh&#x2F;id_rsa .&#x2F;</span><br><span class="line">cp &#x2F;root&#x2F;.docker&#x2F;config.json .&#x2F;</span><br></pre></td></tr></table></figure><p>创建运维私有仓库，打开我们的harbor.od.com创建一个infra的私有仓库：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfh9bsnziqj30h305x74g.jpg" alt="infra私有仓库"></p><p> 然后build镜像：过程漫长，可以抽根烟，喝杯茶了</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker build . -t harbor.od.com&#x2F;infra&#x2F;jenkins:v2.190.3</span><br><span class="line">docker push harbor.od.com&#x2F;infra&#x2F;jenkins:v2.190.3</span><br></pre></td></tr></table></figure><h3 id="为jenkins创建名称空间"><a href="#为jenkins创建名称空间" class="headerlink" title="为jenkins创建名称空间"></a>为jenkins创建名称空间</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 jenkins]# kubectl create ns infra</span><br><span class="line">namespace&#x2F;infra created</span><br><span class="line">[root@wang-200 jenkins]# kubectl get ns | grep infra</span><br><span class="line">infra             Active   13s</span><br></pre></td></tr></table></figure><p>创建一条secret，用于访问我们的私有仓库infra：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl create secret docker-registry harbor --docker-server&#x3D;harbor.od.com --docker-username&#x3D;admin --docker-password&#x3D;Harbor12345 -n infra</span><br></pre></td></tr></table></figure><p>解释一下上面的命令：创建一条secret，资源类型是docker-registry，名字是  harbor，docker-server=harbor.od.com ，docker-username=admin  ，docker-password=Harbor12345 -n 指定私有仓库名称infra</p><h3 id="创建nfs共享数据盘"><a href="#创建nfs共享数据盘" class="headerlink" title="创建nfs共享数据盘"></a>创建nfs共享数据盘</h3><p> 为了让jenkins中一些需要持久化的数据，能够存储，我们需要使用共享存储，然后进行挂载：这里使用最简单的NFS共享存储，因为k8s默认支持nfs模块</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 jenkins]# mkdir &#x2F;data&#x2F;nfs&#x2F;v1&#x2F;jenkins_home</span><br><span class="line">[root@wang-200 jenkins]# vi &#x2F;etc&#x2F;exports</span><br><span class="line">&#x2F;data&#x2F;nfs&#x2F;v1&#x2F;jenkins_home 192.168.70.1&#x2F;24(insecure,rw,sync,no_root_squash)</span><br><span class="line"></span><br><span class="line">[root@wang-200 jenkins]# systemctl reload nfs</span><br></pre></td></tr></table></figure><h3 id="准备jenkins资源配置清单"><a href="#准备jenkins资源配置清单" class="headerlink" title="准备jenkins资源配置清单"></a>准备jenkins资源配置清单</h3><p>创建ymal文件夹</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 jenkins]# mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;jenkins&#x2F;</span><br><span class="line">[root@wang-200 jenkins]# cd &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;jenkins&#x2F;</span><br></pre></td></tr></table></figure><p>这里挂载了宿主机的docker.sock，使容器内的docker客户端可以直接与宿主机的docker引擎进行通信</p><p>在使用私有仓库的时候，资源清单中，一定要声明：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">imagePullSecrets:</span><br><span class="line">- name: harbor</span><br></pre></td></tr></table></figure><p><strong>deploy.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;jenkins&#x2F;deploy.yaml</span><br><span class="line">kind: Deployment</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: infra</span><br><span class="line">  labels: </span><br><span class="line">    name: jenkins</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels: </span><br><span class="line">      name: jenkins</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels: </span><br><span class="line">        app: jenkins </span><br><span class="line">        name: jenkins</span><br><span class="line">    spec:</span><br><span class="line">      volumes:</span><br><span class="line">      - name: data</span><br><span class="line">        nfs: </span><br><span class="line">          server: wang-200</span><br><span class="line">          path: &#x2F;data&#x2F;nfs&#x2F;v1&#x2F;jenkins_home</span><br><span class="line">      - name: docker</span><br><span class="line">        hostPath: </span><br><span class="line">          path: &#x2F;run&#x2F;docker.sock   </span><br><span class="line">          type: &#39;&#39;</span><br><span class="line">      containers:</span><br><span class="line">      - name: jenkins</span><br><span class="line">        image: harbor.od.com&#x2F;infra&#x2F;jenkins:v2.190.3</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          protocol: TCP</span><br><span class="line">        env:</span><br><span class="line">        - name: JAVA_OPTS</span><br><span class="line">          value: -Xmx512m -Xms512m</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: data</span><br><span class="line">          mountPath: &#x2F;var&#x2F;jenkins_home</span><br><span class="line">        - name: docker</span><br><span class="line">          mountPath: &#x2F;run&#x2F;docker.sock</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: harbor</span><br><span class="line">      securityContext: </span><br><span class="line">        runAsUser: 0</span><br><span class="line">  strategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate: </span><br><span class="line">      maxUnavailable: 1</span><br><span class="line">      maxSurge: 1</span><br><span class="line">  revisionHistoryLimit: 7</span><br><span class="line">  progressDeadlineSeconds: 600</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>svc.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;jenkins&#x2F;svc.yaml</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata: </span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    app: jenkins</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>ingress.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;jenkins&#x2F;ingress.yaml</span><br><span class="line">kind: Ingress</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">metadata: </span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: infra</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: jenkins.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend: </span><br><span class="line">          serviceName: jenkins</span><br><span class="line">          servicePort: 80</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单"><a href="#应用资源配置清单" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><p><strong>执行命令</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;jenkins&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;jenkins&#x2F;svc.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;jenkins&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p><strong>运行过程</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 jenkins]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;jenkins&#x2F;deploy.yaml</span><br><span class="line">deployment.extensions&#x2F;jenkins created</span><br><span class="line">[root@wang-200 jenkins]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;jenkins&#x2F;svc.yaml</span><br><span class="line">service&#x2F;jenkins created</span><br><span class="line">[root@wang-200 jenkins]# </span><br><span class="line">[root@wang-200 jenkins]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;jenkins&#x2F;ingress.yaml</span><br><span class="line">ingress.extensions&#x2F;jenkins created</span><br></pre></td></tr></table></figure><p>查看我们创建的pod：这个启动时间还是挺长的，大概要几分钟时间</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 jenkins]# kubectl get pod -n infra</span><br><span class="line">NAME                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">jenkins-65b9b6d56-46qnt   1&#x2F;1     Running   0          56s</span><br></pre></td></tr></table></figure><p> 检查jenkins需要持久化的数据是否保存下来了：wang-200</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfhd910i5fj30tw035q41.jpg" alt="jenkins数据块"></p><h4 id="访问UI界面"><a href="#访问UI界面" class="headerlink" title="访问UI界面"></a>访问UI界面</h4><p><a href="http://jenkins.od.com/" target="_blank" rel="noopener">http://jenkins.od.com/</a></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfhdfve9urj31j40su78w.jpg" alt="jenkins安装界面"></p><h4 id="替换jenkins更新源"><a href="#替换jenkins更新源" class="headerlink" title="替换jenkins更新源"></a>替换jenkins更新源</h4><p>建议更新数据源后再下载插件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;data&#x2F;nfs&#x2F;v1&#x2F;jenkins_home&#x2F;</span><br><span class="line">sed -i &#39;s&#x2F;http:\&#x2F;\&#x2F;updates.jenkins-ci.org\&#x2F;download&#x2F;https:\&#x2F;\&#x2F;mirrors.tuna.tsinghua.edu.cn\&#x2F;jenkins&#x2F;g&#39; updates&#x2F;default.json &amp;&amp; sed -i &#39;s&#x2F;http:\&#x2F;\&#x2F;www.google.com&#x2F;https:\&#x2F;\&#x2F;www.baidu.com&#x2F;g&#39; updates&#x2F;default.json</span><br></pre></td></tr></table></figure><p>删除pod,重启Jenkins</p><p>然后再安装默认插件，发现速度特别快 :)</p><h4 id="安装jenkins"><a href="#安装jenkins" class="headerlink" title="安装jenkins"></a>安装jenkins</h4><p><strong>创建第一个管理员用户admin</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfheakxe26j31kb0u0tey.jpg" alt="登陆成功后界面"></p><p>安全配置：</p><p>搜索蓝海插件并安装：Blue Ocean</p><h3 id="验证jenkins容器状态"><a href="#验证jenkins容器状态" class="headerlink" title="验证jenkins容器状态"></a>验证jenkins容器状态</h3><p><strong>进入pod中</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pod -n infra</span><br><span class="line">NAME                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">jenkins-65b9b6d56-9h54h   1&#x2F;1     Running   0          5m38s</span><br><span class="line">[root@wang-200 ~]# kubectl -n infra exec -it jenkins-65b9b6d56-9h54h &#x2F;bin&#x2F;bash</span><br></pre></td></tr></table></figure><p><strong>是否是root用户</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@jenkins-65b9b6d56-9h54h:&#x2F;# whoami</span><br><span class="line">root</span><br></pre></td></tr></table></figure><p><strong>时区是否是东八区</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@jenkins-65b9b6d56-9h54h:&#x2F;# date</span><br><span class="line">Fri Jun  5 14:31:59 CST 2020</span><br></pre></td></tr></table></figure><p><strong>是否使用宿主机docker引擎,在容器内查看宿主机上的docker资源情况</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@jenkins-65b9b6d56-9h54h:&#x2F;# docker ps</span><br><span class="line">CONTAINER ID        IMAGE                               COMMAND                  CREATED             STATUS              PORTS                NAMES</span><br><span class="line">65f0f58f370e        bf4556c81529                        &quot;&#x2F;sbin&#x2F;tini -- &#x2F;usr&#x2F;…&quot;   7 minutes ago       Up 7 minutes                             k8s_jenkins_jenkins-65b9b6d56-9h54h_infra_d7f03239-744c-4f66-a5d4-d94fe42c5eaf_0</span><br></pre></td></tr></table></figure><p><strong>是否能免密访问gitee</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@jenkins-65b9b6d56-9h54h:&#x2F;# ssh -i &#x2F;root&#x2F;.ssh&#x2F;id_rsa -T git@gitee.com</span><br><span class="line">Hi wangzt! You&#39;ve successfully authenticated, but GITEE.COM does not provide shell access</span><br></pre></td></tr></table></figure><p><strong>是否能访问harbor私有仓库</strong> </p><p>原因是我们挂载了宿主机的docker config.json</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@jenkins-65b9b6d56-9h54h:&#x2F;# docker login harbor.od.com</span><br><span class="line">Authenticating with existing credentials...</span><br><span class="line">WARNING! Your password will be stored unencrypted in &#x2F;root&#x2F;.docker&#x2F;config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https:&#x2F;&#x2F;docs.docker.com&#x2F;engine&#x2F;reference&#x2F;commandline&#x2F;login&#x2F;#credentials-store</span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br></pre></td></tr></table></figure><p>完成验证以上内容后，证明我们基于本次实验环境的jenkins容器已经安装配置完成了。</p><h2 id="安装配置maven"><a href="#安装配置maven" class="headerlink" title="安装配置maven"></a>安装配置maven</h2><blockquote><p>maven 官方地址：<a href="https://archive.apache.org/dist/maven/maven-3/" target="_blank" rel="noopener">官方地址</a>  <a href="https://archive.apache.org/dist/maven/maven-3/3.6.1/binaries/apache-maven-3.6.1-bin.tar.gz" target="_blank" rel="noopener">maven-3.6.1</a></p></blockquote><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp wang-200:&#x2F;data&#x2F;soft&#x2F;centos7&#x2F;apache-maven-3.6.1-bin.tar.gz &#x2F;opt&#x2F;src&#x2F;</span><br><span class="line">mkdir &#x2F;data&#x2F;nfs&#x2F;v1&#x2F;jenkins_home&#x2F;maven-3.6.1</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;apache-maven-3.6.1-bin.tar.gz -C &#x2F;data&#x2F;nfs&#x2F;v1&#x2F;jenkins_home&#x2F;maven-3.6.1</span><br><span class="line">cd &#x2F;data&#x2F;nfs&#x2F;v1&#x2F;jenkins_home&#x2F;maven-3.6.1</span><br><span class="line">mv apache-maven-3.6.1&#x2F;* .</span><br><span class="line">rm -rf apache-maven-3.6.1</span><br></pre></td></tr></table></figure><p>初始化maven配置：</p><p><code>vim /data/nfs/v1/jenkins_home/maven-3.6.1/conf/settings.xml</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;mirror&gt;</span><br><span class="line">  &lt;id&gt;nexus-aliyun&lt;&#x2F;id&gt;</span><br><span class="line">  &lt;mirrorOf&gt;*&lt;&#x2F;mirrorOf&gt;</span><br><span class="line">  &lt;name&gt;Nexus aliyun&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;url&gt;http:&#x2F;&#x2F;maven.aliyun.com&#x2F;nexus&#x2F;content&#x2F;groups&#x2F;public&lt;&#x2F;url&gt;</span><br><span class="line">&lt;&#x2F;mirror&gt;</span><br></pre></td></tr></table></figure><h2 id="制作JAVA运行基础镜像"><a href="#制作JAVA运行基础镜像" class="headerlink" title="制作JAVA运行基础镜像"></a>制作JAVA运行基础镜像</h2><p>dubbo微服务底包镜像</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull stanleyws&#x2F;jre8:8u112</span><br><span class="line">docker tag fa3a085d6ef1 harbor.od.com&#x2F;public&#x2F;jre:8u112</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;jre:8u112</span><br></pre></td></tr></table></figure><p>创建Dockerfile：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;dockerfile&#x2F;jre8</span><br><span class="line">cd &#x2F;data&#x2F;dockerfile&#x2F;jre8</span><br></pre></td></tr></table></figure><p><code>vi Dockerfile</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">FROM harbor.od.com&#x2F;public&#x2F;jre:8u112</span><br><span class="line">RUN &#x2F;bin&#x2F;cp &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai &#x2F;etc&#x2F;localtime &amp;&amp;\</span><br><span class="line">    echo &#39;Asia&#x2F;Shanghai&#39; &gt;&#x2F;etc&#x2F;timezone</span><br><span class="line">ADD config.yml &#x2F;opt&#x2F;prom&#x2F;config.yml</span><br><span class="line">ADD jmx_javaagent-0.3.1.jar &#x2F;opt&#x2F;prom&#x2F;</span><br><span class="line">WORKDIR &#x2F;opt&#x2F;project_dir</span><br><span class="line">ADD entrypoint.sh &#x2F;entrypoint.sh</span><br><span class="line">CMD [&quot;&#x2F;entrypoint.sh&quot;]</span><br></pre></td></tr></table></figure><p>创建Dockerfile所需文件：</p><p><code>vi config.yml</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">--- </span><br><span class="line">rules: </span><br><span class="line"> - pattern: &#39;.*&#39;</span><br></pre></td></tr></table></figure><p>下载jmx_javaagent,监控jvm信息</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;repo1.maven.org&#x2F;maven2&#x2F;io&#x2F;prometheus&#x2F;jmx&#x2F;jmx_prometheus_javaagent&#x2F;0.3.1&#x2F;jmx_prometheus_javaagent-0.3.1.jar -O jmx_javaagent-0.3.1.jar</span><br></pre></td></tr></table></figure><p>创建entrypoint.sh：<strong>使用exec 来运行java的jar包，能够使脚本将自己的pid 为‘1’ 传递给java进程，避免docker容器因没有前台进程而退出。并且不要加&amp;符。</strong></p><p>vim entrypoint.sh</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">M_OPTS&#x3D;&quot;-Duser.timezone&#x3D;Asia&#x2F;Shanghai -javaagent:&#x2F;opt&#x2F;prom&#x2F;jmx_javaagent-0.3.1.jar&#x3D;$(hostname -i):$&#123;M_PORT:-&quot;12346&quot;&#125;:&#x2F;opt&#x2F;prom&#x2F;config.yml&quot;</span><br><span class="line">C_OPTS&#x3D;$&#123;C_OPTS&#125;</span><br><span class="line">JAR_BALL&#x3D;$&#123;JAR_BALL&#125;</span><br><span class="line">exec java -jar $&#123;M_OPTS&#125; $&#123;C_OPTS&#125; $&#123;JAR_BALL&#125;</span><br></pre></td></tr></table></figure><p>执行权限</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod u+x entrypoint.sh</span><br></pre></td></tr></table></figure><p>执行docker build：base仓库自行创建，权限公开</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker build . -t harbor.od.com&#x2F;base&#x2F;jre8:8u112</span><br><span class="line">docker push harbor.od.com&#x2F;base&#x2F;jre8:8u112</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 老男孩 </category>
          
          <category> 实战交付 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 老男孩 </tag>
            
            <tag> 实战交付 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>七、k8s-错误收集</title>
      <link href="/2020/06/04/%E4%B8%83%E3%80%81k8s-%E9%94%99%E8%AF%AF%E6%94%B6%E9%9B%86/"/>
      <url>/2020/06/04/%E4%B8%83%E3%80%81k8s-%E9%94%99%E8%AF%AF%E6%94%B6%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h2 id="错误收集"><a href="#错误收集" class="headerlink" title="错误收集"></a>错误收集</h2><h2 id="etcd启动报错"><a href="#etcd启动报错" class="headerlink" title="etcd启动报错"></a>etcd启动报错</h2><p>etcd启动报错信息如下</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-22 etcd-v3.1.20]# .&#x2F;etcd-server-startup.sh </span><br><span class="line">2020-05-31 23:45:33.653591 I | etcdmain: etcd Version: 3.1.20</span><br><span class="line">2020-05-31 23:45:33.653780 I | etcdmain: Git SHA: 992dbd4d1</span><br><span class="line">2020-05-31 23:45:33.653814 I | etcdmain: Go Version: go1.8.7</span><br><span class="line">2020-05-31 23:45:33.653842 I | etcdmain: Go OS&#x2F;Arch: linux&#x2F;amd64</span><br><span class="line">2020-05-31 23:45:33.653872 I | etcdmain: setting maximum number of CPUs to 4, total number of available CPUs is 4</span><br><span class="line">2020-05-31 23:45:33.654030 N | etcdmain: the server is already initialized as member before, starting as etcd member...</span><br><span class="line">2020-05-31 23:45:33.654099 I | embed: peerTLS: cert &#x3D; .&#x2F;certs&#x2F;etcd-peer.pem, key &#x3D; .&#x2F;certs&#x2F;etcd-peer-key.pem, ca &#x3D; .&#x2F;certs&#x2F;ca.pem, trusted-ca &#x3D; .&#x2F;certs&#x2F;ca.pem, client-cert-auth &#x3D; true</span><br><span class="line">2020-05-31 23:45:33.656743 I | embed: listening for peers on https:&#x2F;&#x2F;192.168.70.22:2380</span><br><span class="line">2020-05-31 23:45:33.656823 W | embed: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while peer key&#x2F;cert files are presented. Ignored key&#x2F;cert files.</span><br><span class="line">2020-05-31 23:45:33.656845 W | embed: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while client cert auth (--client-cert-auth) is enabled. Ignored client cert auth for this url.</span><br><span class="line">2020-05-31 23:45:33.656968 I | embed: listening for client requests on 127.0.0.1:2379</span><br><span class="line">2020-05-31 23:45:33.657162 I | embed: listening for client requests on 192.168.70.22:2379</span><br><span class="line">2020-05-31 23:45:33.665642 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated</span><br><span class="line">2020-05-31 23:45:33.701913 C | etcdmain: member f97020d34db750ca has already been bootstrapped</span><br></pre></td></tr></table></figure><p><strong>解决办法：</strong></p><p>是因为一开始本机etcd已经注册进去了，当我修改配置文件后启动服务，因为配置和以前不一样，所以不让我启动，将历史记录删除就可以了</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 老男孩 </category>
          
          <category> 二进制安装 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 二进制安装 </tag>
            
            <tag> 老男孩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>六、k8s-集群平滑升级</title>
      <link href="/2020/06/04/%E5%85%AD%E3%80%81k8s-%E9%9B%86%E7%BE%A4%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7/"/>
      <url>/2020/06/04/%E5%85%AD%E3%80%81k8s-%E9%9B%86%E7%BE%A4%E5%B9%B3%E6%BB%91%E5%8D%87%E7%BA%A7/</url>
      
        <content type="html"><![CDATA[<h2 id="k8s-集群平滑升级"><a href="#k8s-集群平滑升级" class="headerlink" title="k8s 集群平滑升级"></a>k8s 集群平滑升级</h2><p>鸣谢：<a href="https://www.cnblogs.com/gaorong/p/11266629.html" target="_blank" rel="noopener">代码小工</a></p><p>官方建议升级过程</p><ol><li>首先阅读相关<code>release node</code>，重点关注其中几部分： Known Issues，Action Requireed，Deprecations and removals。社区会将一些变化highlight到这里，阅读这些变化可以明确自己需要采取哪些行动。</li><li>kubernetes 建议不断地进行小版本升级，而不是一次进行大的版本跳跃。具体的兼容策略是：  slave组件可以与master组件最多延迟两个小版本(minor  version)，但是不能比master组件新。client不能与master组件落后一个小版本，但是可以高一个版本，也就是说：   v1.3的master可以与v1.1，v1.2，v1.3的slave组件一起使用，与v1.2，v1.3，v1.4   client一起使用。官方建议每次升级不要跨越两个版本，升级顺序为: master，addons，salve。</li><li>slave节点的升级是滚动升级，官方建议首先使用<code>kubectl drain</code>驱逐pod之后，然后升级kubelet，因为kubelet会有一些状态信息文件存储在node节点上，社区并不保证这些状态文件在版本间的兼容性。</li><li>apiserver升级之前需要确保resource version被正常支持，目前kubernetes会逐步废弃掉，例如:  DaemonSet，Deployment，ReplicaSet  所使用的  extensions/v1beta1，apps/v1beta1，apps/v1beta2   将会在v1.16中完全废弃掉，届时，如果你再去读取这些版本的资源，apiserver将不会认识这些资源，任何的增删改查都无法进行，只能通过<code>etcdctl</code>进行删除。目前社区正在开发迁移工具，并且在支持该工具之前，所有的版本移除操作都会被冻结，所以目前(2019.5.20)来说是相对安全的。</li><li><strong>(有争议)</strong>虽然kubernetes建议先升级master组件，然后再升级node组件，但是实际应用过程中建议先停掉controller-manager，然后升级master组件，node组件，最后再升级controller-manager，因为controller-manager中会进行一些状态的调谐(reconcile)，对于actual status不符合desire  status的对象会触发一些操作。升级过程中尽管我们会进行充分的测试，但是也难免出现一些非预期的情况下，例如apiserver中某些资源对象的兼容性不好，或者其中的某些字段进行调整，触发controller-manager执行非预期操作，例如重建一个deployment下所有的pod，更糟糕的是，如果此时kubelet还未升级，就可能不认识新版本一些资源对象中的新增的某些字段，此时老的pod被删掉了，但是新的pod没有起来，就造成一定的服务中断。(后面会有相关的案例)</li></ol><p><strong>官方建议每次升级不要跨越两个版本，升级顺序为: master，addons，salve。</strong></p><p><strong>注意：生产根据业务来规划升级时间，这里以wang-21为例。</strong></p><h3 id="环境描述"><a href="#环境描述" class="headerlink" title="环境描述"></a>环境描述</h3><p>目前使用版本为<code>V1.15.2</code>,升级版本为<code>V1.15.4</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get node</span><br><span class="line">NAME               STATUS   ROLES         AGE   VERSION</span><br><span class="line">wang-23.host.com   Ready    master,node   28h   v1.15.2</span><br><span class="line">wang-24.host.com   Ready    node          28h   v1.15.2</span><br></pre></td></tr></table></figure><p>显示的版本号实际上是kubelet的版本号</p><h3 id="准备-V1-15-4版本软件"><a href="#准备-V1-15-4版本软件" class="headerlink" title="准备 V1.15.4版本软件"></a>准备 <code>V1.15.4</code>版本软件</h3><blockquote><p>官网地址 <em><a href="https://dl.k8s.io/v1.15.4/kubernetes-server-linux-amd64.tar.gz" target="_blank" rel="noopener">https://dl.k8s.io/v1.15.4/kubernetes-server-linux-amd64.tar.gz</a></em> </p></blockquote><p><code>[root@wang-200 ~]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp wang-200:&#x2F;data&#x2F;soft&#x2F;k8s&#x2F;kubernetes-server-linux-amd64-v1.15.4.tar.gz &#x2F;opt&#x2F;src&#x2F;kubernetes-server-linux-amd64-v1.15.4.tar.gz</span><br><span class="line"></span><br><span class="line"># 默认是kubernetes目录</span><br><span class="line">mkdir &#x2F;opt&#x2F;kubernetes-v1.15.4</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;kubernetes-server-linux-amd64-v1.15.4.tar.gz -C &#x2F;opt&#x2F;kubernetes-v1.15.4&#x2F; # 指定文件夹，否则会覆盖&#x2F;opt&#x2F;kubernetes这个文件夹</span><br><span class="line"></span><br><span class="line">mv &#x2F;opt&#x2F;kubernetes-v1.15.4&#x2F;kubernetes&#x2F;* &#x2F;opt&#x2F;kubernetes-v1.15.4&#x2F;</span><br><span class="line">rm -rf &#x2F;opt&#x2F;kubernetes-v1.15.4&#x2F;kubernetes*</span><br><span class="line">rm -rf &#x2F;opt&#x2F;kubernetes-v1.15.4&#x2F;server&#x2F;bin&#x2F;*_tag</span><br><span class="line">rm -rf &#x2F;opt&#x2F;kubernetes-v1.15.4&#x2F;server&#x2F;bin&#x2F;*.tar</span><br></pre></td></tr></table></figure><p><strong>最终文件结构</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# tree &#x2F;opt&#x2F;kubernetes-v1.15.4</span><br><span class="line">&#x2F;opt&#x2F;kubernetes-v1.15.4</span><br><span class="line">├── addons</span><br><span class="line">├── LICENSES</span><br><span class="line">└── server</span><br><span class="line">    └── bin</span><br><span class="line">        ├── apiextensions-apiserver</span><br><span class="line">        ├── cloud-controller-manager</span><br><span class="line">        ├── hyperkube</span><br><span class="line">        ├── kubeadm</span><br><span class="line">        ├── kube-apiserver</span><br><span class="line">        ├── kube-controller-manager</span><br><span class="line">        ├── kubectl</span><br><span class="line">        ├── kubelet</span><br><span class="line">        ├── kube-proxy</span><br><span class="line">        ├── kube-scheduler</span><br><span class="line">        └── mounter</span><br><span class="line"></span><br><span class="line">3 directories, 12 files</span><br></pre></td></tr></table></figure><h3 id="升级控制节点"><a href="#升级控制节点" class="headerlink" title="升级控制节点"></a>升级控制节点</h3><p><strong>生存环境需要修改nginx.conf,把升级节点从upstream中注释掉</strong></p><p><strong>拷贝脚本与证书</strong></p><p>升级master节点wang-21</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp -r wang-200:&#x2F;opt&#x2F;kubernetes-v1.15.4 &#x2F;opt&#x2F;kubernetes-v1.15.4</span><br><span class="line">cp -a &#x2F;opt&#x2F;kubernetes-v1.15.2&#x2F;server&#x2F;bin&#x2F;certs &#x2F;opt&#x2F;kubernetes-v1.15.4&#x2F;server&#x2F;bin&#x2F;</span><br><span class="line">cp -a &#x2F;opt&#x2F;kubernetes-v1.15.2&#x2F;server&#x2F;bin&#x2F;conf &#x2F;opt&#x2F;kubernetes-v1.15.4&#x2F;server&#x2F;bin&#x2F;</span><br><span class="line">cp &#x2F;opt&#x2F;kubernetes-v1.15.2&#x2F;server&#x2F;bin&#x2F;*.sh &#x2F;opt&#x2F;kubernetes-v1.15.4&#x2F;server&#x2F;bin&#x2F;</span><br><span class="line">rm -f &#x2F;opt&#x2F;kubernetes</span><br><span class="line">ln -s &#x2F;opt&#x2F;kubernetes-v1.15.4 &#x2F;opt&#x2F;kubernetes</span><br></pre></td></tr></table></figure><p>重启服务</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">supervisorctl restart kube-apiserver-70-21 kube-scheduler-70-21 kube-controller-manager-70-21</span><br><span class="line"># systemctl restart supervisord</span><br><span class="line">kubectl get cs</span><br></pre></td></tr></table></figure><p>如果启动失败，就<code>ps -ef | grep kube</code>,看看还有没有孤儿进程，如果有就杀死再重启。</p><h3 id="升级工作节点"><a href="#升级工作节点" class="headerlink" title="升级工作节点"></a>升级工作节点</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get node</span><br><span class="line">NAME               STATUS   ROLES         AGE   VERSION</span><br><span class="line">wang-23.host.com   Ready    master,node   29h   v1.15.2</span><br><span class="line">wang-24.host.com   Ready    node          29h   v1.15.2</span><br></pre></td></tr></table></figure><h4 id="删除节点"><a href="#删除节点" class="headerlink" title="删除节点"></a>删除节点</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl delete node wang-24.host.com</span><br><span class="line">node &quot;wang-24.host.com&quot; deleted</span><br><span class="line">[root@wang-200 ~]# kubectl get node</span><br><span class="line">NAME               STATUS   ROLES         AGE   VERSION</span><br><span class="line">wang-23.host.com   Ready    master,node   30h   v1.15.2</span><br></pre></td></tr></table></figure><h4 id="拷贝脚本与证书"><a href="#拷贝脚本与证书" class="headerlink" title="拷贝脚本与证书"></a><strong>拷贝脚本与证书</strong></h4><p>升级master节点wang-21</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp -r wang-200:&#x2F;opt&#x2F;kubernetes-v1.15.4 &#x2F;opt&#x2F;kubernetes-v1.15.4</span><br><span class="line">cp -a &#x2F;opt&#x2F;kubernetes-v1.15.2&#x2F;server&#x2F;bin&#x2F;certs &#x2F;opt&#x2F;kubernetes-v1.15.4&#x2F;server&#x2F;bin&#x2F;</span><br><span class="line">cp -a &#x2F;opt&#x2F;kubernetes-v1.15.2&#x2F;server&#x2F;bin&#x2F;conf &#x2F;opt&#x2F;kubernetes-v1.15.4&#x2F;server&#x2F;bin&#x2F;</span><br><span class="line">cp &#x2F;opt&#x2F;kubernetes-v1.15.2&#x2F;server&#x2F;bin&#x2F;*.sh &#x2F;opt&#x2F;kubernetes-v1.15.4&#x2F;server&#x2F;bin&#x2F;</span><br><span class="line">rm -f &#x2F;opt&#x2F;kubernetes</span><br><span class="line">ln -s &#x2F;opt&#x2F;kubernetes-v1.15.4 &#x2F;opt&#x2F;kubernetes</span><br></pre></td></tr></table></figure><h4 id="重启服务"><a href="#重启服务" class="headerlink" title="重启服务"></a>重启服务</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-24 ~]# supervisorctl restart kube-kubelet-70-24 kube-proxy-70-24</span><br></pre></td></tr></table></figure><p>再次验证，发现版本已经升级</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get node</span><br><span class="line">NAME               STATUS   ROLES         AGE     VERSION</span><br><span class="line">wang-23.host.com   Ready    master,node   30h     v1.15.2</span><br><span class="line">wang-24.host.com   Ready    &lt;none&gt;        2m10s   v1.15.4</span><br></pre></td></tr></table></figure><p>我们把所有控制节点和工作节点升级完，服务就算升级完成了。</p><h3 id="恭喜你，二进制部署结束了"><a href="#恭喜你，二进制部署结束了" class="headerlink" title="恭喜你，二进制部署结束了"></a>恭喜你，二进制部署结束了</h3><h2 id="k8s-快速扩展节点"><a href="#k8s-快速扩展节点" class="headerlink" title="k8s 快速扩展节点"></a>k8s 快速扩展节点</h2><h3 id="修改主机基本信息"><a href="#修改主机基本信息" class="headerlink" title="修改主机基本信息"></a><a href="http://wangzhangtao.com/2020/05/28/一、k8s-实验环境说明/#修改wang-200运维节点系统配置" target="_blank" rel="noopener">修改主机基本信息</a></h3><p>执行命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo wang123 | passwd --stdin root</span><br><span class="line">curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;modify_server.sh | sh</span><br></pre></td></tr></table></figure><h4 id="添加dns主机名解析"><a href="#添加dns主机名解析" class="headerlink" title="添加dns主机名解析"></a>添加dns主机名解析</h4><h3 id="部署nfs服务"><a href="#部署nfs服务" class="headerlink" title="部署nfs服务"></a><a href="http://wangzhangtao.com/2020/05/30/二、k8s-前期环境准备/#部署nfs服务" target="_blank" rel="noopener">部署nfs服务</a></h3><p>执行命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y nfs-utils</span><br></pre></td></tr></table></figure><h3 id="部署-docker环境"><a href="#部署-docker环境" class="headerlink" title="部署 docker环境"></a><a href="http://wangzhangtao.com/2020/05/30/二、k8s-前期环境准备/#部署-docker环境" target="_blank" rel="noopener">部署 docker环境</a></h3><p>执行命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_docker.sh | sh</span><br></pre></td></tr></table></figure><h3 id="部署supervisor"><a href="#部署supervisor" class="headerlink" title="部署supervisor"></a><a href="http://wangzhangtao.com/2020/05/30/二、k8s-前期环境准备/#部署supervisor" target="_blank" rel="noopener">部署supervisor</a></h3><p>执行命令</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_supervisor.sh | sh</span><br></pre></td></tr></table></figure><h3 id="部署-kubelet服务"><a href="#部署-kubelet服务" class="headerlink" title="部署 kubelet服务"></a><a href="http://www.wangzhangtao.com/2020/06/02/四、k8s-部署node组件/#部署-kubelet服务" target="_blank" rel="noopener">部署 kubelet服务</a></h3><h4 id="考呗kubelet文件"><a href="#考呗kubelet文件" class="headerlink" title="考呗kubelet文件"></a>考呗kubelet文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;opt&#x2F;kubernetes-v1.15.2</span><br><span class="line">scp -r wang-200:&#x2F;opt&#x2F;kubernetes-v1.15.2&#x2F;* &#x2F;opt&#x2F;kubernetes-v1.15.2&#x2F;</span><br><span class="line">ln -s &#x2F;opt&#x2F;kubernetes-v1.15.2&#x2F; &#x2F;opt&#x2F;kubernetes</span><br><span class="line">ln -s &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kubectl &#x2F;usr&#x2F;bin&#x2F;kubectl</span><br><span class="line">kubectl version</span><br></pre></td></tr></table></figure><h4 id="创建kubelet启动脚本-wang-24例"><a href="#创建kubelet启动脚本-wang-24例" class="headerlink" title="创建kubelet启动脚本(wang-24例)"></a>创建kubelet启动脚本(wang-24例)</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kubelet.sh</span><br><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">.&#x2F;kubelet \\</span><br><span class="line">  --anonymous-auth&#x3D;false \\</span><br><span class="line">  --cgroup-driver systemd \\</span><br><span class="line">  --cluster-dns 10.2.0.2 \\</span><br><span class="line">  --cluster-domain cluster.local \\</span><br><span class="line">  --runtime-cgroups&#x3D;&#x2F;systemd&#x2F;system.slice \\</span><br><span class="line">  --kubelet-cgroups&#x3D;&#x2F;systemd&#x2F;system.slice \\</span><br><span class="line">  --fail-swap-on&#x3D;&quot;false&quot; \\</span><br><span class="line">  --client-ca-file .&#x2F;certs&#x2F;ca.pem \\</span><br><span class="line">  --tls-cert-file .&#x2F;certs&#x2F;kubelet.pem \\</span><br><span class="line">  --tls-private-key-file .&#x2F;certs&#x2F;kubelet-key.pem \\</span><br><span class="line">  --hostname-override wang-24.host.com \\</span><br><span class="line">  --image-gc-high-threshold 20 \\</span><br><span class="line">  --image-gc-low-threshold 10 \\</span><br><span class="line">  --kubeconfig .&#x2F;conf&#x2F;kubelet.kubeconfig \\</span><br><span class="line">  --log-dir &#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-kubelet \\</span><br><span class="line">  --pod-infra-container-image harbor.od.com&#x2F;public&#x2F;pause:latest \\</span><br><span class="line">  --root-dir &#x2F;data&#x2F;kubelet</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 适配对应的主机名</span><br><span class="line">sed -i &quot;s&amp;wang-24.host.com&amp;$&#123;HOSTNAME&#125;&amp;&quot; &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kubelet.sh</span><br><span class="line"></span><br><span class="line">chmod +x &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kubelet.sh </span><br><span class="line">mkdir -p &#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-kubelet &#x2F;data&#x2F;kubelet</span><br></pre></td></tr></table></figure><h4 id="创建-supervisor配置"><a href="#创建-supervisor配置" class="headerlink" title="创建 supervisor配置"></a>创建 supervisor配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;supervisord.d&#x2F;kube-kubelet.ini</span><br><span class="line">[program:kube-kubelet-70-24]</span><br><span class="line">command&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kubelet.sh         ; the program (relative uses PATH, can take args)</span><br><span class="line">numprocs&#x3D;1                                            ; number of processes copies to start (def 1)</span><br><span class="line">directory&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin                  ; directory to cwd to before exec (def no cwd) </span><br><span class="line">autostart&#x3D;true                                        ; start at supervisord start (default: true)</span><br><span class="line">autorestart&#x3D;true                                      ; retstart at unexpected quit (default: true)</span><br><span class="line">startsecs&#x3D;22                                          ; number of secs prog must stay running (def. 1)</span><br><span class="line">startretries&#x3D;3                                        ; max # of serial start failures (default 3)</span><br><span class="line">exitcodes&#x3D;0,2                                         ; &#39;expected&#39; exit codes for process (default 0,2)</span><br><span class="line">stopsignal&#x3D;QUIT                                       ; signal used to kill process (default TERM)</span><br><span class="line">stopwaitsecs&#x3D;10                                       ; max num secs to wait b4 SIGKILL (default 10)</span><br><span class="line">user&#x3D;root                                             ; setuid to this UNIX account to run the program</span><br><span class="line">redirect_stderr&#x3D;false                                 ; redirect proc stderr to stdout (default false)</span><br><span class="line">stdout_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-kubelet&#x2F;kubelet.stdout.log       ; stdout log path, NONE for none; default AUTO</span><br><span class="line">stdout_logfile_maxbytes&#x3D;64MB                          ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stdout_logfile_backups&#x3D;4                              ; # of stdout logfile backups (default 10)</span><br><span class="line">stdout_capture_maxbytes&#x3D;1MB                           ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stdout_events_enabled&#x3D;false                           ; emit events on stdout writes (default false)</span><br><span class="line">stderr_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-kubelet&#x2F;kubelet.stderr.log       ; stderr log path, NONE for none; default AUTO</span><br><span class="line">stderr_logfile_maxbytes&#x3D;64MB                          ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stderr_logfile_backups&#x3D;4                              ; # of stderr logfile backups (default 10)</span><br><span class="line">stderr_capture_maxbytes&#x3D;1MB                           ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stderr_events_enabled&#x3D;false                           ; emit events on stderr writes (default false)</span><br><span class="line">stopasgroup&#x3D;true                            ;默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程</span><br><span class="line">killasgroup&#x3D;true                            ;默认为false，向进程组发送kill信号，包括子进程</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 修改适应其他主机</span><br><span class="line">sed -i &quot;s&amp;kube-kubelet-70-24&amp;kube-kubelet-70-$&#123;HOSTNUM&#125;&amp;&quot; &#x2F;etc&#x2F;supervisord.d&#x2F;kube-kubelet.ini</span><br><span class="line"></span><br><span class="line">supervisorctl update</span><br><span class="line">supervisorctl status</span><br></pre></td></tr></table></figure><p>到运维主机查看kubelet是否注册成功：<code>kubectl get node</code></p><h3 id="给主机打上角色标签"><a href="#给主机打上角色标签" class="headerlink" title="给主机打上角色标签"></a>给主机打上角色标签</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># kubectl label node wang-23.host.com node-role.kubernetes.io&#x2F;master&#x3D;</span><br><span class="line">kubectl label node wang-23.host.com node-role.kubernetes.io&#x2F;node&#x3D;</span><br></pre></td></tr></table></figure><h3 id="部署-kube-proxy服务"><a href="#部署-kube-proxy服务" class="headerlink" title="部署 kube-proxy服务"></a><a href=".com/2020/06/02/四、k8s-部署node组件/#部署-kube-proxy服务">部署 kube-proxy服务</a></h3><h4 id="配置ipvs-转发"><a href="#配置ipvs-转发" class="headerlink" title="配置ipvs 转发"></a>配置ipvs 转发</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;ipvs.sh | sh</span><br></pre></td></tr></table></figure><h4 id="创建kube-proxy启动脚本"><a href="#创建kube-proxy启动脚本" class="headerlink" title="创建kube-proxy启动脚本"></a>创建kube-proxy启动脚本</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-proxy.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">.&#x2F;kube-proxy \\</span><br><span class="line">  --cluster-cidr 172.16.0.0&#x2F;16 \\</span><br><span class="line">  --hostname-override wang-24.host.com \\</span><br><span class="line">  --proxy-mode&#x3D;ipvs \\</span><br><span class="line">  --ipvs-scheduler&#x3D;nq \\</span><br><span class="line">  --kubeconfig .&#x2F;conf&#x2F;kube-proxy.kubeconfig</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sed -i &quot;s&amp;wang-24.host.com&amp;$&#123;HOSTNAME&#125;&amp;&quot; &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-proxy.sh</span><br><span class="line"></span><br><span class="line">chmod +x &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-proxy.sh </span><br><span class="line">mkdir -p &#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-proxy</span><br></pre></td></tr></table></figure><h4 id="创建supervisor配置"><a href="#创建supervisor配置" class="headerlink" title="创建supervisor配置"></a>创建supervisor配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;supervisord.d&#x2F;kube-proxy.ini</span><br><span class="line">[program:kube-proxy-70-24]</span><br><span class="line">command&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-proxy.sh                 ; the program (relative uses PATH, can take args)</span><br><span class="line">numprocs&#x3D;1                                                       ; number of processes copies to start (def 1)</span><br><span class="line">directory&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin                             ; directory to cwd to before exec (def no cwd)</span><br><span class="line">autostart&#x3D;true                                                   ; start at supervisord start (default: true)</span><br><span class="line">autorestart&#x3D;true                                                 ; retstart at unexpected quit (default: true)</span><br><span class="line">startsecs&#x3D;22                                                     ; number of secs prog must stay running (def. 1)</span><br><span class="line">startretries&#x3D;3                                                   ; max # of serial start failures (default 3)</span><br><span class="line">exitcodes&#x3D;0,2                                                    ; &#39;expected&#39; exit codes for process (default 0,2)</span><br><span class="line">stopsignal&#x3D;QUIT                                                  ; signal used to kill process (default TERM)</span><br><span class="line">stopwaitsecs&#x3D;10                                                  ; max num secs to wait b4 SIGKILL (default 10)</span><br><span class="line">user&#x3D;root                                                        ; setuid to this UNIX account to run the program</span><br><span class="line">redirect_stderr&#x3D;false                                            ; redirect proc stderr to stdout (default false)</span><br><span class="line">stdout_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-proxy&#x2F;proxy.stdout.log     ; stdout log path, NONE for none; default AUTO</span><br><span class="line">stdout_logfile_maxbytes&#x3D;64MB                                     ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stdout_logfile_backups&#x3D;4                                         ; # of stdout logfile backups (default 10)</span><br><span class="line">stdout_capture_maxbytes&#x3D;1MB                                      ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stdout_events_enabled&#x3D;false                                      ; emit events on stdout writes (default false)</span><br><span class="line">stderr_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-proxy&#x2F;proxy.stderr.log     ; stderr log path, NONE for none; default AUTO</span><br><span class="line">stderr_logfile_maxbytes&#x3D;64MB                                     ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stderr_logfile_backups&#x3D;4                                         ; # of stderr logfile backups (default 10)</span><br><span class="line">stderr_capture_maxbytes&#x3D;1MB                                      ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stderr_events_enabled&#x3D;false                                      ; emit events on stderr writes (default false)</span><br><span class="line">stopasgroup&#x3D;true                            ;默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程</span><br><span class="line">killasgroup&#x3D;true                            ;默认为false，向进程组发送kill信号，包括子进程</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 修改主机名，适应对应的主机</span><br><span class="line">sed -i &quot;s&amp;kube-proxy-70-24&amp;kube-proxy-70-$&#123;HOSTNUM&#125;&amp;&quot; &#x2F;etc&#x2F;supervisord.d&#x2F;kube-proxy.ini</span><br><span class="line"></span><br><span class="line">supervisorctl update</span><br><span class="line">sleep 5</span><br><span class="line"></span><br><span class="line">supervisorctl status</span><br><span class="line">netstat -anput | grep kube-proxy</span><br></pre></td></tr></table></figure><h3 id="K8S的CNI网络插件-Flannel"><a href="#K8S的CNI网络插件-Flannel" class="headerlink" title="K8S的CNI网络插件-Flannel"></a><a href="http://wangzhangtao.com/2020/06/03/五、k8s-部署addons组件/#K8S的CNI网络插件-Flannel" target="_blank" rel="noopener">K8S的CNI网络插件-Flannel</a></h3><h4 id="考呗flanneld文件"><a href="#考呗flanneld文件" class="headerlink" title="考呗flanneld文件"></a>考呗flanneld文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp wang-200:&#x2F;data&#x2F;soft&#x2F;k8s&#x2F;flannel-v0.11.0-linux-amd64.tar.gz &#x2F;opt&#x2F;src&#x2F;</span><br><span class="line">mkdir &#x2F;opt&#x2F;flannel-v0.11.0</span><br><span class="line">ln -s &#x2F;opt&#x2F;flannel-v0.11.0&#x2F; &#x2F;opt&#x2F;flannel</span><br><span class="line"># tar xf &#x2F;opt&#x2F;src&#x2F;flannel-v0.11.0-linux-amd64.tar.gz -C &#x2F;opt&#x2F;flannel-v0.11.0&#x2F;</span><br><span class="line">scp -r wang-200:&#x2F;opt&#x2F;flannel&#x2F;* &#x2F;opt&#x2F;flannel&#x2F;</span><br><span class="line"></span><br><span class="line">sed -i &quot;s&amp;172.16.24.1&#x2F;24&amp;172.16.$&#123;HOSTNUM&#125;.1&#x2F;24&amp;&quot; &#x2F;opt&#x2F;flannel&#x2F;subnet.env</span><br></pre></td></tr></table></figure><h4 id="创建flanneld启动脚本"><a href="#创建flanneld启动脚本" class="headerlink" title="创建flanneld启动脚本"></a>创建flanneld启动脚本</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;flannel&#x2F;flanneld.sh</span><br><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">.&#x2F;flanneld \\</span><br><span class="line">  --public-ip&#x3D;192.168.70.24 \\</span><br><span class="line">  --etcd-endpoints&#x3D;https:&#x2F;&#x2F;192.168.70.21:2379,https:&#x2F;&#x2F;192.168.70.22:2379,https:&#x2F;&#x2F;192.168.70.23:2379 \\</span><br><span class="line">  --etcd-keyfile&#x3D;.&#x2F;cert&#x2F;client-key.pem \\</span><br><span class="line">  --etcd-certfile&#x3D;.&#x2F;cert&#x2F;client.pem \\</span><br><span class="line">  --etcd-cafile&#x3D;.&#x2F;cert&#x2F;ca.pem \\</span><br><span class="line">  --iface&#x3D;ens160 \\</span><br><span class="line">  --subnet-file&#x3D;.&#x2F;subnet.env \\</span><br><span class="line">  --healthz-port&#x3D;2401</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sed -i &quot;s&amp;--public-ip&#x3D;192.168.70.24&amp;--public-ip&#x3D;192.168.70.$&#123;HOSTNUM&#125;&amp;&quot; &#x2F;opt&#x2F;flannel&#x2F;flanneld.sh</span><br><span class="line"></span><br><span class="line">chmod +x &#x2F;opt&#x2F;flannel&#x2F;flanneld.sh </span><br><span class="line">mkdir -p &#x2F;data&#x2F;logs&#x2F;flanneld</span><br></pre></td></tr></table></figure><h4 id="创建supervisor配置-1"><a href="#创建supervisor配置-1" class="headerlink" title="创建supervisor配置"></a>创建supervisor配置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;supervisord.d&#x2F;flannel.ini</span><br><span class="line">[program:flanneld-70-24]</span><br><span class="line">command&#x3D;&#x2F;opt&#x2F;flannel&#x2F;flanneld.sh            ; the program (relative uses PATH, can take args)</span><br><span class="line">numprocs&#x3D;1                                  ; number of processes copies to start (def 1)</span><br><span class="line">directory&#x3D;&#x2F;opt&#x2F;flannel                      ; directory to cwd to before exec (def no cwd)</span><br><span class="line">autostart&#x3D;true                              ; start at supervisord start (default: true)</span><br><span class="line">autorestart&#x3D;true                            ; retstart at unexpected quit (default: true)</span><br><span class="line">startsecs&#x3D;30                                ; number of secs prog must stay running (def. 1)</span><br><span class="line">startretries&#x3D;3                              ; max # of serial start failures (default 3)</span><br><span class="line">exitcodes&#x3D;0,2                               ; &#39;expected&#39; exit codes for process (default 0,2)</span><br><span class="line">stopsignal&#x3D;QUIT                             ; signal used to kill process (default TERM)</span><br><span class="line">stopwaitsecs&#x3D;10                             ; max num secs to wait b4 SIGKILL (default 10)</span><br><span class="line">user&#x3D;root                                   ; setuid to this UNIX account to run the program</span><br><span class="line">redirect_stderr&#x3D;true                        ; redirect proc stderr to stdout (default false)</span><br><span class="line">stdout_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;flanneld&#x2F;flanneld.stdout.log       ; stderr log path, NONE for none; default AUTO</span><br><span class="line">stdout_logfile_maxbytes&#x3D;64MB                ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stdout_logfile_backups&#x3D;4                    ; # of stdout logfile backups (default 10)</span><br><span class="line">stdout_capture_maxbytes&#x3D;1MB                 ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stdout_events_enabled&#x3D;false                 ; emit events on stdout writes (default false)</span><br><span class="line">stopasgroup&#x3D;true                            ;默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程</span><br><span class="line">killasgroup&#x3D;true                            ;默认为false，向进程组发送kill信号，包括子进程</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sed -i &quot;s&amp;flanneld-70-24&amp;flanneld-70-$&#123;HOSTNUM&#125;&amp;&quot; &#x2F;etc&#x2F;supervisord.d&#x2F;flannel.ini</span><br><span class="line"></span><br><span class="line">supervisorctl update</span><br><span class="line">sleep 5</span><br><span class="line"></span><br><span class="line">supervisorctl status</span><br><span class="line">route -n</span><br></pre></td></tr></table></figure><h4 id="优化iptables"><a href="#优化iptables" class="headerlink" title="优化iptables"></a>优化iptables</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> </span><br><span class="line">yum install -y iptables-services</span><br><span class="line">systemctl start iptables</span><br><span class="line">systemctl enable iptables</span><br><span class="line"></span><br><span class="line">iptables -t nat -D POSTROUTING -s 172.16.$&#123;HOSTNUM&#125;.0&#x2F;24 ! -o docker0 -j MASQUERADE</span><br><span class="line">iptables -t nat -I POSTROUTING -s 172.16.$&#123;HOSTNUM&#125;.0&#x2F;24 ! -d 172.16.0.0&#x2F;16 ! -o docker0 -j MASQUERADE</span><br><span class="line"></span><br><span class="line">iptables -D INPUT -j REJECT --reject-with icmp-host-prohibited</span><br><span class="line">iptables -D FORWARD -j REJECT --reject-with icmp-host-prohibited </span><br><span class="line"></span><br><span class="line">iptables-save &gt; &#x2F;etc&#x2F;sysconfig&#x2F;iptables</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 老男孩 </category>
          
          <category> 二进制安装 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 二进制安装 </tag>
            
            <tag> 老男孩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>五、k8s-部署Addons组件</title>
      <link href="/2020/06/03/%E4%BA%94%E3%80%81k8s-%E9%83%A8%E7%BD%B2addons%E7%BB%84%E4%BB%B6/"/>
      <url>/2020/06/03/%E4%BA%94%E3%80%81k8s-%E9%83%A8%E7%BD%B2addons%E7%BB%84%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<h2 id="K8S的CNI网络插件-Flannel"><a href="#K8S的CNI网络插件-Flannel" class="headerlink" title="K8S的CNI网络插件-Flannel"></a>K8S的CNI网络插件-Flannel</h2><p>集群规划</p><table><thead><tr><th>主机名</th><th>IP地址</th><th>角色</th></tr></thead><tbody><tr><td>wang-21.host.com（为了展现效果，暂不部署）</td><td>192.168.70.21</td><td>flannel</td></tr><tr><td>wang-22.host.com（为了展现效果，暂不部署）</td><td>192.168.70.22</td><td>flannel</td></tr><tr><td>wang-23.host.com</td><td>192.168.70.23</td><td>flannel</td></tr><tr><td>wang-24.host.com</td><td>192.168.70.24</td><td>flannel</td></tr></tbody></table><p>注意：这里部署以wang-24.host.com主机为例，其他运算节点类似</p><h3 id="在运维主机下载软件，解压"><a href="#在运维主机下载软件，解压" class="headerlink" title="在运维主机下载软件，解压"></a>在运维主机下载软件，解压</h3><p>源码包<em><a href="https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz" target="_blank" rel="noopener">https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz</a></em></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# wget -O &#x2F;data&#x2F;soft&#x2F;k8s&#x2F;flannel-v0.11.0-linux-amd64.tar.gz https:&#x2F;&#x2F;github.com&#x2F;coreos&#x2F;flannel&#x2F;releases&#x2F;download&#x2F;v0.11.0&#x2F;flannel-v0.11.0-linux-amd64.tar.gz  # 喝一杯茶</span><br></pre></td></tr></table></figure><h3 id="拷贝安装包到服务器"><a href="#拷贝安装包到服务器" class="headerlink" title="拷贝安装包到服务器"></a>拷贝安装包到服务器</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp wang-200:&#x2F;data&#x2F;soft&#x2F;k8s&#x2F;flannel-v0.11.0-linux-amd64.tar.gz &#x2F;opt&#x2F;src&#x2F;</span><br><span class="line">mkdir &#x2F;opt&#x2F;flannel-v0.11.0</span><br><span class="line">ln -s &#x2F;opt&#x2F;flannel-v0.11.0&#x2F; &#x2F;opt&#x2F;flannel</span><br><span class="line"># scp -r wang-200:&#x2F;opt&#x2F;flannel&#x2F;* &#x2F;opt&#x2F;flannel&#x2F;</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;flannel-v0.11.0-linux-amd64.tar.gz -C &#x2F;opt&#x2F;flannel-v0.11.0&#x2F;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-24 ~]# ls &#x2F;opt&#x2F;flannel-v0.11.0&#x2F;</span><br><span class="line">flanneld  mk-docker-opts.sh  README.md</span><br></pre></td></tr></table></figure><h4 id="拷贝证书"><a href="#拷贝证书" class="headerlink" title="拷贝证书"></a>拷贝证书</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;opt&#x2F;flannel&#x2F;cert</span><br><span class="line">scp wang-200:&#x2F;opt&#x2F;certs&#x2F;ca.pem &#x2F;opt&#x2F;flannel&#x2F;cert&#x2F; </span><br><span class="line">scp wang-200:&#x2F;opt&#x2F;certs&#x2F;client.pem &#x2F;opt&#x2F;flannel&#x2F;cert&#x2F;  </span><br><span class="line">scp wang-200:&#x2F;opt&#x2F;certs&#x2F;client-key.pem &#x2F;opt&#x2F;flannel&#x2F;cert&#x2F;</span><br></pre></td></tr></table></figure><h3 id="操作etcd，增加wang-gw"><a href="#操作etcd，增加wang-gw" class="headerlink" title="操作etcd，增加wang-gw"></a>操作etcd，增加wang-gw</h3><p>启动flannel之前，需要在etcd中添加网络配置记录</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 写入etcd</span><br><span class="line">[root@wang-23 ~]# etcdctl set &#x2F;coreos.com&#x2F;network&#x2F;config &#39;&#123;&quot;Network&quot;: &quot;172.16.0.0&#x2F;16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;host-gw&quot;&#125;&#125;&#39;    </span><br><span class="line">&#123;&quot;Network&quot;: &quot;172.16.0.0&#x2F;16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;host-gw&quot;&#125;&#125;</span><br><span class="line"></span><br><span class="line"># 查看</span><br><span class="line">[root@wang-23 ~]# etcdctl  get &#x2F;coreos.com&#x2F;network&#x2F;config</span><br><span class="line">&#123;&quot;Network&quot;: &quot;172.16.0.0&#x2F;16&quot;, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;host-gw&quot;&#125;&#125;</span><br></pre></td></tr></table></figure><p>wang-gw：直接路由的方式，将容器网络的路由信息直接更新到主机的路由表中，仅适用于二层直接可达的网络</p><h3 id="创建配置文件"><a href="#创建配置文件" class="headerlink" title="创建配置文件"></a>创建配置文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;flannel&#x2F;subnet.env</span><br><span class="line">FLANNEL_NETWORK&#x3D;172.16.0.0&#x2F;16        </span><br><span class="line">FLANNEL_SUBNET&#x3D;172.16.24.1&#x2F;24        </span><br><span class="line">FLANNEL_MTU&#x3D;1500</span><br><span class="line">FLANNEL_IPMASQ&#x3D;false</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 修改对应的主机</span><br><span class="line">sed -i &quot;s&amp;172.16.24.1&#x2F;24&amp;172.16.$&#123;HOSTNUM&#125;.1&#x2F;24&amp;&quot; &#x2F;opt&#x2F;flannel&#x2F;subnet.env</span><br></pre></td></tr></table></figure><blockquote><p><em>注意：lannel集群各主机的配置略有不同，SUBNET需要更改</em></p><p>FLANNEL_NETWORK=172.16.0.0/16        #pod资源的IP范围<br>FLANNEL_SUBNET=172.16.24.1/24        #本机的IP范围</p></blockquote><h3 id="创建flanneld启动脚本"><a href="#创建flanneld启动脚本" class="headerlink" title="创建flanneld启动脚本"></a>创建flanneld启动脚本</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;flannel&#x2F;flanneld.sh</span><br><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">.&#x2F;flanneld \\</span><br><span class="line">  --public-ip&#x3D;192.168.70.24 \\</span><br><span class="line">  --etcd-endpoints&#x3D;https:&#x2F;&#x2F;192.168.70.21:2379,https:&#x2F;&#x2F;192.168.70.22:2379,https:&#x2F;&#x2F;192.168.70.23:2379 \\</span><br><span class="line">  --etcd-keyfile&#x3D;.&#x2F;cert&#x2F;client-key.pem \\</span><br><span class="line">  --etcd-certfile&#x3D;.&#x2F;cert&#x2F;client.pem \\</span><br><span class="line">  --etcd-cafile&#x3D;.&#x2F;cert&#x2F;ca.pem \\</span><br><span class="line">  --iface&#x3D;ens160 \\</span><br><span class="line">  --subnet-file&#x3D;.&#x2F;subnet.env \\</span><br><span class="line">  --healthz-port&#x3D;2401</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sed -i &quot;s&amp;--public-ip&#x3D;192.168.70.24&amp;--public-ip&#x3D;192.168.70.$&#123;HOSTNUM&#125;&amp;&quot; &#x2F;opt&#x2F;flannel&#x2F;flanneld.sh</span><br></pre></td></tr></table></figure><blockquote><p>–public-ip=192.168.70.21 主机IP地址<br>–iface=ens160  注意虚拟机网卡</p></blockquote><p><strong>授权、创建日志目录</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod +x &#x2F;opt&#x2F;flannel&#x2F;flanneld.sh </span><br><span class="line">mkdir -p &#x2F;data&#x2F;logs&#x2F;flanneld</span><br></pre></td></tr></table></figure><h3 id="创建supervisor配置"><a href="#创建supervisor配置" class="headerlink" title="创建supervisor配置"></a>创建supervisor配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;supervisord.d&#x2F;flannel.ini</span><br><span class="line">[program:flanneld-70-24]</span><br><span class="line">command&#x3D;&#x2F;opt&#x2F;flannel&#x2F;flanneld.sh            ; the program (relative uses PATH, can take args)</span><br><span class="line">numprocs&#x3D;1                                  ; number of processes copies to start (def 1)</span><br><span class="line">directory&#x3D;&#x2F;opt&#x2F;flannel                      ; directory to cwd to before exec (def no cwd)</span><br><span class="line">autostart&#x3D;true                              ; start at supervisord start (default: true)</span><br><span class="line">autorestart&#x3D;true                            ; retstart at unexpected quit (default: true)</span><br><span class="line">startsecs&#x3D;30                                ; number of secs prog must stay running (def. 1)</span><br><span class="line">startretries&#x3D;3                              ; max # of serial start failures (default 3)</span><br><span class="line">exitcodes&#x3D;0,2                               ; &#39;expected&#39; exit codes for process (default 0,2)</span><br><span class="line">stopsignal&#x3D;QUIT                             ; signal used to kill process (default TERM)</span><br><span class="line">stopwaitsecs&#x3D;10                             ; max num secs to wait b4 SIGKILL (default 10)</span><br><span class="line">user&#x3D;root                                   ; setuid to this UNIX account to run the program</span><br><span class="line">redirect_stderr&#x3D;true                        ; redirect proc stderr to stdout (default false)</span><br><span class="line">stdout_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;flanneld&#x2F;flanneld.stdout.log       ; stderr log path, NONE for none; default AUTO</span><br><span class="line">stdout_logfile_maxbytes&#x3D;64MB                ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stdout_logfile_backups&#x3D;4                    ; # of stdout logfile backups (default 10)</span><br><span class="line">stdout_capture_maxbytes&#x3D;1MB                 ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stdout_events_enabled&#x3D;false                 ; emit events on stdout writes (default false)</span><br><span class="line">stopasgroup&#x3D;true                            ;默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程</span><br><span class="line">killasgroup&#x3D;true                            ;默认为false，向进程组发送kill信号，包括子进程</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sed -i &quot;s&amp;flanneld-70-24&amp;flanneld-70-$&#123;HOSTNUM&#125;&amp;&quot; &#x2F;etc&#x2F;supervisord.d&#x2F;flannel.ini</span><br></pre></td></tr></table></figure><h3 id="启动服务并检查"><a href="#启动服务并检查" class="headerlink" title="启动服务并检查"></a>启动服务并检查</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-24 ~]# supervisorctl update</span><br><span class="line">flanneld-70-24: added process group</span><br><span class="line">[root@wang-24 ~]# supervisorctl status</span><br><span class="line">flanneld-70-24                   STARTING  </span><br><span class="line">kube-kubelet-70-24               RUNNING   pid 52982, uptime 1:38:54</span><br><span class="line">kube-proxy-70-24                 RUNNING   pid 53669, uptime 1:01:36</span><br></pre></td></tr></table></figure><p><strong>检查etcd数据库</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-23 ~]# etcdctl ls &#x2F;coreos.com&#x2F;network&#x2F;subnets</span><br><span class="line">&#x2F;coreos.com&#x2F;network&#x2F;subnets&#x2F;172.16.24.0-24</span><br><span class="line">[root@wang-23 ~]# etcdctl get &#x2F;coreos.com&#x2F;network&#x2F;subnets&#x2F;172.16.24.0-24</span><br><span class="line">&#123;&quot;PublicIP&quot;:&quot;192.168.70.24&quot;,&quot;BackendType&quot;:&quot;host-gw&quot;&#125;</span><br></pre></td></tr></table></figure><h3 id="安装部署集群其他节点-略过"><a href="#安装部署集群其他节点-略过" class="headerlink" title="安装部署集群其他节点(略过)"></a>安装部署集群其他节点(略过)</h3><h3 id="检查路由"><a href="#检查路由" class="headerlink" title="检查路由"></a>检查路由</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-24 flannel]# route -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">0.0.0.0         192.168.70.1    0.0.0.0         UG    100    0        0 ens160</span><br><span class="line">172.16.23.0     192.168.70.23   255.255.255.0   UG    0      0        0 ens160</span><br><span class="line">172.16.24.0     0.0.0.0         255.255.255.0   U     0      0        0 docker0</span><br><span class="line">192.168.70.0    0.0.0.0         255.255.255.0   U     100    0        0 ens160</span><br></pre></td></tr></table></figure><p>访问其他主机上的pod</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-24 flannel]# curl -I 172.16.23.2</span><br><span class="line">HTTP&#x2F;1.1 200 OK</span><br><span class="line">Server: nginx&#x2F;1.7.9</span><br><span class="line">Date: Wed, 03 Jun 2020 03:44:27 GMT</span><br><span class="line">Content-Type: text&#x2F;html</span><br><span class="line">Content-Length: 612</span><br><span class="line">Last-Modified: Tue, 23 Dec 2014 16:25:09 GMT</span><br><span class="line">Connection: keep-alive</span><br><span class="line">ETag: &quot;54999765-264&quot;</span><br><span class="line">Accept-Ranges: bytes</span><br></pre></td></tr></table></figure><h3 id="K8S资源配置清单的内网http服务"><a href="#K8S资源配置清单的内网http服务" class="headerlink" title="K8S资源配置清单的内网http服务"></a>K8S资源配置清单的内网http服务</h3><p>创建nginx配置文件</p><p><code>[root@wang-200 ~]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;doc.k8s.od.com.conf  </span><br><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  doc.k8s.od.com;</span><br><span class="line">    # index index.html index.htm index.jsp;</span><br><span class="line">    # root &#x2F;data&#x2F;k8s;</span><br><span class="line"></span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        autoindex on;</span><br><span class="line">        autoindex_exact_size off;</span><br><span class="line">        default_type text&#x2F;plain;</span><br><span class="line">        root &#x2F;data&#x2F;k8s&#x2F;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    access_log &#x2F;data&#x2F;logs&#x2F;k8s-doc.log;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>创建目录并加载代码</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;data&#x2F;k8s&#x2F;install &#x2F;data&#x2F;k8s&#x2F;yaml</span><br><span class="line">mkdir -p &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;coredns</span><br><span class="line">nginx -t</span><br><span class="line">nginx -s reload</span><br></pre></td></tr></table></figure><p><strong>浏览器访问测试</strong>`</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfeyylg1wsj31920d0767.jpg" alt="image-20200603120508191"></p><h3 id="在各运算节点上优化iptables规则"><a href="#在各运算节点上优化iptables规则" class="headerlink" title="在各运算节点上优化iptables规则"></a>在各运算节点上优化iptables规则</h3><p>注意：iptables规则各主机的略有不同，其他运算节点上执行时注意修改。</p><p><strong>安装iptables</strong></p><p><code>vim /data/k8s/install/install_iptables.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> </span><br><span class="line">yum install -y iptables-services</span><br><span class="line">systemctl start iptables</span><br><span class="line">systemctl enable iptables</span><br><span class="line"></span><br><span class="line">iptables -t nat -D POSTROUTING -s 172.16.$&#123;HOSTNUM&#125;.0&#x2F;24 ! -o docker0 -j MASQUERADE</span><br><span class="line">iptables -t nat -I POSTROUTING -s 172.16.$&#123;HOSTNUM&#125;.0&#x2F;24 ! -d 172.16.0.0&#x2F;16 ! -o docker0 -j MASQUERADE</span><br><span class="line"></span><br><span class="line">iptables -D INPUT -j REJECT --reject-with icmp-host-prohibited</span><br><span class="line">iptables -D FORWARD -j REJECT --reject-with icmp-host-prohibited </span><br><span class="line"></span><br><span class="line">iptables-save &gt; &#x2F;etc&#x2F;sysconfig&#x2F;iptables</span><br></pre></td></tr></table></figure><p>运行安装脚本</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -sSL http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;install&#x2F;install_iptables.sh | sh</span><br></pre></td></tr></table></figure><h3 id="手动添加路由规则-略过，仅供参考"><a href="#手动添加路由规则-略过，仅供参考" class="headerlink" title="手动添加路由规则(略过，仅供参考)"></a>手动添加路由规则(略过，仅供参考)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">route add -net 172.16.23.0&#x2F;24 gw 192.168.70.23 dev ens160</span><br><span class="line">route add -net 172.16.24.0&#x2F;24 gw 192.168.70.24 dev ens160</span><br><span class="line">iptables -t filter -I FORWARD -d 172.16.0.0&#x2F;16 -j ACCEPT</span><br></pre></td></tr></table></figure><h2 id="K8S的服务发现插件-CoreDNS"><a href="#K8S的服务发现插件-CoreDNS" class="headerlink" title="K8S的服务发现插件-CoreDNS"></a>K8S的服务发现插件-CoreDNS</h2><h3 id="准备coredns镜像"><a href="#准备coredns镜像" class="headerlink" title="准备coredns镜像"></a>准备coredns镜像</h3><p><code>[root@wang-200 k8s]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull coredns&#x2F;coredns:1.6.5</span><br><span class="line">docker tag coredns&#x2F;coredns:1.6.5 harbor.od.com&#x2F;public&#x2F;coredns:v1.6.5</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;coredns:v1.6.5</span><br></pre></td></tr></table></figure><h3 id="准备资源配置清单"><a href="#准备资源配置清单" class="headerlink" title="准备资源配置清单"></a>准备资源配置清单</h3><p><strong>进入存放资源清单目录</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# cd &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;coredns&#x2F;</span><br></pre></td></tr></table></figure><p><strong>rbac.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;coredns&#x2F;rbac.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">      kubernetes.io&#x2F;cluster-service: &quot;true&quot;</span><br><span class="line">      addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    kubernetes.io&#x2F;bootstrapping: rbac-defaults</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">  name: system:coredns</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - endpoints</span><br><span class="line">  - services</span><br><span class="line">  - pods</span><br><span class="line">  - namespaces</span><br><span class="line">  verbs:</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    rbac.authorization.kubernetes.io&#x2F;autoupdate: &quot;true&quot;</span><br><span class="line">  labels:</span><br><span class="line">    kubernetes.io&#x2F;bootstrapping: rbac-defaults</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: EnsureExists</span><br><span class="line">  name: system:coredns</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:coredns</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>configmap.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;coredns&#x2F;configmap.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">data:</span><br><span class="line">  Corefile: |</span><br><span class="line">    .:53 &#123;</span><br><span class="line">        errors</span><br><span class="line">        log</span><br><span class="line">        health</span><br><span class="line">        ready</span><br><span class="line">        kubernetes cluster.local 10.2.0.0&#x2F;16</span><br><span class="line">        forward . 192.168.70.12</span><br><span class="line">        cache 30</span><br><span class="line">        loop</span><br><span class="line">        reload</span><br><span class="line">        loadbalance</span><br><span class="line">       &#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>deployment.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;coredns&#x2F;deployment.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: coredns</span><br><span class="line">    kubernetes.io&#x2F;name: &quot;CoreDNS&quot;</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: coredns</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: coredns</span><br><span class="line">    spec:</span><br><span class="line">      priorityClassName: system-cluster-critical</span><br><span class="line">      serviceAccountName: coredns</span><br><span class="line">      containers:</span><br><span class="line">      - name: coredns</span><br><span class="line">        image: harbor.od.com&#x2F;public&#x2F;coredns:v1.6.5</span><br><span class="line">        args:</span><br><span class="line">        - -conf</span><br><span class="line">        - &#x2F;etc&#x2F;coredns&#x2F;Corefile</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: config-volume</span><br><span class="line">          mountPath: &#x2F;etc&#x2F;coredns</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 53</span><br><span class="line">          name: dns</span><br><span class="line">          protocol: UDP</span><br><span class="line">        - containerPort: 53</span><br><span class="line">          name: dns-tcp</span><br><span class="line">          protocol: TCP</span><br><span class="line">        - containerPort: 9153</span><br><span class="line">          name: metrics</span><br><span class="line">          protocol: TCP</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: &#x2F;health</span><br><span class="line">            port: 8080</span><br><span class="line">            scheme: HTTP</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">          successThreshold: 1</span><br><span class="line">          failureThreshold: 5</span><br><span class="line">      dnsPolicy: Default</span><br><span class="line">      volumes:</span><br><span class="line">        - name: config-volume</span><br><span class="line">          configMap:</span><br><span class="line">            name: coredns</span><br><span class="line">            items:</span><br><span class="line">            - key: Corefile</span><br><span class="line">              path: Corefile</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>service.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;coredns&#x2F;service.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: coredns</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: coredns</span><br><span class="line">    kubernetes.io&#x2F;cluster-service: &quot;true&quot;</span><br><span class="line">    kubernetes.io&#x2F;name: &quot;CoreDNS&quot;</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: coredns</span><br><span class="line">  clusterIP: 10.2.0.2</span><br><span class="line">  ports:</span><br><span class="line">  - name: dns</span><br><span class="line">    port: 53</span><br><span class="line">    protocol: UDP</span><br><span class="line">  - name: dns-tcp</span><br><span class="line">    port: 53</span><br><span class="line">  - name: metrics</span><br><span class="line">    port: 9153</span><br><span class="line">    protocol: TCP</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>检查资源配置清单</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 coredns]# ll</span><br><span class="line">总用量 16</span><br><span class="line">-rw-r--r--. 1 root root  320 6月   3 14:10 configmap.yaml</span><br><span class="line">-rw-r--r--. 1 root root 1294 6月   3 14:11 deployment.yaml</span><br><span class="line">-rw-r--r--. 1 root root  954 6月   3 14:10 rbac.yaml</span><br><span class="line">-rw-r--r--. 1 root root  384 6月   3 14:11 service.yaml</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单"><a href="#应用资源配置清单" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><p><strong>应用命令</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;coredns&#x2F;rbac.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;coredns&#x2F;configmap.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;coredns&#x2F;deployment.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;coredns&#x2F;service.yaml</span><br></pre></td></tr></table></figure><p><strong>显示过程</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 coredns]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;coredns&#x2F;rbac.yaml</span><br><span class="line">serviceaccount&#x2F;coredns created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io&#x2F;system:coredns created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;system:coredns created</span><br><span class="line">[root@wang-200 coredns]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;coredns&#x2F;configmap.yaml</span><br><span class="line">configmap&#x2F;coredns created</span><br><span class="line">[root@wang-200 coredns]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;coredns&#x2F;deployment.yaml</span><br><span class="line">deployment.apps&#x2F;coredns created</span><br><span class="line">[root@wang-200 coredns]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;coredns&#x2F;service.yaml</span><br><span class="line">service&#x2F;coredns created</span><br></pre></td></tr></table></figure><h3 id="查看创建的资源"><a href="#查看创建的资源" class="headerlink" title="查看创建的资源"></a>查看创建的资源</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 coredns]# kubectl get all -n kube-system</span><br><span class="line">NAME                          READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod&#x2F;coredns-9bc44c684-9b9kj   1&#x2F;1     Running   0          80s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NAME              TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE</span><br><span class="line">service&#x2F;coredns   ClusterIP   10.2.0.2     &lt;none&gt;        53&#x2F;UDP,53&#x2F;TCP,9153&#x2F;TCP   78s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NAME                      READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps&#x2F;coredns   1&#x2F;1     1            1           80s</span><br><span class="line"></span><br><span class="line">NAME                                DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset.apps&#x2F;coredns-9bc44c684   1         1         1       80s</span><br></pre></td></tr></table></figure><h3 id="验证coredns"><a href="#验证coredns" class="headerlink" title="验证coredns"></a>验证coredns</h3><p><strong>执行命令</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dig -t A wang-21.host.com @192.168.70.12 +short</span><br><span class="line">dig -t A wang-21.host.com @10.2.0.2 +short</span><br><span class="line">dig -t A www.baidu.com @192.168.70.12 +short</span><br><span class="line">dig -t A www.baidu.com @10.2.0.2 +short</span><br></pre></td></tr></table></figure><p><strong>显示过程</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-24 flannel]# dig -t A wang-21.host.com @192.168.70.12 +short</span><br><span class="line">192.168.70.21</span><br><span class="line">[root@wang-24 flannel]# dig -t A wang-21.host.com @10.2.0.2 +short</span><br><span class="line">192.168.70.21</span><br><span class="line">[root@wang-24 flannel]# dig -t A www.baidu.com @192.168.70.12 +short</span><br><span class="line">www.a.shifen.com.</span><br><span class="line">180.101.49.11</span><br><span class="line">180.101.49.12</span><br><span class="line">[root@wang-24 flannel]# dig -t A www.baidu.com @10.2.0.2 +short</span><br><span class="line">www.a.shifen.com.</span><br><span class="line">180.101.49.12</span><br><span class="line">180.101.49.11</span><br></pre></td></tr></table></figure><h2 id="K8S的服务暴露插件-Traefik"><a href="#K8S的服务暴露插件-Traefik" class="headerlink" title="K8S的服务暴露插件-Traefik"></a>K8S的服务暴露插件-Traefik</h2><h3 id="ingress控制器"><a href="#ingress控制器" class="headerlink" title="ingress控制器"></a>ingress控制器</h3><p>​        Traefik是一个用Golang开发的轻量级的Http反向代理和负载均衡器。由于可以自动配置和刷新backend节点，目前可以被绝大部分容器平台支持，例如Kubernetes，Swarm，Rancher等。由于traefik会实时与Kubernetes API交互,所以对于Service的节点变化，traefik的反应会更加迅速。总体来说traefik可以在Kubernetes中完美的运行.</p><h3 id="准备traefik镜像"><a href="#准备traefik镜像" class="headerlink" title="准备traefik镜像"></a><strong>准备traefik镜像</strong></h3><p><code>[root@wang-200 ~]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull traefik:v1.7.2</span><br><span class="line">docker tag traefik:v1.7.2 harbor.od.com&#x2F;public&#x2F;traefik:v1.7.2</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;traefik:v1.7.2</span><br></pre></td></tr></table></figure><h3 id="准备资源配置清单-1"><a href="#准备资源配置清单-1" class="headerlink" title="准备资源配置清单"></a>准备资源配置清单</h3><p>官网yaml文件地址：<a href="https://github.com/containous/traefik/tree/v1.7/examples/k8s" target="_blank" rel="noopener">https://github.com/containous/traefik/tree/v1.7/examples/k8s</a></p><p><strong>创建并进入资源清单目录</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir -p &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;traefik</span><br></pre></td></tr></table></figure><p><strong>rbac.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;traefik&#x2F;rbac.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-ingress-controller</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1beta1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-ingress-controller</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - services</span><br><span class="line">      - endpoints</span><br><span class="line">      - secrets</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - extensions</span><br><span class="line">    resources:</span><br><span class="line">      - ingresses</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-ingress-controller</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: traefik-ingress-controller</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: traefik-ingress-controller</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>daemonset.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;traefik&#x2F;daemonset.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-ingress</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: traefik-ingress</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: traefik-ingress</span><br><span class="line">        name: traefik-ingress</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: traefik-ingress-controller</span><br><span class="line">      terminationGracePeriodSeconds: 60</span><br><span class="line">      containers:</span><br><span class="line">      - image: harbor.od.com&#x2F;public&#x2F;traefik:v1.7.2</span><br><span class="line">        name: traefik-ingress</span><br><span class="line">        ports:</span><br><span class="line">        - name: controller</span><br><span class="line">          containerPort: 80 </span><br><span class="line">          hostPort: 81              </span><br><span class="line">        - name: admin-web      </span><br><span class="line">          containerPort: 8080</span><br><span class="line">        securityContext:</span><br><span class="line">          capabilities:</span><br><span class="line">            drop:</span><br><span class="line">            - ALL</span><br><span class="line">            add:</span><br><span class="line">            - NET_BIND_SERVICE</span><br><span class="line">        args:</span><br><span class="line">        - --api</span><br><span class="line">        - --kubernetes</span><br><span class="line">        - --logLevel&#x3D;INFO</span><br><span class="line">        - --insecureskipverify&#x3D;true</span><br><span class="line">        - --kubernetes.endpoint&#x3D;https:&#x2F;&#x2F;api.k8s.od.com:8443      </span><br><span class="line">        - --accesslog</span><br><span class="line">        - --accesslog.filepath&#x3D;&#x2F;var&#x2F;log&#x2F;traefik_access.log</span><br><span class="line">        - --traefiklog</span><br><span class="line">        - --traefiklog.filepath&#x3D;&#x2F;var&#x2F;log&#x2F;traefik.log</span><br><span class="line">        - --metrics.prometheus</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>service.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;traefik&#x2F;service.yaml</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-ingress-service</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: traefik-ingress</span><br><span class="line">  ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 80</span><br><span class="line">      name: controller</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 8080</span><br><span class="line">      name: admin-web</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>ingress.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;traefik&#x2F;ingress.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: traefik-web-ui</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io&#x2F;ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: traefik.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: traefik-ingress-service</span><br><span class="line">          servicePort: 8080</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单-1"><a href="#应用资源配置清单-1" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><p><strong>执行命令</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;traefik&#x2F;rbac.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;traefik&#x2F;daemonset.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;traefik&#x2F;service.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;traefik&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p><strong>执行过程</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-21 ~]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;traefik&#x2F;rbac.yaml</span><br><span class="line">serviceaccount&#x2F;traefik-ingress-controller created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io&#x2F;traefik-ingress-controller created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;traefik-ingress-controller created</span><br><span class="line"></span><br><span class="line">[root@wang-21 ~]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;traefik&#x2F;daemonset.yaml</span><br><span class="line">daemonset.extensions&#x2F;traefik-ingress created</span><br><span class="line"></span><br><span class="line">[root@wang-21 ~]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;traefik&#x2F;service.yaml</span><br><span class="line">service&#x2F;traefik-ingress-service created</span><br><span class="line"></span><br><span class="line">[root@wang-21 ~]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;traefik&#x2F;ingress.yaml</span><br><span class="line">ingress.extensions&#x2F;traefik-web-ui created</span><br></pre></td></tr></table></figure><p><strong>检查创建的资源</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-23 ~]# kubectl get pods -n kube-system </span><br><span class="line">NAME                      READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-9bc44c684-9b9kj   1&#x2F;1     Running   0          3h16m</span><br><span class="line">traefik-ingress-nn4vn     1&#x2F;1     Running   0          3h8m</span><br><span class="line">traefik-ingress-tnw7n     1&#x2F;1     Running   0          3h8m</span><br></pre></td></tr></table></figure><p><strong>如果报错如下：</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Warning  FailedCreatePodSandBox  29s (x6207 over 3h5m)  kubelet, wang-24.host.com  (combined from similar events): Failed create pod sandbox: rpc error: code &#x3D; Unknown desc &#x3D; failed to start sandbox container for pod &quot;traefik-ingress-tnw7n&quot;: Error response from daemon: driver failed programming external connectivity on endpoint k8s_POD_traefik-ingress-tnw7n_kube-system_45d3c9bf-0633-44f0-8048-91061b74a7e2_6215 (b578fba313008f91ed117d5d5112c74ca2d07b5e460c20a862b7ad07207f5f72):  (iptables failed: iptables --wait -t filter -A DOCKER ! -i docker0 -o docker0 -p tcp -d 172.16.24.4 --dport 80 -j ACCEPT: iptables: No chain&#x2F;target&#x2F;match by that name.</span><br></pre></td></tr></table></figure><blockquote><p>解决方法：重启docker</p></blockquote><h3 id="添加A记录解析"><a href="#添加A记录解析" class="headerlink" title="添加A记录解析"></a>添加A记录解析</h3><p>配置Nginx反向</p><p><code>wang-12</code>和<code>wang-11</code>两台主机上的nginx均需要配置</p><p><code>[root@wang-11 ~]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;od.com.conf</span><br><span class="line">upstream default_backend_traefik &#123;</span><br><span class="line">    server 192.168.70.23:81    max_fails&#x3D;3 fail_timeout&#x3D;10s;</span><br><span class="line">    server 192.168.70.24:81    max_fails&#x3D;3 fail_timeout&#x3D;10s;</span><br><span class="line">&#125;</span><br><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name *.od.com;</span><br><span class="line">    access_log &#x2F;data&#x2F;logs&#x2F;nginx&#x2F;od.com.log;</span><br><span class="line"></span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        proxy_pass http:&#x2F;&#x2F;default_backend_traefik;</span><br><span class="line">        proxy_set_header Host       \$http_host;</span><br><span class="line">        proxy_set_header x-forwarded-for \$proxy_add_x_forwarded_for;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h4 id="重新加载配置文件"><a href="#重新加载配置文件" class="headerlink" title="重新加载配置文件"></a>重新加载配置文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;data&#x2F;logs&#x2F;nginx</span><br><span class="line">nginx -t</span><br><span class="line">nginx -s reload</span><br></pre></td></tr></table></figure><blockquote><p> <strong>注：泛域名，访问任何业务域，会调用vip，分发流量至traefik81端口</strong></p></blockquote><h3 id="浏览器访问"><a href="#浏览器访问" class="headerlink" title="浏览器访问"></a>浏览器访问</h3><p><a href="http://traefik.od.com/dashboard/" target="_blank" rel="noopener">http://traefik.od.com/dashboard/</a></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gff8ezfmdsj31ro0u0dlq.jpg" alt="image-20200603173213883"></p><h2 id="K8S的GUI资源管理插件-dashboard"><a href="#K8S的GUI资源管理插件-dashboard" class="headerlink" title="K8S的GUI资源管理插件-dashboard"></a>K8S的GUI资源管理插件-dashboard</h2><h3 id="准备dashboard镜像"><a href="#准备dashboard镜像" class="headerlink" title="准备dashboard镜像"></a>准备dashboard镜像</h3><p><code>[root@wang-200 ~]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># docker search kubernetes-dashboard-amd64 其他地址可能快点</span><br><span class="line">docker pull k8scn&#x2F;kubernetes-dashboard-amd64:v1.8.3</span><br><span class="line">docker tag k8scn&#x2F;kubernetes-dashboard-amd64:v1.8.3 harbor.od.com&#x2F;public&#x2F;kubernetes-dashboard:v1.8.3</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;kubernetes-dashboard:v1.8.3</span><br></pre></td></tr></table></figure><h3 id="准备资源配置清单-2"><a href="#准备资源配置清单-2" class="headerlink" title="准备资源配置清单"></a>准备资源配置清单</h3><p><strong>创建并进入dashboard资源清单目录</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dashboard</span><br></pre></td></tr></table></figure><p><strong>rbac.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dashboard&#x2F;rbac.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">  name: kubernetes-dashboard-admin</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard-admin</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: kubernetes-dashboard-admin</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>deployment.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dashboard&#x2F;deployment.yaml </span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">    kubernetes.io&#x2F;cluster-service: &quot;true&quot;</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: kubernetes-dashboard</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: kubernetes-dashboard</span><br><span class="line">      annotations:</span><br><span class="line">        scheduler.alpha.kubernetes.io&#x2F;critical-pod: &#39;&#39;</span><br><span class="line">    spec:</span><br><span class="line">      priorityClassName: system-cluster-critical</span><br><span class="line">      containers:</span><br><span class="line">      - name: kubernetes-dashboard</span><br><span class="line">        image: harbor.od.com&#x2F;public&#x2F;kubernetes-dashboard:v1.8.3</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 300Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 50m</span><br><span class="line">            memory: 100Mi</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8443</span><br><span class="line">          protocol: TCP</span><br><span class="line">        args:</span><br><span class="line">          # PLATFORM-SPECIFIC ARGS HERE</span><br><span class="line">          - --auto-generate-certificates</span><br><span class="line">          - --token-ttl&#x3D;43200 </span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: tmp-volume</span><br><span class="line">          mountPath: &#x2F;tmp</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            scheme: HTTPS</span><br><span class="line">            path: &#x2F;</span><br><span class="line">            port: 8443</span><br><span class="line">          initialDelaySeconds: 30</span><br><span class="line">          timeoutSeconds: 30</span><br><span class="line">      volumes:</span><br><span class="line">      - name: tmp-volume</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">      serviceAccountName: kubernetes-dashboard-admin</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: &quot;CriticalAddonsOnly&quot;</span><br><span class="line">        operator: &quot;Exists&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>service.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dashboard&#x2F;service.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">    kubernetes.io&#x2F;cluster-service: &quot;true&quot;</span><br><span class="line">    addonmanager.kubernetes.io&#x2F;mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">  ports:</span><br><span class="line">  - port: 443</span><br><span class="line">    targetPort: 8443</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>ingress.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;dashboard&#x2F;ingress.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io&#x2F;ingress.class: traefik</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: dashboard.od.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: kubernetes-dashboard</span><br><span class="line">          servicePort: 443</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单-2"><a href="#应用资源配置清单-2" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dashboard&#x2F;rbac.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dashboard&#x2F;deployment.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dashboard&#x2F;service.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dashboard&#x2F;ingress.yaml</span><br></pre></td></tr></table></figure><p><strong>执行过程</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dashboard&#x2F;rbac.yaml</span><br><span class="line">serviceaccount&#x2F;kubernetes-dashboard-admin created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;kubernetes-dashboard-admin created</span><br><span class="line">[root@wang-200 ~]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dashboard&#x2F;deployment.yaml</span><br><span class="line">deployment.apps&#x2F;kubernetes-dashboard created</span><br><span class="line">[root@wang-200 ~]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dashboard&#x2F;service.yaml</span><br><span class="line">service&#x2F;kubernetes-dashboard created</span><br><span class="line">[root@wang-200 ~]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;dashboard&#x2F;ingress.yaml</span><br><span class="line">ingress.extensions&#x2F;kubernetes-dashboard created</span><br></pre></td></tr></table></figure><p><strong>查看创建的资源</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pods -n kube-system|grep dashboard</span><br><span class="line">kubernetes-dashboard-59dfb9868d-hsgwn   1&#x2F;1     Running   0          35s</span><br><span class="line">[root@wang-200 ~]# kubectl get svc -n kube-system|grep dashboard</span><br><span class="line">kubernetes-dashboard      ClusterIP   10.2.244.60    &lt;none&gt;        443&#x2F;TCP                  41s</span><br><span class="line">[root@wang-200 ~]# kubectl get ingress -n kube-system|grep dashboard</span><br><span class="line">kubernetes-dashboard   dashboard.od.com             80      46s</span><br></pre></td></tr></table></figure><h3 id="配置认证"><a href="#配置认证" class="headerlink" title="配置认证"></a>配置认证</h3><p>使用token(令牌)需要https协议，因为treafik代理的http，所以要创建证书去https访问dashboard</p><p><strong>cfssl签发证书</strong></p><p><code>[root@wang-200 ~]# cd /opt/certs/</code></p><p>设置证书信息</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;certs&#x2F;od.com-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;*.od.com&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;od&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;ops&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>签发证书</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 certs]# cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;server od.com-csr.json |cfssl-json -bare od.com</span><br><span class="line">[root@wang-200 certs]# ls od.com*</span><br><span class="line">od.com.csr  od.com-csr.json  od.com-key.pem  od.com.pem</span><br></pre></td></tr></table></figure><p><strong>拷贝证书</strong></p><p>代理节点(wang-12, wang-11)</p><p><code>[root@wang-11 ~]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;nginx&#x2F;certs</span><br><span class="line">scp wang-200:&#x2F;opt&#x2F;certs&#x2F;od.com.pem &#x2F;etc&#x2F;nginx&#x2F;certs&#x2F;</span><br><span class="line">scp wang-200:&#x2F;opt&#x2F;certs&#x2F;od.com-key.pem &#x2F;etc&#x2F;nginx&#x2F;certs&#x2F;</span><br></pre></td></tr></table></figure><p>创建nginx 配置</p><p><code>[root@wang-11 ~]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;dashboard.od.com.conf</span><br><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name dashboard.od.com;   </span><br><span class="line">    rewrite ^(.*)$ https:&#x2F;&#x2F;\$&#123;server_name&#125;\$1 permanent;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen 443 ssl;</span><br><span class="line">    server_name dashboard.od.com;</span><br><span class="line">    access_log &#x2F;data&#x2F;logs&#x2F;nginx&#x2F;dashboard.od.com.log;</span><br><span class="line"></span><br><span class="line">    ssl_certificate &quot;certs&#x2F;od.com.pem&quot;;</span><br><span class="line">    ssl_certificate_key &quot;certs&#x2F;od.com-key.pem&quot;;</span><br><span class="line">    ssl_session_cache shared:SSL:1m;</span><br><span class="line">    ssl_session_timeout  10m;</span><br><span class="line">    ssl_ciphers HIGH:!aNULL:!MD5;</span><br><span class="line">    ssl_prefer_server_ciphers on;</span><br><span class="line"></span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        proxy_pass http:&#x2F;&#x2F;default_backend_traefik;</span><br><span class="line">        proxy_set_header Host       \$http_host;</span><br><span class="line">        proxy_set_header x-forwarded-for \$proxy_add_x_forwarded_for;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>加载配置</strong></p><p><code>[root@wang-11 ~]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nginx -t</span><br><span class="line">nginx -s reload</span><br></pre></td></tr></table></figure><h4 id="访问页面"><a href="#访问页面" class="headerlink" title="访问页面"></a>访问页面</h4><p><a href="https://dashboard.od.com" target="_blank" rel="noopener">https://dashboard.od.com</a></p><p>登陆页面，可以使用token令牌登陆</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfg7eyf6qjj317i0p6juc.jpg" alt="登陆窗口"></p><p>登陆以后展示页面</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfg7g9tk03j310m0l4di5.jpg" alt="在这里插入图片描述"></p><h2 id="K8S的资源监控插件-Heapster（官方已废弃）"><a href="#K8S的资源监控插件-Heapster（官方已废弃）" class="headerlink" title="K8S的资源监控插件-Heapster（官方已废弃）"></a>K8S的资源监控插件-Heapster（官方已废弃）</h2><p>该项目将被官方废弃（RETIRED），在1.8版本以后由metricserver替代</p><p>准备heapster镜像</p><p><code>[root@wang-200 ~]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull quay.io&#x2F;bitnami&#x2F;heapster:1.5.4</span><br><span class="line">docker tag quay.io&#x2F;bitnami&#x2F;heapster:1.5.4 harbor.od.com&#x2F;public&#x2F;heapster:v1.5.4</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;heapster:v1.5.4</span><br></pre></td></tr></table></figure><h3 id="准备资源配置清单-3"><a href="#准备资源配置清单-3" class="headerlink" title="准备资源配置清单"></a>准备资源配置清单</h3><p><strong>创建并进入资源配置清单目录</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;heapster</span><br></pre></td></tr></table></figure><p><strong>rbac.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;heapster&#x2F;rbac.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: heapster</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: heapster</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:heapster</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: heapster</span><br><span class="line">  namespace: kube-system</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>deploy.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;heapster&#x2F;deploy.yaml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: heapster</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        task: monitoring</span><br><span class="line">        k8s-app: heapster</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: heapster</span><br><span class="line">      containers:</span><br><span class="line">      - name: heapster</span><br><span class="line">        image: harbor.od.com&#x2F;public&#x2F;heapster:v1.5.4</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        command:</span><br><span class="line">        - &#x2F;opt&#x2F;bitnami&#x2F;heapster&#x2F;bin&#x2F;heapster</span><br><span class="line">        - --source&#x3D;kubernetes:https:&#x2F;&#x2F;kubernetes.default</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>svc.yaml</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;k8s&#x2F;yaml&#x2F;heapster&#x2F;svc.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    task: monitoring</span><br><span class="line">    kubernetes.io&#x2F;cluster-service: &#39;true&#39;</span><br><span class="line">    kubernetes.io&#x2F;name: Heapster</span><br><span class="line">  name: heapster</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 8082</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: heapster</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置清单-3"><a href="#应用资源配置清单-3" class="headerlink" title="应用资源配置清单"></a>应用资源配置清单</h3><p><strong>执行命令</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;heapster&#x2F;rbac.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;heapster&#x2F;deploy.yaml</span><br><span class="line">kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;heapster&#x2F;svc.yaml</span><br></pre></td></tr></table></figure><p><strong>命令执行过程</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;heapster&#x2F;rbac.yaml</span><br><span class="line">serviceaccount&#x2F;heapster created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;heapster created</span><br><span class="line"></span><br><span class="line">[root@wang-200 ~]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;heapster&#x2F;deploy.yaml</span><br><span class="line">deployment.extensions&#x2F;heapster created</span><br><span class="line"></span><br><span class="line">[root@wang-200 ~]# kubectl apply -f http:&#x2F;&#x2F;doc.k8s.od.com&#x2F;yaml&#x2F;heapster&#x2F;svc.yaml</span><br><span class="line">service&#x2F;heapster created</span><br></pre></td></tr></table></figure><p><strong>查看创建的资源</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pods -n kube-system|grep heapster</span><br><span class="line">heapster-b5b9f794-snkp5                 1&#x2F;1     Running   0          42s</span><br></pre></td></tr></table></figure><h3 id="重启dashboard，访问"><a href="#重启dashboard，访问" class="headerlink" title="重启dashboard，访问"></a>重启dashboard，访问</h3><p><a href="https://dashboard.od.com" target="_blank" rel="noopener">https://dashboard.od.com</a></p><p>CPU使用率和内存使用率</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfg88dmqe4j30xm08a750.jpg" alt="image-20200604141129557"></p><p>容器组资源</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfg88peza6j30x8093gn5.jpg" alt="image-20200604141148174"></p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 老男孩 </category>
          
          <category> 二进制安装 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 二进制安装 </tag>
            
            <tag> 老男孩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>四、k8s-部署node组件</title>
      <link href="/2020/06/02/%E5%9B%9B%E3%80%81k8s-%E9%83%A8%E7%BD%B2node%E7%BB%84%E4%BB%B6/"/>
      <url>/2020/06/02/%E5%9B%9B%E3%80%81k8s-%E9%83%A8%E7%BD%B2node%E7%BB%84%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<h2 id="准备pause基础镜像"><a href="#准备pause基础镜像" class="headerlink" title="准备pause基础镜像"></a>准备pause基础镜像</h2><h3 id="为什么需要这个pause基础镜像？"><a href="#为什么需要这个pause基础镜像？" class="headerlink" title="为什么需要这个pause基础镜像？"></a>为什么需要这个pause基础镜像？</h3><p>原因：需要用一个pause基础镜像把这台机器的pod拉起来，因为kubelet是干活的节点，它帮我们调度docker引擎，边车模式，让kebelet控制一个小镜像，先于我们的业务容器起来，让它帮我们业务容器去设置：UTC、NET、IPC，让它先把命名空间占上，业务容易还没起来的时候，pod的ip已经分配出来 </p><h3 id="下载pause镜像"><a href="#下载pause镜像" class="headerlink" title="下载pause镜像"></a>下载pause镜像</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull kubernetes&#x2F;pause</span><br></pre></td></tr></table></figure><h3 id="提交至docker私有仓库（harbor）中"><a href="#提交至docker私有仓库（harbor）中" class="headerlink" title="提交至docker私有仓库（harbor）中"></a>提交至docker私有仓库（harbor）中</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# docker tag kubernetes&#x2F;pause harbor.od.com&#x2F;public&#x2F;pause</span><br><span class="line">[root@wang-200 ~]# docker push harbor.od.com&#x2F;public&#x2F;pause</span><br><span class="line">The push refers to repository [harbor.od.com&#x2F;public&#x2F;pause]</span><br><span class="line">5f70bf18a086: Mounted from public&#x2F;nginx </span><br><span class="line">e16a89738269: Pushed </span><br><span class="line">latest: digest: sha256:b31bfb4d0213f254d361e0079deaaebefa4f82ba7aa76ef82e90b4935ad5b105 size: 938</span><br></pre></td></tr></table></figure><h2 id="部署-kubelet服务"><a href="#部署-kubelet服务" class="headerlink" title="部署 kubelet服务"></a>部署 kubelet服务</h2><h3 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h3><table><thead><tr><th>主机名</th><th>IP地址</th><th>角色</th></tr></thead><tbody><tr><td>wang-21.host.com（为了展现效果，暂不部署）</td><td>192.168.70.21</td><td>kubelet</td></tr><tr><td>wang-22.host.com（为了展现效果，暂不部署）</td><td>192.168.70.22</td><td>kubelet</td></tr><tr><td>wang-23.host.com</td><td>192.168.70.23</td><td>kubelet</td></tr><tr><td>wang-24.host.com</td><td>192.168.70.24</td><td>kubelet</td></tr></tbody></table><h3 id="创建签名证书"><a href="#创建签名证书" class="headerlink" title="创建签名证书"></a>创建签名证书</h3><p><strong>创建生成证书签名请求（csr）的json配置文件</strong></p><p> <code>[root@wang-200 certs]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;certs&#x2F;kubelet-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;k8s-kubelet&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">    &quot;127.0.0.1&quot;,</span><br><span class="line">    &quot;192.168.70.21&quot;,</span><br><span class="line">    &quot;192.168.70.22&quot;,</span><br><span class="line">    &quot;192.168.70.23&quot;,</span><br><span class="line">    &quot;192.168.70.24&quot;,</span><br><span class="line">    &quot;192.168.70.25&quot;,</span><br><span class="line">    &quot;192.168.70.26&quot;,</span><br><span class="line">    &quot;192.168.70.27&quot;,</span><br><span class="line">    &quot;192.168.70.28&quot;,</span><br><span class="line">    &quot;192.168.70.29&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;od&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;ops&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>生成证书与私钥文件</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 certs]# cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;server kubelet-csr.json | cfssl-json -bare kubelet</span><br><span class="line"></span><br><span class="line">[root@wang-200 certs]# ls kubelet*</span><br><span class="line">kubelet.csr  kubelet-csr.json  kubelet-key.pem  kubelet.pem</span><br></pre></td></tr></table></figure><h3 id="创建配置文件kubelet-kubeconfig"><a href="#创建配置文件kubelet-kubeconfig" class="headerlink" title="创建配置文件kubelet.kubeconfig"></a>创建配置文件kubelet.kubeconfig</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;conf&#x2F;</span><br><span class="line">cd &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;conf&#x2F;</span><br></pre></td></tr></table></figure><p><strong>set-cluster</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl config set-cluster myk8s \</span><br><span class="line">  --certificate-authority&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;certs&#x2F;ca.pem \</span><br><span class="line">  --embed-certs&#x3D;true \</span><br><span class="line">  --server&#x3D;https:&#x2F;&#x2F;api.k8s.od.com:8443 \</span><br><span class="line">  --kubeconfig&#x3D;kubelet.kubeconfig</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#返回结果：</span><br><span class="line">Cluster &quot;myk8s&quot; set.</span><br></pre></td></tr></table></figure><p><strong>set-credentials</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl config set-credentials k8s-node \</span><br><span class="line">  --client-certificate&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;certs&#x2F;client.pem \</span><br><span class="line">  --client-key&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;certs&#x2F;client-key.pem \</span><br><span class="line">  --embed-certs&#x3D;true \</span><br><span class="line">  --kubeconfig&#x3D;kubelet.kubeconfig</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#返回结果：</span><br><span class="line">User &quot;k8s-node&quot; set.</span><br></pre></td></tr></table></figure><p><strong>set-context</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl config set-context myk8s-context \</span><br><span class="line">  --cluster&#x3D;myk8s \</span><br><span class="line">  --user&#x3D;k8s-node \</span><br><span class="line">  --kubeconfig&#x3D;kubelet.kubeconfig</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#返回结果：</span><br><span class="line">Context &quot;myk8s-context&quot; modified.</span><br></pre></td></tr></table></figure><p><strong>use-context</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 conf]# kubectl config use-context myk8s-context --kubeconfig&#x3D;kubelet.kubeconfig</span><br><span class="line">Switched to context &quot;myk8s-context&quot;.</span><br></pre></td></tr></table></figure><p><strong>查看生成的文件</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 conf]# ls</span><br><span class="line">kubelet.kubeconfig</span><br></pre></td></tr></table></figure><p><strong>拷贝文件到本地</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;conf&#x2F;kubelet.kubeconfig ~&#x2F;.kube&#x2F;config</span><br></pre></td></tr></table></figure><h3 id="集群角色绑定到用户"><a href="#集群角色绑定到用户" class="headerlink" title="集群角色绑定到用户"></a>集群角色绑定到用户</h3><p><strong>1.创建资源配置清单</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;conf&#x2F;k8s-node.yaml</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: k8s-node</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:node</span><br><span class="line">subjects:</span><br><span class="line">- apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: User</span><br><span class="line">  name: k8s-node</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>2.根据配置文件创建用户</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建角色后会存到etcd里</span><br><span class="line">[root@wang-200 ~]# kubectl apply -f &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;conf&#x2F;k8s-node.yaml</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;k8s-node created</span><br></pre></td></tr></table></figure><p><strong>3.查询集群角色</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get clusterrolebinding k8s-node</span><br><span class="line">NAME       AGE</span><br><span class="line">k8s-node   37s</span><br></pre></td></tr></table></figure><h3 id="拷贝安装包到服务器"><a href="#拷贝安装包到服务器" class="headerlink" title="拷贝安装包到服务器"></a>拷贝安装包到服务器</h3><p>如果和控制节点在一台机器上，这一步可省略</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir &#x2F;opt&#x2F;kubernetes-v1.15.2</span><br><span class="line">scp -r wang-200:&#x2F;opt&#x2F;kubernetes-v1.15.2&#x2F;* &#x2F;opt&#x2F;kubernetes-v1.15.2&#x2F;</span><br><span class="line">ln -s &#x2F;opt&#x2F;kubernetes-v1.15.2&#x2F; &#x2F;opt&#x2F;kubernetes</span><br><span class="line">ln -s &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kubectl &#x2F;usr&#x2F;bin&#x2F;kubectl</span><br></pre></td></tr></table></figure><p><strong>拷贝kueblet证书</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp wang-200:&#x2F;opt&#x2F;certs&#x2F;kubelet.pem &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;certs&#x2F;</span><br><span class="line">scp wang-200:&#x2F;opt&#x2F;certs&#x2F;kubelet-key.pem &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;certs&#x2F;</span><br><span class="line">scp wang-200:&#x2F;opt&#x2F;certs&#x2F;ca.pem &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;certs&#x2F;</span><br><span class="line">scp wang-200:&#x2F;opt&#x2F;certs&#x2F;client.pem &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;certs&#x2F;</span><br><span class="line">scp wang-200:&#x2F;opt&#x2F;certs&#x2F;client-key.pem &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;certs&#x2F;</span><br><span class="line">scp wang-200:&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;conf&#x2F;kubelet.kubeconfig &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;conf&#x2F;kubelet.kubeconfig</span><br></pre></td></tr></table></figure><h3 id="创建kubelet启动脚本-wang-24例"><a href="#创建kubelet启动脚本-wang-24例" class="headerlink" title="创建kubelet启动脚本(wang-24例)"></a>创建kubelet启动脚本(wang-24例)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kubelet.sh</span><br><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">.&#x2F;kubelet \\</span><br><span class="line">  --anonymous-auth&#x3D;false \\</span><br><span class="line">  --cgroup-driver systemd \\</span><br><span class="line">  --cluster-dns 10.2.0.2 \\</span><br><span class="line">  --cluster-domain cluster.local \\</span><br><span class="line">  --runtime-cgroups&#x3D;&#x2F;systemd&#x2F;system.slice \\</span><br><span class="line">  --kubelet-cgroups&#x3D;&#x2F;systemd&#x2F;system.slice \\</span><br><span class="line">  --fail-swap-on&#x3D;&quot;false&quot; \\</span><br><span class="line">  --client-ca-file .&#x2F;certs&#x2F;ca.pem \\</span><br><span class="line">  --tls-cert-file .&#x2F;certs&#x2F;kubelet.pem \\</span><br><span class="line">  --tls-private-key-file .&#x2F;certs&#x2F;kubelet-key.pem \\</span><br><span class="line">  --hostname-override wang-24.host.com \\</span><br><span class="line">  --image-gc-high-threshold 20 \\</span><br><span class="line">  --image-gc-low-threshold 10 \\</span><br><span class="line">  --kubeconfig .&#x2F;conf&#x2F;kubelet.kubeconfig \\</span><br><span class="line">  --log-dir &#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-kubelet \\</span><br><span class="line">  --pod-infra-container-image harbor.od.com&#x2F;public&#x2F;pause:latest \\</span><br><span class="line">  --root-dir &#x2F;data&#x2F;kubelet</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 适配对应的主机名</span><br><span class="line">sed -i &quot;s&amp;wang-24.host.com&amp;$&#123;HOSTNAME&#125;&amp;&quot; &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kubelet.sh</span><br></pre></td></tr></table></figure><blockquote><p>*注意：kubelet集群各主机的启动脚本略不同，其他节点注意修改：–hostname-override</p></blockquote><blockquote><p> –anonymous-auth=false    # 匿名登陆，这里设置为不允许</p><p> –cgroup-driver systemd     # 这里需要和docker的daemon.json保持一直</p><p> –fail-swap-on=”false”          # 设置为不关闭swap分区也正常启动，正常需要关闭swap分区的。</p><p> –hostname-override wang-21.host.com      # 主机名</p><p> –pod-infra-container-image harbor.od.com/public/pause:latest       # pause地址</p></blockquote><p><strong>授权并创建日志目录</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod +x &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kubelet.sh </span><br><span class="line">mkdir -p &#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-kubelet &#x2F;data&#x2F;kubelet</span><br></pre></td></tr></table></figure><h3 id="创建-supervisor配置"><a href="#创建-supervisor配置" class="headerlink" title="创建 supervisor配置"></a>创建 supervisor配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;supervisord.d&#x2F;kube-kubelet.ini</span><br><span class="line">[program:kube-kubelet-70-24]</span><br><span class="line">command&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kubelet.sh         ; the program (relative uses PATH, can take args)</span><br><span class="line">numprocs&#x3D;1                                            ; number of processes copies to start (def 1)</span><br><span class="line">directory&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin                  ; directory to cwd to before exec (def no cwd) </span><br><span class="line">autostart&#x3D;true                                        ; start at supervisord start (default: true)</span><br><span class="line">autorestart&#x3D;true                                      ; retstart at unexpected quit (default: true)</span><br><span class="line">startsecs&#x3D;22                                          ; number of secs prog must stay running (def. 1)</span><br><span class="line">startretries&#x3D;3                                        ; max # of serial start failures (default 3)</span><br><span class="line">exitcodes&#x3D;0,2                                         ; &#39;expected&#39; exit codes for process (default 0,2)</span><br><span class="line">stopsignal&#x3D;QUIT                                       ; signal used to kill process (default TERM)</span><br><span class="line">stopwaitsecs&#x3D;10                                       ; max num secs to wait b4 SIGKILL (default 10)</span><br><span class="line">user&#x3D;root                                             ; setuid to this UNIX account to run the program</span><br><span class="line">redirect_stderr&#x3D;false                                 ; redirect proc stderr to stdout (default false)</span><br><span class="line">stdout_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-kubelet&#x2F;kubelet.stdout.log       ; stdout log path, NONE for none; default AUTO</span><br><span class="line">stdout_logfile_maxbytes&#x3D;64MB                          ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stdout_logfile_backups&#x3D;4                              ; # of stdout logfile backups (default 10)</span><br><span class="line">stdout_capture_maxbytes&#x3D;1MB                           ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stdout_events_enabled&#x3D;false                           ; emit events on stdout writes (default false)</span><br><span class="line">stderr_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-kubelet&#x2F;kubelet.stderr.log       ; stderr log path, NONE for none; default AUTO</span><br><span class="line">stderr_logfile_maxbytes&#x3D;64MB                          ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stderr_logfile_backups&#x3D;4                              ; # of stderr logfile backups (default 10)</span><br><span class="line">stderr_capture_maxbytes&#x3D;1MB                           ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stderr_events_enabled&#x3D;false                           ; emit events on stderr writes (default false)</span><br><span class="line">stopasgroup&#x3D;true                            ;默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程</span><br><span class="line">killasgroup&#x3D;true                            ;默认为false，向进程组发送kill信号，包括子进程</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 修改适应其他主机</span><br><span class="line">sed -i &quot;s&amp;kube-kubelet-70-24&amp;kube-kubelet-70-$&#123;HOSTNUM&#125;&amp;&quot; &#x2F;etc&#x2F;supervisord.d&#x2F;kube-kubelet.ini</span><br></pre></td></tr></table></figure><blockquote><p><em>注意：其他主机部署时请注意修改program标签</em></p></blockquote><h3 id="启动服务并检查"><a href="#启动服务并检查" class="headerlink" title="启动服务并检查"></a>启动服务并检查</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-24 ~]# supervisorctl update</span><br><span class="line">kube-kubelet-70-24: added process group</span><br><span class="line">[root@wang-24 ~]# supervisorctl status</span><br><span class="line">kube-kubelet-70-24               RUNNING   pid 52982, uptime 0:00:25</span><br></pre></td></tr></table></figure><h3 id="部署启动集群其他主机-略过"><a href="#部署启动集群其他主机-略过" class="headerlink" title="部署启动集群其他主机(略过)"></a>部署启动集群其他主机(略过)</h3><p><strong>检查运算节点集群</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get node</span><br><span class="line">NAME               STATUS   ROLES    AGE    VERSION</span><br><span class="line">wang-23.host.com   Ready    &lt;none&gt;   11s    v1.15.2</span><br><span class="line">wang-24.host.com   Ready    &lt;none&gt;   5m2s   v1.15.2</span><br></pre></td></tr></table></figure><h2 id="给主机打上角色标签"><a href="#给主机打上角色标签" class="headerlink" title="给主机打上角色标签"></a>给主机打上角色标签</h2><p>标签功能是特色管理功能之一</p><h3 id="为主机打标签"><a href="#为主机打标签" class="headerlink" title="为主机打标签"></a>为主机打标签</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl label node wang-23.host.com node-role.kubernetes.io&#x2F;master&#x3D; </span><br><span class="line">node&#x2F;wang-23.host.com labeled</span><br><span class="line">[root@wang-200 ~]# kubectl label node wang-23.host.com node-role.kubernetes.io&#x2F;node&#x3D;</span><br><span class="line">node&#x2F;wang-23.host.com labeled</span><br><span class="line">[root@wang-200 ~]# kubectl label node wang-24.host.com node-role.kubernetes.io&#x2F;node&#x3D;</span><br><span class="line">node&#x2F;wang-24.host.com labeled</span><br></pre></td></tr></table></figure><h3 id="查看主机标签"><a href="#查看主机标签" class="headerlink" title="查看主机标签"></a>查看主机标签</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get node</span><br><span class="line">NAME               STATUS   ROLES         AGE   VERSION</span><br><span class="line">wang-23.host.com   Ready    master,node   14m   v1.15.2</span><br><span class="line">wang-24.host.com   Ready    node          18m   v1.15.2</span><br></pre></td></tr></table></figure><h2 id="部署-kube-proxy服务"><a href="#部署-kube-proxy服务" class="headerlink" title="部署 kube-proxy服务"></a>部署 kube-proxy服务</h2><p>集群规划</p><table><thead><tr><th>主机名</th><th>IP地址</th><th>角色</th></tr></thead><tbody><tr><td>wang-21.host.com（为了展现效果，暂不部署）</td><td>192.168.70.21</td><td>kube-proxy</td></tr><tr><td>wang-22.host.com（为了展现效果，暂不部署）</td><td>192.168.70.22</td><td>kube-proxy</td></tr><tr><td>wang-23.host.com</td><td>192.168.70.23</td><td>kube-proxy</td></tr><tr><td>wang-24.host.com</td><td>192.168.70.24</td><td>kube-proxy</td></tr></tbody></table><p>注意：这里部署以wang-24主机为例，其他运算节点类似</p><h3 id="签发kube-proxy证书"><a href="#签发kube-proxy证书" class="headerlink" title="签发kube-proxy证书"></a>签发kube-proxy证书</h3><p><strong>创建生成证书签名请求（csr）的json配置文件</strong></p><p><code>[root@wang-200 ~]# cd /opt/certs/</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; kube-proxy-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;system:kube-proxy&quot;,</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;od&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;ops&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>生成kubelet证书和私钥</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 certs]# cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;client kube-proxy-csr.json |cfssl-json -bare kube-proxy-client</span><br><span class="line"></span><br><span class="line">[root@wang-200 certs]# ls kube-proxy-*</span><br><span class="line">kube-proxy-client.csr  kube-proxy-client-key.pem  kube-proxy-client.pem  kube-proxy-csr.json</span><br></pre></td></tr></table></figure><h3 id="拷贝kube-proxy证书"><a href="#拷贝kube-proxy证书" class="headerlink" title="拷贝kube-proxy证书"></a>拷贝kube-proxy证书</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp wang-200:&#x2F;opt&#x2F;certs&#x2F;kube-proxy-client.pem &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;certs&#x2F;</span><br><span class="line">scp wang-200:&#x2F;opt&#x2F;certs&#x2F;kube-proxy-client-key.pem &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;certs&#x2F;</span><br></pre></td></tr></table></figure><h3 id="创建配置文件kube-proxy-kubeconfig"><a href="#创建配置文件kube-proxy-kubeconfig" class="headerlink" title="创建配置文件kube-proxy.kubeconfig"></a>创建配置文件kube-proxy.kubeconfig</h3><p>只需要在<code>wang-200</code>主机操作，再将生成的配置文件拷贝到各规划节点即可。</p><p><strong>1.切换至conf目录</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 certs]# cd &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;conf</span><br></pre></td></tr></table></figure><p><strong>set-cluster</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl config set-cluster myk8s \</span><br><span class="line">  --certificate-authority&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;certs&#x2F;ca.pem \</span><br><span class="line">  --embed-certs&#x3D;true \</span><br><span class="line">  --server&#x3D;https:&#x2F;&#x2F;api.k8s.od.com:8443 \</span><br><span class="line">  --kubeconfig&#x3D;kube-proxy.kubeconfig</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 返回结果</span><br><span class="line">Cluster &quot;myk8s&quot; set.</span><br></pre></td></tr></table></figure><p><strong>set-credentials</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl config set-credentials kube-proxy \</span><br><span class="line">  --client-certificate&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;certs&#x2F;kube-proxy-client.pem \</span><br><span class="line">  --client-key&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;certs&#x2F;kube-proxy-client-key.pem \</span><br><span class="line">  --embed-certs&#x3D;true \</span><br><span class="line">  --kubeconfig&#x3D;kube-proxy.kubeconfig</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 返回结果</span><br><span class="line">User &quot;kube-proxy&quot; set.</span><br></pre></td></tr></table></figure><p><strong>set-context</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl config set-context myk8s-context \</span><br><span class="line">  --cluster&#x3D;myk8s \</span><br><span class="line">  --user&#x3D;kube-proxy \</span><br><span class="line">  --kubeconfig&#x3D;kube-proxy.kubeconfig</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F;返回结果</span><br><span class="line">Context &quot;myk8s-context&quot; created.</span><br></pre></td></tr></table></figure><p><strong>use-context</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 conf]# kubectl config use-context myk8s-context --kubeconfig&#x3D;kube-proxy.kubeconfig</span><br><span class="line">Switched to context &quot;myk8s-context&quot;.</span><br></pre></td></tr></table></figure><p><strong>查看生成配置文件</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 conf]# ll kube-proxy.kubeconfig</span><br><span class="line">-rw-------. 1 root root 6252 6月   3 10:06 kube-proxy.kubeconfig</span><br></pre></td></tr></table></figure><h3 id="配置ipvs-转发"><a href="#配置ipvs-转发" class="headerlink" title="配置ipvs 转发"></a>配置ipvs 转发</h3><p><strong>查看ipvs模块是否开启</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-24 ~]# lsmod |grep ip_vs</span><br><span class="line"># 无结果,说明未开启</span><br></pre></td></tr></table></figure><p><strong>开启ipvs模块</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;data&#x2F;shell&#x2F;ipvs.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">ipvs_mods_dir&#x3D;&quot;&#x2F;usr&#x2F;lib&#x2F;modules&#x2F;\$(uname -r)&#x2F;kernel&#x2F;net&#x2F;netfilter&#x2F;ipvs&quot;</span><br><span class="line">for i in \$(ls \$ipvs_mods_dir|grep -o &quot;^[^.]*&quot;)</span><br><span class="line">do</span><br><span class="line">  &#x2F;sbin&#x2F;modinfo -F filename \$i &amp;&gt;&#x2F;dev&#x2F;null</span><br><span class="line">  if [ \$? -eq 0 ];then</span><br><span class="line">    &#x2F;sbin&#x2F;modprobe \$i</span><br><span class="line">  fi</span><br><span class="line">done</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sh &#x2F;data&#x2F;shell&#x2F;ipvs.sh</span><br></pre></td></tr></table></figure><p><strong>检测ipvs模块是否开启</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-24 ~]# lsmod |grep ip_vs</span><br><span class="line">ip_vs_wrr              12697  0 </span><br><span class="line">ip_vs_wlc              12519  0 </span><br><span class="line">ip_vs_sh               12688  0 </span><br><span class="line">ip_vs_sed              12519  0 </span><br><span class="line">ip_vs_rr               12600  0 </span><br><span class="line">ip_vs_pe_sip           12740  0 </span><br><span class="line">nf_conntrack_sip       33860  1 ip_vs_pe_sip</span><br><span class="line">ip_vs_nq               12516  0 </span><br><span class="line">ip_vs_lc               12516  0 </span><br><span class="line">ip_vs_lblcr            12922  0 </span><br><span class="line">ip_vs_lblc             12819  0 </span><br><span class="line">ip_vs_ftp              13079  0 </span><br><span class="line">ip_vs_dh               12688  0 </span><br><span class="line">ip_vs                 145497  24 ip_vs_dh,ip_vs_lc,ip_vs_nq,ip_vs_rr,ip_vs_sh,ip_vs_ftp,ip_vs_sed,ip_vs_wlc,ip_vs_wrr,ip_vs_pe_sip,ip_vs_lblcr,ip_vs_lblc</span><br><span class="line">nf_nat                 26787  3 ip_vs_ftp,nf_nat_ipv4,nf_nat_masquerade_ipv4</span><br><span class="line">nf_conntrack          133095  8 ip_vs,nf_nat,nf_nat_ipv4,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,nf_conntrack_sip,nf_conntrack_ipv4</span><br><span class="line">libcrc32c              12644  4 xfs,ip_vs,nf_nat,nf_conntrack</span><br></pre></td></tr></table></figure><h3 id="创建kube-proxy启动脚本"><a href="#创建kube-proxy启动脚本" class="headerlink" title="创建kube-proxy启动脚本"></a>创建kube-proxy启动脚本</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-proxy.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">.&#x2F;kube-proxy \\</span><br><span class="line">  --cluster-cidr 172.16.0.0&#x2F;16 \\</span><br><span class="line">  --hostname-override wang-24.host.com \\</span><br><span class="line">  --proxy-mode&#x3D;ipvs \\</span><br><span class="line">  --ipvs-scheduler&#x3D;nq \\</span><br><span class="line">  --kubeconfig .&#x2F;conf&#x2F;kube-proxy.kubeconfig</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sed -i &quot;s&amp;wang-24.host.com&amp;$&#123;HOSTNAME&#125;&amp;&quot; &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-proxy.sh</span><br></pre></td></tr></table></figure><blockquote><p><em>注意：其他主机部署时请注意修改<code>--hostname-override</code>该主机的主机名</em></p></blockquote><h3 id="检查配置、授权、创建日志目录"><a href="#检查配置、授权、创建日志目录" class="headerlink" title="检查配置、授权、创建日志目录"></a>检查配置、授权、创建日志目录</h3><p><strong>拷贝kube-proxy.kubeconfig 到wang-24</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod +x &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-proxy.sh </span><br><span class="line">mkdir -p &#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-proxy</span><br><span class="line">scp wang-200:&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;conf&#x2F;kube-proxy.kubeconfig &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;conf&#x2F;</span><br></pre></td></tr></table></figure><h3 id="创建supervisor配置"><a href="#创建supervisor配置" class="headerlink" title="创建supervisor配置"></a>创建supervisor配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;supervisord.d&#x2F;kube-proxy.ini</span><br><span class="line">[program:kube-proxy-70-24]</span><br><span class="line">command&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-proxy.sh                 ; the program (relative uses PATH, can take args)</span><br><span class="line">numprocs&#x3D;1                                                       ; number of processes copies to start (def 1)</span><br><span class="line">directory&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin                             ; directory to cwd to before exec (def no cwd)</span><br><span class="line">autostart&#x3D;true                                                   ; start at supervisord start (default: true)</span><br><span class="line">autorestart&#x3D;true                                                 ; retstart at unexpected quit (default: true)</span><br><span class="line">startsecs&#x3D;22                                                     ; number of secs prog must stay running (def. 1)</span><br><span class="line">startretries&#x3D;3                                                   ; max # of serial start failures (default 3)</span><br><span class="line">exitcodes&#x3D;0,2                                                    ; &#39;expected&#39; exit codes for process (default 0,2)</span><br><span class="line">stopsignal&#x3D;QUIT                                                  ; signal used to kill process (default TERM)</span><br><span class="line">stopwaitsecs&#x3D;10                                                  ; max num secs to wait b4 SIGKILL (default 10)</span><br><span class="line">user&#x3D;root                                                        ; setuid to this UNIX account to run the program</span><br><span class="line">redirect_stderr&#x3D;false                                            ; redirect proc stderr to stdout (default false)</span><br><span class="line">stdout_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-proxy&#x2F;proxy.stdout.log     ; stdout log path, NONE for none; default AUTO</span><br><span class="line">stdout_logfile_maxbytes&#x3D;64MB                                     ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stdout_logfile_backups&#x3D;4                                         ; # of stdout logfile backups (default 10)</span><br><span class="line">stdout_capture_maxbytes&#x3D;1MB                                      ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stdout_events_enabled&#x3D;false                                      ; emit events on stdout writes (default false)</span><br><span class="line">stderr_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-proxy&#x2F;proxy.stderr.log     ; stderr log path, NONE for none; default AUTO</span><br><span class="line">stderr_logfile_maxbytes&#x3D;64MB                                     ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stderr_logfile_backups&#x3D;4                                         ; # of stderr logfile backups (default 10)</span><br><span class="line">stderr_capture_maxbytes&#x3D;1MB                                      ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stderr_events_enabled&#x3D;false                                      ; emit events on stderr writes (default false)</span><br><span class="line">stopasgroup&#x3D;true                            ;默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程</span><br><span class="line">killasgroup&#x3D;true                            ;默认为false，向进程组发送kill信号，包括子进程</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 修改主机名，适应对应的主机</span><br><span class="line">sed -i &quot;s&amp;kube-proxy-70-24&amp;kube-proxy-70-$&#123;HOSTNUM&#125;&amp;&quot; &#x2F;etc&#x2F;supervisord.d&#x2F;kube-proxy.ini</span><br></pre></td></tr></table></figure><blockquote><p><em>注意：其他主机部署时请注意修改program标签</em></p></blockquote><h3 id="启动并检测结果"><a href="#启动并检测结果" class="headerlink" title="启动并检测结果"></a>启动并检测结果</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-24 ~]# supervisorctl update                          </span><br><span class="line">kube-proxy-70-24: added process group</span><br><span class="line">[root@wang-24 ~]# supervisorctl status                          </span><br><span class="line">kube-kubelet-70-24               RUNNING   pid 52982, uptime 0:37:22</span><br><span class="line">kube-proxy-70-24                 STARTING</span><br><span class="line">[root@wang-24 ~]# netstat -anput | grep kube-proxy</span><br><span class="line">tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      7522&#x2F;.&#x2F;kube-proxy   </span><br><span class="line">tcp        0      0 192.168.70.24:42688     192.168.70.10:8443      ESTABLISHED 7522&#x2F;.&#x2F;kube-proxy   </span><br><span class="line">tcp6       0      0 :::10256                :::*                    LISTEN      7522&#x2F;.&#x2F;kube-proxy</span><br></pre></td></tr></table></figure><h3 id="安装部署集群其他主机（略过）"><a href="#安装部署集群其他主机（略过）" class="headerlink" title="安装部署集群其他主机（略过）"></a>安装部署集群其他主机（略过）</h3><h2 id="验证k8s集群-测试结果"><a href="#验证k8s集群-测试结果" class="headerlink" title="验证k8s集群 (测试结果)"></a>验证k8s集群 (测试结果)</h2><h3 id="创建资源配置清单"><a href="#创建资源配置清单" class="headerlink" title="创建资源配置清单"></a>创建资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;root&#x2F;yaml&#x2F;nginx-ds.yml</span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ds</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx-ds</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: my-nginx</span><br><span class="line">        image: harbor.od.com&#x2F;public&#x2F;nginx:v1.7.9</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="应用资源配置"><a href="#应用资源配置" class="headerlink" title="应用资源配置"></a>应用资源配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl create -f &#x2F;root&#x2F;yaml&#x2F;nginx-ds.yml </span><br><span class="line">daemonset.extensions&#x2F;nginx-ds created</span><br></pre></td></tr></table></figure><h3 id="验证资源配置清单"><a href="#验证资源配置清单" class="headerlink" title="验证资源配置清单"></a>验证资源配置清单</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get pod</span><br><span class="line">NAME             READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-ds-8p4gg   1&#x2F;1     Running   0          57s</span><br><span class="line">nginx-ds-vjct6   1&#x2F;1     Running   0          57s</span><br><span class="line">[root@wang-200 ~]# kubectl get pod -o wide</span><br><span class="line">NAME             READY   STATUS    RESTARTS   AGE   IP            NODE               NOMINATED NODE   READINESS GATES</span><br><span class="line">nginx-ds-8p4gg   1&#x2F;1     Running   0          59s   172.16.23.2   wang-23.host.com   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-ds-vjct6   1&#x2F;1     Running   0          59s   172.16.24.2   wang-24.host.com   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h3 id="访问pod资源测试"><a href="#访问pod资源测试" class="headerlink" title="访问pod资源测试"></a>访问pod资源测试</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-24 ~]# curl -I 172.16.24.2</span><br><span class="line">HTTP&#x2F;1.1 200 OK</span><br><span class="line">Server: nginx&#x2F;1.7.9</span><br><span class="line">Date: Wed, 03 Jun 2020 02:21:14 GMT</span><br><span class="line">Content-Type: text&#x2F;html</span><br><span class="line">Content-Length: 612</span><br><span class="line">Last-Modified: Tue, 23 Dec 2014 16:25:09 GMT</span><br><span class="line">Connection: keep-alive</span><br><span class="line">ETag: &quot;54999765-264&quot;</span><br><span class="line">Accept-Ranges: bytes</span><br><span class="line"></span><br><span class="line">[root@wang-24 ~]# curl -I 172.16.23.2</span><br><span class="line">curl: (7) Failed connect to 172.16.23.2:80; 连接超时</span><br></pre></td></tr></table></figure><p>问题：跨宿主机的pod资源，无法访问。</p><p>解决方案：通过CNI网络插件实现POD资源能够跨宿主机就行通信</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 老男孩 </category>
          
          <category> 二进制安装 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 二进制安装 </tag>
            
            <tag> 老男孩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>三、k8s-部署master组件</title>
      <link href="/2020/05/31/%E4%B8%89%E3%80%81k8s-%E9%83%A8%E7%BD%B2master%E7%BB%84%E4%BB%B6/"/>
      <url>/2020/05/31/%E4%B8%89%E3%80%81k8s-%E9%83%A8%E7%BD%B2master%E7%BB%84%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<h2 id="部署etcd服务"><a href="#部署etcd服务" class="headerlink" title="部署etcd服务"></a>部署etcd服务</h2><h3 id="etcd集群规划"><a href="#etcd集群规划" class="headerlink" title="etcd集群规划"></a>etcd集群规划</h3><table><thead><tr><th>主机名</th><th>IP地址</th><th>角色</th></tr></thead><tbody><tr><td>wang-21.host.com</td><td>192.168.70.21</td><td>etcd lead</td></tr><tr><td>wang-22.host.com</td><td>192.168.70.22</td><td>etcd follow</td></tr><tr><td>wang-23.host.com</td><td>192.168.70.23</td><td>etcd follow</td></tr></tbody></table><h3 id="etcd-签发证书-wang-200主机"><a href="#etcd-签发证书-wang-200主机" class="headerlink" title="etcd 签发证书 (wang-200主机)"></a>etcd 签发证书 (wang-200主机)</h3><p><strong>创建基于根证书的config配置文件</strong></p><p><code>vim /opt/certs/ca-config.json</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;signing&quot;: &#123;</span><br><span class="line">        &quot;default&quot;: &#123;</span><br><span class="line">            &quot;expiry&quot;: &quot;175200h&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;profiles&quot;: &#123;</span><br><span class="line">            &quot;server&quot;: &#123;</span><br><span class="line">                &quot;expiry&quot;: &quot;175200h&quot;,</span><br><span class="line">                &quot;usages&quot;: [</span><br><span class="line">                    &quot;signing&quot;,</span><br><span class="line">                    &quot;key encipherment&quot;,</span><br><span class="line">                    &quot;server auth&quot;</span><br><span class="line">                ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;client&quot;: &#123;</span><br><span class="line">                &quot;expiry&quot;: &quot;175200h&quot;,</span><br><span class="line">                &quot;usages&quot;: [</span><br><span class="line">                    &quot;signing&quot;,</span><br><span class="line">                    &quot;key encipherment&quot;,</span><br><span class="line">                    &quot;client auth&quot;</span><br><span class="line">                ]</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;peer&quot;: &#123;</span><br><span class="line">                &quot;expiry&quot;: &quot;175200h&quot;,</span><br><span class="line">                &quot;usages&quot;: [</span><br><span class="line">                    &quot;signing&quot;,</span><br><span class="line">                    &quot;key encipherment&quot;,</span><br><span class="line">                    &quot;server auth&quot;,</span><br><span class="line">                    &quot;client auth&quot;</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>证书类型</p><p>client certificate：客户端使用，用于服务端认证客户端,如etcdctl、etcd proxy、fleetctl、docker客户端</p><p>server certificate：服务端使用，客户端以此验证服务端身份,例如docker服务端、kube-apiserver</p><p>peer certificate：双向证书，用于etcd集群成员间通信</p></blockquote><p><strong>创建生成自签证书的签名请求(csr)的 json配置文件</strong></p><p><code>[root@wang-200 certs]# vim /opt/certs/etcd-peer-csr.json</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;k8s-etcd&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">        &quot;192.168.70.21&quot;,</span><br><span class="line">        &quot;192.168.70.22&quot;,</span><br><span class="line">        &quot;192.168.70.23&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;od&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;ops&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>hosts 段把有可能成为etcd节点的主机都填写进去。</p></blockquote><p><strong>生成证书</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 certs]# cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;peer etcd-peer-csr.json|cfssl-json -bare etcd-peer</span><br><span class="line">[root@wang-200 certs]# ls |grep etcd</span><br><span class="line">etcd-peer.csr</span><br><span class="line">etcd-peer-csr.json</span><br><span class="line">etcd-peer-key.pem</span><br><span class="line">etcd-peer.pem</span><br></pre></td></tr></table></figure><h3 id="安装etcd服务"><a href="#安装etcd服务" class="headerlink" title="安装etcd服务"></a>安装etcd服务</h3><p><strong>下载安装包</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 certs]# mkdir &#x2F;data&#x2F;soft&#x2F;k8s&#x2F;</span><br><span class="line">[root@wang-200 certs]# wget -O &#x2F;data&#x2F;soft&#x2F;k8s&#x2F;etcd-v3.1.20-linux-amd64.tar.gz https:&#x2F;&#x2F;github.com&#x2F;etcd-io&#x2F;etcd&#x2F;releases&#x2F;download&#x2F;v3.1.20&#x2F;etcd-v3.1.20-linux-amd64.tar.gz #较慢</span><br></pre></td></tr></table></figure><p>拷贝到指定服务器并安装</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp wang-200:&#x2F;data&#x2F;soft&#x2F;k8s&#x2F;etcd-v3.1.20-linux-amd64.tar.gz &#x2F;data&#x2F;soft&#x2F;</span><br><span class="line">tar xf &#x2F;data&#x2F;soft&#x2F;etcd-v3.1.20-linux-amd64.tar.gz -C &#x2F;opt&#x2F;</span><br><span class="line">mv &#x2F;opt&#x2F;etcd-v3.1.20-linux-amd64 &#x2F;opt&#x2F;etcd-v3.1.20</span><br><span class="line">ln -s &#x2F;opt&#x2F;etcd-v3.1.20 &#x2F;opt&#x2F;etcd</span><br><span class="line">ln -s &#x2F;opt&#x2F;etcd&#x2F;etcdctl &#x2F;usr&#x2F;bin&#x2F;etcdctl</span><br></pre></td></tr></table></figure><p>创建etcd用户和相关目录</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">useradd -M -s &#x2F;sbin&#x2F;nologin etcd</span><br><span class="line">mkdir -p &#x2F;opt&#x2F;etcd&#x2F;certs&#x2F;  # 存放etcd证书</span><br><span class="line">mkdir -p &#x2F;data&#x2F;etcd&#x2F;  # 存放etcd数据</span><br><span class="line">mkdir -p &#x2F;data&#x2F;logs&#x2F;etcd-server  # 存放etcd日志</span><br><span class="line">chown -R etcd:etcd &#x2F;data&#x2F;etcd &#x2F;data&#x2F;logs&#x2F;etcd-server&#x2F;</span><br></pre></td></tr></table></figure><p>拷贝证书与私钥文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;opt&#x2F;etcd&#x2F;certs&#x2F;</span><br><span class="line">scp wang-200:&#x2F;opt&#x2F;certs&#x2F;ca.pem .</span><br><span class="line">scp wang-200:&#x2F;opt&#x2F;certs&#x2F;etcd-peer-key.pem . </span><br><span class="line">scp wang-200:&#x2F;opt&#x2F;certs&#x2F;etcd-peer.pem .</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-21 certs]# ls</span><br><span class="line">ca.pem  etcd-peer-key.pem  etcd-peer.pem</span><br></pre></td></tr></table></figure><h3 id="创建etcd服务启动脚本"><a href="#创建etcd服务启动脚本" class="headerlink" title="创建etcd服务启动脚本"></a>创建etcd服务启动脚本</h3><div class="hide-toggle" style="border: 1px solid bg"><div class="hide-button toggle-title" style="background-color: bg;color: color"><i class="fa fa-caret-right fa-fw"></i><span>直接粘贴脚本</span></div>    <div class="hide-content"><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;etcd&#x2F;etcd-server-startup.sh</span><br><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">.&#x2F;etcd --name etcd-server-70-21 \\</span><br><span class="line">       --data-dir &#x2F;data&#x2F;etcd&#x2F;etcd-server \\</span><br><span class="line">       --listen-peer-urls https:&#x2F;&#x2F;192.168.70.21:2380 \\</span><br><span class="line">       --listen-client-urls https:&#x2F;&#x2F;192.168.70.21:2379,http:&#x2F;&#x2F;127.0.0.1:2379 \\</span><br><span class="line">       --quota-backend-bytes 8000000000 \\</span><br><span class="line">       --initial-advertise-peer-urls https:&#x2F;&#x2F;192.168.70.21:2380 \\</span><br><span class="line">       --advertise-client-urls https:&#x2F;&#x2F;192.168.70.21:2379,http:&#x2F;&#x2F;127.0.0.1:2379 \\</span><br><span class="line">       --initial-cluster  etcd-server-70-21&#x3D;https:&#x2F;&#x2F;192.168.70.21:2380,etcd-server-70-22&#x3D;https:&#x2F;&#x2F;192.168.70.22:2380,etcd-server-70-23&#x3D;https:&#x2F;&#x2F;192.168.70.23:2380 \\</span><br><span class="line">       --ca-file .&#x2F;certs&#x2F;ca.pem \\</span><br><span class="line">       --cert-file .&#x2F;certs&#x2F;etcd-peer.pem \\</span><br><span class="line">       --key-file .&#x2F;certs&#x2F;etcd-peer-key.pem \\</span><br><span class="line">       --client-cert-auth  \\</span><br><span class="line">       --trusted-ca-file .&#x2F;certs&#x2F;ca.pem \\</span><br><span class="line">       --peer-ca-file .&#x2F;certs&#x2F;ca.pem \\</span><br><span class="line">       --peer-cert-file .&#x2F;certs&#x2F;etcd-peer.pem \\</span><br><span class="line">       --peer-key-file .&#x2F;certs&#x2F;etcd-peer-key.pem \\</span><br><span class="line">       --peer-client-cert-auth \\</span><br><span class="line">       --peer-trusted-ca-file .&#x2F;certs&#x2F;ca.pem \\</span><br><span class="line">       --log-output stdout</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sed -i &quot;2s&amp;etcd-server-70-21&amp;etcd-server-70-$&#123;HOSTNUM&#125;&amp;&quot; &#x2F;opt&#x2F;etcd&#x2F;etcd-server-startup.sh</span><br><span class="line">sed -i &quot;4,8s&amp;192.168.70.21&amp;192.168.70.$&#123;HOSTNUM&#125;&amp;&quot; &#x2F;opt&#x2F;etcd&#x2F;etcd-server-startup.sh</span><br><span class="line">chmod +x &#x2F;opt&#x2F;etcd&#x2F;etcd-server-startup.sh</span><br><span class="line">chown -R etcd:etcd &#x2F;opt&#x2F;etcd-v3.1.20&#x2F;</span><br></pre></td></tr></table></figure></div></div><p><code>vim /opt/etcd/etcd-server-startup.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">.&#x2F;etcd --name etcd-server-70-21 \</span><br><span class="line">       --data-dir &#x2F;data&#x2F;etcd&#x2F;etcd-server \</span><br><span class="line">       --listen-peer-urls https:&#x2F;&#x2F;192.168.70.21:2380 \</span><br><span class="line">       --listen-client-urls https:&#x2F;&#x2F;192.168.70.21:2379,http:&#x2F;&#x2F;127.0.0.1:2379 \</span><br><span class="line">       --quota-backend-bytes 8000000000 \</span><br><span class="line">       --initial-advertise-peer-urls https:&#x2F;&#x2F;192.168.70.21:2380 \</span><br><span class="line">       --advertise-client-urls https:&#x2F;&#x2F;192.168.70.21:2379,http:&#x2F;&#x2F;127.0.0.1:2379 \</span><br><span class="line">       --initial-cluster  etcd-server-70-21&#x3D;https:&#x2F;&#x2F;192.168.70.21:2380,etcd-server-70-22&#x3D;https:&#x2F;&#x2F;192.168.70.22:2380,etcd-server-70-23&#x3D;https:&#x2F;&#x2F;192.168.70.23:2380 \</span><br><span class="line">       --ca-file .&#x2F;certs&#x2F;ca.pem \</span><br><span class="line">       --cert-file .&#x2F;certs&#x2F;etcd-peer.pem \</span><br><span class="line">       --key-file .&#x2F;certs&#x2F;etcd-peer-key.pem \</span><br><span class="line">       --client-cert-auth  \</span><br><span class="line">       --trusted-ca-file .&#x2F;certs&#x2F;ca.pem \</span><br><span class="line">       --peer-ca-file .&#x2F;certs&#x2F;ca.pem \</span><br><span class="line">       --peer-cert-file .&#x2F;certs&#x2F;etcd-peer.pem \</span><br><span class="line">       --peer-key-file .&#x2F;certs&#x2F;etcd-peer-key.pem \</span><br><span class="line">       --peer-client-cert-auth \</span><br><span class="line">       --peer-trusted-ca-file .&#x2F;certs&#x2F;ca.pem \</span><br><span class="line">       --log-output stdout</span><br></pre></td></tr></table></figure><blockquote><p>#各etcd节点，脚本不同的地方(#根据宿主机IP变化)<br>    –name   etcd-server-16-21<br>    –listen-peer-urls<br>    –listen-client-urls<br>    –initial-advertise-peer-urls<br>    –advertise-client-urls</p></blockquote><p><strong>调整权限</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod +x &#x2F;opt&#x2F;etcd&#x2F;etcd-server-startup.sh</span><br><span class="line">chown -R etcd:etcd &#x2F;opt&#x2F;etcd-v3.1.20&#x2F;</span><br></pre></td></tr></table></figure><h3 id="创建etcd-server的启动配置"><a href="#创建etcd-server的启动配置" class="headerlink" title="创建etcd-server的启动配置"></a>创建etcd-server的启动配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;supervisord.d&#x2F;etcd-server.ini</span><br><span class="line">[program:etcd-server-70-21]                 ; 根据主机改变</span><br><span class="line">command&#x3D;&#x2F;opt&#x2F;etcd&#x2F;etcd-server-startup.sh    ; the program (relative uses PATH, can take args)</span><br><span class="line">numprocs&#x3D;1                                  ; number of processes copies to start (def 1)</span><br><span class="line">directory&#x3D;&#x2F;opt&#x2F;etcd                         ; directory to cwd to before exec (def no cwd)</span><br><span class="line">autostart&#x3D;true                              ; start at supervisord start (default: true)</span><br><span class="line">autorestart&#x3D;true                            ; retstart at unexpected quit (default: true)</span><br><span class="line">startsecs&#x3D;30                                ; number of secs prog must stay running (def. 1)</span><br><span class="line">startretries&#x3D;3                              ; max # of serial start failures (default 3)</span><br><span class="line">exitcodes&#x3D;0,2                               ; &#39;expected&#39; exit codes for process (default 0,2)</span><br><span class="line">stopsignal&#x3D;QUIT                             ; signal used to kill process (default TERM)</span><br><span class="line">stopwaitsecs&#x3D;10                             ; max num secs to wait b4 SIGKILL (default 10)</span><br><span class="line">user&#x3D;etcd                                   ; setuid to this UNIX account to run the program</span><br><span class="line">redirect_stderr&#x3D;true                        ; redirect proc stderr to stdout (default false)</span><br><span class="line">stdout_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;etcd-server&#x2F;etcd.stdout.log     ; stdout log path, NONE for none; default AUTO</span><br><span class="line">stdout_logfile_maxbytes&#x3D;64MB                ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stdout_logfile_backups&#x3D;4                    ; # of stdout logfile backups (default 10)</span><br><span class="line">stdout_capture_maxbytes&#x3D;1MB                 ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stdout_events_enabled&#x3D;false                 ; emit events on stdout writes (default false)</span><br><span class="line">stderr_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;etcd-server&#x2F;etcd.stderr.log      ; stderr log path, NONE for none; default AUTO</span><br><span class="line">stderr_logfile_maxbytes&#x3D;64MB                ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stderr_logfile_backups&#x3D;4                    ; # of stderr logfile backups (default 10)</span><br><span class="line">stderr_capture_maxbytes&#x3D;1MB                 ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stderr_events_enabled&#x3D;false                 ; emit events on stderr writes (default false）</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sed -i &quot;s&amp;etcd-server-70-21&amp;etcd-server-70-$&#123;HOSTNUM&#125;&amp;&quot; &#x2F;etc&#x2F;supervisord.d&#x2F;etcd-server.ini</span><br></pre></td></tr></table></figure><h3 id="启动etcd"><a href="#启动etcd" class="headerlink" title="启动etcd"></a>启动etcd</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-21 certs]# supervisorctl update</span><br><span class="line">etcd-server-70-21: added process group</span><br><span class="line">[root@wang-21 certs]# supervisorctl status</span><br><span class="line">etcd-server-70-21                STARTING  </span><br><span class="line">[root@wang-21 certs]# netstat -luntp|grep etcd</span><br><span class="line">tcp        0      0 192.168.70.21:2379      0.0.0.0:*               LISTEN      57940&#x2F;.&#x2F;etcd        </span><br><span class="line">tcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      57940&#x2F;.&#x2F;etcd        </span><br><span class="line">tcp        0      0 192.168.70.21:2380      0.0.0.0:*               LISTEN      57940&#x2F;.&#x2F;etcd</span><br></pre></td></tr></table></figure><h3 id="检查集群状态"><a href="#检查集群状态" class="headerlink" title="检查集群状态"></a>检查集群状态</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看集群健康状态</span><br><span class="line">[root@wang-21 ~]# &#x2F;opt&#x2F;etcd&#x2F;etcdctl cluster-health</span><br><span class="line">member 220fa81c6e7fa2e1 is healthy: got healthy result from http:&#x2F;&#x2F;127.0.0.1:2379</span><br><span class="line">member 394ece5834c57ea6 is healthy: got healthy result from http:&#x2F;&#x2F;127.0.0.1:2379</span><br><span class="line">member f97020d34db750ca is healthy: got healthy result from http:&#x2F;&#x2F;127.0.0.1:2379</span><br><span class="line">cluster is healthy</span><br><span class="line"></span><br><span class="line"># 查看etcd集群列表</span><br><span class="line">[root@wang-21 ~]# &#x2F;opt&#x2F;etcd&#x2F;etcdctl member list</span><br><span class="line">220fa81c6e7fa2e1: name&#x3D;etcd-server-70-23 peerURLs&#x3D;https:&#x2F;&#x2F;192.168.70.23:2380 clientURLs&#x3D;http:&#x2F;&#x2F;127.0.0.1:2379,https:&#x2F;&#x2F;192.168.70.23:2379 isLeader&#x3D;false</span><br><span class="line">394ece5834c57ea6: name&#x3D;etcd-server-70-21 peerURLs&#x3D;https:&#x2F;&#x2F;192.168.70.21:2380 clientURLs&#x3D;http:&#x2F;&#x2F;127.0.0.1:2379,https:&#x2F;&#x2F;192.168.70.21:2379 isLeader&#x3D;true</span><br><span class="line">f97020d34db750ca: name&#x3D;etcd-server-70-22 peerURLs&#x3D;https:&#x2F;&#x2F;192.168.70.22:2380 clientURLs&#x3D;http:&#x2F;&#x2F;127.0.0.1:2379,https:&#x2F;&#x2F;192.168.70.22:2379 isLeader&#x3D;false</span><br></pre></td></tr></table></figure><h2 id="部署-kube-apiserver服务"><a href="#部署-kube-apiserver服务" class="headerlink" title="部署 kube-apiserver服务"></a>部署 kube-apiserver服务</h2><h3 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h3><table><thead><tr><th>主机名</th><th>IP地址</th><th>角色</th></tr></thead><tbody><tr><td>wang-21.host.com</td><td>192.168.70.21</td><td>kube-apiserver</td></tr><tr><td>wang-22.host.com</td><td>192.168.70.22</td><td>kube-apiserver</td></tr><tr><td>wang-23.host.com</td><td>192.168.70.23</td><td>kube-apiserver</td></tr><tr><td>wang-11.host.com</td><td>192.168.70.11</td><td>4层负载均衡</td></tr><tr><td>wang-12.host.com</td><td>192.168.70.12</td><td>4层负载均衡</td></tr></tbody></table><p>这里<code>192.168.70.11</code>和<code>192.168.70.12</code>使用nginx做4层负载均衡器，用keepalived跑一个vip：<code>192.168.70.10</code>,代理两个kube-apiserver，实现高可用</p><h3 id="运维主机签发证书"><a href="#运维主机签发证书" class="headerlink" title="运维主机签发证书"></a>运维主机签发证书</h3><h4 id="签发client证书"><a href="#签发client证书" class="headerlink" title="签发client证书"></a>签发client证书</h4><p><strong>1. 创建生成证书签名请求（csr）的json配置文件</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;certs&#x2F;client-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;k8s-node&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;wang&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;ops&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>2. 生成client证书和私钥</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 certs]# cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;client client-csr.json | cfssl-json -bare client</span><br><span class="line">[root@wang-200 certs]# ls client*</span><br><span class="line">client.csr  client-csr.json  client-key.pem  client.pem</span><br></pre></td></tr></table></figure><h4 id="签发apiserver证书"><a href="#签发apiserver证书" class="headerlink" title="签发apiserver证书"></a>签发apiserver证书</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;certs&#x2F;apiserver-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;apiserver&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">        &quot;127.0.0.1&quot;,</span><br><span class="line">        &quot;10.2.0.1&quot;,</span><br><span class="line">        &quot;kubernetes.default&quot;,</span><br><span class="line">        &quot;kubernetes.default.svc&quot;,</span><br><span class="line">        &quot;kubernetes.default.svc.cluster&quot;,</span><br><span class="line">        &quot;kubernetes.default.svc.cluster.local&quot;,</span><br><span class="line">        &quot;api.k8s.od.com&quot;,</span><br><span class="line">        &quot;192.168.70.10&quot;,</span><br><span class="line">        &quot;192.168.70.11&quot;,</span><br><span class="line">        &quot;192.168.70.12&quot;,</span><br><span class="line">        &quot;192.168.70.21&quot;,</span><br><span class="line">        &quot;192.168.70.22&quot;,</span><br><span class="line">        &quot;192.168.70.23&quot;,</span><br><span class="line">        &quot;192.168.70.24&quot;,</span><br><span class="line">        &quot;192.168.70.25&quot;,</span><br><span class="line">        &quot;192.168.70.26&quot;,</span><br><span class="line">        &quot;192.168.70.27&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;od&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;ops&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>2. 生成kube-apiserver证书和私钥</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 certs]# cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;server apiserver-csr.json |cfssl-json -bare apiserver</span><br><span class="line">[root@wang-200 certs]# ls apiserver*</span><br><span class="line">apiserver.csr  apiserver-csr.json  apiserver-key.pem  apiserver.pem</span><br></pre></td></tr></table></figure><h3 id="在运维主机下载kubernetes安装包"><a href="#在运维主机下载kubernetes安装包" class="headerlink" title="在运维主机下载kubernetes安装包"></a>在运维主机下载kubernetes安装包</h3><blockquote><p>下载地址：<a href="https://github.com/kubernetes/kubernetes/releases" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/releases</a></p></blockquote><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget -O &#x2F;data&#x2F;soft&#x2F;k8s&#x2F;kubernetes-server-linux-amd64-v1.15.2.tar.gz https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kubernetes&#x2F;archive&#x2F;v1.15.2.tar.gz # 吃顿午餐</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# tar xf &#x2F;data&#x2F;soft&#x2F;k8s&#x2F;kubernetes-server-linux-amd64-v1.15.2.tar.gz -C &#x2F;opt&#x2F;</span><br><span class="line">[root@wang-200 ~]# mv &#x2F;opt&#x2F;kubernetes &#x2F;opt&#x2F;kubernetes-v1.15.2</span><br><span class="line">[root@wang-200 ~]# ln -s &#x2F;opt&#x2F;kubernetes-v1.15.2 &#x2F;opt&#x2F;kubernetes</span><br><span class="line">[root@wang-200 ~]# ln -s &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kubectl &#x2F;usr&#x2F;bin&#x2F;kubectl</span><br><span class="line">[root@wang-200 ~]# cd &#x2F;opt&#x2F;kubernetes</span><br><span class="line">[root@wang-200 kubernetes-v1.15.2]# rm kubernetes-src.tar.gz </span><br><span class="line">[root@wang-200 kubernetes]# rm -f server&#x2F;bin&#x2F;*.tar</span><br><span class="line">[root@wang-200 kubernetes]# rm -f server&#x2F;bin&#x2F;*_tag</span><br><span class="line">[root@wang-200 kubernetes]# tree</span><br><span class="line">.</span><br><span class="line">├── addons</span><br><span class="line">├── LICENSES</span><br><span class="line">└── server</span><br><span class="line">    └── bin</span><br><span class="line">        ├── apiextensions-apiserver</span><br><span class="line">        ├── cloud-controller-manager</span><br><span class="line">        ├── hyperkube</span><br><span class="line">        ├── kubeadm</span><br><span class="line">        ├── kube-apiserver</span><br><span class="line">        ├── kube-controller-manager</span><br><span class="line">        ├── kubectl</span><br><span class="line">        ├── kubelet</span><br><span class="line">        ├── kube-proxy</span><br><span class="line">        ├── kube-scheduler</span><br><span class="line">        └── mounter</span><br><span class="line"></span><br><span class="line">3 directories, 12 files</span><br></pre></td></tr></table></figure><p><strong>拷贝证书</strong></p><p><code>[root@wang-200 ~]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建证书目录</span><br><span class="line">mkdir -p &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;certs&#x2F;</span><br><span class="line">cd &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;certs&#x2F;</span><br><span class="line"></span><br><span class="line"># 拷贝证书</span><br><span class="line">cp &#x2F;opt&#x2F;certs&#x2F;apiserver.pem .   </span><br><span class="line">cp &#x2F;opt&#x2F;certs&#x2F;apiserver-key.pem .   </span><br><span class="line">cp &#x2F;opt&#x2F;certs&#x2F;client.pem .              </span><br><span class="line">cp &#x2F;opt&#x2F;certs&#x2F;client-key.pem .    </span><br><span class="line">cp &#x2F;opt&#x2F;certs&#x2F;ca.pem .            </span><br><span class="line">cp &#x2F;opt&#x2F;certs&#x2F;ca-key.pem .</span><br></pre></td></tr></table></figure><p><strong>查看拷贝的证书</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 certs]# ls</span><br><span class="line">apiserver-key.pem  apiserver.pem  ca-key.pem  ca.pem  client-key.pem  client.pem</span><br></pre></td></tr></table></figure><h3 id="拷贝安装包"><a href="#拷贝安装包" class="headerlink" title="拷贝安装包"></a>拷贝安装包</h3><p><code>[root@wang-21 ~]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scp -r wang-200:&#x2F;opt&#x2F;kubernetes-v1.15.2 &#x2F;opt&#x2F;kubernetes-v1.15.2</span><br><span class="line">ln -s &#x2F;opt&#x2F;kubernetes-v1.15.2&#x2F; &#x2F;opt&#x2F;kubernetes</span><br><span class="line">ln -s &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kubectl &#x2F;usr&#x2F;bin&#x2F;kubectl</span><br></pre></td></tr></table></figure><h3 id="创建配置"><a href="#创建配置" class="headerlink" title="创建配置"></a>创建配置</h3><p>k8s资源配置清单，专门给k8s做日志审计</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建资源配置清单目录</span><br><span class="line">mkdir &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;conf</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;conf&#x2F;audit.yaml</span><br><span class="line">apiVersion: audit.k8s.io&#x2F;v1beta1 # This is required.</span><br><span class="line">kind: Policy</span><br><span class="line"># Don&#39;t generate audit events for all requests in RequestReceived stage.</span><br><span class="line">omitStages:</span><br><span class="line">  - &quot;RequestReceived&quot;</span><br><span class="line">rules:</span><br><span class="line">  # Log pod changes at RequestResponse level</span><br><span class="line">  - level: RequestResponse</span><br><span class="line">    resources:</span><br><span class="line">    - group: &quot;&quot;</span><br><span class="line">      # Resource &quot;pods&quot; doesn&#39;t match requests to any subresource of pods,</span><br><span class="line">      # which is consistent with the RBAC policy.</span><br><span class="line">      resources: [&quot;pods&quot;]</span><br><span class="line">  # Log &quot;pods&#x2F;log&quot;, &quot;pods&#x2F;status&quot; at Metadata level</span><br><span class="line">  - level: Metadata</span><br><span class="line">    resources:</span><br><span class="line">    - group: &quot;&quot;</span><br><span class="line">      resources: [&quot;pods&#x2F;log&quot;, &quot;pods&#x2F;status&quot;]</span><br><span class="line">  # Don&#39;t log requests to a configmap called &quot;controller-leader&quot;</span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">    - group: &quot;&quot;</span><br><span class="line">      resources: [&quot;configmaps&quot;]</span><br><span class="line">      resourceNames: [&quot;controller-leader&quot;]</span><br><span class="line">  # Don&#39;t log watch requests by the &quot;system:kube-proxy&quot; on endpoints or services</span><br><span class="line">  - level: None</span><br><span class="line">    users: [&quot;system:kube-proxy&quot;]</span><br><span class="line">    verbs: [&quot;watch&quot;]</span><br><span class="line">    resources:</span><br><span class="line">    - group: &quot;&quot; # core API group</span><br><span class="line">      resources: [&quot;endpoints&quot;, &quot;services&quot;]</span><br><span class="line">  # Don&#39;t log authenticated requests to certain non-resource URL paths.</span><br><span class="line">  - level: None</span><br><span class="line">    userGroups: [&quot;system:authenticated&quot;]</span><br><span class="line">    nonResourceURLs:</span><br><span class="line">    - &quot;&#x2F;api*&quot; # Wildcard matching.</span><br><span class="line">    - &quot;&#x2F;version&quot;</span><br><span class="line">  # Log the request body of configmap changes in kube-system.</span><br><span class="line">  - level: Request</span><br><span class="line">    resources:</span><br><span class="line">    - group: &quot;&quot; # core API group</span><br><span class="line">      resources: [&quot;configmaps&quot;]</span><br><span class="line">    # This rule only applies to resources in the &quot;kube-system&quot; namespace.</span><br><span class="line">    # The empty string &quot;&quot; can be used to select non-namespaced resources.</span><br><span class="line">    namespaces: [&quot;kube-system&quot;]</span><br><span class="line">  # Log configmap and secret changes in all other namespaces at the Metadata level.</span><br><span class="line">  - level: Metadata</span><br><span class="line">    resources:</span><br><span class="line">    - group: &quot;&quot; # core API group</span><br><span class="line">      resources: [&quot;secrets&quot;, &quot;configmaps&quot;]</span><br><span class="line">  # Log all other resources in core and extensions at the Request level.</span><br><span class="line">  - level: Request</span><br><span class="line">    resources:</span><br><span class="line">    - group: &quot;&quot; # core API group</span><br><span class="line">    - group: &quot;extensions&quot; # Version of group should NOT be included.</span><br><span class="line">  # A catch-all rule to log all other requests at the Metadata level.</span><br><span class="line">  - level: Metadata</span><br><span class="line">    # Long-running requests like watches that fall under this rule will not</span><br><span class="line">    # generate an audit event in RequestReceived.</span><br><span class="line">    omitStages:</span><br><span class="line">      - &quot;RequestReceived&quot;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="创建启动脚本"><a href="#创建启动脚本" class="headerlink" title="创建启动脚本"></a>创建启动脚本</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-apiserver.sh</span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">.&#x2F;kube-apiserver \\</span><br><span class="line">  --apiserver-count 3 \\</span><br><span class="line">  --audit-log-path &#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-apiserver&#x2F;audit-log \\</span><br><span class="line">  --audit-policy-file .&#x2F;conf&#x2F;audit.yaml \\</span><br><span class="line">  --authorization-mode RBAC \\</span><br><span class="line">  --client-ca-file .&#x2F;certs&#x2F;ca.pem \\</span><br><span class="line">  --requestheader-client-ca-file .&#x2F;certs&#x2F;ca.pem \\</span><br><span class="line">  --runtime-config&#x3D;settings.k8s.io&#x2F;v1alpha1&#x3D;true \\</span><br><span class="line">  --enable-admission-plugins NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset \\</span><br><span class="line">  --etcd-cafile .&#x2F;certs&#x2F;ca.pem \\</span><br><span class="line">  --etcd-certfile .&#x2F;certs&#x2F;client.pem \\</span><br><span class="line">  --etcd-keyfile .&#x2F;certs&#x2F;client-key.pem \\</span><br><span class="line">  --etcd-servers https:&#x2F;&#x2F;192.168.70.21:2379,https:&#x2F;&#x2F;192.168.70.22:2379,https:&#x2F;&#x2F;192.168.70.23:2379 \\</span><br><span class="line">  --service-account-key-file .&#x2F;certs&#x2F;ca-key.pem \\</span><br><span class="line">  --service-cluster-ip-range 10.2.0.0&#x2F;16 \\</span><br><span class="line">  --service-node-port-range 3000-29999 \\</span><br><span class="line">  --target-ram-mb&#x3D;1024 \\</span><br><span class="line">  --kubelet-client-certificate .&#x2F;certs&#x2F;client.pem \\</span><br><span class="line">  --kubelet-client-key .&#x2F;certs&#x2F;client-key.pem \\</span><br><span class="line">  --log-dir  &#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-apiserver \\</span><br><span class="line">  --tls-cert-file .&#x2F;certs&#x2F;apiserver.pem \\</span><br><span class="line">  --tls-private-key-file .&#x2F;certs&#x2F;apiserver-key.pem \\</span><br><span class="line">  --v 2</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 调整权限与目录</span><br><span class="line">chmod +x &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-apiserver.sh</span><br><span class="line">mkdir -p &#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-apiserver</span><br></pre></td></tr></table></figure><blockquote><p>####################### 参数说明 #######################<br>  –apiserver-count 2             // apiserver数量<br>  –audit-log-path               // 日志存放位置<br>  –audit-policy-file            // 日志审计规则文件<br>  –authorization-mode     RBAC    // RBAC –基于角色访问的控制<br>  #详细参数说明:  ./kube-apiserver  help</p></blockquote><h3 id="创建supervisor配置"><a href="#创建supervisor配置" class="headerlink" title="创建supervisor配置"></a>创建supervisor配置</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;supervisord.d&#x2F;kube-apiserver.ini</span><br><span class="line">[program:kube-apiserver-70-21]</span><br><span class="line">command&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-apiserver.sh            ; the program (relative uses PATH, can take args)</span><br><span class="line">numprocs&#x3D;1                                                      ; number of processes copies to start (def 1)</span><br><span class="line">directory&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin                            ; directory to cwd to before exec (def no cwd)</span><br><span class="line">autostart&#x3D;true                                                  ; start at supervisord start (default: true)</span><br><span class="line">autorestart&#x3D;true                                                ; retstart at unexpected quit (default: true)</span><br><span class="line">startsecs&#x3D;30                                                    ; number of secs prog must stay running (def. 1)</span><br><span class="line">startretries&#x3D;3                                                  ; max # of serial start failures (default 3)</span><br><span class="line">exitcodes&#x3D;0,2                                                   ; &#39;expected&#39; exit codes for process (default 0,2)</span><br><span class="line">stopsignal&#x3D;QUIT                                                 ; signal used to kill process (default TERM)</span><br><span class="line">stopwaitsecs&#x3D;10                                                 ; max num secs to wait b4 SIGKILL (default 10)</span><br><span class="line">user&#x3D;root                                                       ; setuid to this UNIX account to run the program</span><br><span class="line">redirect_stderr&#x3D;true                                            ; redirect proc stderr to stdout (default false)</span><br><span class="line">stdout_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-apiserver&#x2F;apiserver.stdout.log        ; stderr log path, NONE for none; default AUTO</span><br><span class="line">stdout_logfile_maxbytes&#x3D;64MB                                    ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stdout_logfile_backups&#x3D;4                                        ; # of stdout logfile backups (default 10)</span><br><span class="line">stdout_capture_maxbytes&#x3D;1MB                                     ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stdout_events_enabled&#x3D;false                                     ; emit events on stdout writes (default false)</span><br><span class="line">stopasgroup&#x3D;true                            ;默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程</span><br><span class="line">killasgroup&#x3D;true                            ;默认为false，向进程组发送kill信号，包括子进程</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 修改成适应自己主机的IP地址</span><br><span class="line">sed -i &quot;1s&amp;kube-apiserver-70-21&amp;kube-apiserver-70-$&#123;HOSTNUM&#125;&amp;&quot; &#x2F;etc&#x2F;supervisord.d&#x2F;kube-apiserver.ini</span><br></pre></td></tr></table></figure><h3 id="启动服务并检查"><a href="#启动服务并检查" class="headerlink" title="启动服务并检查"></a>启动服务并检查</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-21 certs]# supervisorctl update</span><br><span class="line">kube-apiserver-70-21: added process group</span><br><span class="line">[root@wang-21 certs]# supervisorctl status</span><br><span class="line">etcd-server-70-21                RUNNING   pid 57939, uptime 1:46:48</span><br><span class="line">kube-apiserver-70-21             STARTING  </span><br><span class="line">[root@wang-21 certs]# netstat -lntup|grep kube-api</span><br><span class="line">tcp        0      0 127.0.0.1:8080          0.0.0.0:*               LISTEN      58186&#x2F;.&#x2F;kube-apiser </span><br><span class="line">tcp6       0      0 :::6443                 :::*                    LISTEN      58186&#x2F;.&#x2F;kube-apiser</span><br></pre></td></tr></table></figure><h3 id="安装部署启动检查另外两台节点"><a href="#安装部署启动检查另外两台节点" class="headerlink" title="安装部署启动检查另外两台节点"></a>安装部署启动检查另外两台节点</h3><h3 id="配置4层反向代理"><a href="#配置4层反向代理" class="headerlink" title="配置4层反向代理"></a>配置4层反向代理</h3><p>操作主机：<code>wang-11, wang-12</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-11 ~]# yum install -y nginx</span><br></pre></td></tr></table></figure><p><strong>配置nginx</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;&gt; &#x2F;etc&#x2F;nginx&#x2F;nginx.conf</span><br><span class="line"># 追加到文件最后</span><br><span class="line">stream &#123;</span><br><span class="line">    upstream kube-apiserver &#123;</span><br><span class="line">        server 192.168.70.21:6443     max_fails&#x3D;3 fail_timeout&#x3D;30s;</span><br><span class="line">        server 192.168.70.22:6443     max_fails&#x3D;3 fail_timeout&#x3D;30s;</span><br><span class="line">        server 192.168.70.23:6443     max_fails&#x3D;3 fail_timeout&#x3D;30s;</span><br><span class="line">    &#125;</span><br><span class="line">    server &#123;</span><br><span class="line">        listen 8443;</span><br><span class="line">        proxy_connect_timeout 2s;</span><br><span class="line">        proxy_timeout 900s;</span><br><span class="line">        proxy_pass kube-apiserver;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>启动nginx</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nginx -t</span><br><span class="line">systemctl start nginx</span><br><span class="line">systemctl enable nginx</span><br></pre></td></tr></table></figure><h2 id="部署keepalive服务"><a href="#部署keepalive服务" class="headerlink" title="部署keepalive服务"></a>部署keepalive服务</h2><p>部署主机：<code>wang-11, wang-12</code></p><h3 id="安装keepalive"><a href="#安装keepalive" class="headerlink" title="安装keepalive"></a>安装keepalive</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-11 ~]# yum -y install keepalived</span><br></pre></td></tr></table></figure><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><h4 id="准备三个文件，探活使用"><a href="#准备三个文件，探活使用" class="headerlink" title="准备三个文件，探活使用"></a>准备三个文件，探活使用</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">token&#x3D;&quot;5c8ffe75f67f5bb326c1f56ac298cea222f25ce7b8dcbf695a3432a564ff0b8f&quot; # 钉钉token</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;keepalived&#x2F;keepalived_check.sh </span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">ip&#x3D;&#96;ifconfig  | grep &#39;inet 192&#39; | awk &#39;&#123;print $2&#125;&#39;&#96;</span><br><span class="line">echo $ip &#96;date&#96; &gt;&gt; &#x2F;data&#x2F;logs&#x2F;keepalive&#x2F;keepalived_check.log</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;keepalived&#x2F;keepalived_master.sh  </span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">ip&#x3D;&#96;ifconfig  | grep &#39;inet 192&#39; | awk &#39;&#123;print $2&#125;&#39;&#96;</span><br><span class="line">curl &quot;https:&#x2F;&#x2F;oapi.dingtalk.com&#x2F;robot&#x2F;send?access_token&#x3D;$token&quot; \</span><br><span class="line">   -H &#39;Content-Type: application&#x2F;json&#39; -d &#39;&#123;&quot;msgtype&quot;: &quot;text&quot;,  &quot;text&quot;: &#123; &quot;content&quot;: &quot;节点&#39;\$ip&#39;的虚拟IP地址192.168.70.10成为master&quot;&#125;&#125;&#39;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;keepalived&#x2F;keepalived_backup.sh </span><br><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">ip&#x3D;&#96;ifconfig  | grep &#39;inet 192&#39; | awk &#39;&#123;print $2&#125;&#39;&#96;</span><br><span class="line">curl &quot;https:&#x2F;&#x2F;oapi.dingtalk.com&#x2F;robot&#x2F;send?access_token&#x3D;$token&quot; \</span><br><span class="line">   -H &#39;Content-Type: application&#x2F;json&#39; -d &#39;&#123;&quot;msgtype&quot;: &quot;text&quot;,  &quot;text&quot;: &#123; &quot;content&quot;: &quot;节点&#39;\$ip&#39;的虚拟IP地址192.168.70.10成为backup&quot;&#125;&#125;&#39;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">chmod +x &#x2F;etc&#x2F;keepalived&#x2F;keepalived_check.sh</span><br><span class="line">chmod +x &#x2F;etc&#x2F;keepalived&#x2F;keepalived_master.sh</span><br><span class="line">chmod +x &#x2F;etc&#x2F;keepalived&#x2F;keepalived_backup.sh</span><br><span class="line">mkdir &#x2F;data&#x2F;logs&#x2F;keepalive&#x2F;</span><br></pre></td></tr></table></figure><h4 id="修改keepalived配置文件"><a href="#修改keepalived配置文件" class="headerlink" title="修改keepalived配置文件"></a>修改keepalived配置文件</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf</span><br><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   notification_email &#123;</span><br><span class="line">     wangzt@od.com</span><br><span class="line">   &#125;</span><br><span class="line">   notification_email_from xueting@od.com</span><br><span class="line">   smtp_server 192.168.70.1  #指定smtp服务器地址</span><br><span class="line">   smtp_connect_timeout 30   #指定smtp连接超时时间</span><br><span class="line">   router_id LVS_DEVEL  #运行keepalived机器的一个标识</span><br><span class="line">   vrrp_skip_check_adv_addr  # 默认是不跳过检查。检查收到的VRRP通告中的所有地址可能会比较耗时，设置此命令的意思是，如果通告与接收的上一个通告来自相同的master路由器，则不执行检查(跳过检查)。</span><br><span class="line">   # vrrp_strict 严格遵守VRRP协议。下列情况将会阻止启动Keepalived：1. 没有VIP地址。2. 单播邻居。3. 在VRRP版本2中有IPv6地址。</span><br><span class="line">   vrrp_garp_interval 0  # 在一个接口发送的两个免费ARP之间的延迟。可以精确到毫秒级。默认是0.</span><br><span class="line">   vrrp_gna_interval 0</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script chk &#123;</span><br><span class="line">     script &quot;&#x2F;etc&#x2F;keepalived&#x2F;keepalived_check.sh&quot; #执行脚本，返回为ture时执行</span><br><span class="line">     interval 10     # 每隔10秒执行一次上面的检测</span><br><span class="line">     weight 5       # 返回值正确，权重加5</span><br><span class="line">     rise 1  #上升1 ？</span><br><span class="line">     fall 2  # 重试两次</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_10 &#123;</span><br><span class="line">    state BACKUP  # 设置节点为主从</span><br><span class="line">    interface ens160  # 网卡名称，必须已存在</span><br><span class="line">    virtual_router_id 10  # 网卡id，keepalive通过此标示确认为一组</span><br><span class="line">    priority 100  # 优先级</span><br><span class="line">    advert_int 5  # 检测间隔</span><br><span class="line">    authentication &#123;  # 认证方式</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 111110</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;  # 虚拟主机IP</span><br><span class="line">        192.168.70.10</span><br><span class="line">    &#125;</span><br><span class="line">    track_script &#123;  # 探活检测</span><br><span class="line">      chk</span><br><span class="line">    &#125;</span><br><span class="line">    notify_master &#x2F;etc&#x2F;keepalived&#x2F;keepalived_master.sh #成为主节点时执行</span><br><span class="line">    notify_backup &#x2F;etc&#x2F;keepalived&#x2F;keepalived_backup.sh #成为备用节点时执行    </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><blockquote><p>注意网卡名称和修改优先级</p></blockquote><h3 id="启动keepalive"><a href="#启动keepalive" class="headerlink" title="启动keepalive"></a>启动keepalive</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-11 ~]# systemctl start keepalived</span><br><span class="line">[root@wang-11 ~]# systemctl enable keepalived</span><br></pre></td></tr></table></figure><h3 id="验证VIP已经绑定端口"><a href="#验证VIP已经绑定端口" class="headerlink" title="验证VIP已经绑定端口"></a>验证VIP已经绑定端口</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-11 ~]# ip a | grep 70.10</span><br><span class="line">    inet 192.168.70.10&#x2F;32 scope global ens160</span><br></pre></td></tr></table></figure><p><strong>这是当节点切换时，我们就能收到告警</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfbupisdvcj30hu05amxy.jpg" alt="钉钉VIP告警"></p><h2 id="部署-kube-controller-manager服务"><a href="#部署-kube-controller-manager服务" class="headerlink" title="部署 kube-controller-manager服务"></a>部署 kube-controller-manager服务</h2><h3 id="集群规划-1"><a href="#集群规划-1" class="headerlink" title="集群规划"></a>集群规划</h3><table><thead><tr><th>主机名</th><th>IP地址</th><th>角色</th></tr></thead><tbody><tr><td>wang-21.host.com</td><td>192.168.70.21</td><td>controller-manager</td></tr><tr><td>wang-22.host.com</td><td>192.168.70.22</td><td>controller-manager</td></tr><tr><td>wang-23.host.com</td><td>192.168.70.23</td><td>controller-manager</td></tr></tbody></table><p>注意：这里部署文档以wang-21主机为例，另外两台主机安装部署方法类似。</p><h3 id="创建服务启动脚本"><a href="#创建服务启动脚本" class="headerlink" title="创建服务启动脚本"></a>创建服务启动脚本</h3><p><code>[root@wang-21 ~]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-controller-manager.sh </span><br><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">.&#x2F;kube-controller-manager \\</span><br><span class="line">  --cluster-cidr 172.16.0.0&#x2F;16 \\</span><br><span class="line">  --leader-elect true \\</span><br><span class="line">  --log-dir &#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-controller-manager \\</span><br><span class="line">  --master http:&#x2F;&#x2F;127.0.0.1:8080 \\</span><br><span class="line">  --service-account-private-key-file .&#x2F;certs&#x2F;ca-key.pem \\</span><br><span class="line">  --service-cluster-ip-range 10.2.0.0&#x2F;16 \\</span><br><span class="line">  --root-ca-file .&#x2F;certs&#x2F;ca.pem \\</span><br><span class="line">  --v 2</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>授权、创建目录</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-21 ~]# chmod +x &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-controller-manager.sh </span><br><span class="line">[root@wang-21 ~]# mkdir -p &#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-controller-manager</span><br></pre></td></tr></table></figure><h3 id="创建supervisor配置-1"><a href="#创建supervisor配置-1" class="headerlink" title="创建supervisor配置"></a>创建supervisor配置</h3><p><code>[root@wang-21 ~]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;supervisord.d&#x2F;kube-controller-manager.ini</span><br><span class="line">[program:kube-controller-manager-70-21]</span><br><span class="line">command&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-controller-manager.sh  ; the program (relative uses PATH, can take args)</span><br><span class="line">numprocs&#x3D;1                                                ; number of processes copies to start (def 1)</span><br><span class="line">directory&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin                      ; directory to cwd to before exec (def no cwd)</span><br><span class="line">autostart&#x3D;true                                            ; start at supervisord start (default: true)</span><br><span class="line">autorestart&#x3D;true                                          ; retstart at unexpected quit (default: true)</span><br><span class="line">startsecs&#x3D;22                                              ; number of secs prog must stay running (def. 1)</span><br><span class="line">startretries&#x3D;3                                            ; max # of serial start failures (default 3)</span><br><span class="line">exitcodes&#x3D;0,2                                             ; &#39;expected&#39; exit codes for process (default 0,2)</span><br><span class="line">stopsignal&#x3D;QUIT                                           ; signal used to kill process (default TERM)</span><br><span class="line">stopwaitsecs&#x3D;10                                           ; max num secs to wait b4 SIGKILL (default 10)</span><br><span class="line">user&#x3D;root                                                 ; setuid to this UNIX account to run the program</span><br><span class="line">redirect_stderr&#x3D;false                                     ; redirect proc stderr to stdout (default false)</span><br><span class="line">stdout_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-controller-manager&#x2F;controll.stdout.log  ; stdout log path, NONE for none; default AUTO</span><br><span class="line">stdout_logfile_maxbytes&#x3D;64MB                            ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stdout_logfile_backups&#x3D;4                                ; # of stdout logfile backups (default 10)</span><br><span class="line">stdout_capture_maxbytes&#x3D;1MB                             ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stdout_events_enabled&#x3D;false                             ; emit events on stdout writes (default false)</span><br><span class="line">stderr_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-controller-manager&#x2F;controll.stderr.log  ; stderr log path, NONE for none; default AUTO</span><br><span class="line">stderr_logfile_maxbytes&#x3D;64MB                            ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stderr_logfile_backups&#x3D;4                                ; # of stderr logfile backups (default 10)</span><br><span class="line">stderr_capture_maxbytes&#x3D;1MB                             ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stderr_events_enabled&#x3D;false                             ; emit events on stderr writes (default false)</span><br><span class="line">stopasgroup&#x3D;true                            ;默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程</span><br><span class="line">killasgroup&#x3D;true                            ;默认为false，向进程组发送kill信号，包括子进程</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">sed -i &quot;1s&amp;kube-controller-manager-70-21&amp;kube-controller-manager-70-$&#123;HOSTNUM&#125;&amp;&quot; &#x2F;etc&#x2F;supervisord.d&#x2F;kube-controller-manager.ini</span><br></pre></td></tr></table></figure><h3 id="启动服务并检查-1"><a href="#启动服务并检查-1" class="headerlink" title="启动服务并检查"></a>启动服务并检查</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-21 ~]# supervisorctl update                                                                                                           </span><br><span class="line">kube-controller-manager-70-21: added process group</span><br><span class="line">[root@wang-21 ~]# supervisorctl status</span><br><span class="line">etcd-server-70-21                RUNNING   pid 57939, uptime 2:51:56</span><br><span class="line">kube-apiserver-70-21             RUNNING   pid 58185, uptime 1:05:13</span><br><span class="line">kube-controller-manager-70-21    STARTING</span><br></pre></td></tr></table></figure><h3 id="安装部署集群其他主机-wang-22-wang-23"><a href="#安装部署集群其他主机-wang-22-wang-23" class="headerlink" title="安装部署集群其他主机(wang-22, wang-23)"></a>安装部署集群其他主机(wang-22, wang-23)</h3><h2 id="部署-kube-scheduler服务"><a href="#部署-kube-scheduler服务" class="headerlink" title="部署 kube-scheduler服务"></a>部署 kube-scheduler服务</h2><h3 id="集群规划-2"><a href="#集群规划-2" class="headerlink" title="集群规划"></a>集群规划</h3><table><thead><tr><th>主机名</th><th>IP地址</th><th>角色</th></tr></thead><tbody><tr><td>wang-21.host.com</td><td>192.168.70.21</td><td>kube-scheduler</td></tr><tr><td>wang-22.host.com</td><td>192.168.70.22</td><td>kube-scheduler</td></tr><tr><td>wang-23.host.com</td><td>192.168.70.23</td><td>kube-scheduler</td></tr></tbody></table><p>注意：这里部署文档以wang-21主机为例，另外两台主机安装部署方法类似。</p><p><code>[root@wang-21 bin]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-scheduler.sh</span><br><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">.&#x2F;kube-scheduler \\</span><br><span class="line">  --leader-elect  \\</span><br><span class="line">  --log-dir &#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-scheduler \\</span><br><span class="line">  --master http:&#x2F;&#x2F;127.0.0.1:8080 \\</span><br><span class="line">  --v 2</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p><strong>授权、创建目录</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chmod +x  &#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-scheduler.sh</span><br><span class="line">mkdir -p &#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-scheduler</span><br></pre></td></tr></table></figure><h3 id="创建supervisor配置-2"><a href="#创建supervisor配置-2" class="headerlink" title="创建supervisor配置"></a>创建supervisor配置</h3><p><code>[root@wang-21 bin]#</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;supervisord.d&#x2F;kube-scheduler.ini</span><br><span class="line">[program:kube-scheduler-70-21]</span><br><span class="line">command&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin&#x2F;kube-scheduler.sh                     ; the program (relative uses PATH, can take args)</span><br><span class="line">numprocs&#x3D;1                                                               ; number of processes copies to start (def 1)</span><br><span class="line">directory&#x3D;&#x2F;opt&#x2F;kubernetes&#x2F;server&#x2F;bin                                     ; directory to cwd to before exec (def no cwd)</span><br><span class="line">autostart&#x3D;true                                                           ; start at supervisord start (default: true)</span><br><span class="line">autorestart&#x3D;true                                                         ; retstart at unexpected quit (default: true)</span><br><span class="line">startsecs&#x3D;22                                                             ; number of secs prog must stay running (def. 1)</span><br><span class="line">startretries&#x3D;3                                                           ; max # of serial start failures (default 3)</span><br><span class="line">exitcodes&#x3D;0,2                                                            ; &#39;expected&#39; exit codes for process (default 0,2)</span><br><span class="line">stopsignal&#x3D;QUIT                                                          ; signal used to kill process (default TERM)</span><br><span class="line">stopwaitsecs&#x3D;10                                                          ; max num secs to wait b4 SIGKILL (default 10)</span><br><span class="line">user&#x3D;root                                                                ; setuid to this UNIX account to run the program</span><br><span class="line">redirect_stderr&#x3D;false                                                    ; redirect proc stderr to stdout (default false)</span><br><span class="line">stdout_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-scheduler&#x2F;scheduler.stdout.log ; stdout log path, NONE for none; default AUTO</span><br><span class="line">stdout_logfile_maxbytes&#x3D;64MB                                             ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stdout_logfile_backups&#x3D;4                                                 ; # of stdout logfile backups (default 10)</span><br><span class="line">stdout_capture_maxbytes&#x3D;1MB                                              ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stdout_events_enabled&#x3D;false                                              ; emit events on stdout writes (default false)</span><br><span class="line">stderr_logfile&#x3D;&#x2F;data&#x2F;logs&#x2F;kubernetes&#x2F;kube-scheduler&#x2F;scheduler.stderr.log ; stderr log path, NONE for none; default AUTO</span><br><span class="line">stderr_logfile_maxbytes&#x3D;64MB                                             ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">stderr_logfile_backups&#x3D;4                                                 ; # of stderr logfile backups (default 10)</span><br><span class="line">stderr_capture_maxbytes&#x3D;1MB                                              ; number of bytes in &#39;capturemode&#39; (default 0)</span><br><span class="line">stderr_events_enabled&#x3D;false                                              ; emit events on stderr writes (default false)</span><br><span class="line">stopasgroup&#x3D;true                            ;默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程</span><br><span class="line">killasgroup&#x3D;true                            ;默认为false，向进程组发送kill信号，包括子进程</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 修改设置自己的主机</span><br><span class="line">sed -i &quot;1s&amp;kube-scheduler-70-21&amp;kube-scheduler-70-$&#123;HOSTNUM&#125;&amp;&quot; &#x2F;etc&#x2F;supervisord.d&#x2F;kube-scheduler.ini</span><br></pre></td></tr></table></figure><h3 id="启动服务并检查-2"><a href="#启动服务并检查-2" class="headerlink" title="启动服务并检查"></a>启动服务并检查</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-21 bin]# supervisorctl update</span><br><span class="line">kube-scheduler-70-21: added process group</span><br><span class="line">[root@wang-21 bin]# supervisorctl status</span><br><span class="line">etcd-server-70-21               RUNNING   pid 52511, uptime 4:08:05</span><br><span class="line">kube-apiserver-70-21            RUNNING   pid 52730, uptime 1:45:06</span><br><span class="line">kube-controller-manager-70-21   RUNNING   pid 52875, uptime 0:08:41</span><br><span class="line">kube-scheduler-70-21            RUNNING   pid 52892, uptime 0:00:31</span><br></pre></td></tr></table></figure><h3 id="安装部署集群其他主机-wang-22-wang-23-1"><a href="#安装部署集群其他主机-wang-22-wang-23-1" class="headerlink" title="安装部署集群其他主机(wang-22,wang-23)"></a>安装部署集群其他主机(wang-22,wang-23)</h3><h3 id="查看主控节点健康状态"><a href="#查看主控节点健康状态" class="headerlink" title="查看主控节点健康状态"></a>查看主控节点健康状态</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-21 bin]# kubectl get cs</span><br><span class="line">NAME                 STATUS    MESSAGE              ERROR</span><br><span class="line">scheduler            Healthy   ok                   </span><br><span class="line">controller-manager   Healthy   ok                   </span><br><span class="line">etcd-1               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;   </span><br><span class="line">etcd-0               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;   </span><br><span class="line">etcd-2               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br></pre></td></tr></table></figure><h2 id="准备k8s用户配置"><a href="#准备k8s用户配置" class="headerlink" title="准备k8s用户配置"></a>准备k8s用户配置</h2><h3 id="准备json文件"><a href="#准备json文件" class="headerlink" title="准备json文件"></a>准备json文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;certs&#x2F;admin-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;cluster-admin&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;wang&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;ops&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="签发证书与私钥"><a href="#签发证书与私钥" class="headerlink" title="签发证书与私钥"></a>签发证书与私钥</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 certs]# cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;client admin-csr.json |cfssl-json -bare admin</span><br><span class="line">2020&#x2F;06&#x2F;02 16:23:28 [INFO] generate received request</span><br><span class="line">2020&#x2F;06&#x2F;02 16:23:28 [INFO] received CSR</span><br><span class="line">2020&#x2F;06&#x2F;02 16:23:28 [INFO] generating key: rsa-2048</span><br><span class="line">2020&#x2F;06&#x2F;02 16:23:29 [INFO] encoded CSR</span><br><span class="line">2020&#x2F;06&#x2F;02 16:23:29 [INFO] signed certificate with serial number 448820123648992987660922165809109462670105492676</span><br><span class="line">2020&#x2F;06&#x2F;02 16:23:29 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable for</span><br><span class="line">websites. For more information see the Baseline Requirements for the Issuance and Management</span><br><span class="line">of Publicly-Trusted Certificates, v.1.1.6, from the CA&#x2F;Browser Forum (https:&#x2F;&#x2F;cabforum.org);</span><br><span class="line">specifically, section 10.2.3 (&quot;Information Requirements&quot;).</span><br><span class="line">[root@wang-200 certs]# ls admin*</span><br><span class="line">admin.csr  admin-csr.json  admin-key.pem  admin.pem</span><br></pre></td></tr></table></figure><h3 id="创建用户"><a href="#创建用户" class="headerlink" title="创建用户"></a>创建用户</h3><p>set-cluster</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl config set-cluster myk8s --certificate-authority&#x3D;&#x2F;opt&#x2F;certs&#x2F;ca.pem --embed-certs&#x3D;true --server&#x3D;https:&#x2F;&#x2F;api.k8s.od.com:8443 --kubeconfig&#x3D;myk8s-config</span><br><span class="line">Cluster &quot;myk8s&quot; set.</span><br></pre></td></tr></table></figure><p>set-credentials</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl config set-credentials cluster-admin --client-certificate&#x3D;&#x2F;opt&#x2F;certs&#x2F;admin.pem --client-key&#x3D;&#x2F;opt&#x2F;certs&#x2F;admin-key.pem --embed-certs&#x3D;true --kubeconfig&#x3D;myk8s-config</span><br><span class="line">User &quot;cluster-admin&quot; set.</span><br></pre></td></tr></table></figure><p>set-context</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl config set-context myk8s-context --cluster&#x3D;myk8s --user&#x3D;cluster-admin --kubeconfig&#x3D;myk8s-config</span><br><span class="line">Context &quot;myk8s-context&quot; created.</span><br></pre></td></tr></table></figure><p>use-context</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl config use-context myk8s-context --kubeconfig&#x3D;myk8s-config</span><br><span class="line">Switched to context &quot;myk8s-context&quot;.</span><br></pre></td></tr></table></figure><p>拷贝文件到家目录</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# cp myk8s-config ~&#x2F;.kube&#x2F;config</span><br></pre></td></tr></table></figure><h3 id="集群角色绑定用户"><a href="#集群角色绑定用户" class="headerlink" title="集群角色绑定用户"></a>集群角色绑定用户</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-21 ~]# kubectl create clusterrolebinding myk8s-admin --clusterrole&#x3D;cluster-admin --user&#x3D;cluster-admin</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io&#x2F;myk8s-admin created</span><br></pre></td></tr></table></figure><h3 id="验证cluster-admin用户"><a href="#验证cluster-admin用户" class="headerlink" title="验证cluster-admin用户"></a>验证cluster-admin用户</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# kubectl get cs                </span><br><span class="line">NAME                 STATUS    MESSAGE              ERROR</span><br><span class="line">scheduler            Healthy   ok                   </span><br><span class="line">controller-manager   Healthy   ok                   </span><br><span class="line">etcd-2               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;   </span><br><span class="line">etcd-0               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;   </span><br><span class="line">etcd-1               Healthy   &#123;&quot;health&quot;: &quot;true&quot;&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 老男孩 </category>
          
          <category> 二进制安装 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 二进制安装 </tag>
            
            <tag> 老男孩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二、k8s-前期环境准备</title>
      <link href="/2020/05/30/%E4%BA%8C%E3%80%81k8s-%E5%89%8D%E6%9C%9F%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/"/>
      <url>/2020/05/30/%E4%BA%8C%E3%80%81k8s-%E5%89%8D%E6%9C%9F%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87/</url>
      
        <content type="html"><![CDATA[<h2 id="部署DNS服务"><a href="#部署DNS服务" class="headerlink" title="部署DNS服务"></a>部署DNS服务</h2><p><code>部署节点wang-12</code></p><h3 id="安装Bind9服务"><a href="#安装Bind9服务" class="headerlink" title="安装Bind9服务"></a>安装Bind9服务</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 ~]# yum install -y bind</span><br></pre></td></tr></table></figure><h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 ~]# cp &#x2F;etc&#x2F;named.conf &#x2F;etc&#x2F;named.conf.bak</span><br><span class="line">[root@wang-12 ~]# vim &#x2F;etc&#x2F;named.conf</span><br><span class="line">12 options &#123;</span><br><span class="line">13         listen-on port 53 &#123; 192.168.70.13; &#125;;    # 监听地址</span><br><span class="line">14         &#x2F;&#x2F;listen-on-v6 port 53 &#123; ::1; &#125;;    # 不监听IV6</span><br><span class="line">21         allow-query     &#123; any; &#125;;            # 允许谁来访问，所有</span><br><span class="line">22         forwarders      &#123; 114.114.114.114; &#125;;    # 上级DNS，添加</span><br><span class="line">33         recursion yes;    # 递归查询</span><br><span class="line">35         dnssec-enable no;    # DNS安全扩展</span><br><span class="line">36         dnssec-validation no;</span><br></pre></td></tr></table></figure><p><strong>检查配置文件</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 ~]# named-checkconf</span><br></pre></td></tr></table></figure><p>没有任何输出，说明配置文件无误</p><h3 id="修改区域配置文件"><a href="#修改区域配置文件" class="headerlink" title="修改区域配置文件"></a>修改区域配置文件</h3><p>添加主机域名<code>host.com</code>，业务域<code>od.com</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 ~]# cp &#x2F;etc&#x2F;named.rfc1912.zones &#x2F;etc&#x2F;named.rfc1912.zones.bak</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt;&gt; &#x2F;etc&#x2F;named.rfc1912.zones</span><br><span class="line"># 文件结尾添加以下内容</span><br><span class="line">zone &quot;host.com&quot; IN &#123;</span><br><span class="line">        type master;</span><br><span class="line">        file &quot;host.com.zone&quot;;</span><br><span class="line">        allow-update &#123; 192.168.70.12; &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">zone &quot;od.com&quot; IN &#123;</span><br><span class="line">        type master;</span><br><span class="line">        file &quot;od.com.zone&quot;;</span><br><span class="line">        allow-update &#123; 192.168.70.12; &#125;;</span><br><span class="line">&#125;;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="添加区域数据文件"><a href="#添加区域数据文件" class="headerlink" title="添加区域数据文件"></a>添加区域数据文件</h3><p><strong>主机域配置文件</strong></p><p><code>vim /var/named/host.com.zone</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ORIGIN host.com.</span><br><span class="line">$TTL 600  ; 10minutes</span><br><span class="line">@    IN   SOA   dns.host.com.  dnsadmin.host.com. (</span><br><span class="line">                2019122301    ; serial</span><br><span class="line">                10800         ; refresh (3 hours)</span><br><span class="line">                900           ; retry (15 minutes)</span><br><span class="line">                604800        ; expire (1 week)</span><br><span class="line">                86400         ; minimum (1 day)</span><br><span class="line">                )</span><br><span class="line">                NS   dns.host.com.</span><br><span class="line">$TTL 60 ; 1 minute</span><br><span class="line">dns             A       192.168.70.12</span><br><span class="line">wang-200        A       192.168.70.200</span><br><span class="line">wang-11         A       192.168.70.11</span><br><span class="line">wang-12         A       192.168.70.12</span><br><span class="line">wang-21         A       192.168.70.21</span><br><span class="line">wang-22         A       192.168.70.22</span><br><span class="line">wang-23         A       192.168.70.23</span><br><span class="line">wang-24         A       192.168.70.24</span><br></pre></td></tr></table></figure><p><strong>业务域配置文件</strong></p><p><code>vim /var/named/od.com.zone</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ORIGIN od.com.</span><br><span class="line">$TTL 600  ; 10minutes</span><br><span class="line">@    IN   SOA   od.com.com.  dnsadmin.od.com. (</span><br><span class="line">                2019122301    ; serial</span><br><span class="line">                10800         ; refresh (3 hours)</span><br><span class="line">                900           ; retry (15 minutes)</span><br><span class="line">                604800        ; expire (1 week)</span><br><span class="line">                86400         ; minimum (1 day)</span><br><span class="line">                )</span><br><span class="line">                NS   dns.od.com.</span><br><span class="line">$TTL 60 ; 1 minute</span><br><span class="line">dns             A       192.168.70.12</span><br><span class="line">api.k8s         A       192.168.70.10</span><br><span class="line">*               A       192.168.70.10</span><br></pre></td></tr></table></figure><h3 id="启动-bind9服务"><a href="#启动-bind9服务" class="headerlink" title="启动 bind9服务"></a>启动 bind9服务</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 ~]# systemctl start named</span><br><span class="line">[root@wang-12 ~]# systemctl enable named</span><br><span class="line">Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;multi-user.target.wants&#x2F;named.service to &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;named.service.</span><br><span class="line">[root@wang-12 ~]# netstat -lntup|grep -w 53</span><br><span class="line">tcp        0      0 192.168.70.12:53        0.0.0.0:*               LISTEN      51102&#x2F;named         </span><br><span class="line">udp        0      0 192.168.70.12:53        0.0.0.0:*                           51102&#x2F;named</span><br></pre></td></tr></table></figure><h3 id="检查DNS可用性"><a href="#检查DNS可用性" class="headerlink" title="检查DNS可用性"></a>检查DNS可用性</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 ~]# dig -t A wang-11.host.com @192.168.70.12 +short  </span><br><span class="line">192.168.70.11</span><br><span class="line">[root@wang-12 ~]# dig -t A api.k8s.od.com @192.168.70.12 +short     </span><br><span class="line">192.168.70.10</span><br></pre></td></tr></table></figure><h3 id="配置DNS客户端"><a href="#配置DNS客户端" class="headerlink" title="配置DNS客户端"></a>配置DNS客户端</h3><p><strong>所有主机都需要修改</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# ansible all -m shell -a &quot;sed -i &#39;s&amp;DNS1&#x3D;114.114.114.114&amp;DNS1&#x3D;192.168.70.12&amp;&#39; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens160 &amp;&amp; systemctl restart network &quot;</span><br></pre></td></tr></table></figure><p><strong>检测结果</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 ~]# cat &#x2F;etc&#x2F;resolv.conf</span><br><span class="line"># Generated by NetworkManager</span><br><span class="line">search host.com</span><br><span class="line">nameserver 192.168.70.12</span><br></pre></td></tr></table></figure><p><strong>测试可用性</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 ~]# ping -c3 wang-11</span><br><span class="line">PING wang-11.host.com (192.168.70.11) 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.70.11 (192.168.70.11): icmp_seq&#x3D;1 ttl&#x3D;64 time&#x3D;0.318 ms</span><br><span class="line"></span><br><span class="line">[root@wang-12 ~]# ping -c3 www.baidu.com</span><br><span class="line">PING www.a.shifen.com (180.101.49.12) 56(84) bytes of data.</span><br><span class="line">64 bytes from 180.101.49.12 (180.101.49.12): icmp_seq&#x3D;1 ttl&#x3D;51 time&#x3D;20.8 ms</span><br></pre></td></tr></table></figure><h2 id="部署签发证书环境"><a href="#部署签发证书环境" class="headerlink" title="部署签发证书环境"></a>部署签发证书环境</h2><p><code>部署主机：wang-200</code></p><h3 id="安装CFSSL"><a href="#安装CFSSL" class="headerlink" title="安装CFSSL"></a>安装CFSSL</h3><p>证书签发工具CFSSL: R1.2</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssl_linux-amd64 -O &#x2F;usr&#x2F;bin&#x2F;cfssl</span><br><span class="line">wget https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssljson_linux-amd64 -O &#x2F;usr&#x2F;bin&#x2F;cfssl-json</span><br><span class="line">wget https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssl-certinfo_linux-amd64  -O  &#x2F;usr&#x2F;bin&#x2F;cfssl-certinfo</span><br><span class="line">chmod +x &#x2F;usr&#x2F;bin&#x2F;cfssl &#x2F;usr&#x2F;bin&#x2F;cfssl-json &#x2F;usr&#x2F;bin&#x2F;cfssl-certinfo</span><br></pre></td></tr></table></figure><h3 id="创建生成CA证书签名请求-csr-的json配置文件"><a href="#创建生成CA证书签名请求-csr-的json配置文件" class="headerlink" title="创建生成CA证书签名请求(csr)的json配置文件"></a>创建生成CA证书签名请求(csr)的json配置文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir &#x2F;opt&#x2F;certs</span><br><span class="line">[root@wang-200 ~]# cd &#x2F;opt&#x2F;certs&#x2F;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;opt&#x2F;certs&#x2F;ca-csr.json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;kubernetes-ca&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">            &quot;ST&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;L&quot;: &quot;beijing&quot;,</span><br><span class="line">            &quot;O&quot;: &quot;wang&quot;,</span><br><span class="line">            &quot;OU&quot;: &quot;ops&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;ca&quot;: &#123;</span><br><span class="line">        &quot;expiry&quot;: &quot;175200h&quot;       </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><div class="note primary">            <p>⚠️：证书的有效时间设置长一点，我的是20年</p>          </div><div class="note info">            <p>CN: Common Name，浏览器使用该字段验证网站是否合法，一般写的是域名。非常重要。浏览器使用该字段验证网站是否合法<br>C: Country， 国家<br>ST: State，州，省<br>L: Locality，地区，城市<br>O: Organization Name，组织名称，公司名称<br>OU: Organization Unit Name，组织单位名称，公司部门</p>          </div><h3 id="生成CA证书和私钥"><a href="#生成CA证书和私钥" class="headerlink" title="生成CA证书和私钥"></a>生成CA证书和私钥</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 certs]# cfssl gencert -initca ca-csr.json |cfssl-json -bare ca</span><br><span class="line">[root@wang-200 certs]# ls</span><br><span class="line">ca.csr  ca-csr.json  ca-key.pem #根证书私钥 ca.pem #根证书</span><br></pre></td></tr></table></figure><h2 id="部署docker镜像私有仓库harbor"><a href="#部署docker镜像私有仓库harbor" class="headerlink" title="部署docker镜像私有仓库harbor"></a>部署docker镜像私有仓库harbor</h2><p>部署主机：<code>wang-200</code></p><h3 id="下载软件并解压"><a href="#下载软件并解压" class="headerlink" title="下载软件并解压"></a>下载软件并解压</h3><blockquote><p>harbor官网github地址：<a href="https://github.com/goharbor/harbor" target="_blank" rel="noopener">https://github.com/goharbor/harbor</a></p></blockquote><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir -p &#x2F;data&#x2F;soft&#x2F;docker</span><br><span class="line">[root@wang-200 ~]# wget -O &#x2F;data&#x2F;soft&#x2F;docker&#x2F;harbor-offline-installer-v1.8.3.tgz https:&#x2F;&#x2F;storage.googleapis.com&#x2F;harbor-releases&#x2F;release-1.8.0&#x2F;harbor-offline-installer-v1.8.3.tgz # 较慢</span><br><span class="line"># https:&#x2F;&#x2F;codeload.github.com&#x2F;goharbor&#x2F;harbor&#x2F;tar.gz&#x2F;v1.8.3 在线安装包，自己摸索安装</span><br></pre></td></tr></table></figure><p><strong>解压并创建软连接</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# tar xf &#x2F;data&#x2F;soft&#x2F;docker&#x2F;harbor-offline-installer-v1.8.3.tgz -C &#x2F;opt&#x2F;</span><br><span class="line">[root@wang-200 ~]# mv &#x2F;opt&#x2F;harbor&#x2F; &#x2F;opt&#x2F;harbor-v1.8.3</span><br><span class="line">[root@wang-200 ~]# ln -s &#x2F;opt&#x2F;harbor-v1.8.3&#x2F; &#x2F;opt&#x2F;harbor</span><br></pre></td></tr></table></figure><h3 id="修改-harbor配置文件"><a href="#修改-harbor配置文件" class="headerlink" title="修改 harbor配置文件"></a>修改 harbor配置文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# cp &#x2F;opt&#x2F;harbor&#x2F;harbor.yml &#x2F;opt&#x2F;harbor&#x2F;harbor.yml.bak</span><br><span class="line">[root@wang-200 ~]# vim &#x2F;opt&#x2F;harbor&#x2F;harbor.yml</span><br><span class="line">[root@zzgw7-200 ~]# egrep -nv &#39;^$|#&#39; &#x2F;opt&#x2F;harbor&#x2F;harbor.yml</span><br><span class="line"> 5:hostname: harbor.od.com# 配置域名访问</span><br><span class="line">10:  port: 180# 修改服务端口,防止端口冲突</span><br><span class="line">27:harbor_admin_password: Harbor12345# harbor登录密码</span><br><span class="line">35:data_volume: &#x2F;data&#x2F;harbor# harbor数据存放路径</span><br><span class="line">82:  location: &#x2F;data&#x2F;harbor&#x2F;logs# harbor日志存放路径</span><br></pre></td></tr></table></figure><h3 id="安装-docker-compose1-25-5"><a href="#安装-docker-compose1-25-5" class="headerlink" title="安装 docker-compose1.25.5"></a>安装 docker-compose1.25.5</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y python3-pip</span><br><span class="line">pip3 install -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple pip -U</span><br><span class="line">pip3 install docker-compose -i https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple</span><br><span class="line">docker-compose -version</span><br></pre></td></tr></table></figure><h3 id="安装-Harbor"><a href="#安装-Harbor" class="headerlink" title="安装 Harbor"></a>安装 Harbor</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# cd &#x2F;opt&#x2F;harbor</span><br><span class="line">[root@wang-200 harbor]# .&#x2F;install.sh</span><br></pre></td></tr></table></figure><h3 id="查看-Harbor启动状态"><a href="#查看-Harbor启动状态" class="headerlink" title="查看 Harbor启动状态"></a>查看 Harbor启动状态</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">[root@wang-200 harbor]<span class="comment"># docker-compose ps</span></span><br><span class="line">      Name                     Command                       State                     Ports          </span><br><span class="line">------------------------------------------------------------------------------------------------------</span><br><span class="line">harbor-core         /harbor/start.sh                 Up (health: starting)                            </span><br><span class="line">harbor-db           /entrypoint.sh postgres          Up (health: starting)   5432/tcp                 </span><br><span class="line">harbor-jobservice   /harbor/start.sh                 Up                                               </span><br><span class="line">harbor-log          /bin/sh -c /usr/<span class="built_in">local</span>/bin/ ...   Up (health: starting)   127.0.0.1:1514-&gt;10514/tcp</span><br><span class="line">harbor-portal       nginx -g daemon off;             Up (health: starting)   80/tcp                   </span><br><span class="line">nginx               nginx -g daemon off;             Up (health: starting)   0.0.0.0:180-&gt;80/tcp      </span><br><span class="line">redis               docker-entrypoint.sh redis ...   Up                      6379/tcp                 </span><br><span class="line">registry            /entrypoint.sh /etc/regist ...   Up (health: starting)   5000/tcp                 </span><br><span class="line">registryctl         /harbor/start.sh                 Up (health: starting)</span><br></pre></td></tr></table></figure><h3 id="配置-harbor的DNS内网解析"><a href="#配置-harbor的DNS内网解析" class="headerlink" title="配置 harbor的DNS内网解析"></a>配置 harbor的DNS内网解析</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># sed &#39;&#x2F;^\*&#x2F;i harbor\t\tA\t192.168.70.200\t; 添加harbor的A记录&#39; &#x2F;var&#x2F;named&#x2F;od.com.zone </span><br><span class="line">[root@wang-12 ~]# vim &#x2F;var&#x2F;named&#x2F;od.com.zone</span><br><span class="line">harbor          A       192.168.70.200  ; 添加harbor的A记录</span><br></pre></td></tr></table></figure><p><strong>重启named服务并验证</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-12 ~]# systemctl restart named    </span><br><span class="line">[root@wang-12 ~]# dig -t A harbor.od.com +short</span><br><span class="line">192.168.70.200</span><br></pre></td></tr></table></figure><h3 id="配置nginx-代理harbor"><a href="#配置nginx-代理harbor" class="headerlink" title="配置nginx 代理harbor"></a>配置nginx 代理harbor</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 soft]# mkdir &#x2F;data&#x2F;logs&#x2F;nginx</span><br></pre></td></tr></table></figure><p><code>vim /etc/nginx/conf.d/harbor.od.com.conf</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name harbor.od.com;</span><br><span class="line">    client_max_body_size 1000m;</span><br><span class="line">    access_log &#x2F;data&#x2F;logs&#x2F;nginx&#x2F;harbor.od.com.log;</span><br><span class="line"></span><br><span class="line">    location &#x2F; &#123;</span><br><span class="line">        proxy_pass http:&#x2F;&#x2F;127.0.0.1:180;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>重启 nginx</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nginx -t</span><br><span class="line">nginx -s reload</span><br></pre></td></tr></table></figure><h3 id="Web-访问登录"><a href="#Web-访问登录" class="headerlink" title="Web 访问登录"></a>Web 访问登录</h3><p>默认登录名：<code>admin</code>     默认密码：<code>Harbor12345</code></p><p>密码可以在harbor.yml中修改</p><img src="http://wangzhangtao.com/img/kubernetes/install/20200331230312971.png" alt="habor登录界面" style="zoom:67%;max-width: 60%;" /><p><strong>创建新项目</strong></p><img src="http://wangzhangtao.com/img/kubernetes/install/20200331230353366.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2lzYV9JSQ==,size_16,color_FFFFFF,t_70" alt="创建项目" style="zoom:67%;max-width: 60%;" /><h3 id="上传镜像测试"><a href="#上传镜像测试" class="headerlink" title="上传镜像测试"></a>上传镜像测试</h3><p><strong>登录harbor仓库</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# docker login harbor.od.com</span><br><span class="line">Username: admin#用户名</span><br><span class="line">Password:         #登录密码</span><br><span class="line">WARNING! Your password will be stored unencrypted in &#x2F;root&#x2F;.docker&#x2F;config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https:&#x2F;&#x2F;docs.docker.com&#x2F;engine&#x2F;reference&#x2F;commandline&#x2F;login&#x2F;#credentials-store</span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br></pre></td></tr></table></figure><p><strong>下载测试镜像并打给镜像打一个tag</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker pull nginx:1.7.9</span><br><span class="line">docker tag nginx:1.7.9 harbor.od.com&#x2F;public&#x2F;nginx:v1.7.9</span><br><span class="line">docker push harbor.od.com&#x2F;public&#x2F;nginx:v1.7.9</span><br></pre></td></tr></table></figure><p><strong>web 页面检查</strong></p><img src="https://img-blog.csdnimg.cn/20200331230447672.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2lzYV9JSQ==,size_16,color_FFFFFF,t_70" alt="web页面检查" style="zoom:67%;max-width: 60%;" /><h2 id="部署nfs服务"><a href="#部署nfs服务" class="headerlink" title="部署nfs服务"></a>部署nfs服务</h2><h3 id="安装-nfs-服务器所需的软件包"><a href="#安装-nfs-服务器所需的软件包" class="headerlink" title="安装 nfs 服务器所需的软件包"></a>安装 nfs 服务器所需的软件包</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# yum install -y nfs-utils</span><br></pre></td></tr></table></figure><h3 id="修改配置文件-1"><a href="#修改配置文件-1" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# mkdir -p &#x2F;data&#x2F;nfs&#x2F;v1  </span><br><span class="line">[root@wang-200 ~]# vim &#x2F;etc&#x2F;exports</span><br><span class="line">&#x2F;data&#x2F;nfs&#x2F;v1 192.168.70.1&#x2F;24(insecure,rw,sync,no_root_squash)  # 免密可读写</span><br></pre></td></tr></table></figure><h3 id="启动nfs服务"><a href="#启动nfs服务" class="headerlink" title="启动nfs服务"></a>启动nfs服务</h3><p>运行nfs服务，还需要rpcbind的支持</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl start rpcbind</span><br><span class="line">systemctl start nfs-server</span><br><span class="line">systemctl enable rpcbind</span><br><span class="line">systemctl enable nfs-server</span><br><span class="line">exportfs -r  # 重新挂载&#x2F;etc&#x2F;exports中的设置，此外同步更新&#x2F;etc&#x2F;exports及&#x2F;var&#x2F;lib&#x2F;nfs&#x2F;xtab中的内容</span><br></pre></td></tr></table></figure><h3 id="检查nfs服务是否可用"><a href="#检查nfs服务是否可用" class="headerlink" title="检查nfs服务是否可用"></a>检查nfs服务是否可用</h3><p><strong>检查是否生效</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# exportfs</span><br><span class="line">&#x2F;data&#x2F;nfs&#x2F;v1    192.168.70.1&#x2F;24</span><br></pre></td></tr></table></figure><p><strong>客户端测试（wang-21）</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-21 ~]# yum install -y nfs-utils</span><br><span class="line">[root@wang-21 ~]# showmount -e 192.168.70.200   </span><br><span class="line">Export list for 192.168.70.200:</span><br><span class="line">&#x2F;data&#x2F;nfs&#x2F;v1 192.168.70.1&#x2F;24</span><br><span class="line">[root@wang-21 ~]# mkdir &#x2F;tmp&#x2F;testnfs</span><br><span class="line">[root@wang-21 ~]# mount -t nfs 192.168.70.200:&#x2F;data&#x2F;nfs&#x2F;v1 &#x2F;tmp&#x2F;testnfs   </span><br><span class="line">[root@wang-21 ~]# echo &quot;hello nfs server&quot; &gt; &#x2F;tmp&#x2F;testnfs&#x2F;hello</span><br></pre></td></tr></table></figure><p><strong>回到wang-200上查验</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# cat &#x2F;data&#x2F;nfs&#x2F;v1&#x2F;hello </span><br><span class="line">hello nfs server</span><br></pre></td></tr></table></figure><h3 id="在所有需要的主机上安装nfs"><a href="#在所有需要的主机上安装nfs" class="headerlink" title="在所有需要的主机上安装nfs"></a>在所有需要的主机上安装nfs</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 jenkins]# ansible all -m shell -a &quot;yum install nfs-utils -y&quot;</span><br></pre></td></tr></table></figure><h2 id="部署-docker环境"><a href="#部署-docker环境" class="headerlink" title="部署 docker环境"></a>部署 docker环境</h2><p><code>部署主机：wang-200,wang-21, wang-22,wang-23</code></p><h3 id="创建安装docker的执行文件"><a href="#创建安装docker的执行文件" class="headerlink" title="创建安装docker的执行文件"></a>创建安装docker的执行文件</h3><p><code>vim /data/shell/install_docker.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装docker服务</span><br><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager --add-repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo</span><br><span class="line">yum -y install docker-ce-cli-18.09.3 docker-ce-18.09.3 # docker-ce-18.06.0.ce -y</span><br><span class="line">sleep 3</span><br><span class="line"></span><br><span class="line"># 配置docker参数，注意不同主机，docker bip不同</span><br><span class="line">mkdir &#x2F;etc&#x2F;docker&#x2F;</span><br><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;graph&quot;: &quot;&#x2F;data&#x2F;docker&quot;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay&quot;,</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;tpd9v3c5.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;insecure-registries&quot;: [&quot;registry.access.redhat.com&quot;,&quot;quay.io&quot;,&quot;harbor.od.com&quot;],</span><br><span class="line">  &quot;bip&quot;: &quot;172.16.200.1&#x2F;24&quot;,</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],</span><br><span class="line">  &quot;live-restore&quot;: true</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sed -i &quot;s&amp;172.16.200.1&#x2F;24&amp;172.16.$&#123;HOSTNUM&#125;.1&#x2F;24&amp;&quot; &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line"></span><br><span class="line"># 启动docker</span><br><span class="line">systemctl start docker</span><br><span class="line"># 添加docker开机自启</span><br><span class="line">systemctl enable docker</span><br><span class="line"></span><br><span class="line"># 查看docker版本号</span><br><span class="line">docker version</span><br></pre></td></tr></table></figure><h3 id="配置执行权限并执行"><a href="#配置执行权限并执行" class="headerlink" title="配置执行权限并执行"></a>配置执行权限并执行</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# chmod +x &#x2F;data&#x2F;shell&#x2F;install_docker.sh</span><br><span class="line">[root@wang-200 ~]# &#x2F;data&#x2F;shell&#x2F;install_docker.sh</span><br><span class="line">[root@wang-200 ~]# ansible worker -m shell -a &quot;curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_docker.sh | sh&quot;</span><br></pre></td></tr></table></figure><h2 id="部署supervisor"><a href="#部署supervisor" class="headerlink" title="部署supervisor"></a>部署supervisor</h2><p>Supervisor是用Python开发的一个client/server服务，是Linux/Unix系统下的一个进程管理工具，不支持Windows系统。它可以很方便的监听、启动、停止、重启一个或多个进程。用Supervisor管理的进程，当一个进程意外被杀死，supervisort监听到进程死后，会自动将它重新拉起，很方便的做到进程自动恢复的功能</p><h3 id="创建安装supervisor执行文件"><a href="#创建安装supervisor执行文件" class="headerlink" title="创建安装supervisor执行文件"></a>创建安装supervisor执行文件</h3><p><code>vim /data/shell/install_supervisor.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y supervisor</span><br><span class="line">systemctl start supervisord.service</span><br><span class="line">systemctl enable supervisord.service</span><br></pre></td></tr></table></figure><h3 id="到服务器执行脚本"><a href="#到服务器执行脚本" class="headerlink" title="到服务器执行脚本"></a>到服务器执行脚本</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# ansible worker -m shell -a &quot;curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_supervisor.sh | sh&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 老男孩 </category>
          
          <category> 二进制安装 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 二进制安装 </tag>
            
            <tag> 老男孩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一、k8s-实验环境说明</title>
      <link href="/2020/05/28/%E4%B8%80%E3%80%81k8s-%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83%E8%AF%B4%E6%98%8E/"/>
      <url>/2020/05/28/%E4%B8%80%E3%80%81k8s-%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83%E8%AF%B4%E6%98%8E/</url>
      
        <content type="html"><![CDATA[<h2 id="文档说明"><a href="#文档说明" class="headerlink" title="文档说明"></a>文档说明</h2><p>本文档为kubernetes二进制安装方式，以老男孩k8s一期培训课程为基础模版，增加了一个k8s控制节点(机器上也有工作节点)，并有少量改动。</p><h3 id="主要参考文档："><a href="#主要参考文档：" class="headerlink" title="主要参考文档："></a>主要参考文档：</h3><ul><li><a href="https://www.jianshu.com/p/ad36d2ad0df0" target="_blank" rel="noopener">OneLpc二进制安装K8S集群</a></li><li><a href="https://www.cnblogs.com/slim-liu/p/11953318.html" target="_blank" rel="noopener">slim_liu的k8s交付实战</a></li><li><a href="https://www.wqblogs.com/2020/01/08/%E9%83%A8%E7%BD%B2kibana/" target="_blank" rel="noopener">虫子k8s生态实战</a></li></ul><h2 id="实验架构图"><a href="#实验架构图" class="headerlink" title="实验架构图"></a>实验架构图</h2><img src="http://wangzhangtao.com/img/kubernetes/install/2020033123005972.png" alt="实验架构图" style="zoom:67%;max-width:80%;" /><h2 id="实验主机说明"><a href="#实验主机说明" class="headerlink" title="实验主机说明"></a>实验主机说明</h2><p>操作系统：7.6.1810； 内核：3.10.0</p><table><thead><tr><th>主机名</th><th>IP地址</th><th>角色</th><th>硬件配置</th></tr></thead><tbody><tr><td>wang-200.host.com (zzgw7-200)</td><td>192.168.70.200</td><td>运维节点</td><td>4核8G-100G</td></tr><tr><td>wang-11.host.com (zzgw7-11)</td><td>192.168.70.11</td><td>k8s代理节点</td><td>2核4G-100G</td></tr><tr><td>wang-12.host.com (zzgw7-12)</td><td>192.168.70.12</td><td>k8s代理节点</td><td>2核4G-100G</td></tr><tr><td>wang-21.host.com (zzgw7-21)</td><td>192.168.70.21</td><td>k8s控制节点</td><td>2核4G-100G</td></tr><tr><td>wang-22.host.com (zzgw7-22)</td><td>192.168.70.22</td><td>k8s控制节点</td><td>2核4G-100G</td></tr><tr><td>wang-23.host.com (zzgw7-21)</td><td>192.168.70.23</td><td>k8s控制节点, 工作节点</td><td>2核4G-100G</td></tr><tr><td>wang-24.host.com (zzgw7-22)</td><td>192.168.70.24</td><td>k8s工作节点</td><td>4核8G-100G</td></tr></tbody></table><h2 id="通过VMWare创建虚拟主机"><a href="#通过VMWare创建虚拟主机" class="headerlink" title="通过VMWare创建虚拟主机"></a>通过VMWare创建虚拟主机</h2><p>wang-11, wang-12,wang-21, wang-22, wang-23, wang-24</p><h2 id="nginx运维仓库"><a href="#nginx运维仓库" class="headerlink" title="nginx运维仓库"></a>nginx运维仓库</h2><h3 id="安装nginx"><a href="#安装nginx" class="headerlink" title="安装nginx"></a>安装nginx</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum -y install nginx</span><br></pre></td></tr></table></figure><p>在wang-200服务器上创建本地安装包和安装文档，方便以后工作</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;doc.od.com.conf </span><br><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  doc.od.com;</span><br><span class="line">    index index.html index.htm index.jsp;</span><br><span class="line">    root &#x2F;data&#x2F;k8s;</span><br><span class="line"></span><br><span class="line">    location &#x2F;shell &#123;</span><br><span class="line">        autoindex on;</span><br><span class="line">        autoindex_exact_size off;</span><br><span class="line">        default_type text&#x2F;plain;</span><br><span class="line">        alias &#x2F;data&#x2F;shell&#x2F;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location &#x2F;soft &#123;</span><br><span class="line">        autoindex on;</span><br><span class="line">        autoindex_exact_size off;</span><br><span class="line">        alias &#x2F;data&#x2F;soft&#x2F;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    access_log &#x2F;data&#x2F;logs&#x2F;nginx&#x2F;doc.log;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="启动-nginx"><a href="#启动-nginx" class="headerlink" title="启动 nginx"></a>启动 nginx</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nginx -t</span><br><span class="line">systemctl start nginx</span><br><span class="line">systemctl enable nginx</span><br></pre></td></tr></table></figure><h2 id="修改wang-200运维节点系统配置"><a href="#修改wang-200运维节点系统配置" class="headerlink" title="修改wang-200运维节点系统配置"></a>修改wang-200运维节点系统配置</h2><p><strong>修改默认密码</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# echo wang123 | passwd --stdin root</span><br><span class="line">[root@localhost ~]# mkdir &#x2F;data&#x2F;shell</span><br></pre></td></tr></table></figure><p><strong>创建文件，方便以后使用</strong></p><p><code>vim /data/shell/modify_server.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改主机名</span><br><span class="line">hostip&#x3D;&#96;ifconfig ens160 | grep 192 | awk &#39;&#123;print $2&#125;&#39;| awk -F&#39;.&#39; &#39;&#123;print $4&#125;&#39; &#96;; hostnamectl set-hostname wang-$hostip.host.com</span><br><span class="line"></span><br><span class="line"># 配置阿里yum源</span><br><span class="line">mkdir &#x2F;etc&#x2F;yum.repos.d&#x2F;bak &amp;&amp; mv &#x2F;etc&#x2F;yum.repos.d&#x2F;*.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;bak</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo</span><br><span class="line"></span><br><span class="line"># 关闭SElinux</span><br><span class="line">sed -i &#39;s&#x2F;SELINUX&#x3D;enforcing&#x2F;SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">setenforce 0</span><br><span class="line"></span><br><span class="line"># 关闭firewalld</span><br><span class="line">systemctl disable firewalld.service  </span><br><span class="line">systemctl stop firewalld.service</span><br><span class="line"></span><br><span class="line"># 安装必要工具</span><br><span class="line">yum install -y wget net-tools telnet tree nmap sysstat lrzsz dos2unix </span><br><span class="line"></span><br><span class="line"># 创建日志目录</span><br><span class="line">mkdir -p &#x2F;data&#x2F;logs &#x2F;data&#x2F;soft</span><br><span class="line"></span><br><span class="line"># 添加两个变量，方便以后使用</span><br><span class="line">HOSTIP&#x3D;&#96;hostname -i | grep -o &#39;192.168.70.[0-9]\&#123;1,3\&#125;&#39; &#96;</span><br><span class="line">HOSTNUM&#x3D;$&#123;HOSTIP##*.&#125;</span><br><span class="line">echo $HOSTIP $HOSTNUM</span><br><span class="line">echo -e &quot;export HOSTIP&#x3D;$HOSTIP \nexport HOSTNUM&#x3D;$HOSTNUM&quot; &gt;&gt; &#x2F;etc&#x2F;bashrc</span><br><span class="line">source &#x2F;etc&#x2F;bashrc</span><br></pre></td></tr></table></figure><p>执行配置文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# chmod +x &#x2F;data&#x2F;shell&#x2F;modify_server.sh</span><br><span class="line">[root@localhost ~]# &#x2F;data&#x2F;shell&#x2F;modify_server.sh  # 方法一</span><br><span class="line">[root@localhost ~]# curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;modify_server.sh | sh  # 方法二</span><br></pre></td></tr></table></figure><h2 id="使用ansible来进行统一管理"><a href="#使用ansible来进行统一管理" class="headerlink" title="使用ansible来进行统一管理"></a>使用ansible来进行统一管理</h2><h3 id="创建一个-SSH-key，用来管理其他服务器"><a href="#创建一个-SSH-key，用来管理其他服务器" class="headerlink" title="创建一个 SSH key，用来管理其他服务器"></a>创建一个 SSH key，用来管理其他服务器</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-19 ~]# ssh-keygen -t rsa -C &quot;wang@qq.com&quot;</span><br></pre></td></tr></table></figure><p>将公钥拷贝到其他管理服务器，比如192.168.70.11</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# ssh-copy-id 192.168.70.11</span><br><span class="line">&#x2F;usr&#x2F;bin&#x2F;ssh-copy-id: INFO: Source of key(s) to be installed: &quot;&#x2F;root&#x2F;.ssh&#x2F;id_rsa.pub&quot;</span><br><span class="line">The authenticity of host &#39;192.168.70.11 (192.168.70.11)&#39; can&#39;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:uAnxlYCLTfdyOL0yA5xwDgSN1V&#x2F;WmyjwJqvixpsFoJ4.</span><br><span class="line">ECDSA key fingerprint is MD5:47:f9:99:e4:3a:30:2b:05:37:b3:29:07:60:57:ef:4d.</span><br><span class="line">Are you sure you want to continue connecting (yes&#x2F;no)? yes</span><br><span class="line">&#x2F;usr&#x2F;bin&#x2F;ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed</span><br><span class="line">&#x2F;usr&#x2F;bin&#x2F;ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys</span><br><span class="line">root@192.168.70.11&#39;s password: </span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &#39;192.168.70.11&#39;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br></pre></td></tr></table></figure><h3 id="安装ansible"><a href="#安装ansible" class="headerlink" title="安装ansible"></a>安装ansible</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# yum -y install ansible</span><br></pre></td></tr></table></figure><h3 id="修改ansible配置文件"><a href="#修改ansible配置文件" class="headerlink" title="修改ansible配置文件"></a>修改ansible配置文件</h3><p><code>vim /etc/ansible/hosts</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[all:children]</span><br><span class="line">yunwei</span><br><span class="line">k8s</span><br><span class="line">[k8s:children]</span><br><span class="line">nginx</span><br><span class="line">master</span><br><span class="line">worker</span><br><span class="line">[yunwei]</span><br><span class="line">192.168.70.200</span><br><span class="line">[nginx]</span><br><span class="line">192.168.70.11</span><br><span class="line">192.168.70.12</span><br><span class="line">[master]</span><br><span class="line">192.168.70.21</span><br><span class="line">192.168.70.22</span><br><span class="line">192.168.70.23</span><br><span class="line">[worker]</span><br><span class="line">192.168.70.21</span><br><span class="line">192.168.70.22</span><br><span class="line">192.168.70.23</span><br><span class="line">192.168.70.24</span><br></pre></td></tr></table></figure><h2 id="使用anbile修改其他服务器"><a href="#使用anbile修改其他服务器" class="headerlink" title="使用anbile修改其他服务器"></a>使用anbile修改其他服务器</h2><h3 id="修改其他服务器的密码"><a href="#修改其他服务器的密码" class="headerlink" title="修改其他服务器的密码"></a>修改其他服务器的密码</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# ansible k8s -m shell -a &quot;echo wang123 | passwd --stdin root&quot;</span><br></pre></td></tr></table></figure><h3 id="调整其他服务器系统配置"><a href="#调整其他服务器系统配置" class="headerlink" title="调整其他服务器系统配置"></a>调整其他服务器系统配置</h3><p>拷贝文件到服务器并运行文件</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# ansible k8s -m shell -a &quot;curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;modify_server.sh | sh&quot;</span><br></pre></td></tr></table></figure><h2 id="域名集合"><a href="#域名集合" class="headerlink" title="域名集合"></a>域名集合</h2><p><a href="http://harbor.od.com/" target="_blank" rel="noopener">http://harbor.od.com/</a>    harbor管理仓库</p><p><a href="http://doc.k8s.od.com/" target="_blank" rel="noopener">http://doc.k8s.od.com/</a>     k8s构建文档</p><p><a href="http://traefik.od.com/dashboard/" target="_blank" rel="noopener">http://traefik.od.com/dashboard/</a>   traefik管理界面(Ingress)</p><p><a href="https://dashboard.od.com" target="_blank" rel="noopener">https://dashboard.od.com</a>    k8s管理界面</p><p><a href="http://jenkins.od.com/" target="_blank" rel="noopener">http://jenkins.od.com/</a>  Jenkins构建平台</p><p><a href="http://dubbo-monitor.od.com/" target="_blank" rel="noopener">http://dubbo-monitor.od.com/</a>  dubbo控制界面</p><p><a href="http://demo.od.com/hello?name=slim" target="_blank" rel="noopener">http://demo.od.com/hello?name=slim</a>   demo测试UI页面</p><p><a href="http://config.od.com/" target="_blank" rel="noopener">http://config.od.com/</a>  apollo客户端</p><p><a href="http://portal.od.com/" target="_blank" rel="noopener">http://portal.od.com/</a>  apollo配置中心</p><p><a href="http://km.od.com/" target="_blank" rel="noopener">http://km.od.com/</a>    kafka管理界面</p><p><a href="http://kibana.od.com/" target="_blank" rel="noopener">http://kibana.od.com/</a>    查看日志</p><p><a href="http://blackbox.od.com/" target="_blank" rel="noopener">http://blackbox.od.com/</a>  服务监控界面</p><p><a href="http://prometheus.od.com/" target="_blank" rel="noopener">http://prometheus.od.com/</a>   prometheus管理界面</p><p><a href="http://grafana.od.com/login" target="_blank" rel="noopener">http://grafana.od.com/login</a>  监控展示界面</p><p><a href="http://alertmanager.od.com" target="_blank" rel="noopener">http://alertmanager.od.com</a>  监控报警页面</p><p>测试域名</p><p><a href="http://config-test.od.com/" target="_blank" rel="noopener">http://config-test.od.com/</a></p><p><a href="http://config-prod.od.com/" target="_blank" rel="noopener">http://config-prod.od.com/</a></p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 老男孩 </category>
          
          <category> 二进制安装 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 二进制安装 </tag>
            
            <tag> 老男孩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>k8stest-二进制安装记录</title>
      <link href="/2020/05/28/k8stest-%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85%E8%AE%B0%E5%BD%95/"/>
      <url>/2020/05/28/k8stest-%E4%BA%8C%E8%BF%9B%E5%88%B6%E5%AE%89%E8%A3%85%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ansible all -m shell -a  &quot;HOSTIP&#x3D;\&#96;hostname -i\&#96;; HOSTNUM&#x3D;\$&#123;HOSTIP##*.&#125;;  echo -e export HOSTIP&#x3D;\$HOSTIP \\\nexport HOSTNUM&#x3D;\$HOSTNUM &gt;&gt; &#x2F;etc&#x2F;bashrc&quot; </span><br><span class="line">ansible all -m shell -a  &quot;systemctl restart docker&quot;</span><br></pre></td></tr></table></figure><p>![habor登录界面](<a href="http://wangzhangtao.com/img/kubernetes/install/" target="_blank" rel="noopener">http://wangzhangtao.com/img/kubernetes/install/</a></p><div class="hide-toggle" style="border: 1px solid bg"><div class="hide-button toggle-title" style="background-color: bg;color: color"><i class="fa fa-caret-right fa-fw"></i><span>display</span></div>    <div class="hide-content"><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">content</span><br><span class="line">[root@wang-200 hexo]# grep since source&#x2F;_data&#x2F;butterfly.yml </span><br><span class="line">since: 2020</span><br></pre></td></tr></table></figure></div></div><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-22 etcd]# &#x2F;opt&#x2F;etcd&#x2F;etcd-server-startup.sh</span><br><span class="line">2020-05-31 17:08:20.411202 I | etcdmain: etcd Version: 3.1.20</span><br><span class="line">2020-05-31 17:08:20.411514 I | etcdmain: Git SHA: 992dbd4d1</span><br><span class="line">2020-05-31 17:08:20.411556 I | etcdmain: Go Version: go1.8.7</span><br><span class="line">2020-05-31 17:08:20.411608 I | etcdmain: Go OS&#x2F;Arch: linux&#x2F;amd64</span><br><span class="line">2020-05-31 17:08:20.411640 I | etcdmain: setting maximum number of CPUs to 4, total number of available CPUs is 4</span><br><span class="line">2020-05-31 17:08:20.411824 N | etcdmain: the server is already initialized as member before, starting as etcd member...</span><br><span class="line">2020-05-31 17:08:20.411932 I | embed: peerTLS: cert &#x3D; .&#x2F;certs&#x2F;etcd-peer.pem, key &#x3D; .&#x2F;certs&#x2F;etcd-peer-key.pem, ca &#x3D; .&#x2F;certs&#x2F;ca.pem, trusted-ca &#x3D; .&#x2F;certs&#x2F;ca.pem, client-cert-auth &#x3D; true</span><br><span class="line">2020-05-31 17:08:20.415840 I | embed: listening for peers on https:&#x2F;&#x2F;192.168.70.22:2380</span><br><span class="line">2020-05-31 17:08:20.415961 W | embed: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while peer key&#x2F;cert files are presented. Ignored key&#x2F;cert files.</span><br><span class="line">2020-05-31 17:08:20.415999 W | embed: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while client cert auth (--client-cert-auth) is enabled. Ignored client cert auth for this url.</span><br><span class="line">2020-05-31 17:08:20.416173 I | embed: listening for client requests on 127.0.0.1:2379</span><br><span class="line">2020-05-31 17:08:20.416400 I | embed: listening for client requests on 192.168.70.22:2379</span><br><span class="line">2020-05-31 17:08:20.429883 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated</span><br><span class="line">2020-05-31 17:08:20.444261 I | etcdmain: --initial-cluster must include etcd-server-70-22&#x3D;https:&#x2F;&#x2F;192.168.70.22:2380 given --initial-advertise-peer-urls&#x3D;https:&#x2F;&#x2F;192.168.70.22:2380</span><br></pre></td></tr></table></figure><p>–initial-cluster 我这一行配置错了</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2020-05-31 17:37:34.355943 C | etcdmain: member f97020d34db750ca has already been bootstrapped</span><br><span class="line">2020-05-31 17:37:37.422736 I | etcdmain: etcd Version: 3.1.20</span><br><span class="line">2020-05-31 17:37:37.422933 I | etcdmain: Git SHA: 992dbd4d1</span><br><span class="line">2020-05-31 17:37:37.422950 I | etcdmain: Go Version: go1.8.7</span><br><span class="line">2020-05-31 17:37:37.422962 I | etcdmain: Go OS&#x2F;Arch: linux&#x2F;amd64</span><br><span class="line">2020-05-31 17:37:37.422976 I | etcdmain: setting maximum number of CPUs to 4, total number of available CPUs is 4</span><br><span class="line">2020-05-31 17:37:37.423091 N | etcdmain: the server is already initialized as member before, starting as etcd member...</span><br><span class="line">2020-05-31 17:37:37.423171 I | embed: peerTLS: cert &#x3D; .&#x2F;certs&#x2F;etcd-peer.pem, key &#x3D; .&#x2F;certs&#x2F;etcd-peer-key.pem, ca &#x3D; .&#x2F;certs&#x2F;ca.pem, trusted-ca &#x3D; .&#x2F;certs&#x2F;ca.pem, client-cert-auth &#x3D; true</span><br><span class="line">2020-05-31 17:37:37.425752 I | embed: listening for peers on https:&#x2F;&#x2F;192.168.70.22:2380</span><br><span class="line">2020-05-31 17:37:37.425873 W | embed: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while peer key&#x2F;cert files are presented. Ignored key&#x2F;cert files.</span><br><span class="line">2020-05-31 17:37:37.425904 W | embed: The scheme of client url http:&#x2F;&#x2F;127.0.0.1:2379 is HTTP while client cert auth (--client-cert-auth) is enabled. Ignored client cert auth for this url.</span><br><span class="line">2020-05-31 17:37:37.426034 I | embed: listening for client requests on 127.0.0.1:2379</span><br><span class="line">2020-05-31 17:37:37.426143 I | embed: listening for client requests on 192.168.70.22:2379</span><br><span class="line">2020-05-31 17:37:37.433828 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated</span><br><span class="line">2020-05-31 17:37:37.466423 C | etcdmain: member f97020d34db750ca has already been bootstrapped</span><br></pre></td></tr></table></figure><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfeyubuvokj30ho06ot9a.jpg" alt="image-20200509153629808"></p><p><strong>Traefik-ingress 发布报错</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Events:</span><br><span class="line">  Type     Reason                  Age                    From                       Message</span><br><span class="line"></span><br><span class="line">----     ------                  ----                   ----                       -------</span><br><span class="line"></span><br><span class="line">  Normal   SandboxChanged          10m (x5904 over 3h5m)  kubelet, wang-24.host.com  Pod sandbox changed, it will be killed and re-created.</span><br><span class="line">  Warning  FailedCreatePodSandBox  29s (x6207 over 3h5m)  kubelet, wang-24.host.com  (combined from similar events): Failed create pod sandbox: rpc error: code &#x3D; Unknown desc &#x3D; failed to start sandbox container for pod &quot;traefik</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  -ingress-tnw7n&quot;: Error response from daemon: driver failed programming external connectivity on endpoint k8s_POD_traefik-ingress-tnw7n_kube-system_45d3c9bf-0633-44f0-8048-91061b74a7e2_6215 (b578fba313008f91ed117d5d5112c74ca2d07b5e460c20a862b7ad07207f5f72):  (iptables failed: iptables --wait -t filter -A DOCKER ! -i docker0 -o docker0 -p tcp -d 172.16.24.4 --dport 80 -j ACCEPT: iptables: No chain&#x2F;target&#x2F;match by that name.</span><br><span class="line"> (exit status 1))</span><br></pre></td></tr></table></figure><p>重启问题解决</p>]]></content>
      
      
      <categories>
          
          <category> kubernetes </category>
          
          <category> 老男孩 </category>
          
          <category> 二进制安装 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kubernetes </tag>
            
            <tag> 二进制安装 </tag>
            
            <tag> 老男孩 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基础环境部署</title>
      <link href="/2020/05/28/%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/"/>
      <url>/2020/05/28/%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h2 id="初始化系统配置"><a href="#初始化系统配置" class="headerlink" title="初始化系统配置"></a>初始化系统配置</h2><h3 id="修改默认密码"><a href="#修改默认密码" class="headerlink" title="修改默认密码"></a>修改默认密码</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# echo wang123 | passwd --stdin root</span><br><span class="line">[root@localhost ~]# mkdir &#x2F;data&#x2F;shell</span><br></pre></td></tr></table></figure><h3 id="创建文件，方便以后使用"><a href="#创建文件，方便以后使用" class="headerlink" title="创建文件，方便以后使用"></a>创建文件，方便以后使用</h3><p><code>vim /data/shell/modify_server.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改主机名</span><br><span class="line">hostip&#x3D;&#96;ifconfig ens160 | grep 192 | awk &#39;&#123;print $2&#125;&#39;| awk -F&#39;.&#39; &#39;&#123;print $4&#125;&#39; &#96;; hostnamectl set-hostname wang-$hostip.host.com</span><br><span class="line"></span><br><span class="line"># 配置阿里yum源</span><br><span class="line">mkdir &#x2F;etc&#x2F;yum.repos.d&#x2F;bak &amp;&amp; mv &#x2F;etc&#x2F;yum.repos.d&#x2F;*.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;bak</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo</span><br><span class="line"></span><br><span class="line"># 关闭SElinux</span><br><span class="line">sed -i &#39;s&#x2F;SELINUX&#x3D;enforcing&#x2F;SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">setenforce 0</span><br><span class="line"></span><br><span class="line"># 关闭firewalld</span><br><span class="line">systemctl disable firewalld.service  </span><br><span class="line">systemctl stop firewalld.service</span><br><span class="line"></span><br><span class="line"># 安装必要工具</span><br><span class="line">yum install -y wget net-tools telnet tree nmap sysstat lrzsz dos2unix </span><br><span class="line"></span><br><span class="line"># 创建日志目录</span><br><span class="line">mkdir -p &#x2F;data&#x2F;logs &#x2F;data&#x2F;soft</span><br><span class="line"></span><br><span class="line"># 添加两个变量，方便以后使用</span><br><span class="line">HOSTIP&#x3D;&#96;hostname -i | grep -o &#39;192.168.70.[0-9]\&#123;1,3\&#125;&#39; &#96;</span><br><span class="line">HOSTNUM&#x3D;$&#123;HOSTIP##*.&#125;</span><br><span class="line">echo $HOSTIP $HOSTNUM</span><br><span class="line">echo -e &quot;export HOSTIP&#x3D;$HOSTIP \nexport HOSTNUM&#x3D;$HOSTNUM&quot; &gt;&gt; &#x2F;etc&#x2F;bashrc</span><br><span class="line">source &#x2F;etc&#x2F;bashrc</span><br></pre></td></tr></table></figure><h3 id="执行配置文件"><a href="#执行配置文件" class="headerlink" title="执行配置文件"></a>执行配置文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# chmod +x &#x2F;data&#x2F;shell&#x2F;modify_server.sh</span><br><span class="line">[root@localhost ~]# &#x2F;data&#x2F;shell&#x2F;modify_server.sh  # 方法一</span><br><span class="line">[root@localhost ~]# curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;modify_server.sh | sh  # 方法二</span><br></pre></td></tr></table></figure><h2 id="nginx运维仓库"><a href="#nginx运维仓库" class="headerlink" title="nginx运维仓库"></a>nginx运维仓库</h2><p>在wang-200服务器上创建本地安装包和安装文档，方便以后工作</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;doc.od.com.conf </span><br><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  doc.od.com;</span><br><span class="line">    # index index.html index.htm index.jsp;</span><br><span class="line">    # root &#x2F;data&#x2F;k8s;</span><br><span class="line"></span><br><span class="line">    location &#x2F;shell &#123;</span><br><span class="line">        autoindex on;</span><br><span class="line">        autoindex_exact_size off;</span><br><span class="line">        default_type text&#x2F;plain;</span><br><span class="line">        alias &#x2F;data&#x2F;shell&#x2F;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location &#x2F;soft &#123;</span><br><span class="line">        autoindex on;</span><br><span class="line">        autoindex_exact_size off;</span><br><span class="line">        alias &#x2F;data&#x2F;soft&#x2F;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    access_log &#x2F;data&#x2F;logs&#x2F;nginx&#x2F;doc.log;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h2 id="部署java"><a href="#部署java" class="headerlink" title="部署java"></a>部署java</h2><blockquote><p>官网地址 <a href="https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html" target="_blank" rel="noopener">JDK 8u202 and earlier</a>   <a href="https://download.oracle.com/otn/java/jdk/8u202-b08/1961070e4c9b4e26a04e7f5a083f551e/jdk-8u202-linux-x64.tar.gz" target="_blank" rel="noopener">jdk-8u202-linux</a>  :)  </p></blockquote><p>下载官网软件包到运维主机 /data/soft/centos7/jdk-8u202-linux-x64.tar.gz</p><p><strong>在运维主机创建jdk安装脚本</strong></p><p><code>vim /data/shell/install_java.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 准备文件夹</span><br><span class="line">mkdir &#x2F;opt&#x2F;src</span><br><span class="line">mkdir &#x2F;usr&#x2F;java</span><br><span class="line"></span><br><span class="line"># 拷贝安装包并解压</span><br><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;jdk-8u202-linux-x64.tar.gz -O &#x2F;opt&#x2F;src&#x2F;jdk-8u202-linux-x64.tar.gz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;jdk-8u202-linux-x64.tar.gz -C &#x2F;usr&#x2F;java&#x2F;</span><br><span class="line">ln -s &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_202 &#x2F;usr&#x2F;java&#x2F;jdk</span><br><span class="line"></span><br><span class="line"># 配置默认全局变量</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; &#x2F;etc&#x2F;profile</span><br><span class="line">#JAVA HOME</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk</span><br><span class="line">export PATH&#x3D;\$PATH:\$JAVA_HOME&#x2F;bin:\$JAVA_HOME&#x2F;sbin</span><br><span class="line">export CLASSPATH&#x3D;\$CLASSPATH:\$JAVA_HOME&#x2F;lib:\$JAVA_HOME&#x2F;lib&#x2F;tools.jar</span><br><span class="line">EOF</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line"></span><br><span class="line"># 检查java是否可用</span><br><span class="line">java -version</span><br></pre></td></tr></table></figure><p>执行远程脚本安装java</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_java.sh | sh</span><br></pre></td></tr></table></figure><h2 id="部署npm"><a href="#部署npm" class="headerlink" title="部署npm"></a>部署npm</h2><blockquote><p>官网下载地址 <a href="https://nodejs.org/dist/v12.16.2/node-v12.16.2-linux-x64.tar.xz" target="_blank" rel="noopener">node-v12.16.2</a></p><p>下载到运维主机 /data/soft/centos7/node-v12.16.2-linux-x64.tar.xz</p></blockquote><p><strong>在运维主机创建jdk安装脚本</strong></p><p><code>vim /data/shell/install_node.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;node-v12.16.2-linux-x64.tar.xz -O &#x2F;opt&#x2F;src&#x2F;node-v12.16.2-linux-x64.tar.xz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;node-v12.16.2-linux-x64.tar.xz -C &#x2F;usr&#x2F;</span><br><span class="line">ln -s &#x2F;usr&#x2F;node-v12.16.2-linux-x64 &#x2F;usr&#x2F;node</span><br><span class="line"></span><br><span class="line"># 配置全局变量</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; &#x2F;etc&#x2F;profile</span><br><span class="line">#NODE HOME</span><br><span class="line">export NODE_HOME&#x3D;&#x2F;usr&#x2F;node</span><br><span class="line">export PATH&#x3D;\$PATH:\$NODE_HOME&#x2F;bin</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 查看并验证node</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line">node -v</span><br><span class="line">npm -v</span><br><span class="line"></span><br><span class="line"># 安装淘宝镜像源</span><br><span class="line">npm install -g cnpm --registry&#x3D;https:&#x2F;&#x2F;registry.npm.taobao.org</span><br><span class="line">cnpm -v</span><br></pre></td></tr></table></figure><p>执行远程脚本安装npm</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_node.sh | sh</span><br></pre></td></tr></table></figure><h2 id="部署golang"><a href="#部署golang" class="headerlink" title="部署golang"></a>部署golang</h2><p>安装包下载地址为：<a href="https://golang.org/dl/。" target="_blank" rel="noopener">https://golang.org/dl/。</a></p><p>如果打不开可以使用这个地址：<a href="https://golang.google.cn/dl/" target="_blank" rel="noopener">https://golang.google.cn/dl/</a>  <a href="https://dl.google.com/go/go1.14.4.linux-amd64.tar.gz" target="_blank" rel="noopener">go1.14.4</a></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;dl.google.com&#x2F;go&#x2F;go1.14.4.linux-amd64.tar.gz -O &#x2F;data&#x2F;soft&#x2F;centos7&#x2F;go1.14.4.linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p>vim /data/shell/install_go.sh </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 解压安装包</span><br><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;go1.14.4.linux-amd64.tar.gz -O &#x2F;opt&#x2F;src&#x2F;go1.14.4.linux-amd64.tar.gz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;go1.14.4.linux-amd64.tar.gz -C &#x2F;usr&#x2F;</span><br><span class="line">mv &#x2F;usr&#x2F;go &#x2F;usr&#x2F;go-1.14.4</span><br><span class="line">ln -s &#x2F;usr&#x2F;go-1.14.4 &#x2F;usr&#x2F;go</span><br><span class="line"></span><br><span class="line"># 设置全局变量</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; &#x2F;etc&#x2F;profile</span><br><span class="line">#NODE HOME</span><br><span class="line">export GO_HOME&#x3D;&#x2F;usr&#x2F;go</span><br><span class="line">export PATH&#x3D;\$PATH:\$GO_HOME&#x2F;bin</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 验证可用性</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line">go version</span><br></pre></td></tr></table></figure><p>远程执行安装脚本</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_go.sh | sh</span><br></pre></td></tr></table></figure><h2 id="部署maven"><a href="#部署maven" class="headerlink" title="部署maven"></a>部署maven</h2><blockquote><p><a href="https://maven.apache.org/download.cgi" target="_blank" rel="noopener">maven官网下载地址</a> <a href="https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz" target="_blank" rel="noopener">maven3.6.3官网快照</a></p></blockquote><img src="http://wangzhangtao.com/img/body/2.dev2添加jenkins和nginx/image-20200811142510094.png" alt="mavne官网下载示意图" style="zoom:67%;max-width: 70%" /><p>下载官网软件包到运维主机 /data/soft/centos7/apache-maven-3.6.3-bin.tar.gz</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;apache&#x2F;maven&#x2F;maven-3&#x2F;3.6.3&#x2F;binaries&#x2F;apache-maven-3.6.3-bin.tar.gz -O &#x2F;data&#x2F;soft&#x2F;centos7&#x2F;apache-maven-3.6.3-bin.tar.gz</span><br></pre></td></tr></table></figure><h3 id="在运维主机创建maven安装脚本"><a href="#在运维主机创建maven安装脚本" class="headerlink" title="在运维主机创建maven安装脚本"></a>在运维主机创建maven安装脚本</h3><p>vim /data/shell/install_maven.sh</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 准备文件夹</span><br><span class="line">mkdir &#x2F;usr&#x2F;maven</span><br><span class="line"></span><br><span class="line"># 拷贝安装包并解压</span><br><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;apache-maven-3.6.3-bin.tar.gz -O &#x2F;opt&#x2F;src&#x2F;apache-maven-3.6.3-bin.tar.gz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;apache-maven-3.6.3-bin.tar.gz -C &#x2F;usr&#x2F;maven&#x2F;</span><br><span class="line">ln -s &#x2F;usr&#x2F;maven&#x2F;apache-maven-3.6.3 &#x2F;usr&#x2F;maven&#x2F;mvn</span><br><span class="line"></span><br><span class="line"># 配置默认全局变量</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; &#x2F;etc&#x2F;profile</span><br><span class="line">#mavne 3.6.3</span><br><span class="line">export M2_HOME&#x3D;&#x2F;usr&#x2F;maven&#x2F;mvn</span><br><span class="line">export PATH&#x3D;\$M2_HOME&#x2F;bin:\$PATH</span><br><span class="line">EOF</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line"></span><br><span class="line"># 检查mvn是否可用</span><br><span class="line">mvn -v</span><br></pre></td></tr></table></figure><h3 id="执行远程脚本安装maven"><a href="#执行远程脚本安装maven" class="headerlink" title="执行远程脚本安装maven"></a>执行远程脚本安装maven</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_maven.sh | sh</span><br></pre></td></tr></table></figure><h2 id="部署docker"><a href="#部署docker" class="headerlink" title="部署docker"></a>部署docker</h2><h3 id="在运维主机创建docker安装脚本"><a href="#在运维主机创建docker安装脚本" class="headerlink" title="在运维主机创建docker安装脚本"></a>在运维主机创建docker安装脚本</h3><p><code>vim /data/shell/install_docker.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装docker服务</span><br><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager --add-repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo</span><br><span class="line">yum -y install docker-ce-cli-18.09.3 docker-ce-18.09.3 # docker-ce-18.06.0.ce -y</span><br><span class="line">sleep 3</span><br><span class="line"></span><br><span class="line"># 配置docker参数，注意不同主机，docker bip不同</span><br><span class="line">mkdir &#x2F;etc&#x2F;docker&#x2F;</span><br><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;graph&quot;: &quot;&#x2F;data&#x2F;docker&quot;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay&quot;,</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;tpd9v3c5.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;insecure-registries&quot;: [&quot;registry.access.redhat.com&quot;,&quot;quay.io&quot;,&quot;harbor.od.com&quot;],</span><br><span class="line">  &quot;bip&quot;: &quot;172.16.200.1&#x2F;24&quot;,</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],</span><br><span class="line">  &quot;live-restore&quot;: true</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sed -i &quot;s&amp;172.16.200.1&#x2F;24&amp;172.16.$&#123;HOSTNUM&#125;.1&#x2F;24&amp;&quot; &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line"></span><br><span class="line"># 启动docker</span><br><span class="line">systemctl start docker</span><br><span class="line"># 添加docker开机自启</span><br><span class="line">systemctl enable docker</span><br><span class="line"></span><br><span class="line"># 查看docker版本号</span><br><span class="line">docker version</span><br></pre></td></tr></table></figure><h3 id="配置执行权限并执行"><a href="#配置执行权限并执行" class="headerlink" title="配置执行权限并执行"></a>配置执行权限并执行</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# chmod +x &#x2F;data&#x2F;shell&#x2F;install_docker.sh</span><br><span class="line">[root@wang-200 ~]# &#x2F;data&#x2F;shell&#x2F;install_docker.sh</span><br><span class="line">[root@wang-200 ~]# ansible worker -m shell -a &quot;curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_docker.sh | sh&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> daily </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> 基础环境 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基础环境部署</title>
      <link href="/2020/05/28/%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/"/>
      <url>/2020/05/28/%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<h2 id="初始化系统配置"><a href="#初始化系统配置" class="headerlink" title="初始化系统配置"></a>初始化系统配置</h2><h3 id="修改默认密码"><a href="#修改默认密码" class="headerlink" title="修改默认密码"></a>修改默认密码</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# echo wang123 | passwd --stdin root</span><br><span class="line">[root@localhost ~]# mkdir &#x2F;data&#x2F;shell</span><br></pre></td></tr></table></figure><h3 id="创建文件，方便以后使用"><a href="#创建文件，方便以后使用" class="headerlink" title="创建文件，方便以后使用"></a>创建文件，方便以后使用</h3><p><code>vim /data/shell/modify_server.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改主机名</span><br><span class="line">hostip&#x3D;&#96;ifconfig ens160 | grep 192 | awk &#39;&#123;print $2&#125;&#39;| awk -F&#39;.&#39; &#39;&#123;print $4&#125;&#39; &#96;; hostnamectl set-hostname wang-$hostip.host.com</span><br><span class="line"></span><br><span class="line"># 配置阿里yum源</span><br><span class="line">mkdir &#x2F;etc&#x2F;yum.repos.d&#x2F;bak &amp;&amp; mv &#x2F;etc&#x2F;yum.repos.d&#x2F;*.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;bak</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo</span><br><span class="line"></span><br><span class="line"># 关闭SElinux</span><br><span class="line">sed -i &#39;s&#x2F;SELINUX&#x3D;enforcing&#x2F;SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">setenforce 0</span><br><span class="line"></span><br><span class="line"># 关闭firewalld</span><br><span class="line">systemctl disable firewalld.service  </span><br><span class="line">systemctl stop firewalld.service</span><br><span class="line"></span><br><span class="line"># 安装必要工具</span><br><span class="line">yum install -y wget net-tools telnet tree nmap sysstat lrzsz dos2unix </span><br><span class="line"></span><br><span class="line"># 创建日志目录</span><br><span class="line">mkdir -p &#x2F;data&#x2F;logs &#x2F;data&#x2F;soft</span><br><span class="line"></span><br><span class="line"># 添加两个变量，方便以后使用</span><br><span class="line">HOSTIP&#x3D;&#96;hostname -i | grep -o &#39;192.168.70.[0-9]\&#123;1,3\&#125;&#39; &#96;</span><br><span class="line">HOSTNUM&#x3D;$&#123;HOSTIP##*.&#125;</span><br><span class="line">echo $HOSTIP $HOSTNUM</span><br><span class="line">echo -e &quot;export HOSTIP&#x3D;$HOSTIP \nexport HOSTNUM&#x3D;$HOSTNUM&quot; &gt;&gt; &#x2F;etc&#x2F;bashrc</span><br><span class="line">source &#x2F;etc&#x2F;bashrc</span><br></pre></td></tr></table></figure><h3 id="执行配置文件"><a href="#执行配置文件" class="headerlink" title="执行配置文件"></a>执行配置文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@localhost ~]# chmod +x &#x2F;data&#x2F;shell&#x2F;modify_server.sh</span><br><span class="line">[root@localhost ~]# &#x2F;data&#x2F;shell&#x2F;modify_server.sh  # 方法一</span><br><span class="line">[root@localhost ~]# curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;modify_server.sh | sh  # 方法二</span><br></pre></td></tr></table></figure><h2 id="nginx运维仓库"><a href="#nginx运维仓库" class="headerlink" title="nginx运维仓库"></a>nginx运维仓库</h2><p>在wang-200服务器上创建本地安装包和安装文档，方便以后工作</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;doc.od.com.conf </span><br><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  doc.od.com;</span><br><span class="line">    # index index.html index.htm index.jsp;</span><br><span class="line">    # root &#x2F;data&#x2F;k8s;</span><br><span class="line"></span><br><span class="line">    location &#x2F;shell &#123;</span><br><span class="line">        autoindex on;</span><br><span class="line">        autoindex_exact_size off;</span><br><span class="line">        default_type text&#x2F;plain;</span><br><span class="line">        alias &#x2F;data&#x2F;shell&#x2F;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location &#x2F;soft &#123;</span><br><span class="line">        autoindex on;</span><br><span class="line">        autoindex_exact_size off;</span><br><span class="line">        alias &#x2F;data&#x2F;soft&#x2F;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    access_log &#x2F;data&#x2F;logs&#x2F;nginx&#x2F;doc.log;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h2 id="部署java"><a href="#部署java" class="headerlink" title="部署java"></a>部署java</h2><blockquote><p>官网地址 <a href="https://www.oracle.com/java/technologies/javase/javase8-archive-downloads.html" target="_blank" rel="noopener">JDK 8u202 and earlier</a>   <a href="https://download.oracle.com/otn/java/jdk/8u202-b08/1961070e4c9b4e26a04e7f5a083f551e/jdk-8u202-linux-x64.tar.gz" target="_blank" rel="noopener">jdk-8u202-linux</a>  :)  </p></blockquote><p>下载官网软件包到运维主机 /data/soft/centos7/jdk-8u202-linux-x64.tar.gz</p><p><strong>在运维主机创建jdk安装脚本</strong></p><p><code>vim /data/shell/install_java.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 准备文件夹</span><br><span class="line">mkdir &#x2F;opt&#x2F;src</span><br><span class="line">mkdir &#x2F;usr&#x2F;java</span><br><span class="line"></span><br><span class="line"># 拷贝安装包并解压</span><br><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;jdk-8u202-linux-x64.tar.gz -O &#x2F;opt&#x2F;src&#x2F;jdk-8u202-linux-x64.tar.gz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;jdk-8u202-linux-x64.tar.gz -C &#x2F;usr&#x2F;java&#x2F;</span><br><span class="line">ln -s &#x2F;usr&#x2F;java&#x2F;jdk1.8.0_202 &#x2F;usr&#x2F;java&#x2F;jdk</span><br><span class="line"></span><br><span class="line"># 配置默认全局变量</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; &#x2F;etc&#x2F;profile</span><br><span class="line">#JAVA HOME</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk</span><br><span class="line">export PATH&#x3D;\$PATH:\$JAVA_HOME&#x2F;bin:\$JAVA_HOME&#x2F;sbin</span><br><span class="line">export CLASSPATH&#x3D;\$CLASSPATH:\$JAVA_HOME&#x2F;lib:\$JAVA_HOME&#x2F;lib&#x2F;tools.jar</span><br><span class="line">EOF</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line"></span><br><span class="line"># 检查java是否可用</span><br><span class="line">java -version</span><br></pre></td></tr></table></figure><p>执行远程脚本安装java</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_java.sh | sh</span><br></pre></td></tr></table></figure><h2 id="部署npm"><a href="#部署npm" class="headerlink" title="部署npm"></a>部署npm</h2><blockquote><p>官网下载地址 <a href="https://nodejs.org/dist/v12.16.2/node-v12.16.2-linux-x64.tar.xz" target="_blank" rel="noopener">node-v12.16.2</a></p><p>下载到运维主机 /data/soft/centos7/node-v12.16.2-linux-x64.tar.xz</p></blockquote><p><strong>在运维主机创建jdk安装脚本</strong></p><p><code>vim /data/shell/install_node.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;node-v12.16.2-linux-x64.tar.xz -O &#x2F;opt&#x2F;src&#x2F;node-v12.16.2-linux-x64.tar.xz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;node-v12.16.2-linux-x64.tar.xz -C &#x2F;usr&#x2F;</span><br><span class="line">ln -s &#x2F;usr&#x2F;node-v12.16.2-linux-x64 &#x2F;usr&#x2F;node</span><br><span class="line"></span><br><span class="line"># 配置全局变量</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; &#x2F;etc&#x2F;profile</span><br><span class="line">#NODE HOME</span><br><span class="line">export NODE_HOME&#x3D;&#x2F;usr&#x2F;node</span><br><span class="line">export PATH&#x3D;\$PATH:\$NODE_HOME&#x2F;bin</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 查看并验证node</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line">node -v</span><br><span class="line">npm -v</span><br><span class="line"></span><br><span class="line"># 安装淘宝镜像源</span><br><span class="line">npm install -g cnpm --registry&#x3D;https:&#x2F;&#x2F;registry.npm.taobao.org</span><br><span class="line">cnpm -v</span><br></pre></td></tr></table></figure><p>执行远程脚本安装npm</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_node.sh | sh</span><br></pre></td></tr></table></figure><h2 id="部署golang"><a href="#部署golang" class="headerlink" title="部署golang"></a>部署golang</h2><p>安装包下载地址为：<a href="https://golang.org/dl/。" target="_blank" rel="noopener">https://golang.org/dl/。</a></p><p>如果打不开可以使用这个地址：<a href="https://golang.google.cn/dl/" target="_blank" rel="noopener">https://golang.google.cn/dl/</a>  <a href="https://dl.google.com/go/go1.14.4.linux-amd64.tar.gz" target="_blank" rel="noopener">go1.14.4</a></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;dl.google.com&#x2F;go&#x2F;go1.14.4.linux-amd64.tar.gz -O &#x2F;data&#x2F;soft&#x2F;centos7&#x2F;go1.14.4.linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p>vim /data/shell/install_go.sh </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 解压安装包</span><br><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;go1.14.4.linux-amd64.tar.gz -O &#x2F;opt&#x2F;src&#x2F;go1.14.4.linux-amd64.tar.gz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;go1.14.4.linux-amd64.tar.gz -C &#x2F;usr&#x2F;</span><br><span class="line">mv &#x2F;usr&#x2F;go &#x2F;usr&#x2F;go-1.14.4</span><br><span class="line">ln -s &#x2F;usr&#x2F;go-1.14.4 &#x2F;usr&#x2F;go</span><br><span class="line"></span><br><span class="line"># 设置全局变量</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; &#x2F;etc&#x2F;profile</span><br><span class="line">#NODE HOME</span><br><span class="line">export GO_HOME&#x3D;&#x2F;usr&#x2F;go</span><br><span class="line">export PATH&#x3D;\$PATH:\$GO_HOME&#x2F;bin</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 验证可用性</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line">go version</span><br></pre></td></tr></table></figure><p>远程执行安装脚本</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_go.sh | sh</span><br></pre></td></tr></table></figure><h2 id="部署maven"><a href="#部署maven" class="headerlink" title="部署maven"></a>部署maven</h2><blockquote><p><a href="https://maven.apache.org/download.cgi" target="_blank" rel="noopener">maven官网下载地址</a> <a href="https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz" target="_blank" rel="noopener">maven3.6.3官网快照</a></p></blockquote><img src="http://wangzhangtao.com/img/body/2.dev2添加jenkins和nginx/image-20200811142510094.png" alt="mavne官网下载示意图" style="zoom:67%;max-width: 70%" /><p>下载官网软件包到运维主机 /data/soft/centos7/apache-maven-3.6.3-bin.tar.gz</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;apache&#x2F;maven&#x2F;maven-3&#x2F;3.6.3&#x2F;binaries&#x2F;apache-maven-3.6.3-bin.tar.gz -O &#x2F;data&#x2F;soft&#x2F;centos7&#x2F;apache-maven-3.6.3-bin.tar.gz</span><br></pre></td></tr></table></figure><h3 id="在运维主机创建maven安装脚本"><a href="#在运维主机创建maven安装脚本" class="headerlink" title="在运维主机创建maven安装脚本"></a>在运维主机创建maven安装脚本</h3><p>vim /data/shell/install_maven.sh</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 准备文件夹</span><br><span class="line">mkdir &#x2F;usr&#x2F;maven</span><br><span class="line"></span><br><span class="line"># 拷贝安装包并解压</span><br><span class="line">wget http:&#x2F;&#x2F;doc.od.com&#x2F;soft&#x2F;centos7&#x2F;apache-maven-3.6.3-bin.tar.gz -O &#x2F;opt&#x2F;src&#x2F;apache-maven-3.6.3-bin.tar.gz</span><br><span class="line">tar xf &#x2F;opt&#x2F;src&#x2F;apache-maven-3.6.3-bin.tar.gz -C &#x2F;usr&#x2F;maven&#x2F;</span><br><span class="line">ln -s &#x2F;usr&#x2F;maven&#x2F;apache-maven-3.6.3 &#x2F;usr&#x2F;maven&#x2F;mvn</span><br><span class="line"></span><br><span class="line"># 配置默认全局变量</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; &#x2F;etc&#x2F;profile</span><br><span class="line">#mavne 3.6.3</span><br><span class="line">export M2_HOME&#x3D;&#x2F;usr&#x2F;maven&#x2F;mvn</span><br><span class="line">export PATH&#x3D;\$M2_HOME&#x2F;bin:\$PATH</span><br><span class="line">EOF</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line"></span><br><span class="line"># 检查mvn是否可用</span><br><span class="line">mvn -v</span><br></pre></td></tr></table></figure><h3 id="执行远程脚本安装maven"><a href="#执行远程脚本安装maven" class="headerlink" title="执行远程脚本安装maven"></a>执行远程脚本安装maven</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_maven.sh | sh</span><br></pre></td></tr></table></figure><h2 id="部署docker"><a href="#部署docker" class="headerlink" title="部署docker"></a>部署docker</h2><h3 id="在运维主机创建docker安装脚本"><a href="#在运维主机创建docker安装脚本" class="headerlink" title="在运维主机创建docker安装脚本"></a>在运维主机创建docker安装脚本</h3><p><code>vim /data/shell/install_docker.sh</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 安装docker服务</span><br><span class="line">yum install -y yum-utils device-mapper-persistent-data lvm2</span><br><span class="line">yum-config-manager --add-repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo</span><br><span class="line">yum -y install docker-ce-cli-18.09.3 docker-ce-18.09.3 # docker-ce-18.06.0.ce -y</span><br><span class="line">sleep 3</span><br><span class="line"></span><br><span class="line"># 配置docker参数，注意不同主机，docker bip不同</span><br><span class="line">mkdir &#x2F;etc&#x2F;docker&#x2F;</span><br><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;graph&quot;: &quot;&#x2F;data&#x2F;docker&quot;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay&quot;,</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;tpd9v3c5.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;insecure-registries&quot;: [&quot;registry.access.redhat.com&quot;,&quot;quay.io&quot;,&quot;harbor.od.com&quot;],</span><br><span class="line">  &quot;bip&quot;: &quot;172.16.200.1&#x2F;24&quot;,</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],</span><br><span class="line">  &quot;live-restore&quot;: true</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sed -i &quot;s&amp;172.16.200.1&#x2F;24&amp;172.16.$&#123;HOSTNUM&#125;.1&#x2F;24&amp;&quot; &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line"></span><br><span class="line"># 启动docker</span><br><span class="line">systemctl start docker</span><br><span class="line"># 添加docker开机自启</span><br><span class="line">systemctl enable docker</span><br><span class="line"></span><br><span class="line"># 查看docker版本号</span><br><span class="line">docker version</span><br></pre></td></tr></table></figure><h3 id="配置执行权限并执行"><a href="#配置执行权限并执行" class="headerlink" title="配置执行权限并执行"></a>配置执行权限并执行</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@wang-200 ~]# chmod +x &#x2F;data&#x2F;shell&#x2F;install_docker.sh</span><br><span class="line">[root@wang-200 ~]# &#x2F;data&#x2F;shell&#x2F;install_docker.sh</span><br><span class="line">[root@wang-200 ~]# ansible worker -m shell -a &quot;curl -sSL http:&#x2F;&#x2F;doc.od.com&#x2F;shell&#x2F;install_docker.sh | sh&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Test</title>
      <link href="/2020/05/20/test/"/>
      <url>/2020/05/20/test/</url>
      
        <content type="html"><![CDATA[<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><p>哪个英文字母最酷？ <span class="hide-inline"><a class="hide-button button--primary button--animated" style="background-color: #FF7242;color: #fff">查看答案  </a><span class="hide-content">因为西装裤(C装酷)</span></span></p><p>门里站着一个人? <span class="hide-inline"><a class="hide-button button--primary button--animated" style="">Click  </a><span class="hide-content">闪</span></span></p><blockquote><p>wang</p><footer><strong>[author[</strong><cite>source]] [link] [source_link_title]</cite></footer></blockquote><figure class="highlight plain"><figcaption><span>[title] [] [url] [link text] [additional options]</span></figcaption><table><tr><td class="code"><pre><span class="line">code snippet</span><br></pre></td></tr></table></figure><figure class="highlight plain"><figcaption><span>_.compact</span><a href="http://underscorejs.org/#compact" target="_blank" rel="noopener">Underscore.js</a></figcaption><table><tr><td class="code"><pre><span class="line">_.compact([0, 1, false, 2, &#39;&#39;, 3]);</span><br><span class="line">&#x3D;&gt; [1, 2, 3]</span><br></pre></td></tr></table></figure><div class="hide-block"><a class="hide-button button--primary button--animated" style="">查看答案 小美女    </a><span class="hide-content"><div class="justified-gallery"><p><img src="https://i.loli.net/2019/12/25/Fze9jchtnyJXMHN.jpg" alt=""><br><img src="https://i.loli.net/2019/12/25/ryLVePaqkYm4TEK.jpg" alt=""><br><img src="https://i.loli.net/2019/12/25/gEy5Zc1Ai6VuO4N.jpg" alt=""><br><img src="https://i.loli.net/2019/12/25/d6QHbytlSYO4FBG.jpg" alt=""><br><img src="https://i.loli.net/2019/12/25/6nepIJ1xTgufatZ.jpg" alt=""><br><img src="https://i.loli.net/2019/12/25/E7Jvr4eIPwUNmzq.jpg" alt=""><br><img src="https://i.loli.net/2019/12/25/mh19anwBSWIkGlH.jpg" alt=""><br><img src="https://i.loli.net/2019/12/25/2tu9JC8ewpBFagv.jpg" alt=""></p>          </div></span></div><p>隐藏</p><div class="hide-toggle" style="border: 1px solid bg"><div class="hide-button toggle-title" style="background-color: bg;color: color"><i class="fa fa-caret-right fa-fw"></i><span>display</span></div>    <div class="hide-content"><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">content</span><br><span class="line">[root@wang-41 hexo]# grep since source&#x2F;_data&#x2F;butterfly.yml </span><br><span class="line">since: 2020</span><br></pre></td></tr></table></figure></div></div><p>mermaid</p><p>使用mermaid标签可以绘制Flowchart（流程图）、Sequence diagram（时序图 ）、Class Diagram（类别图）、State Diagram（状态图）、Gantt（甘特图）和Pie Chart（圆形图</p><div class="mermaid">          内容          </div><div class="mermaid">          pie    title Key elements in Product X    "Calcium" : 42.96    "Potassium" : 50.05    "Magnesium" : 10.01    "Iron" :  5          </div><blockquote><p>张雪阳,遇见你是我的幸运，愿我们更开心快乐<br>明天去哪玩？</p></blockquote><div class="gallery-group-main">  <figure class="gallery-group">  <img class="gallery-group-img" src='https://i.loli.net/2019/11/10/T7Mu8Aod3egmC4Q.png'>  <figcaption>  <div class="gallery-group-name">壁纸</div>  <p>收藏的一些壁纸</p>  <a href='/Gallery/wallpaper'></a>  </figcaption>  </figure>    <figure class="gallery-group">  <img class="gallery-group-img" src='https://i.loli.net/2019/12/25/8t97aVlp4hgyBGu.jpg'>  <figcaption>  <div class="gallery-group-name">漫威</div>  <p>关于漫威的图片</p>  <a href='/Gallery/marvel'></a>  </figcaption>  </figure>    <figure class="gallery-group">  <img class="gallery-group-img" src='https://i.loli.net/2019/12/25/hOqbQ3BIwa6KWpo.jpg'>  <figcaption>  <div class="gallery-group-name">OH MY GIRL</div>  <p>关于OH MY GIRL的图片</p>  <a href='/Gallery/ohmygirl'></a>  </figcaption>  </figure>  </div>]]></content>
      
      
      <categories>
          
          <category> live </category>
          
          <category> bar </category>
          
      </categories>
      
      
        <tags>
            
            <tag> test </tag>
            
            <tag> bar </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生活中处处充满爱</title>
      <link href="/2020/05/02/%E7%94%9F%E6%B4%BB%E4%B8%AD%E5%A4%84%E5%A4%84%E5%85%85%E6%BB%A1%E7%88%B1/"/>
      <url>/2020/05/02/%E7%94%9F%E6%B4%BB%E4%B8%AD%E5%A4%84%E5%A4%84%E5%85%85%E6%BB%A1%E7%88%B1/</url>
      
        <content type="html"><![CDATA[<h2 id="很高兴遇见你"><a href="#很高兴遇见你" class="headerlink" title="很高兴遇见你"></a>很高兴遇见你</h2>]]></content>
      
      
      <categories>
          
          <category> live </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/05/01/hello-world/"/>
      <url>/2020/05/01/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> live </category>
          
          <category> bar </category>
          
      </categories>
      
      
        <tags>
            
            <tag> test </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
